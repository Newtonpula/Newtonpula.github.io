<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Markovian Spiking Neural Network</title>
    <link href="/2024/11/26/Markovian-Spiking-Neural-Network/"/>
    <url>/2024/11/26/Markovian-Spiking-Neural-Network/</url>
    
    <content type="html"><![CDATA[<h1 id="markovian-spiking-neural-network">Markovian Spiking NeuralNetwork</h1><h2 id="surrogate-for-linear-integrate-and-fire-lif-model">Surrogate forlinear integrate-and-fire (LIF) model</h2><p>See original code.</p><h2 id="surrogate-for-fitzhugh-nagumo-fhn-model">Surrogate forFitzHugh-Nagumo (FHN) model</h2><p>We refer to the system <span class="math display">\[    \begin{cases}        \frac{\mathrm{d}V}{\mathrm{d}t}&amp;=  -\frac{1}{\tau}\left((V-V_R)(V-V_{th})(V-V_M)+w \right) +I_{ext}(t) \\        \frac{\mathrm{d}w}{\mathrm{d}t} &amp;= b(V-c)-\gamma w    \end{cases} \tag{1}\]</span></p><p>while we'll see the parameters in the evolution of <spanclass="math inline">\(w\)</span> does not matter in the Markoviansurrogate. We set <span class="math inline">\(V_R=-70\)</span>mV theresting potential, <span class="math inline">\(V_{th}=-50\)</span>mV thefiring threshold, <span class="math inline">\(V_M=40\)</span>mV themagnitude of the peak of the spike.</p><p>The discretization of the state space is as follows: <spanclass="math inline">\(V\)</span> space is discretized into <spanclass="math inline">\(V_M-V_I-1\)</span> states (<strong>I found itdifficult to deal with the boundary, or the stable fixedpoint</strong>), <span class="math inline">\([V_I+1,V_M-1]\)</span>.Here, <span class="math inline">\(V_I\)</span> is the magnitude ofhyperpolarization after a spike. It's not arbitrary and must be negativeenough (<strong>so it's not ideal, and I think it's inherited from thedeficit of the original FitzHugh-Nagumo model</strong>). <spanclass="math inline">\(w\)</span> space is discretized into <spanclass="math inline">\(2\)</span> states, <spanclass="math inline">\(0\)</span> and <spanclass="math inline">\(w_M\)</span>. <spanclass="math inline">\(w_M\)</span> is determined by <spanclass="math inline">\(V_I\)</span>, given by <spanclass="math display">\[    w_{M} = -(V_{I}-V_{R})(V_{I}-V_{th})(V_{I}-V_{M})&gt;0\]</span></p><p>So we have <spanclass="math inline">\(V_{I}&lt;V_{R}&lt;V_{th}&lt;V_{M}\)</span>. Now weexplain how the membrane potential evolves.</p><ul><li>Each neuron receives independent Poisson kicks from an externalsource with rate <span class="math inline">\(\lambda^{E}\)</span> or<span class="math inline">\(\lambda^{I}\)</span>.</li><li>Each neuron receives synaptic input from within the population.Similar to the original Markovian model, denote <spanclass="math inline">\(H^{E}\)</span> and <spanclass="math inline">\(H^{I}\)</span> as numbers of kicks 'in play'.</li><li>The membrane potential evolves as follows:<ul><li>If <span class="math inline">\(V_{I}&lt; V &lt; V_{th}\)</span>,then <span class="math inline">\(w=0\)</span> and <spanclass="math inline">\(V\)</span> evolves under the cubic term, externalPoisson kicks and synaptic input from other neurons.</li><li>If <span class="math inline">\(V_{th}\leq V &lt; V_{M}\)</span>,then <span class="math inline">\(V\)</span> evolves solely under thecubic term.</li><li>If <span class="math inline">\(V=V_{M}\)</span>, then <spanclass="math inline">\(w\)</span> is set to <spanclass="math inline">\(w_{M}\)</span> with a rate <spanclass="math inline">\(\frac{1}{\tau_{w}}\)</span>, we think the neuronfires.</li><li>If <span class="math inline">\(V_{I}&lt;V\leqslant V_{M}\)</span>and <span class="math inline">\(w=w_{M}\)</span>, then the neuron is inthe phase of re-polarization and hyperpolarization. <spanclass="math inline">\(V\)</span> evolves solely under the cubic term. Wethink the neuron is in a refractory period.</li><li>If <span class="math inline">\(V=V_{I}\)</span>, then <spanclass="math inline">\(w\)</span> is set to <spanclass="math inline">\(0\)</span> with the rate <spanclass="math inline">\(\frac{1}{\tau_{w}}\)</span>.</li></ul></li></ul><p>The evolution of <span class="math inline">\(V\)</span> isquestionable and needs to be discussed.</p><h3 id="issue-questions-about-the-original-code">Issue: questions aboutthe original code</h3><ul><li>Is the code really match the paper? Modelling the independentexponential clock of a synaptic input in play as a term in the ratematrix?</li></ul><h3 id="issue-linear-non-superimposability">Issue: linearnon-superimposability</h3><p>The construction of the transition matrix involves calculation of therate of a leaky/cubic term. Separating the leaky term is straightforwardin leaky integrate-and-fire model, but it's more complicated if theleaky term is not linear.</p><p>In leaky integrate-and-fire model, the leaky term is <spanclass="math display">\[    \frac{\mathrm{d}V}{\mathrm{d}t} = -\frac{1}{\tau}V\]</span></p><p>Notice that we deliberately set the resting potential to <spanclass="math inline">\(0\)</span>. In the case <spanclass="math inline">\(V&gt;0\)</span>,</p><p><span class="math display">\[    V(t)=V(0)\exp \left( -\frac{t}{\tau} \right)\]</span></p><p>Set <span class="math inline">\(V(t)=V-1, V(0)=V\)</span>, then <spanclass="math display">\[    t = \tau \ln \left( 1+\frac{1}{V-1} \right)\]</span></p><p>So the rate is given by <span class="math display">\[    \frac{1}{t} = \frac{1}{\tau\ln \left( 1+\frac{1}{V-1} \right) }\tag{2}\]</span></p><p>Similarly, in the case <span class="math inline">\(V&lt;0\)</span>,the rate of <span class="math inline">\(V\)</span> to <spanclass="math inline">\(V+1\)</span> is given by <spanclass="math display">\[    \frac{1}{t} = \frac{1}{\tau \ln \left( 1-\frac{1}{V+1} \right) }\tag{3}\]</span></p><p>However, such rate is not linearly additive. Since the 'leaky' termin <span class="math inline">\((1)\)</span> is a cubic term, how tocalculate the rate it represents is a question.</p><p>To illustrate the linear non-superimposability, consider a quadratic'leaky' term <span class="math display">\[    \frac{\mathrm{d}V}{\mathrm{d}t} = - V(V+1)\]</span></p><p>and <span class="math inline">\(V&gt;0\)</span>. Consider <spanclass="math inline">\(V(V+1)\)</span> as a whole, the rate of <spanclass="math inline">\(V\)</span> to <spanclass="math inline">\(V-1\)</span> is given by <spanclass="math display">\[    \frac{1}{2\ln V-\ln (V-1)-\ln (V+1)} \tag{4}\]</span></p><p>Separate <span class="math inline">\(V(V+1)\)</span> into <spanclass="math inline">\(V^{2}\)</span> and <spanclass="math inline">\(V\)</span>, the rate of <spanclass="math inline">\(V\)</span> to <spanclass="math inline">\(V-1\)</span> is given by <spanclass="math display">\[    \frac{1}{\ln V -\ln (V-1)} + V(V-1) \tag{5}\]</span></p><p>Obviously, <span class="math inline">\((4)\)</span> and <spanclass="math inline">\((5)\)</span> are not equal though they are bothquite close to <span class="math inline">\(V(V+1)\)</span></p><p>Several options to calculate the rate of the cubic term in <spanclass="math inline">\((1)\)</span> are: - Consider <spanclass="math inline">\((V-V_R)(V-V_{th})(V-V_M)\)</span> as a whole,separate it with <span class="math inline">\(w\)</span>. - Separate<span class="math inline">\(O(V^{3})\)</span>, <spanclass="math inline">\(O(V^{2})\)</span>, <spanclass="math inline">\(O(V)\)</span>, <spanclass="math inline">\(O(1)\)</span> and <spanclass="math inline">\(w\)</span> terms. - Directly use the absolutevalue of RHS of <span class="math inline">\((1)\)</span> as therate.</p><h3 id="issue-boundary-stable-fixed-points">Issue: boundary &amp; stablefixed points</h3><p>We must be really careful of the boundary and fixed points, i.e.,<span class="math inline">\(V=V_{I}, V=V_{R}, V=V_{th}, V=V_{M}\)</span>though <span class="math inline">\(V=V_{I}\)</span> and <spanclass="math inline">\(V=V_{M}\)</span> are impossible due to our choicesof state space.</p><p>In <span class="math inline">\((2)\)</span>, <spanclass="math inline">\(V\)</span> cannot be <spanclass="math inline">\(0,1\)</span>. In <spanclass="math inline">\((3)\)</span>, <spanclass="math inline">\(V\)</span> cannot be <spanclass="math inline">\(-1,0\)</span>. This is relatively simple. However,if we use the first option above to calculate the rate of the cubicterm, then two functions are <span class="math display">\[    \frac{1}{\tau \left( p \log (1+\frac{1}{V-V_{R}-1})+q\log(1+\frac{1}{V-V_{th}-1}) + r\log (1+\frac{1}{V-V_{M}-1}) \right) }\tag{6}\]</span></p><p>where <span class="math inline">\(p,q,r\)</span> is the solution to<span class="math display">\[    \begin{pmatrix}    1 &amp; 1 &amp; 1 \\    V_{th}+V_{M} &amp; V_{R} + V_{M} &amp; V_{R} + V_{th} \\    V_{th}V_{M} &amp; V_{R}V_{M} &amp; V_{R}V_{th}    \end{pmatrix}    \begin{pmatrix}    p \\ q \\ r    \end{pmatrix}    =    \begin{pmatrix}    0 \\ 0 \\ 1    \end{pmatrix}\]</span></p><p>Similarly, the rate of <span class="math inline">\(V\)</span> to<span class="math inline">\(V+1\)</span> is given by <spanclass="math display">\[    \frac{1}{\tau \left( p \log (1-\frac{1}{V-V_{R}+1})+q\log(1-\frac{1}{V-V_{th}+1}) + r\log (1-\frac{1}{V-V_{M}+1}) \right) }\tag{7}\]</span></p><p>In <span class="math inline">\((6)\)</span>, <spanclass="math inline">\(V\)</span> cannot be <spanclass="math inline">\(V_{R}, V_{R}+1, V_{th}, V_{th}+1, V_{M}\)</span>.In <span class="math inline">\((7)\)</span>, <spanclass="math inline">\(V\)</span> cannot be <spanclass="math inline">\(V_{R}-1, V_{R}, V_{th}-1, V_{th}, V_{M}-1,V_{M}\)</span>. It's laborious to tackle all the cases.</p><p>Current approach to deal with the boundary: The state space of <spanclass="math inline">\(V\)</span> is narrowed to <spanclass="math inline">\([V_{I}+1, V_{M}-1]\)</span>. The neuron evolves onthe cubic term, Poisson kicks and synaptic inputs around <spanclass="math inline">\(V_{R}\)</span>.</p><ul><li>Around <span class="math inline">\(V=V_{R}\)</span>, the cubic termdoes not contribute if <span class="math inline">\(V=V_{R},V_{R}+1\)</span> when <span class="math inline">\(V\)</span> is going to<span class="math inline">\(V-1\)</span>, and <spanclass="math inline">\(V=V_{R}, V_{R}-1\)</span> when <spanclass="math inline">\(V\)</span> is going to <spanclass="math inline">\(V+1\)</span>.</li><li>Around <span class="math inline">\(V=V_{th}\)</span>, the cubic termdoes not contribute if <span class="math inline">\(V=V_{th},V_{th}+1\)</span> when <span class="math inline">\(V\)</span> is goingto <span class="math inline">\(V-1\)</span>, and <spanclass="math inline">\(V=V_{th}, V_{th}-1\)</span> when <spanclass="math inline">\(V\)</span> is going to <spanclass="math inline">\(V+1\)</span>. Moreover, since <spanclass="math inline">\(V=V_{th}\)</span> is a fixed point when <spanclass="math inline">\(w=0\)</span>, the neuron still receives Poissonkicks and synaptic inputs at <spanclass="math inline">\(V=V_{th}\)</span>, and evolves solely under thecubic term when <span class="math inline">\(V\geqslantV_{th}+1\)</span>.</li><li>Around <span class="math inline">\(V=V_{M}\)</span>, we think theneuron fires if <span class="math inline">\(V=V_{M}-1\)</span>, so thereis no <span class="math inline">\(V\)</span> to <spanclass="math inline">\(V+1\)</span> issue in <spanclass="math inline">\((7)\)</span>.</li><li>Around <span class="math inline">\(V=V_{I}\)</span>, there isactually no issue since <span class="math inline">\(V=V_{I}\)</span> isnot a singular point of <span class="math inline">\((6)\)</span>. Recallthat we separate the cubic term with <spanclass="math inline">\(w=w_{M}\)</span>, and the rate of <spanclass="math inline">\(w_{M}\)</span> is given by <spanclass="math inline">\(w_M/\tau\)</span>.</li></ul><p><strong>After discussion</strong>, we think the above two issues arenot critical. We are seeking for an approximation of the originaldynaimcs, the rate for the leaky/cubic term is not necessary to beaccurate and various forms can be used.</p><h3 id="issue-scaling-of-the-system">Issue: scaling of the system</h3><h4 id="scaling-of-v-and-w">scaling of <spanclass="math inline">\(V\)</span> and <spanclass="math inline">\(w\)</span></h4><p>The original and variants of the FitzHugh-Nagumo model are notdesigned for modeling exact process of action potentials. They aremathematically ideal models to study the feature of periodic firingqualitatively. In specific, <span class="math inline">\(V_R, V_{th},V_M\)</span> are usually take small values, e.g., <spanclass="math inline">\(0,0.5,1.0\)</span>. The values can be seen asunder unit 0.1V.</p><p>We typically look at voltage under unit mV. How can we switch betweentwo different scalings? Denote <span class="math inline">\(V, w\)</span>as the scale under unit 0.1V, and <span class="math inline">\(\bar{V},\bar{w}\)</span> as the scale under unit mV. The same treatment isapplied to other parameters.</p><p><span class="math display">\[    \begin{cases}        \frac{\mathrm{d}V}{\mathrm{d}t}&amp;=  -\frac{1}{\tau}\left((V-V_R)(V-V_{th})(V-V_M)+w \right) +I_{ext}(t) \\        \frac{\mathrm{d}w}{\mathrm{d}t} &amp;= b(V-c)-\gamma w    \end{cases}\]</span></p><p><span class="math inline">\(\bar{V}=100V\)</span>, we may also assume<span class="math inline">\(\bar{w}=100w\)</span>. Then the systembecomes <span class="math display">\[    \begin{cases}        \frac{\mathrm{d}\bar{V}}{\mathrm{d}t} =-\frac{1}{10000\tau}(\bar{V}-\bar{V}_{R})(\bar{V}-\bar{V}_{th})(\bar{V}-\bar{V}_{M})-\frac{1}{\tau}\bar{w} + 100 I_{ext}(t)\\        \frac{\mathrm{d}\bar{w}}{\mathrm{d}t} = b(\bar{V}-100c) - \gamma\bar{w} \tag{8}    \end{cases}\]</span></p><p>As can be seen, the system is not linear, so the scaling is notinvariant.</p><h4 id="scaling-of-time">scaling of time</h4><p>Baltanas and Casado studied the bursting behaviour of theFitzHugh-Nagumo model subject to quasi-monochromatic noise.Specifically, the input <span class="math inline">\(I_{ext}(t)\)</span>to the system is <span class="math inline">\(A_c + x(t)\)</span>, where<span class="math inline">\(A_c\)</span> is a critical constant and<span class="math inline">\(x(t)\)</span> is a noisy damped oscillation:<span class="math display">\[    \frac{\mathrm{d}^{2}x}{\mathrm{d}t^{2}}+2\Gamma\frac{\mathrm{d}x}{\mathrm{d}t} + \omega^{2} x=\xi(t) \tag{9}\]</span></p><p>Here, <span class="math inline">\(\xi(t)\)</span> is a Gaussian whitenoise with zero mean and correlation <span class="math inline">\(\langle\xi(t)\xi(s)\rangle = \Delta \delta(t-s)\)</span>.</p><p>The authors took <span class="math inline">\(\Gamma=0.03,\Delta=0.005, \omega=1\)</span> and observed bursting behaviour in <spanclass="math inline">\((1)\)</span>. We empirically observed that with<span class="math inline">\(\tau=1\)</span> and <spanclass="math inline">\(\tau_w=100\)</span>. However, the length of aspike is about 40 ms, inconsistent with a real spike, which is about 2~3ms. How to scale <span class="math inline">\(\tau, \tau_{w}, \Gamma,\omega, \Delta\)</span> to make the system more realistic? We simplyscale <span class="math inline">\(\tau=0.1\)</span> and <spanclass="math inline">\(\tau_{w}=10\)</span>.</p><p>It is a well-known fact that the solution to <spanclass="math inline">\((8)\)</span> with <spanclass="math inline">\(\xi(t)=0\)</span> is <span class="math display">\[    x(t) = A_0 \exp \left( -\Gamma t \right) \cos \left(\sqrt{\omega^{2}-\Gamma^{2}} t+ \phi \right)\]</span></p><p>If we defined the Hamiltonian <spanclass="math inline">\(H(x,\dot{x})=\frac{1}{2}(\dot{x}^2+\omega^{2}x^{2})\)</span>,then the Hamiltonian equation is <span class="math display">\[    \mathrm{d}H=-2\Gamma\left( H- \frac{\Delta}{4\Gamma}\right)\mathrm{d}t + \sqrt{\Delta H} \mathrm{d}W_t \tag{10}\]</span></p><p>In the long term the expected value of <spanclass="math inline">\(H\)</span> tends towards <spanclass="math inline">\(\frac{\Delta}{4\Gamma}\)</span>. Combined with thedefinition of <span class="math inline">\(H\)</span>, we see that if weset <span class="math inline">\(\omega=10\)</span> and want to keep themagnitude of oscillation, we should set <spanclass="math inline">\(\Delta=0.005\times 100=0.5\)</span>.</p><h3 id="issue-time-alignment">Issue: time alignment</h3><p>We first discretize the state space as an ordered pair <spanclass="math inline">\((V,w)\)</span>. However, it's not a good idea ifmore variables are to be incorporated or even we have more states in<span class="math inline">\(w\)</span> (recall that we only have <spanclass="math inline">\(2\)</span> states of <spanclass="math inline">\(w\)</span>). A better idea is to separate thestate space of <span class="math inline">\(V\)</span> and <spanclass="math inline">\(w\)</span>. Since the evolution of <spanclass="math inline">\(V\)</span> and <spanclass="math inline">\(w\)</span> depends on each other, the transitionvector must be calculated each time the state changes.</p><p>The best we can do is it to precompute the transition probability dueto the leaky/cubic term. Each time the state shall change, we grab thetransition vector and add parts related to the other variable andexternal input.</p><p>One of the problems is that transition rate of <spanclass="math inline">\(V\)</span> and <spanclass="math inline">\(w\)</span> are not the same. How to align <spanclass="math inline">\(V\)</span> and <spanclass="math inline">\(w\)</span>? One way is to sample two random number<span class="math inline">\(\Delta t_V\)</span>, <spanclass="math inline">\(\Delta t_w\)</span> from two exponentialdistributions with rates corresponding to <spanclass="math inline">\(V\)</span> and <spanclass="math inline">\(w\)</span>. If <span class="math inline">\(\Deltat_V&lt; \Delta t_w\)</span>, then we update <spanclass="math inline">\(V\)</span>, otherwise we update <spanclass="math inline">\(w\)</span>.</p><h3 id="issue-range-of-w.">Issue: range of <spanclass="math inline">\(w\)</span>.</h3><p>In the rescaled system <span class="math inline">\((8)\)</span>, thestationary value of <span class="math inline">\(\bar{w}\)</span> isgiven by <spanclass="math inline">\(\frac{b}{\gamma}(\bar{V}-100c)\)</span>. However,we obviously don't need the range of <spanclass="math inline">\(\bar{w}\)</span> to be that big from simpleobservation of the trajectory and the vector field.</p><p>To constrain the range of <spanclass="math inline">\(\bar{w}\)</span>, we set a threshold of theleaky/cubic term.</p><h2 id="reference">Reference</h2>]]></content>
    
    
    <categories>
      
      <category>Neuronal Dynamics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Neuroscience</tag>
      
      <tag>Stochastic Process</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Eigenvalues of Tree Graph</title>
    <link href="/2024/01/29/Eigenvalues-of-Tree-Graph/"/>
    <url>/2024/01/29/Eigenvalues-of-Tree-Graph/</url>
    
    <content type="html"><![CDATA[<h1 id="eigenvalues-of-tree-graph">Eigenvalues of Tree Graph</h1><p>This blog version is slightly incomplete compared to the pdf versionon <a href="https://github.com/Newtonpula/Tree_Graph">github</a>. I putall related code there too.</p><h2 id="introduction">Introduction</h2><p>The Laplacian of <span class="math inline">\(G\)</span> is defined as<span class="math inline">\(Q = D - M\)</span>, where <spanclass="math inline">\(D\)</span> is the diagonal matrix of outdegrees ofvertices and <span class="math inline">\(M\)</span> is the adjacencymatrix of <span class="math inline">\(G\)</span>.</p><p>An <span class="math inline">\(l\)</span>-walk, or <spanclass="math inline">\(l\)</span>-path, in a directed graph <spanclass="math inline">\(G=(V,E)\)</span> means a directed <spanclass="math inline">\(l\)</span>-walk, that is a sequence <spanclass="math inline">\((u_0,e_1,u_1,\cdots ,e_{l}, u_l)\)</span> ofvertices and edges of <span class="math inline">\(G\)</span> such thatfor each <span class="math inline">\(1\leqslant i\leqslant l\)</span>,edge <span class="math inline">\(e_i\)</span> has initial vertex <spanclass="math inline">\(u_{i-1}\)</span> and terminal vertex <spanclass="math inline">\(u_i\)</span>. The walk is closed if <spanclass="math inline">\(u_0=u_l\)</span>.</p><p>If <span class="math inline">\(S\)</span> is a nonempty subset of<span class="math inline">\(V\)</span>, we denote by <spanclass="math inline">\(G_{S}\)</span> the induced subgraph of <spanclass="math inline">\(G\)</span> on the vertex set <spanclass="math inline">\(S\)</span>, that is the directed graph obtainedfrom <span class="math inline">\(G\)</span> by only keeping vertices in<span class="math inline">\(S\)</span> and edges with both endpoints in<span class="math inline">\(S\)</span>.</p><h2 id="tree-graph">tree graph</h2><p>In this section we recall some basic concepts and introduce a fewprevious results.</p><h3 id="spanning-trees-and-forests">Spanning trees and forests</h3><p>Let <span class="math inline">\(G = (E,V)\)</span> be a finitedirected graph. For each edge we denote <spanclass="math inline">\(s(e)\)</span> its source and <spanclass="math inline">\(t(e)\)</span> its target. A (rooted) spanning treeof <span class="math inline">\(G\)</span> is a subgraph of <spanclass="math inline">\(G\)</span> containing all vertices with no cycle.There is one vertex <span class="math inline">\(r\)</span>, called theroot, having outdegree <span class="math inline">\(0\)</span> and theother vertices have outdegree <span class="math inline">\(1\)</span>. Bythe nature of tree, the spanning tree of <spanclass="math inline">\(G\)</span> has <spanclass="math inline">\(|V|-1\)</span> edges.</p><p>Generally, if $W V $ is a nonempty subset, a forest of <spanclass="math inline">\(G\)</span> rooted in <spanclass="math inline">\(W\)</span> is defined as the following subgraph:it contains all vertices, with no cycle, and the vertices in <spanclass="math inline">\(W\)</span> have outdegree <spanclass="math inline">\(0\)</span> while the other vertices have outdegree<span class="math inline">\(1\)</span>.</p><h4 id="tree-graph-mathcaltg">Tree graph <spanclass="math inline">\(\mathcal{T}G\)</span></h4><p>The tree graph of <span class="math inline">\(G\)</span>, denoted<span class="math inline">\(\mathcal{T}G\)</span>, is the directed graphwhose vertices are the spanning trees of <spanclass="math inline">\(G\)</span> and whose edges are obtained by thefollowing construction:</p><p>Let <span class="math inline">\(\mathbf{a}\)</span> be a spanningtree of <span class="math inline">\(G\)</span> with root <spanclass="math inline">\(r\)</span>. For an edge <spanclass="math inline">\(e\in E\)</span> with <spanclass="math inline">\(s(e)=r\)</span>, let <spanclass="math inline">\(\mathbf{b}\)</span> be the subgraph of <spanclass="math inline">\(G\)</span> obtained by adding edge <spanclass="math inline">\(e\)</span> to <spanclass="math inline">\(\mathbf{a}\)</span> then deleting the edge comingout of <span class="math inline">\(t(e)\)</span> in <spanclass="math inline">\(\mathbf{a}\)</span>.</p><p>It's easy to check that <spanclass="math inline">\(\mathbf{b}\)</span> is a spanning tree of <spanclass="math inline">\(G\)</span>, with root <spanclass="math inline">\(t(e)\)</span>.</p><p><img src="/img/tree_graph.png" /></p><p>In the following we will denote <spanclass="math inline">\(\mathcal{T}V\)</span> the set of vertices of <spanclass="math inline">\(\mathcal{T}G\)</span>, i.e., the set of spanningtrees of <span class="math inline">\(G\)</span>.</p><p>There is a natural map <span class="math inline">\(p\)</span> from<span class="math inline">\(\mathcal{T}G\)</span> to <spanclass="math inline">\(G\)</span>. <span class="math inline">\(p\)</span>maps each vertex of <span class="math inline">\(\mathcal{T}G\)</span> toits root, and maps each edge of <spanclass="math inline">\(\mathcal{T}G\)</span> to the edge <spanclass="math inline">\(e\)</span> of <spanclass="math inline">\(G\)</span> for its construction.</p><p>Now we give here some elementary properties of the tree graph. Seeprevious papers for proofs.</p><ul><li><span class="math inline">\(\mathcal{T}G\)</span> is simple and hasno loop.</li><li>The graph <span class="math inline">\(\mathcal{T}G\)</span> isstrongly connected if <span class="math inline">\(G\)</span> is.</li><li>The graph <span class="math inline">\(\mathcal{T}G\)</span> is a <ahref="https://en.wikipedia.org/wiki/Covering_graph">covering graph</a>of <span class="math inline">\(G\)</span>.</li><li>The set of edges of <spanclass="math inline">\(\mathcal{T}G\)</span> can be partitioned intoedge-disjoint simple cycles, which project onto simple cycles of <spanclass="math inline">\(G\)</span>. If <spanclass="math inline">\(C\)</span> is a simple cycle of <spanclass="math inline">\(G\)</span>, with vertex set <spanclass="math inline">\(W\)</span>, then the number of simple cycles of<span class="math inline">\(\mathcal{T}G\)</span> lying above <spanclass="math inline">\(C\)</span> is equal to the number of forestsrooted in <span class="math inline">\(W\)</span>.</li><li>The graph <span class="math inline">\(\mathcal{T}G\)</span> isEulerian: The number of outgoing or incoming edges of a vertex <spanclass="math inline">\(\mathbf{a}\)</span> are both equal to the numberof outgoing edges of the root of <spanclass="math inline">\(\mathbf{a}\)</span> in <spanclass="math inline">\(G\)</span>.</li></ul><p>If $W V $, The matrix-tree theorem gives the generating function ofspanning forests of <span class="math inline">\(G\)</span> rooted in<span class="math inline">\(W\)</span> . Moreover, it's easy to know thetotal number of those spanning forests: Denote <spanclass="math inline">\(Q^{W}\)</span> as the matrix obtained from theLaplacian matrix <span class="math inline">\(Q\)</span> of <spanclass="math inline">\(G\)</span> by deleting rows and columns indexed byelements of <span class="math inline">\(W\)</span>, then the number ofspanning forests of <span class="math inline">\(G\)</span> rooted in<span class="math inline">\(W\)</span> is given by <spanclass="math inline">\(\det (Q^{W})\)</span>.</p><p>In particular, let <span class="math inline">\(W = \{r\}\)</span>,then the number of spanning trees of <spanclass="math inline">\(G\)</span> rooted in <spanclass="math inline">\(r\)</span> is given by <spanclass="math inline">\(\det (Q^{r})\)</span>. For convenience in thefollowing we use the notation <span class="math inline">\(Q_{W} = Q^{V\backslash W}\)</span> to denote the matrix extracted from the Laplacianmatrix <span class="math inline">\(Q\)</span> of <spanclass="math inline">\(G\)</span> by keeping only rows and columnsindexed by elements of <span class="math inline">\(W\)</span>.</p><p>It is important to notice that the matrix-tree theorem is valid forboth directed and undirected graphs.</p><p>By the matrix-tree theorem, we can easily derive all spanning treesof <span class="math inline">\(G\)</span>, then construct the tree graph<span class="math inline">\(\mathcal{T}G\)</span>.</p><h2 id="eigenvalues-of-tree-graph-1">Eigenvalues of Tree Graph</h2><p>Though astonishing, eigenvalues of the adjacency matrix of the treegraph <span class="math inline">\(\mathcal{T}G\)</span> can be computedexplicitly. This can be expected from the following fact: the <spanclass="math inline">\((i,j)\)</span> entry of the matrix <spanclass="math inline">\(M\)</span>, where <spanclass="math inline">\(M\)</span> is the adjacency matrix of <spanclass="math inline">\(G\)</span>, equals the number of <spanclass="math inline">\(l\)</span>-walks in <spanclass="math inline">\(G\)</span> which start at <spanclass="math inline">\(v_i\)</span> and end at <spanclass="math inline">\(v_j\)</span>. Thus, the number of closed <spanclass="math inline">\(l\)</span>-walks in <spanclass="math inline">\(\mathcal{T}G\)</span>, which we will denote by<span class="math inline">\(w(\mathcal{T}G, l)\)</span>, equals thetrace of <span class="math inline">\(M^{l}\)</span> and hence the sum ofthe <span class="math inline">\(l\)</span>th powers of the eigenvaluesof <span class="math inline">\(M\)</span>.</p><p>C. Athanasiadis proved the following theorem about eigenvalues of<span class="math inline">\(\mathcal{M}\)</span>, i.e., the adjacencymatrix of <span class="math inline">\(\mathcal{T}G\)</span>.<div class="note note-info">            <p>In Biane and Chapuy's paper, they wrote the theorem wrongly(Proposition 3.1)</p>          </div></p><p><strong>Theorem</strong> The nonzero eigenvalues of <spanclass="math inline">\(\mathcal{M}\)</span> are eigenvalues of <spanclass="math inline">\(M_{X};X \subseteq V\)</span> where <spanclass="math inline">\(M\)</span> is the adjacency matrix of <spanclass="math inline">\(G\)</span>. For an eigenvalue <spanclass="math inline">\(\gamma\)</span>, let <spanclass="math inline">\(m_{X}(\gamma)\)</span> denotes its multiplicity in<span class="math inline">\(M_{X}\)</span>, then its multiplicity in<span class="math inline">\(\mathcal{M}\)</span> is <spanclass="math display">\[    \sum_{X \subseteq V }^{} m_{X}(\gamma) \det(Q_{V\backslash X}-I)\]</span></p><p>Here we make the convention that <spanclass="math inline">\(\det(Q_{\emptyset}-I) = 1\)</span>.</p><p>The original proof from Athanasiadis is a little confusing, so wemake it more clear here. First we need to calculate <spanclass="math inline">\(w(\mathcal{T}G,l)\)</span>.</p><p><strong>Theorem</strong> For a directed graph <spanclass="math inline">\(G\)</span> on the vertex set <spanclass="math inline">\(V\)</span> we have <span class="math display">\[    w(\mathcal{T}G, l) = \sum_{S \subseteq V} w(G_{S}, l)\det(Q_{V\backslash S}-I).\]</span></p><p><strong>proof</strong> A closed <spanclass="math inline">\(l\)</span>-walk in <spanclass="math inline">\(\mathcal{T}G\)</span> is determined by a spanningtree <span class="math inline">\(T_0\)</span> on <spanclass="math inline">\(G\)</span> with root <spanclass="math inline">\(r\)</span>, and a closed <spanclass="math inline">\(l\)</span>-walk <spanclass="math inline">\(W=(u_0,e_1,u_1,\cdots ,e_l,u_l)\)</span> in <spanclass="math inline">\(G\)</span> with <spanclass="math inline">\(u_0=u_l=r\)</span>. Let <spanclass="math inline">\(a=(u_0,u_1,\cdots ,u_l)\)</span>. The sequence<span class="math inline">\(a\)</span> determines the roots of the trees<span class="math inline">\(T_0, T_1,\cdots ,T_{l}=T_0\)</span> to bevisited during the walk in <spanclass="math inline">\(\mathcal{T}G\)</span>.</p><p>Fix a closed <span class="math inline">\(l\)</span>-walk <spanclass="math inline">\(W\)</span> in <spanclass="math inline">\(G\)</span> together with the sequence <spanclass="math inline">\(a\)</span>. Following Athanasiadis's argument(Theorem 2.2) the number of trees <spanclass="math inline">\(T_0\)</span> which will yield a closed <spanclass="math inline">\(l\)</span>-walk in <spanclass="math inline">\(\mathcal{T}G\)</span> is the number <spanclass="math inline">\(\tau(G,U)\)</span> of spanning forests on <spanclass="math inline">\(G\)</span> with root set <spanclass="math inline">\(U=\{u_0,u_1,\cdots ,u_l\}\)</span>. Finally, thenumber of closed <span class="math inline">\(l\)</span>-walks in <spanclass="math inline">\(\mathcal{T}G\)</span> is <spanclass="math display">\[    w(\mathcal{T}G,l) = \sum_{U \subseteq V }^{} g(G_{U},l)\tau(G,U),\]</span></p><p>where <span class="math inline">\(g(G,l)\)</span> stands for thenumber of closed <span class="math inline">\(l\)</span>-walks in a graph<span class="math inline">\(G\)</span> visiting all of its vertices.</p><p>We may explain (1) more explicitly. Notice that a closed <spanclass="math inline">\(l\)</span>-walk in <spanclass="math inline">\(G\)</span> can have repeated vertices, so if <spanclass="math inline">\(G_{U}\)</span> has a closed <spanclass="math inline">\(l\)</span>-walk visiting all of its vertices, then<span class="math inline">\(\lvert U \rvert\)</span> can be divided by<span class="math inline">\(l\)</span>. Also, <spanclass="math inline">\(G_{U}\)</span> can have more than one closed <spanclass="math inline">\(l\)</span>-walk. The inclusion-exclusion principlegives <span class="math display">\[    g(G_{U},l) = \sum_{S\subseteq U }^{} (-1)^{\lvert U-S \rvert}w(G_{S},l).\]</span></p><p>Hence, after using (2) to compute <spanclass="math inline">\(g(G_{U},l)\)</span> and changing the order ofsummation, (1) becomes <span class="math display">\[    w(\mathcal{T}G,l) = \sum_{S\subseteq V }^{} w(G_{S},l) \sum_{S\subseteq U \subseteq V  }^{} (-1)^{\lvert U-S \rvert }\tau(G,U).\]</span></p><p>Recall that by the matrix-tree theorem, <spanclass="math inline">\(\tau(G,U) = \det(Q_{V\backslash U})\)</span>, thus<span class="math display">\[    w(\mathcal{T}G,l) = \sum_{S \subseteq V} w(G_{S},l)\sum_{S \subseteqU \subseteq V  }^{} (-1)^{\lvert U-S \rvert }\det (Q_{V\backslash U}) =\sum_{S \subseteq V} w(G_{S},l)\det(Q_{V\backslash S}-I).\]</span></p><p>Now that we have the number of closed <spanclass="math inline">\(l\)</span>-walks in <spanclass="math inline">\(\mathcal{T}G\)</span>, we can compute the trace of<span class="math inline">\(\mathcal{M}^{l}\)</span> and <spanclass="math inline">\(M^{l}\)</span>. We need the following lemma tocomplete the proof.</p><p><strong>lemma</strong> Suppose that for some nonzero complex numbers<span class="math inline">\(a_i\)</span>, <spanclass="math inline">\(b_j\)</span>, where <spanclass="math inline">\(1\leqslant i\leqslant r\)</span> and <spanclass="math inline">\(1\leqslant j\leqslant s\)</span> we have <spanclass="math display">\[    \sum_{i=1}^{r} a_i^{l} = \sum_{j=1}^{s} b_j^{l}\]</span></p><p>for all positive integers <span class="math inline">\(l\)</span>.Then <span class="math inline">\(r=s\)</span> and the <spanclass="math inline">\(a_i\)</span> are a permutation of the <spanclass="math inline">\(b_j\)</span>.</p><p>The proof can be found in Athanasiadis's paper.</p><p>From theorem 3.2 and Lemma 3.3 we immediately get theorem 3.1.</p><h2 id="example-of-complete-graph">Example of Complete Graph</h2><p>The complete directed graph <spanclass="math inline">\(K_{n}\)</span> is the graph on the vertex set<span class="math inline">\(V=[n] = \{1,2,\cdots ,n\}\)</span> withexactly one directed edge from <span class="math inline">\(i\)</span> to<span class="math inline">\(j\)</span> for each <spanclass="math inline">\(i\neq j\)</span>.</p><h3 id="eigenvalues-of-mathcaltk_n">Eigenvalues of <spanclass="math inline">\(\mathcal{T}K_{n}\)</span></h3><p><strong>proposition</strong> With <spanclass="math inline">\(G=K_{n}\)</span>, <spanclass="math inline">\(n\geqslant 2\)</span>, <spanclass="math inline">\(\mathcal{M}\)</span> has eigenvalues <spanclass="math inline">\(-1, 1,\cdots ,n-1\)</span>. The multiplicity of<span class="math inline">\(i\)</span> is <span class="math display">\[    m_{\mathcal{M}}(i) =        \begin{cases}            i{n \choose i+1}(n-1)^{n-i-2}, \quad 1\leqslant i\leqslantn-1 \\            n^{n-1}-(n-1)^{n-1}, \quad i=-1.        \end{cases}\]</span></p><p><strong>proof</strong> First we calculate the eigenvalues of <spanclass="math inline">\(K_{n}\)</span>. Note that <spanclass="math inline">\((1,\cdots ,1)^{\mathsf{T}}\)</span> is aneigenvector of <span class="math inline">\(M\)</span> with eigenvalue<span class="math inline">\(n-1\)</span>. Besides, <spanclass="math inline">\(J_n\)</span>, the <spanclass="math inline">\(n\times n\)</span> matrix with all entries equalto <span class="math inline">\(1\)</span>, has a kernel of dimension<span class="math inline">\(n-1\)</span>. Thus the eigenvalues of <spanclass="math inline">\(K_{n}\)</span> are <spanclass="math inline">\(n-1\)</span> with multiplicity <spanclass="math inline">\(1\)</span> and <spanclass="math inline">\(-1\)</span> with multiplicity <spanclass="math inline">\(n-1\)</span>. Hence, by theorem 3.1, theeigenvalues of <span class="math inline">\(\mathcal{M}\)</span> are<span class="math inline">\(-1, 1,\cdots ,n-1\)</span>. Moreover theeigenvalue <span class="math inline">\(m-1\)</span>, <spanclass="math inline">\(2\leqslant m\leqslant n\)</span> has multiplicity<span class="math display">\[    {n \choose m}\det (Q_{V\backslash [m]}-I) = {n \choosem}(m-1)(n-1)^{n-m-1}.\]</span></p><p>which is easy to obtain by consider eigenvalues of <spanclass="math inline">\(Q_{V\backslash [m]}-I\)</span>.</p><p><span class="math inline">\(-1\)</span> has multiplicity <spanclass="math display">\[    \begin{aligned}        &amp;\sum_{m=2}^{n} {n \choose m}(m-1)\det (Q_{V\backslash[m]}-I)  \\        &amp;= \sum_{m=1}^{n} (m-1)^{2} {n\choose m} (n-1)^{n-m-1} =n^{n-1}-(n-1)^{n-1}.    \end{aligned}\]</span></p><p>The multiplicities we have so far add up to <spanclass="math inline">\(n^{n-1}\)</span>, so <spanclass="math inline">\(0\)</span> is not an eigenvalue of <spanclass="math inline">\(\mathcal{M}\)</span> and the propositionfollows.</p><h3 id="properties-of-mathcaltk_n">Properties of <spanclass="math inline">\(\mathcal{T}K_{n}\)</span></h3><p>Properties of <span class="math inline">\(K_{n}\)</span> have alwaysbeen interesting. We have a lot more to say about <spanclass="math inline">\(\mathcal{T}K_{n}\)</span>. First, since <spanclass="math inline">\(\mathcal{T}K_{n}\)</span> is a covering graph of<span class="math inline">\(K_{n}\)</span>, we have the followingproposition.</p><p><strong>proposition</strong> <spanclass="math inline">\(\mathcal{T}K_{n}\)</span> is a regular graph whereeach vertex has outdegree and indegree <spanclass="math inline">\(n-1\)</span>.</p><p>However, we remind readers that while every vertex in <spanclass="math inline">\(\mathcal{T}K_{n}\)</span> has outdegree <spanclass="math inline">\(n-1\)</span> is obvious from the construction ofedges in <span class="math inline">\(\mathcal{T}K_{n}\)</span>, theconstant indegree of each vertex is not trivial at all. Given a spanningtree <span class="math inline">\(T_0\)</span> of <spanclass="math inline">\(K_{n}\)</span>, writing down all <spanclass="math inline">\(T_1,\cdots ,T_{n-1}\)</span> that <spanclass="math inline">\(T_i \rightarrow T_0\)</span> in <spanclass="math inline">\(\mathcal{T}K_{n}\)</span> is not that easy. Thedifficulty lies in irreversibility of the construction of edges in <spanclass="math inline">\(\mathcal{T}K_{n}\)</span>. Actually, we can getthe following proposition.</p><p><strong>proposition</strong> Let <spanclass="math inline">\(T_1\)</span> is a spanning tree of <spanclass="math inline">\(K_{n}\)</span> with root <spanclass="math inline">\(r_1\)</span>, <spanclass="math inline">\(T_2\)</span> is a spanning tree of <spanclass="math inline">\(K_{n}\)</span> with root <spanclass="math inline">\(r_2\)</span>. There is an edge <spanclass="math inline">\(T_1\rightarrow T_2\)</span> in <spanclass="math inline">\(\mathcal{T}K_n\)</span>. Then there is an edge<span class="math inline">\(T_2 \rightarrow T_1\)</span> in <spanclass="math inline">\(\mathcal{T}K_{n}\)</span> if and only if <spanclass="math inline">\(r_2 \rightarrow r_1\)</span> in <spanclass="math inline">\(T_1\)</span>. Also, by definition, we have <spanclass="math inline">\(r_1 \rightarrow r_2\)</span> in <spanclass="math inline">\(T_2\)</span>. In the remaining we will say thatthere is a bi-edge between <span class="math inline">\(T_1\)</span> and<span class="math inline">\(T_2\)</span>.</p><p><strong>proof</strong> Suppose <spanclass="math inline">\(r_2\rightarrow r_1\)</span> in <spanclass="math inline">\(T_1\)</span>. Then in <spanclass="math inline">\(T_2\)</span>, we have <spanclass="math inline">\(r_1\rightarrow r_2\)</span>, and <spanclass="math inline">\(r_1\)</span> has no other outgoing edges. Thus, ifwe add <span class="math inline">\(r_2 \rightarrow r_1\)</span> in <spanclass="math inline">\(T_2\)</span> and delete <spanclass="math inline">\(r_1 \rightarrow r_2\)</span> in <spanclass="math inline">\(T_2\)</span>, we get exactly <spanclass="math inline">\(T_1\)</span>. Hence, <spanclass="math inline">\(T_2 \rightarrow T_1\)</span> in <spanclass="math inline">\(\mathcal{T}K_{n}\)</span>. Conversely, suppose<span class="math inline">\(T_2 \rightarrow T_1\)</span> in <spanclass="math inline">\(\mathcal{T}K_{n}\)</span>. By definition, <spanclass="math inline">\(r_2\rightarrow r_1\)</span> in <spanclass="math inline">\(T_1\)</span>.</p><p>From the above proposition, we know that given <spanclass="math inline">\(T_0\)</span>, the number of <spanclass="math inline">\(T\)</span> such that <spanclass="math inline">\(T_0\)</span> and <spanclass="math inline">\(T\)</span> has a bi-edge equals the indegree of<span class="math inline">\(r_0\)</span> in <spanclass="math inline">\(T_0\)</span>, where <spanclass="math inline">\(r_0\)</span> is the root of <spanclass="math inline">\(T_0\)</span>.</p><p>For any <span class="math inline">\(m\)</span>, we now considernumber of rooted trees with <span class="math inline">\(m\)</span>bi-edges. There is a helpful simple decomposition of rooted trees: Let<span class="math inline">\(T\)</span> be a rooted tree. If we removeall edges incident with the root, we get the root together with a set oftrees. Number of trees equals indegree of the root.</p><p><strong>proposition</strong> With <spanclass="math inline">\(G=K_{n}\)</span>, number of trees with <spanclass="math inline">\(m\)</span> bi-edges is <spanclass="math display">\[    nm {n-1 \choose m}(n-1)^{n-2-m}\]</span></p><p><strong>proof</strong> First, there are <spanclass="math inline">\(n\)</span> choices of root, denoted as <spanclass="math inline">\(r_0\)</span>. Second, by the above discussion,there are <span class="math inline">\({n-1\choose m}\)</span> choices ofroots after the decomposition, denoted as <spanclass="math inline">\(r_1,\cdots ,r_m\)</span>. Finally, excluding <spanclass="math inline">\(r_0\)</span>, number of forests rooted in <spanclass="math inline">\(r_1,\cdots ,r_m\)</span> equals to <spanclass="math inline">\(\det (n-1)I_{n-1-m}-J_{n-1-m}\)</span>, which is<span class="math inline">\(m(n-1)^{n-2-m}\)</span>. By multiplicityprinciple, we get the proposition.</p><p><strong>proposition</strong> With <spanclass="math inline">\(G=K_n\)</span>, if <spanclass="math inline">\(T_0\rightarrow T_1\)</span>, <spanclass="math inline">\(T_1\rightarrow T_2\)</span> in <spanclass="math inline">\(\mathcal{T}G\)</span>, then <spanclass="math inline">\(T_0\nrightarrow T_2\)</span>.</p><p><strong>proof</strong> Denote roots of <spanclass="math inline">\(T_0,T_1,T_2\)</span> as <spanclass="math inline">\(r_0,r_1,r_2\)</span> and they are distinct. If<span class="math inline">\(T_0\rightarrow T_1\)</span> and <spanclass="math inline">\(T_1\rightarrow T_2\)</span>, then <spanclass="math inline">\(r_0\rightarrow r_1\rightarrow r_2\)</span>. Thus,if we add an edge from <span class="math inline">\(r_0\)</span> to <spanclass="math inline">\(r_2\)</span>, the spanning tree we get isdifferent from <span class="math inline">\(T_2\)</span>.</p><h3 id="three-conjectures">Three Conjectures</h3><p>Properties of <span class="math inline">\(\mathcal{T}K_{n}\)</span>are much more than we have discussed. In particular, while eigenvaluesof <span class="math inline">\(\mathcal{T}K_{n}\)</span> have beencomputed, the Jordan canonical form of <spanclass="math inline">\(\mathcal{T}K_{n}\)</span> has not been obtained.By calculating <span class="math inline">\(n=3,4,5,6\)</span> cases, wemake three conjectures here.</p><p><strong>conjecture 1</strong> For eigenvalues <spanclass="math inline">\(\lambda=1,2,\cdots ,n-1\)</span> of <spanclass="math inline">\(\mathcal{T}K_{n}\)</span>, the Jordan blocks areall of size <span class="math inline">\(1\)</span>.</p><p><strong>conjecture 2</strong> For eigenvalue <spanclass="math inline">\(\lambda=-1\)</span>, the Jordan blocks are of size<span class="math inline">\(1,2,\cdots ,n-1\)</span>. Moreover, thereare exactly <span class="math inline">\(n-1\)</span> Jordan blocks ofsize <span class="math inline">\(n-1\)</span>.</p><p><strong>conjecture 3</strong> For eigenvalue <spanclass="math inline">\(\lambda=-1\)</span>, <spanclass="math inline">\(\operatorname{rank}((\mathcal{M}+I)^{n-1})=\operatorname{rank}((\mathcal{M}+I)^{n})=\operatorname{rank}((\mathcal{M}+I)^{n+1})=\cdots=(n-1)^{n-1}\)</span>.</p><p><img src="/img/tree_graph_2.png" /></p><p>We hope to prove or falsify these three conjectures later.</p><h3 id="classification-of-vertices-of-mathcaltk_n">Classification ofVertices of <span class="math inline">\(\mathcal{T}K_{n}\)</span></h3><p>In Biane and Chapuy's paper, they obtain the number of spanning treesof <span class="math inline">\(\mathcal{T}K_{n}\)</span>, namely thenumber of vertices of graph <spanclass="math inline">\(\mathcal{T}^{2}K_{n}\)</span>: <spanclass="math display">\[    n^{n-2}\prod_{k=1}^{n-1} ((n-k)n^{k-1})^{(k-1)(n-1)^{n-k-1}{n\choosek}}\]</span></p><p>If we want to calculate eigenvalues of <spanclass="math inline">\(\mathcal{T}^{2}K_n\)</span>, we need firstclassify vertices of <spanclass="math inline">\(\mathcal{T}K_{n}\)</span>. By relabeling thisreduces to the problem of classifying unlabeled rooted trees with <spanclass="math inline">\(n\)</span> vertices. Namely, we only considernumber of equivalence classes under graph isomorphism.</p><p>There is no simple formula for counting unlabeled trees. Denote <spanclass="math inline">\(r(x)\)</span> be the generating function fornumber of unlabeled rooted trees, i.e., <spanclass="math inline">\(r(x)=\sum_{n=1}^{\infty} r_nx^{n}\)</span> where<span class="math inline">\(r_n\)</span> is the number of unlabeledrooted trees with <span class="math inline">\(n\)</span> vertices. Thenwe have the following theorem.</p><p><strong>Theorem</strong> The generating function <spanclass="math inline">\(r(x)\)</span> for unlabeled rooted trees satisfies<span class="math display">\[        r(x) = x h[r(x)] = x \exp \left( \sum_{k=1}^{\infty}\frac{r(x^{k})}{k} \right) .       \]</span></p><p>Please refer to Gessel's paper for definition for <spanclass="math inline">\(h(A(x))\)</span> and proof of this theorem. Wefind that <span class="math display">\[    \begin{aligned}        r(x) = x + x^{2} &amp;+ 2x^{3} + 4x^{4} + 9x^{5} + 20 x^{6} +48x^{7} + 115x^{8} + 286x^{9} \\        &amp;+ 719x^{10} + 1842 x^{11} + 4766x^{12} + 12486x^{13} +32973x^{14} \\        &amp;+ 87811x^{15} + 235381 x^{16} + 634847x^{17} +1721159x^{18} \\        &amp;+ 4688676 x^{19} + 1282622x^{20} + 35221832x^{21} + \cdots    \end{aligned}\]</span></p><p>These numbers of sequence A000081 in the Online Encyclopedia ofInteger Sequences (OEIS).</p>]]></content>
    
    
    <categories>
      
      <category>graph theory</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Network Effects in Neurostimulation (2)</title>
    <link href="/2023/10/18/Network-Effects-in-Neurostimulation-2/"/>
    <url>/2023/10/18/Network-Effects-in-Neurostimulation-2/</url>
    
    <content type="html"><![CDATA[<h1 id="network-effects-in-neurostimulation-2">Network Effects inNeurostimulation (2)</h1><h2 id="dataset-used-to-training">Dataset used to training</h2><p>The dataset we used is from Paulk, 2022, in which singlepulse-induced cortico-cortical evoked potentials (CCEPs) were recorded.The data is from F-TRACT database.</p><h2 id="reasons-of-training-with-original-deco-model">Reasons oftraining with original Deco model</h2><p>As mentioned at the end of <a href="">the previous blog</a>, Iderived a modified model.</p><h2id="correlation-is-significantly-higher-in-critical-region">Correlationis significantly higher in critical region</h2><h3 id="analysis-of-the-phase-transition-line">Analysis of the phasetransition line</h3><h3 id="calculate-the-phase-transition-line">Calculate the phasetransition line</h3><p><span class="math display">\[    \begin{aligned}        \frac{\mathrm{d}S_{i}(t)}{\mathrm{d}t} &amp;=-\frac{S_{i}}{\tau_{S}} + (1-S_{i})\gamma H(x_i) \\        H(x_i) &amp;= \frac{ax_{i}-b}{1-\exp (-d(ax_{i}-b))} \\        x_{i} &amp;= wJ_{N}S_{i} + GJ_{N}\sum_{j}^{} C_{ij}S_{j}    \end{aligned}\]</span> <!--For training, $H(x)$ is replaced by $$    \tilde{H}(x) = \frac{1}{d}\ln(1+\exp d(ax-b))$$--></p><ul><li>First we consider <span class="math inline">\(G=0\)</span></li></ul><p><span class="math display">\[\tau_S \gamma (1-S_i)H(wJ_N S_i) = S_i\]</span></p><p>At critical point, <span class="math display">\[\tau_S \gamma H(wJ_N S_i)+1 = \tau_S \gamma (1-S_i) wJ_N\frac{\mathrm{d}H}{\mathrm{d}x}(wJ_N S_i)\]</span></p><p><span class="math display">\[    w = w_0 \approx 2.9185\]</span></p><ul><li>For <span class="math inline">\(G&gt;0\)</span>, denote <spanclass="math inline">\(\lambda\)</span> as the spectral radius ofstruc_conn_matrix.</li></ul><p>If there exists a high stable state, we have the following lemma:<strong>Lemma</strong> <span class="math inline">\(\exists k \in\{1,\cdots ,N\}\)</span>, such that <span class="math display">\[    \sum_{j=1}^{N} \mathbf{C}_{kj}S_{j} \leqslant \lambda S_{k}.\]</span></p><p><strong>proof</strong> Since <span class="math inline">\(C\)</span>is symmetric, we have(https://en.wikipedia.org/wiki/Spectral_radius)</p><p><span class="math display">\[    \left\| \mathbf{C}\mathbf{S} \right\|_{2}\leqslant \lambda \left\|\mathbf{S} \right\|_{2},\]</span></p><p>i.e., <span class="math display">\[    \sqrt{\sum_{k=1}^{N} \left( \sum_{j=1}^{N} C_{kj}S_j \right)^{2}}\leqslant \lambda \sqrt{\sum_{k=1}^{N} S_k^{2}}\]</span></p><p>Thus, <span class="math inline">\(\exists k \in \{1,\cdots,N\}\)</span>, such that <span class="math display">\[    \sum_{j=1}^{N} C_{kj}S_j \leqslant \lambda S_{k}    \]</span></p><p>The existence of the high stable state is equivalent to <spanclass="math display">\[    S_i = \tau_S \gamma (1-S_i)H(J_N (w S_i + G\sum_{j=1}^{N}C_{ij}S_j))\]</span></p><p>Since <span class="math inline">\(H\)</span> is increasing, by thelemma we know that <span class="math inline">\(\exists k \in \{1,\cdots,N\}\)</span>, <span class="math display">\[    S_k = \tau_S \gamma (1-S_k)H(J_N (w S_k + G\sum_{j=1}^{N}C_{kj}S_j)) \leqslant  \tau_S \gamma (1-S_k)H(J_N (w + \lambda G)S_k)\]</span></p><p>This can only happen when <span class="math inline">\(w + \lambda G\geqslant w_0\)</span>.</p><!--An intuitive approach is let $\mathbf{S}$ be the eigenvector corresponding to $\lambda$, then we have$$    x_i = J_{N}(wS_i + G \sum_{j}^{} C_{ij}S_{j}) = J_{N}(w S_{i} + \lambda G S_{i}) = J_{N}(w + \lambda G)S_{i}$$the dynamics of $S_{i}$ seems untangled:$$    \frac{\mathrm{d}S_{i}}{\mathrm{d}t} = -\frac{S_{i}}{\tau_{S}} + (1-S_{i}) \gamma H(J_{N}(w + \lambda G)S_{i})$$From the situation of $G=0$, the phase transition point should be $$    w + \lambda G  = w_{0}$$However, the approach suffers from the problem of existence of a high stable state. The problem is comprised of two parts:- Whether there exists a high stable state.- If there exists, why can the initial state of $\mathbf{S}$ (S_init = 0.999) converge to the high stable state.--><h2id="information-in-n2-component-implies-effective-connectivity">Informationin N2 component implies effective connectivity</h2><h2 id="further-problems">Further problems</h2><h2 id="refrence">Refrence</h2><p>Paulk, A.C., Zelmann, R., Croker, B., et al. (2022). Local anddistant cortical responses to single pulse intracranial stimulation inthe human brain are differentially modulated by specific stimulationparameters. Brain Stimulation. 15, 491-508</p>]]></content>
    
    
    <categories>
      
      <category>Neuronal Dynamics</category>
      
      <category>Neurostimulation</category>
      
      <category>CCEP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Neuroscience</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Takens&#39;s Theorem</title>
    <link href="/2023/10/04/Takens-s-Theorem/"/>
    <url>/2023/10/04/Takens-s-Theorem/</url>
    
    <content type="html"><![CDATA[<h1 id="takenss-theorem">Takens's Theorem</h1><p>Takens's theorem shows connection between space and time, which is animportant result in dynamical system. It also relates to ergodic theoryand have applications in many fields. The proof is much obscure sinceit's mainly based on ideas from differential topology. The originalproof is by, of course,Takens<sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="F. Takens, Detecting strange attractors in turbulence, in Dynamical Systems and Turbulence, Warwick 1980, vol. 898, D. Rand and L.-S. Young, Eds., in Lecture Notes in Mathematics, vol. 898. , Berlin, Heidelberg: Springer Berlin Heidelberg, 1981, pp. 366381. doi: 10.1007/BFb0091924.">[8]</span></a></sup>.Sauer, Yorke andCasdgali<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="T. Sauer, J. A. Yorke, and M. Casdagli, Embedology, J Stat Phys, vol. 65, no. 34, pp. 579616, Nov. 1991, doi: 10.1007/BF01053745.">[6]</span></a></sup>generalized Takens's work. One big point is that manifolds aregeneralized to subset of <spanclass="math inline">\(\mathbb{R}^{k}\)</span> with certain box-countingdimension. Here we do not give the proof of Takens's theorem. Bothoriginal and explanatory materialsexist<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="J. P. Huke, Embedding Nonlinear Dynamical Systems: A Guide to Takens Theorem. Mar. 2006. [Online]. Available: https://api.semanticscholar.org/CorpusID:55183186">[2]</span></a></sup><sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="J. Penalva Vadell, Takenss theorem proof and applications.pdf. 2018.">[5]</span></a></sup><sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="T. Sauer, J. A. Yorke, and M. Casdagli, Embedology, J Stat Phys, vol. 65, no. 34, pp. 579616, Nov. 1991, doi: 10.1007/BF01053745.">[6]</span></a></sup><sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="F. Takens, Detecting strange attractors in turbulence, in Dynamical Systems and Turbulence, Warwick 1980, vol. 898, D. Rand and L.-S. Young, Eds., in Lecture Notes in Mathematics, vol. 898. , Berlin, Heidelberg: Springer Berlin Heidelberg, 1981, pp. 366381. doi: 10.1007/BFb0091924.">[8]</span></a></sup>.We focus on applications of Takens's theorem, including reconstructionof attractor from time seriesdata<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="H. Ma, S. Leng, K. Aihara, W. Lin, and L. Chen, Randomly distributed embedding making short-term high-dimensional data predictable, Proc. Natl. Acad. Sci. U.S.A., vol. 115, no. 43, Oct. 2018, doi: 10.1073/pnas.1802987115.">[4]</span></a></sup>and change-pointdetection<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="J.-W. Hou, H.-F. Ma, D. He, J. Sun, Q. Nie, and W. Lin, Harvesting random embedding for high-frequency change-point detection in temporal complex systems, National Science Review, vol. 9, no. 4, p. nwab228, May 2022, doi: 10.1093/nsr/nwab228.">[1]</span></a></sup>.</p><h2 id="box-counting-dimension">box-counting dimension</h2><p>The Minkowski-Bouigand dimension, also known as Minkowski dimensionor box-counting dimension.</p><p>Denote <span class="math inline">\(N(\varepsilon)\)</span> is thenumber of boxes of side length <spanclass="math inline">\(\varepsilon\)</span> required to cover the set.Then the box-counting dimension is defined as <spanclass="math display">\[    \dim_{\text{box}}(S):=\lim_{\varepsilon\to 0}\frac{\logN(\varepsilon)}{\log 1 / \varepsilon}\]</span></p><p>If <span class="math inline">\(S\)</span> is a smooth space (amanifold) of integer dimension <span class="math inline">\(d\)</span>,then <span class="math inline">\(N(1 / n) \thickapprox Cn^{d}\)</span>,which corresponds with the normal definition of dimension.</p><p>When the above limit does not exist, upper and lower box-countingdimensions can be defined, and they are strongly related to theHausdorff dimension.</p><h2 id="some-definitions">some definitions</h2><p>First we need to clarify what is an attractor.</p><p><strong>Def 1</strong> Let <spanclass="math inline">\(\phi(t,x)\)</span> is a flow. A <strong>positivelyinvariant set</strong> <span class="math inline">\(A\)</span> from aflow <span class="math inline">\(\phi(t,x)\)</span> is a set such thatif <span class="math inline">\(\phi(t_0,x) \in A\)</span> for some <spanclass="math inline">\(t_0\)</span>, then <spanclass="math inline">\(\phi(t,x)\in A\)</span> for all <spanclass="math inline">\(t\geqslant t_0\)</span>.</p><p><strong>Def 2</strong> A <strong>stable set</strong> from acontinuous dynamical system of flow <spanclass="math inline">\(\phi(t,x)\)</span> is a set such that there existsa neighborhood <span class="math inline">\(B\)</span> of <spanclass="math inline">\(S\)</span> satisfying that if <spanclass="math inline">\(\phi(t_0,x)\in B\)</span>, then <spanclass="math inline">\(\phi(t,x)\in B\)</span> for all <spanclass="math inline">\(t\geqslant t_0\)</span>.</p><p>Futhermore, if there exists a neighborhood <spanclass="math inline">\(B\)</span> such that, for every neighborhood <spanclass="math inline">\(B&#39; \subset B\)</span>, if <spanclass="math inline">\(\phi(t_0,x) \in B\)</span>, then there exists<span class="math inline">\(t_1\geqslant t_0\)</span> such that <spanclass="math inline">\(\phi(t_0,x)\in B&#39;\)</span> for every <spanclass="math inline">\(t\geqslant t_1\)</span>, then <spanclass="math inline">\(S\)</span> is also an <strong>asymptoticallystable set</strong>.</p><p><strong>Def 3</strong> An <strong>attracting set</strong> of an ODEis a closed, positively invariant and asymptotically stable set. Anattractor of an ODE is an attracting set which contains a denseorbit.</p><p><strong>Def 4</strong> let <span class="math inline">\(M\)</span> bea manifold. The set of <span class="math inline">\(C^{r}\)</span>functions from <span class="math inline">\(M\)</span> to itself whichare also diffeomorphisms (have <spanclass="math inline">\(C^{r}\)</span> inverse) is called <spanclass="math inline">\(Diff^{r}(M)\)</span>.</p><h2 id="takens-embedding-theorem">Takens' embedding theorem</h2><p><strong>Theorem 1</strong> Let <span class="math inline">\(M\)</span>be a compact manifold of dimension <spanclass="math inline">\(m\)</span>. For pairs <spanclass="math inline">\((\phi, y)\)</span>, with <spanclass="math inline">\(\phi \in Diff^{2}(M)\)</span>, <spanclass="math inline">\(y \in C^{2}(M, \mathbb{R})\)</span>, it is ageneric property that the map <span class="math inline">\(\Phi_{(\phi,y)}\colon M \rightarrow \mathbb{R}^{2m+1}\)</span>, defined by <spanclass="math display">\[    \Phi_{(\phi, y)}(x) = (y(x), y(\phi(x)), \cdots , y(\phi^{2m}(x)))\]</span></p><p>is an embedding. Recall Whitney embedding theorem. This mean that<span class="math inline">\(\Phi_{(\phi,y)}(M)\)</span> is diffeomorphicto <span class="math inline">\(M\)</span> (in the sense of <spanclass="math inline">\(C^{2}\)</span>)</p><p>Here 'generic' means open and dense, and we use the <spanclass="math inline">\(C^{1}\)</span> topology. We refer to the functions<span class="math inline">\(y\in C^{2}(M,\mathbb{R})\)</span> as<strong>measurement functions</strong>.</p><p>Another version focusing on one particular <spanclass="math inline">\(\phi\)</span> is that:</p><p><strong>Theorem 2</strong> Let <span class="math inline">\(M\)</span>be as above. Let <span class="math inline">\(\phi \colon M\rightarrowM\)</span> be a diffeomorphism, with the properties: - the periodicpoints of <span class="math inline">\(\phi\)</span> with periods lessthan or equal to <span class="math inline">\(2m\)</span> are finite innumber; - if <span class="math inline">\(x\)</span> is any periodicpoint with period <span class="math inline">\(k\leqslant 2m\)</span>then the eigenvalues of the derivative of <spanclass="math inline">\(\phi^{k}\)</span> at <spanclass="math inline">\(x\)</span> are all distinct.</p><p>Then for generic <span class="math inline">\(y \in C^{2}(M,\mathbb{R})\)</span>, the map <spanclass="math inline">\(\Phi_{(\phi,y)}\colon M \rightarrow\mathbb{R}^{2m+1}\)</span>, defined as in Theorem 1, is anembedding.</p><p><strong>Remark</strong> When theorem 1 gives stronger results,theorem 2 gives a sufficient condition for the diffeomorphism <spanclass="math inline">\(\phi\)</span>, this may be informative inapplications. Though the expression and proof of Takens's theorem is allabout existence, we usually have alternatives for <spanclass="math inline">\(\phi\)</span> and <spanclass="math inline">\(y\)</span> in many set ups: if <spanclass="math inline">\(M\)</span> is the strange attractor of a dynamicalsystem, then <span class="math inline">\(\phi\)</span> is usually theflow with a certain time delay, and <spanclass="math inline">\(y\)</span> is the observable or measurement.<strong>However</strong>, we do not know if such <spanclass="math inline">\(\phi\)</span> and <spanclass="math inline">\(y\)</span> are appropriate.</p><h2 id="applications-of-takenss-theorem">Applications of Takens'stheorem</h2><p>I saw two papers<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="H. Ma, S. Leng, K. Aihara, W. Lin, and L. Chen, Randomly distributed embedding making short-term high-dimensional data predictable, Proc. Natl. Acad. Sci. U.S.A., vol. 115, no. 43, Oct. 2018, doi: 10.1073/pnas.1802987115.">[4]</span></a></sup><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="J.-W. Hou, H.-F. Ma, D. He, J. Sun, Q. Nie, and W. Lin, Harvesting random embedding for high-frequency change-point detection in temporal complex systems, National Science Review, vol. 9, no. 4, p. nwab228, May 2022, doi: 10.1093/nsr/nwab228.">[1]</span></a></sup>before I determined to write this post. A framework, randomlydistributed embedding (RDE) was proposed to reconstruct state space andpredict future states. Later, the same group developed anotherframework, temporal change-point detection (TCD), to detectchange-points, where RDE served as the very first step. They are notdirect application of Takens's theorem, but they are based on the ideaof embedding theorem.</p><h3 id="reconstruction-of-state-space-and-prediction">Reconstruction ofstate space and prediction</h3><p>The first idea roots in the theory of generalized embedding, whereone can reconstruct the original system by both delayed embedding andnon-delayed embedding.</p><p>Suppose we have a time series <spanclass="math inline">\(\{\bm{x}(t)\}\in \mathbb{R}^{n\times\mathbb{R}}\)</span>. <span class="math inline">\(x_s(t)\)</span> is oneof the interested observables of <spanclass="math inline">\(\bm{x}\)</span>. Then - A delayed embedding: <spanclass="math inline">\(\mathcal{N}=\{x_s(t),x_s(t+\tau),\cdots,x_s(t+(E-1)\tau)\}\)</span>. - A non-delayed embedding: <spanclass="math inline">\(\mathcal{M}=\{x_{k_1}(t),x_{k_2}(t),\cdots,x_{k_E}(t)\}\)</span>.</p><p>Here, theoretically, the embedding dimension <spanclass="math inline">\(E&gt;2d\)</span> is required and <spanclass="math inline">\(d\)</span> is the box-counting dimension of thesystem dynamics. The determination of <spanclass="math inline">\(E\)</span> in practice, however, is not easy.</p><p>In priciple, there exists a diffeomorphism <spanclass="math inline">\(\bm{\Psi}\colon \mathcal{M}\rightarrow\mathcal{N}\)</span>. Specifically, <span class="math display">\[    x_s(t+\tau)=\psi(x_{k_1}(t),x_{k_2}(t),\cdots ,x_{k_E}(t))\]</span></p><p>How to reconstruct the state space and do one-step prediction? Theprocess consists of several steps:</p><ol type="1"><li>Randomly select a tuple <spanclass="math inline">\(\bm{k}=(k_1,\cdots ,k_{E})\)</span></li><li>fit a predictor <span class="math inline">\(\psi{\bm{k}}\)</span> byminimizing <span class="math inline">\(\left\|x_s(t+\tau)-\psi(x_{k_1}(t),x_{k_2}(t),\cdots ,x_{k_E}(t)) \right\|_{},t=t_1,\cdots,t_q\)</span>. The authors used Gaussian Process Regression(GPR) to fit the predictor.</li><li>Make a one-step prediction as <spanclass="math inline">\(\hat{x}_s^{\bm{k}}(T+\tau)=\psi_{\bm{k}}(x_{k_1}(T),x_{k_2}(T),\cdots,x_{k_E}(T))\)</span>.</li><li>Repeat steps 1-3 by randomly picking up another <spanclass="math inline">\(r\)</span> tuples. Now we have <spanclass="math inline">\(r\)</span> predictions <spanclass="math inline">\(\{\hat{x}_s^{\bm{k}}(T+\tau)\}\)</span>.</li><li>Statistical analysis of <spanclass="math inline">\(\{\hat{x}_s^{\bm{k}}(T+\tau)\}\)</span>. Excludethe outliers by calculating the quartiles of the predictions.</li><li>Using kernel density estimation to estimate the probability densityfunction of the predictions, <spanclass="math inline">\(p(\hat{x}_s(T+\tau))\)</span>.</li><li>Set the final predictions by calculating the expectation of theabove density function <spanclass="math inline">\(\mathbb{E}_{p}(\hat{x}_s(T+\tau))\)</span>.</li></ol><h4id="using-false-nearest-neighbor-criterion-to-determine-embedding-dimension">Usingfalse nearest neighbor criterion to determine embedding dimension</h4><p>The purpose to choose a large enough <spanclass="math inline">\(E\)</span> is to eliminate all self-crossings ofthe attractor. <span class="math inline">\(E&gt;2d\)</span> is obviouslysufficient, but often we don't know the exact value of <spanclass="math inline">\(d\)</span> and <spanclass="math inline">\(E\leqslant 2d\)</span> is enough. The falsenearest neighbor criterion (FNN) was proposed by Kennel, Brown andAbarbanel<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="M. B. Kennel, R. Brown, and H. D. I. Abarbanel, Determining embedding dimension for phase-space reconstruction using a geometrical construction, Phys. Rev. A, vol. 45, no. 6, pp. 34033411, Mar. 1992, doi: 10.1103/PhysRevA.45.3403.">[3]</span></a></sup>to determine <span class="math inline">\(E\)</span> in a model-freeway.</p><p>Denote <spanclass="math inline">\(\bm{x}_s(t,E)=(x_s(t),x_s(t+\tau),\cdots,x_s(t+(E-1)\tau))\)</span>. For a fix <spanclass="math inline">\(t\)</span>, let <spanclass="math inline">\(\bm{x}_s(t^{(r)},E)\)</span> be the <spanclass="math inline">\(r\)</span>th nearest neighbor of <spanclass="math inline">\(\bm{x}_s(t,E)\)</span>. The square of theEuclidian distance between <spanclass="math inline">\(\bm{x}_s(t,E)\)</span> and <spanclass="math inline">\(\bm{x}_s(t^{(r)},E)\)</span> is</p><p><span class="math display">\[    R_{E}^{2}(t,r)=\sum_{k=0}^{E-1}[x_s(t+k\tau)-x_s(t^{(r)}+k\tau)]^{2}\]</span></p><p>The idea is if <spanclass="math inline">\(\bm{x}_s(t^{(r)},E)\)</span> is a false nearestneighbor, meaning it's a self-crossing due to projection, then <spanclass="math inline">\(R_{E+1}^{2}(t,r)-R_{E}^{2}(t,r)\)</span> is largewhen <span class="math inline">\(E\)</span> passed the critical value.It is defined as a false nearest neighbor if <spanclass="math display">\[    \left( \frac{R_{E+1}^{2}(t,r)-R_{E}^{2}(t,r)}{R_{E}^{2}(t,r)}\right)^{\frac{1}{2}}=\frac{\lvert x_s(t+E\tau)-x_s(t^{(r)}+E\tau)\rvert }{R_{E}(t,r)} &gt; R_{tol}\]</span></p><p>Another situation is the limited data set size. The authors usedanother criterion to avoid false nearest neighbors that are not close inthe original attractor. <span class="math display">\[    \frac{R_{E+1}(t)}{R_{A}}&gt;A_{tol}\]</span></p><p><span class="math inline">\(R_{A}\)</span> is a measure of the sizeof the attractor <span class="math display">\[    R_{A}^{2}=\frac{1}{N}\sum_{n=1}^{N} [x_s(t_n)-\bar{x}_s]^{2} \\    \bar{x}_s=\frac{1}{N}\sum_{n=1}^{N} x_s(t_n)\]</span></p><p>where <span class="math inline">\(t_n\)</span> are all time points ofthe time series. <span class="math inline">\(E\)</span> is chosen sothat there is no false nearest neighbor.</p><h4id="using-kernel-density-estimation-to-estimate-the-probability-density-function">Usingkernel density estimation to estimate the probability densityfunction</h4><p>The underlying probability density function is estimated as <spanclass="math display">\[    \hat{f}(x)=\sum_{i}^{} \alpha_i K(x-x_i)\]</span></p><p>where <span class="math inline">\(K\)</span> is a kernel, typically aGaussian, centered at the data points, <span class="math inline">\(x_i,i=1,\cdots ,n\)</span>, and <spanclass="math inline">\(\alpha_i\)</span> are weighting coefficients,typically uniform <spanclass="math inline">\(\alpha=\frac{1}{n}\)</span>. A good discussion ofkernel estimation techniques can befound<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="D. W. Scott, Multivariate Density Estimation: Theory, Practice, and Visualization, 1st ed. in Wiley Series in Probability and Statistics. Wiley, 1992. doi: 10.1002/9780470316849.">[7]</span></a></sup>.</p><h3 id="change-point-detection">Change-point detection</h3><p>TBC.</p><h2 id="reference">Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>J.-W. Hou, H.-F. Ma, D. He,J. Sun, Q. Nie, and W. Lin, Harvesting random embedding forhigh-frequency change-point detection in temporal complex systems,National Science Review, vol. 9, no. 4, p. nwab228, May 2022, doi:10.1093/nsr/nwab228.<a href="#fnref:1" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:2" class="footnote-text"><span>J. P. Huke, EmbeddingNonlinear Dynamical Systems: A Guide to Takens Theorem. Mar. 2006.[Online]. Available: https://api.semanticscholar.org/CorpusID:55183186<a href="#fnref:2" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:3" class="footnote-text"><span>M. B. Kennel, R. Brown, andH. D. I. Abarbanel, Determining embedding dimension for phase-spacereconstruction using a geometrical construction, Phys. Rev. A, vol. 45,no. 6, pp. 34033411, Mar. 1992, doi: 10.1103/PhysRevA.45.3403.<a href="#fnref:3" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:4" class="footnote-text"><span>H. Ma, S. Leng, K. Aihara,W. Lin, and L. Chen, Randomly distributed embedding making short-termhigh-dimensional data predictable, Proc. Natl. Acad. Sci. U.S.A., vol.115, no. 43, Oct. 2018, doi: 10.1073/pnas.1802987115.<a href="#fnref:4" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:5" class="footnote-text"><span>J. Penalva Vadell, Takensstheorem proof and applications.pdf. 2018.<a href="#fnref:5" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:6" class="footnote-text"><span>T. Sauer, J. A. Yorke, andM. Casdagli, Embedology, J Stat Phys, vol. 65, no. 34, pp. 579616,Nov. 1991, doi: 10.1007/BF01053745.<a href="#fnref:6" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:7" class="footnote-text"><span>D. W. Scott, MultivariateDensity Estimation: Theory, Practice, and Visualization, 1st ed. inWiley Series in Probability and Statistics. Wiley, 1992. doi:10.1002/9780470316849.<a href="#fnref:7" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:8" class="footnote-text"><span>F. Takens, Detectingstrange attractors in turbulence, in Dynamical Systems and Turbulence,Warwick 1980, vol. 898, D. Rand and L.-S. Young, Eds., in Lecture Notesin Mathematics, vol. 898. , Berlin, Heidelberg: Springer BerlinHeidelberg, 1981, pp. 366381. doi: 10.1007/BFb0091924.<a href="#fnref:8" rev="footnote" class="footnote-backref"></a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>dynamical system</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Network Effects in Neurostimulation (1)</title>
    <link href="/2023/09/30/Network-Effects-in-Neurostimulation-1/"/>
    <url>/2023/09/30/Network-Effects-in-Neurostimulation-1/</url>
    
    <content type="html"><![CDATA[<h1 id="network-effects-in-neurostimulation-1">Network Effects inNeurostimulation (1)</h1><!--Large scale models are used to characterize the dynamics of large populations of neurons, particularly across different brain regions. In that sense, all neurons in each brain region are summarized as one single neuron population. These models are often comprised of a local circuit system and interactions between different neuron populations, with stochasticity coming from noise.--><p>In our project about network effects of neurostimulation, we careabout indirect effects of stimulation on the brain. That's reasonablesince - there is significant difference between SC and EC. - Clinically,N2 component of cortico-cortical evoked potentials (CCEPs) isobserved<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Lemarchal, Jean-Didier et al. (2022) A brain atlas of axonal and synaptic delays based on modelling of cortico-cortical evoked potentials. Brain.">[6]</span></a></sup></p><blockquote><p>... In addition, we did not primarily consider indirectcortico-subcortico-cortical pathways that would rather be implicated inthe later N2 component ...<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Lemarchal, Jean-Didier et al. (2022) A brain atlas of axonal and synaptic delays based on modelling of cortico-cortical evoked potentials. Brain.">[6]</span></a></sup></p></blockquote><p>We hope to build a large scale model of cortex, which shows similar2-hop network effectis with the real data when applying a virtualstimulus. Furthermore, the model is expected to be used to develop aclose-loop network control method.</p><h2 id="decos-first-model-shows-poor-propagation-ratio">Deco's firstmodel shows poor propagation ratio</h2><p>The model proposed byDeco<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Gustavo Deco, Adrin Ponce-Alvarez et al. (2013) Resting-State Functional Connectivity Emerges from structurally and Dynamically Shaped Slow Linear Flunctuations. The Journal of Neuroscience.">[3]</span></a></sup>used a dynamical mean field approach to reduce thecomplexity<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Kong-Fatt Wong, Xiao-Jing Wang. (2006) A Recurrent Network Mechanism of Time Integration in Perceptual Decisions. The Journal of Neuroscience.">[5]</span></a></sup>.The model is described by the following equations:</p><p><span class="math display">\[    \frac{\mathrm{d}S_{i}(t)}{\mathrm{d}t}  = -\frac{S_{i}}{\tau_{S}}+(1-S_i)\gamma H(x_i) +\sigma \nu_i(t) \tag{1}\]</span></p><p><span class="math display">\[    H(x_i) = \frac{ax_{i}-b}{1-\exp (-d(ax_{i}-b))} \tag{2}\]</span></p><p><span class="math display">\[    x_i = wJ_{N}S_i + GJ_{N}\sum_{j}^{} C_{ij}S_{j} + I_0. \tag{3}\]</span></p><p>Here <span class="math inline">\(H(x_i)\)</span> and <spanclass="math inline">\(S_{i}\)</span> denote the population firing rateand the average synaptic gating variable at the loccal cortical area<span class="math inline">\(i\)</span>, hence <spanclass="math inline">\(S_{i} \in [0,1]\)</span>.</p><p>Parameters in (1) - (3) are strongly correlated with dynamics andbifurcation diagram of the model. Heterogeneity across cortices has beenconsidered byKong<sup id="fnref:11" class="footnote-ref"><a href="#fn:11" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Xiaolu Kong, Ru Kong et al. (2021) Sensory-motor cortices shape functional connectivity dynamics in the human brain. nature communications.">[11]</span></a></sup>,who developed a spatially heterogeneous Deco model. Their aim is tominimize the difference between empirical and simulated FC as well assharp transitions in FC patterns, which characterized by FCD.</p><h3 id="hard-sigmoidal-function-issue">Hard sigmoidal functionissue</h3><p>When doing a simulation of (1) - (3), various numerical methods canbe used. We set <span class="math inline">\(\Delta t = 1 / 2000s\)</span>, which is small enough to avoid stability issue. Hence, wedon't worry about using EulerMaruyama method:</p><p><span class="math display">\[    S_i(t+\Delta t) \thickapprox S_i(t) + \left(-\frac{S_i(t)}{\tau_{S}}+(1-S_i(t))\gamma H(x_i) \right) \Delta t +\sigma \xi \sqrt{\Delta t}, \tag{4}\]</span></p><p>where <span class="math inline">\(\xi \sim\mathcal{N}(0,1)\)</span>.</p><p>The RHS of (4) can goes negative or larger than 1 regardless of anynumerical method (EulerMaruyama, Runge-Kutta, Exponential-Euler, etc).That is unreasonable, so we applied a hard sigmoidal function: <spanclass="math display">\[    S_i(t+\Delta t) = \min (\max (\text{RHS of (4)}, 0), 1).\tag{5}\]</span></p><p>However, whether to apply the hard sigmoidal function has asignificant impact on dynamics.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/large_scale_model/fc_s_no_sigmoidal.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/large_scale_model/fc_s_sigmoidal.png" /></div></div></div><p>We can see that the network is in a bistable state, one with lowactivity and the other with relatively high activity. The network makestransitions between these two states.</p><p>Multistability is widespread in such nonlinear dynamical systems, oneof which impressed me most is bistable states in low-rankRNN<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Adrian Valente, Jonanthan W. Pillow. (2022) Extracting computational mechanisms from neural data using low-rank RNNs. NeurIPS.">[1]</span></a></sup><sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Francesca Mastrogiuseppe, Srdjan Ostojic. (2018) Linking Connectivity, Dynamics, and Computations in Low-Rank Recurrent Neural Networks. Neuron.">[2]</span></a></sup><sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Manuel Beiran, Alexis Dubreuil et al. (2021) Shaping Dynamics With Multiple Populations in Low-Rank Recurrent Networks. Neural Computation.">[8]</span></a></sup>.If stochasticity is deleted, <span class="math inline">\(I_0=0, \gamma =0.641, a = 270, b=108, d=0.154\)</span> as in Deco's paper, RHS of (3)has the value <span class="math inline">\(6.46 \times 10^{-6}\)</span>when all <span class="math inline">\(S_i=0\)</span>, which isnegligible. Given that <spanclass="math inline">\(S_i(t)/\tau_{S}\)</span> is linear, <spanclass="math inline">\((1-S_i(t))\gamma H(x_i)\)</span> is sub-linearnear <span class="math inline">\(S_i=0\)</span>, there indeed exists alow activity stable state.</p><p>The real problem is that the high activity state does not correspondto reality since <span class="math inline">\(S_i\)</span> or <spanclass="math inline">\(H(x_i)\)</span> is too large for a resting-stateneural network.</p><p>Whether to apply the hard sigmoidal function is not decisive in theemergence of transition between two stable states. We found anotherparameter set that shows such transitions with applying the hardsigmoidal function.</p><div class="note note-info">            <p>It is mathematically interesting to investigate why adding a hardsigmoidal function reduces the likelihood that the network will switchbetween two stable states.</p>          </div><h2 id="low-propagation-ratio-in-decos-model">Low propagation ratio inDeco's model</h2><p>We choose to apply the hard sigmoidal function and trained parameters(<span class="math inline">\(w, \sigma, I_{0}, G\)</span>) based onKong'scode<sup id="fnref:11" class="footnote-ref"><a href="#fn:11" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Xiaolu Kong, Ru Kong et al. (2021) Sensory-motor cortices shape functional connectivity dynamics in the human brain. nature communications.">[11]</span></a></sup>to suit empirical FC and FCD. The result shows poor correspondencebetween simulated stimulus time series and real ones, and we think thereason is that the model has a low propagation ratio.</p><h3 id="toy-model-test">toy model test</h3><p>To illustrate the propagation ratio, we use a toy model with onlythree nodes: A-B-C. The connection for A-B and B-C are the same, whilethere is no connection between A and C. The structural connectivitymatrix is normalized to have spectral radius 0.999. Moreover, noises aredeleted, <span class="math inline">\(I_0\)</span> is set to <spanclass="math inline">\(0\)</span> to indicate the propagation moreclearly.</p><p>After the 3-node model becomes stable (denoted as time 0), a stimulusis applied to node A and we observe its propagation along the path. ACpropagation ratio is defined as <span class="math display">\[    \text{AC propagation ratio} =\frac{\mathbb{E}(S_C(t),t&gt;0)-\mathbb{E}(S_{C}(t),t&lt;0)}{\mathbb{E}(S_{A}(t), t&gt;0)-\mathbb{E}(S_{A}(t),t&lt;0)}.\]</span></p><p><span class="math inline">\(G=0.3, w=0.9\)</span> as similar with ourtrained results:</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/large_scale_model/prop_ratio_1_1.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/large_scale_model/prop_ratio_1_2.png" /></div></div></div><p><span class="math inline">\(G=5.0, w=-2.0\)</span> which is notrealistic:</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/large_scale_model/prop_ratio_2_1.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/large_scale_model/prop_ratio_2_2.png" /></div></div></div><p>A phase diagram is available:</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/large_scale_model/prop_ratio_3_1.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/large_scale_model/prop_ratio_3_2.png" /></div></div></div><p>Here, End-up value is the mean of <span class="math inline">\(S_A,S_B, S_C\)</span> in the end, so a clear phase transition can be seen.The propagation ratio is high only when <spanclass="math inline">\(G\)</span> is large and <spanclass="math inline">\(w&lt;0\)</span>.</p><h2 id="decos-second-model-shows-better-propagation-ratio">Deco's secondmodel shows better propagation ratio</h2><p>Deco proposed anothermodel<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Gustavo Deco, Adrin Ponce-Alvarez et al. (2014) How Local Excitation-Inhibition Ratio Impacts the Whole Brain Dynamics. The Journal of Neuroscience.">[4]</span></a></sup>similar to the first one, but with excitatory population and inhibitorypopulation. Heterogeneous version of the model is implemented byDemirta etal.<sup id="fnref:9" class="footnote-ref"><a href="#fn:9" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Murat Demirta, Joshua B. Burt et al. (2019) Hierarchical Heterogeneity across Human Cortex Shapes Large-Scale Neural Dynamics. Neuron.">[9]</span></a></sup>The model is described by the following equations:</p><p><span class="math display">\[    \frac{\mathrm{d}S_i^{E}(t)}{\mathrm{d}t} =-\frac{S_i^{E}(t)}{\tau_{E}}+(1-S_i^{E}(t))\gamma H^{E}(x_i^{E}(t))+\sigma \nu_i(t) \tag{6}\]</span></p><p><span class="math display">\[    \frac{\mathrm{d}S_i^{I}(t)}{\mathrm{d}t} =-\frac{S_i^{I}(t)}{\tau_{I}}+H^{I}(x_i^{I}(t)) +\sigma \nu_i(t) \tag{7}\]</span></p><p><span class="math display">\[    x_i^{E}(t) = w^{E}I_0 + w^{EE}S_i^{E}(t)+gJ_{N} \sum_{j}^{}C_{ij}S_j^{E}(t)-w^{IE}S_i^{I}(t) \tag{8}\]</span></p><p><span class="math display">\[  x_i^{I}(t) = w^{I}I_0 + w^{EI}S_i^{E}(t) - S_i^{I}(t) \tag{9}\]</span></p><p>and <span class="math inline">\(H^{E}(x_i^{E}),H^{I}(x_i^{I})\)</span> is the same as (2) with different <spanclass="math inline">\(a,b,d\)</span>. Here, we can regard <spanclass="math inline">\(w^{II}=1\)</span>.</p><p>Deco's model with E-I populations shows boost in propagation ratio inthe toy model.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/large_scale_model/prop_ratio_4_1.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/large_scale_model/prop_ratio_4_2.png" /></div></div></div><p>Phase diagrams are as follows (<span class="math inline">\(G\)</span>versus <span class="math inline">\(w^{EI}\)</span> as an example, otherkey parameters: <span class="math inline">\(w^{EE} = 5, w^{IE} =1\)</span>), where propagation ratio of high activity endings isdeleted:</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/large_scale_model/prop_ratio_5_1.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/large_scale_model/prop_ratio_5_2.png" /></div></div></div><p>Though this model exhibits better propagation ratio, the problem isdifficulty in model fitting. We are also trying other models.</p><h3id="a-possible-approach-to-simplify-decos-model-with-e-i-populations">Apossible approach to simplify Deco's model with E-I populations</h3><p>Demirta et al. use parameters as follows:</p><table><thead><tr class="header"><th></th><th>Excitatory Populations</th><th>Inhibitory Populations</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(I_{0}\)</span></td><td>0.382 nA</td><td>-</td></tr><tr class="even"><td><span class="math inline">\(J\)</span></td><td>0.15 nA</td><td>-</td></tr><tr class="odd"><td><span class="math inline">\(\gamma\)</span></td><td>0.641</td><td>-</td></tr><tr class="even"><td><span class="math inline">\(w^{E}\)</span></td><td>1.0</td><td>-</td></tr><tr class="odd"><td><span class="math inline">\(\tau_{E}\)</span></td><td>0.1 s</td><td>-</td></tr><tr class="even"><td><span class="math inline">\(a_{E}\)</span></td><td>310 nC<span class="math inline">\(^{-1}\)</span></td><td>-</td></tr><tr class="odd"><td><span class="math inline">\(b_{E}\)</span></td><td>125 Hz</td><td>-</td></tr><tr class="even"><td><span class="math inline">\(d_{E}\)</span></td><td>0.16 s</td><td>-</td></tr><tr class="odd"><td><span class="math inline">\(w^{I}\)</span></td><td>-</td><td>0.7</td></tr><tr class="even"><td><span class="math inline">\(\tau_{I}\)</span></td><td>-</td><td>0.01 s</td></tr><tr class="odd"><td><span class="math inline">\(a_{I}\)</span></td><td>-</td><td>615 nC<span class="math inline">\(^{-1}\)</span></td></tr><tr class="even"><td><span class="math inline">\(b_{I}\)</span></td><td>-</td><td>177 Hz</td></tr><tr class="odd"><td><span class="math inline">\(d_{I}\)</span></td><td>-</td><td>0.087 s</td></tr></tbody></table><p>Not so strictly, <span class="math inline">\(\tau_{I} \ll\tau_{E}\)</span>, so we can use adiabatic approximation for <spanclass="math inline">\(S_i^{I}(t)\)</span>, i.e., <spanclass="math inline">\(S_i^{I}(t) = \tau_{I}H(x_i^{I}(t))\)</span>. Withall assumptions aforementioned, (6)-(8) become:</p><p><span class="math display">\[    \frac{\mathrm{d}S_i^{E}(t)}{\mathrm{d}t} =-\frac{S_i^{E}(t)}{\tau_{E}}+(1-S_i^{E}(t))\gamma H^{E}(x_i^{E}(t))\tag{10}\]</span></p><p><span class="math display">\[  x_i^{I}(t) = w^{EI}S_i^{E}(t) - \tau_{I}H^{I}(x_i^{I}(t)) \tag{11}\]</span></p><p><span class="math display">\[    \begin{aligned}          x_i^{E}(t) &amp;= w^{EE}S_i^{E}(t)+gJ_{N} \sum_{j}^{}C_{ij}S_j^{E}(t)-w^{IE}\tau_{I}H^{I}(x_i^{I}(t)) \\          &amp;= (w^{EE} - w^{IE}w^{EI})S_i^{E}(t) + gJ_{N}\sum_{j}^{}C_{ij}S_{j}^{E}(t) + w^{IE}x_{i}^{I}(t)    \end{aligned} \tag{12}\]</span></p><p>Consider the function: <span class="math display">\[    g(x) = x + \tau_{I}\frac{a_{I}x - b_{I}}{1-\exp(-d_{I}(a_{I}x-b_{I}))} = x + 0.01 \frac{615x-177}{1-\exp(-0.087(615x-177))} \tag{13}\]</span></p><p><img src="/img/large_scale_model/activation.png" /></p><p>Apart from <span class="math inline">\(x \in (0.2, 0.32)\)</span>,<span class="math inline">\(g(x)\)</span> can be well approximated by<span class="math display">\[    \tilde{g}(x) =    \begin{cases}        x, \quad x &lt; 177 / 615 \\        x + 0.01 (615x-177), \quad x\geqslant 177 / 615    \end{cases}\]</span></p><p>Hence, (11) has an approximate solution:</p><p><span class="math display">\[    \begin{cases}        x_i^{I}(t) = w^{EI}S_i^{E}(t), \quadw^{EI}S_{i}^{E}(t)&lt;177/615 \thickapprox 0.288 \\        x_i^{I}(t) = 0.140(w^{EI}S_{i}^{E}(t) + 1.77), \quadw^{EI}S_{i}^{E}(t)\geqslant 0.288    \end{cases}\]</span></p><p>which corresponds to a non-continuous vector field in (12). So amodified Deco model without E-I populations is derived. We only need tocheck <span class="math inline">\(S_i^{E}(t)\)</span> every time stepsand update <span class="math inline">\(x_i^{E}(t)\)</span>accordingly.</p><p>Adding noise and <span class="math inline">\(I_0\)</span> back, themodel can be described by the following equations: <spanclass="math display">\[    \frac{\mathrm{d}S_i(t)}{\mathrm{d}t} = - \frac{S_i(t)}{\tau_{S}} +(1-S_i(t))\gamma H(x_i(t)) + \sigma \nu_i(t) \tag{14}\]</span></p><p><span class="math display">\[  H(x_i(t)) = \frac{a_{E}x - b_{E}}{1-\exp (-d_{E}(a_{E}x - b_{E}))}\tag{15}\]</span></p><p><span class="math display">\[  x_i(t) = w_0 I_0 + w_1 S_i(t) + gJ_{N}\sum_{j}^{} C_{ij}S_{j}(t) + w_2y_i(t) \tag{16}\]</span></p><p><span class="math display">\[  y_i(t) = \min(w_3S_i(t) + w_4I_0, (w_3S_{i}(t) + w_4I_0 + 1.77) /7.15) \tag{17}\]</span></p><p>The modified Deco Model shows good propagation ratio (<spanclass="math inline">\(w_0 = 1, w_1 = -3, w_2 = 1, w_3 = 7, w_4 = 0.7, G= 6\)</span>):</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/large_scale_model/prop_ratio_6_1.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/large_scale_model/prop_ratio_6_2.png" /></div></div></div><p>Phase diagrams are as follows (<span class="math inline">\(G\)</span>versus <span class="math inline">\(w_1\)</span> as an example, other keyparameters: <span class="math inline">\(w_0 = 1, w_2 = 1, w_3 = 7, w_4 =0.7\)</span>):</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/large_scale_model/prop_ratio_7_1.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/large_scale_model/prop_ratio_7_2.png" /></div></div></div><p>To make unit consistent, <span class="math inline">\(w_1\)</span> canbe considered as <spanclass="math inline">\(w^{EE}w^{II}-w^{EI}w^{IE}\)</span>. Further stepsinclude specification of parameters. <span class="math inline">\(w_0 =w^{E}\)</span> and <span class="math inline">\(w_1 = w^{I}\)</span>represent strength of background input, so they are negligible while<span class="math inline">\(I_0 = 0\)</span>. <spanclass="math inline">\(w_1 = w^{EE}w^{II} - w^{EI}w^{IE}, w_2=w^{IE},w_3=w^{EI}, G\)</span> are trainable parameters. Among them <spanclass="math inline">\(w_1\)</span> and <spanclass="math inline">\(G\)</span> is the key.</p><p>More experiments need to be done to verify the effectiveness of thisapproach.</p><h3 id="a-resonable-explanation-for-negative-w">A resonable explanationfor negative w</h3><p>In (3), negative <span class="math inline">\(w\)</span> seems notrealistic. However, in (12), <span class="math inline">\(w^{EE}w^{II} -w^{IE}w^{EI}\)</span> is reasonable to be negative, so we can regardnegative <span class="math inline">\(w\)</span> in (3) as a result ofE-I balance. That also corresponds with phase diagrams above: when <spanclass="math inline">\(w\)</span> is negative and <spanclass="math inline">\(G\)</span> is large to compensate for <spanclass="math inline">\(w\)</span>, the network ends up in a reasonablelow activity state and shows high propagation ratio.</p><h2 id="other-models">Other models</h2><p>We also apply other models to our project, including vanilla RNN,Joglekar'smodel<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Madhura R. Joglekar, Jorge F. Mejias et al. (2018) Inter-areal Balanced Amplification Enhances Signal Propagation in a Large-Scale Circuit Model of the Primate Cortex. Neuron.">[7]</span></a></sup>,Chaudhuri'smodel<sup id="fnref:10" class="footnote-ref"><a href="#fn:10" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Rishidev Chaudhuri, Kenneth Knoblauch et al. (2015) A Large-Scalue Circuit Mechanism for Hierarchical Dynamical Processing in the Primate Cortex. Neuron.">[10]</span></a></sup>.Further experiments are expected.</p><h2 id="reference">Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Adrian Valente, Jonanthan W.Pillow. (2022) Extracting computational mechanisms from neural datausing low-rank RNNs. NeurIPS.<a href="#fnref:1" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Francesca Mastrogiuseppe,Srdjan Ostojic. (2018) Linking Connectivity, Dynamics, and Computationsin Low-Rank Recurrent Neural Networks. Neuron.<a href="#fnref:2" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Gustavo Deco, AdrinPonce-Alvarez et al. (2013) Resting-State Functional ConnectivityEmerges from structurally and Dynamically Shaped Slow LinearFlunctuations. The Journal of Neuroscience.<a href="#fnref:3" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Gustavo Deco, AdrinPonce-Alvarez et al. (2014) How Local Excitation-Inhibition RatioImpacts the Whole Brain Dynamics. The Journal of Neuroscience.<a href="#fnref:4" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Kong-Fatt Wong, Xiao-JingWang. (2006) A Recurrent Network Mechanism of Time Integration inPerceptual Decisions. The Journal of Neuroscience.<a href="#fnref:5" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Lemarchal, Jean-Didier etal. (2022) A brain atlas of axonal and synaptic delays based onmodelling of cortico-cortical evoked potentials. Brain.<a href="#fnref:6" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:7" class="footnote-text"><span>Madhura R. Joglekar, JorgeF. Mejias et al. (2018) Inter-areal Balanced Amplification EnhancesSignal Propagation in a Large-Scale Circuit Model of the Primate Cortex.Neuron. <a href="#fnref:7" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:8" class="footnote-text"><span>Manuel Beiran, AlexisDubreuil et al. (2021) Shaping Dynamics With Multiple Populations inLow-Rank Recurrent Networks. Neural Computation.<a href="#fnref:8" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:9" class="footnote-text"><span>Murat Demirta, Joshua B.Burt et al. (2019) Hierarchical Heterogeneity across Human Cortex ShapesLarge-Scale Neural Dynamics. Neuron.<a href="#fnref:9" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:10" class="footnote-text"><span>Rishidev Chaudhuri, KennethKnoblauch et al. (2015) A Large-Scalue Circuit Mechanism forHierarchical Dynamical Processing in the Primate Cortex. Neuron.<a href="#fnref:10" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:11" class="footnote-text"><span>Xiaolu Kong, Ru Kong et al.(2021) Sensory-motor cortices shape functional connectivity dynamics inthe human brain. nature communications.<a href="#fnref:11" rev="footnote" class="footnote-backref"></a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Neuronal Dynamics</category>
      
      <category>Neurostimulation</category>
      
      <category>CCEP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Neuroscience</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2023/06/21/%E5%B9%B3%E5%9D%87%E5%9C%BA%E6%9C%97%E4%B9%8B%E4%B8%87%E5%8A%A8%E5%8A%9B%E5%AD%A6%E7%9A%84%E6%95%B0%E5%80%BC%E6%A8%A1%E6%8B%9F/"/>
    <url>/2023/06/21/%E5%B9%B3%E5%9D%87%E5%9C%BA%E6%9C%97%E4%B9%8B%E4%B8%87%E5%8A%A8%E5%8A%9B%E5%AD%A6%E7%9A%84%E6%95%B0%E5%80%BC%E6%A8%A1%E6%8B%9F/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><p><strong></strong>MFLD. MFLD.. Fokker-Planck.</p><h2 id="-fokker-planck-"> Fokker-Planck</h2><p><sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="E, W., Li, T. \&amp; Vanden-Eijinden, E. 2019. Applied Stochastic Analysis. RI: AMS.">[2]</span></a></sup><sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Lelivre, T. \&amp; Stoltz, G. (2016) Partial differential equations and stochastic methods in molecular dynamics. Acta Numerica. Cambridge University Press, 25, pp. 681880.">[4]</span></a></sup></p><h3 id=""></h3><p> <span class="math display">\[    \dot{X}_t=b(X_t,t)+\sigma(X_t,t)\dot{W}_t,\tag{1}\]</span>  <span class="math inline">\(\dot{W}_t\)</span>. <span class="math inline">\(m(t)=0\)</span> <span class="math inline">\(K(s,t)=\delta(t-s)\)</span>.</p><p>1 <span class="math display">\[    \mathrm{d}X_t=b(X_t,t)\mathrm{d}t+\sigma(X_t,t)\mathrm{d}W_t.\tag{2}\]</span> 2 <span class="math display">\[    X_t=X_0+\int_{0}^{t} b(X_s,s) \mathrm{d}s+\int_{0}^{t} \sigma(X_s,s)\mathrm{d}W_s. \tag{3}\]</span> 3 <spanclass="math display">\[    \int_{0}^{t} \sigma(X_s,s) \mathrm{d}W_s=\lim_{\lvert \Delta\rvert  \to 0}\sum_{j}^{} \sigma(X_{t_j},t_j)\left(W_{t_{j+1}}-W_{t_{j}} \right)\]</span>.</p><p><strong></strong>  <spanclass="math inline">\(\bm{X}_t\)</span>  <spanclass="math inline">\(\mathrm{d}\bm{X}_t=\bm{b}(t)\mathrm{d}t+\bm{\sigma}(t)\mathrm{d}\bm{W}_t\)</span><span class="math inline">\(\bm{X}_t\in \mathbb{R}^{n}\)</span><spanclass="math inline">\(\bm{\sigma}\in \mathbb{R}^{n\timesm}\)</span><span class="math inline">\(\bm{W}\in\mathbb{R}^{m}\)</span>  <span class="math inline">\(m\)</span>.  <spanclass="math inline">\(Y_t=f(\bm{X}_t)\)</span> <spanclass="math inline">\(f\in C^{\infty}(\mathbb{R}^{d})\)</span>.  <spanclass="math display">\[    \mathrm{d}Y_t=\left( \bm{b}\cdot \nablaf+\frac{1}{2}\bm{\sigma\sigma}^{\mathsf{T}}\colon \nabla ^{2}f \right)\mathrm{d}t+\nabla f\cdot \bm{\sigma}\cdot \mathrm{d}\bm{W}_t.\tag{4}\]</span></p><h3id=""></h3><p> <span class="math inline">\(\mathbb{R}^{d}\)</span> <span class="math display">\[    \mathrm{d}\bm{X}_t=\bm{b}(\bm{X}_t)\mathrm{d}t+\bm{\sigma}(\bm{X}_t)\mathrm{d}\bm{W}_t,\tag{5}\]</span>  <span class="math inline">\(\bm{X}_0\in\mathbb{R}^{d}\)</span>. <span class="math inline">\(\bm{W}_t\in\mathbb{R}^{m}\)</span> <spanclass="math inline">\(\bm{b}\colon \mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\)</span> <span class="math inline">\(\bm{\sigma}\colon\mathbb{R}^{d} \rightarrow \mathbb{R}^{d\times m}\)</span>5.5 <span class="math display">\[    \mathcal{L}=\bm{b}\cdot \nabla +\frac{1}{2}\bm{\sigma\sigma}^{\mathsf{T}}\colon \nabla ^{2},\]</span> <span class="math inline">\(\mathcal{L}\)</span> <span class="math inline">\((\bm{X}_t)_{t\geqslant 0}\)</span>.  <span class="math inline">\(\colon\)</span> Frobenius  <spanclass="math inline">\(C^{\infty}\)</span>  <spanclass="math inline">\(\varphi\colon \mathbb{R}^{d}\rightarrow\mathbb{R}\)</span> <span class="math display">\[    \mathcal{L}\varphi=\sum_{i=1}^{d} b_i\partial_{x_i}\varphi+\frac{1}{2}\sum_{i=1}^{d}\sum_{j=1}^{d}\sum_{k=1}^{m} \sigma_{i,k}\sigma_{j,k}\partial_{x_i,x_j}\varphi.\]</span> 56</p><p><strong></strong>  <span class="math inline">\(\varphi\inC^{\infty}_{c}(\mathbb{R}^{d})\)</span>.  <spanclass="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}[\mathbb{E}_{\bm{W}}(\varphi(\bm{X}_t))]\bigg|_{t=0}=\mathcal{L}\varphi(\bm{X})\bigg|_{\bm{X}=\bm{X}_0}.\tag{7}\]</span></p><p><strong></strong>  <span class="math display">\[    \mathrm{d}\varphi(\bm{X}_t)=\mathcal{L}\varphi(\bm{X}_t)\mathrm{d}t+(\nabla\varphi(\bm{X}_t))^{\mathsf{T}}\sigma(\bm{X}_t)\mathrm{d}\bm{W}_t,\]</span>  <span class="math inline">\(\varphi\inC_{c}^{\infty}(\mathbb{R}^{d})\)</span> <spanclass="math display">\[    \mathbb{E}_{\bm{W}}\left( \int_{0}^{t} (\nabla\varphi(\bm{X}_s))^{\mathsf{T}}\bm{\sigma}(\bm{X}_s) \mathrm{d}\bm{W}_s\right) =0\]</span> .  <spanclass="math display">\[    \mathbb{E}_{\bm{W}}[\varphi(\bm{X}_t)]-\varphi(\bm{X}_0)=\int_{0}^{t}\mathbb{E}_{\bm{W}}[(\mathcal{L}\varphi)(\bm{X}_s) ]\mathrm{d}s, \tag{8}\]</span>  <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}(\mathbb{E}_{\bm{W}}[\varphi(\bm{X}_t)])\bigg|_{t=0}=\lim_{t\to0}\frac{\mathbb{E}_{\bm{W}}[\varphi(\bm{X}_t)]-\varphi(\bm{X}_0)}{t}=\mathcal{L}\varphi(\bm{X})\bigg|_{\bm{X}=\bm{X}_0}.\]</span></p><p>5SDE<span class="math inline">\(\mathcal{L}\)</span> <spanclass="math inline">\(\mathcal{L}\)</span> .5 <spanclass="math inline">\(x_t\)</span> .5. <span class="math inline">\(t=0\)</span>  <spanclass="math inline">\(x\)</span>  <spanclass="math inline">\(p(0,x)\)</span> <spanclass="math inline">\(p\)</span> 5 Fokker-Planck <span class="math inline">\(\mathcal{L}\)</span>.</p><p>5 <spanclass="math inline">\(\mathcal{L}\)</span>  <spanclass="math inline">\(\pi\)</span> 5<span class="math inline">\(\varphi\inC_{c}^{\infty}(\mathbb{R}^{d})\)</span> <spanclass="math display">\[    \int_{}^{} \mathcal{L}\varphi \mathrm{d}\pi=0. \tag{9}\]</span> 7 <span class="math inline">\(\pi\)</span> <span class="math inline">\(x_0\sim \pi\)</span>  <spanclass="math inline">\(\mathbb{E}(\varphi(x_t))=\mathbb{E}(\varphi(x_{0}))\)</span>.</p><h3 id="fokker-planck-">Fokker-Planck </h3><p>4 <spanclass="math inline">\(\mathbb{R}^{d}\)</span>  <spanclass="math inline">\((\bm{X}_t)_{t\geqslant 0}\)</span>.  <spanclass="math inline">\(t\)</span>  <spanclass="math inline">\(\bm{X}_t\)</span>  <spanclass="math inline">\(\psi(t,\bm{X})\)</span><spanclass="math inline">\(\psi(0,\bm{X})=\psi_{0}(\bm{X})\)</span>.</p><p><strong></strong>  <spanclass="math inline">\(\bm{X}_t\)</span>.  <spanclass="math inline">\(\bm{X}_t\)</span> Fokker-Planck .</p><p><span class="math inline">\(\psi\)</span> Fokker-Planck  Kolmogorov  <spanclass="math display">\[    \partial_{t}\psi=\mathcal{L}^{\dag}\psi, \quad \psi(0)=\psi_0,\tag{10}\]</span>  <span class="math inline">\(\mathcal{L}^{\dag}\)</span> <span class="math inline">\(\mathcal{L}\)</span>  <spanclass="math inline">\(L^{2}\)</span>  <spanclass="math display">\[    \mathcal{L}^{\dag}=-\operatorname{div}(\bm{b}\\cdot)+\frac{1}{2}\nabla ^{2}\colon(\bm{\sigma\sigma}^{\mathsf{T}}\cdot).\]</span>  <span class="math inline">\(\varphi\inC^{\infty}(\mathbb{R}^{d})\)</span> <span class="math display">\[    \mathcal{L}^{\dag}\psi=-\sum_{i=1}^{d}\partial_{x_i}(b_i\psi)+\frac{1}{2}\sum_{i,j=1}^{d} \sum_{k=1}^{m}\partial_{x_i,x_j}(\sigma_{i,k}\sigma_{j,k}\psi).\]</span> <span class="math inline">\(L^{2}\)</span> <span class="math inline">\(f,g \inC^{\infty}_{c}(\mathbb{R}^{d})\)</span> <spanclass="math display">\[    \int_{\mathbb{R}^{d}}^{} \mathcal{L}f(x)g(x)\mathrm{d}x=\int_{\mathbb{R}^{d}}^{} f(x)\mathcal{L}^{\dag}g(x)\mathrm{d}x.\]</span> 75.  <spanclass="math inline">\(h&gt;0\)</span>  <spanclass="math inline">\(\varphi\inC^{\infty}_{c}(\mathbb{R}^{d})\)</span> <span class="math display">\[    \frac{\mathbb{E}_{\bm{W}}[\varphi(\bm{X}_{t+h})]-\mathbb{E}_{\bm{W}}[\varphi(\bm{X}_t)]}{h}=\frac{1}{h}\int_{t}^{t+h}\mathbb{E}_{\bm{W}}[(\mathcal{L}\varphi)(\bm{X}_s)] \mathrm{d}s,\]</span>  <spanclass="math inline">\(\psi\)</span> <span class="math display">\[    \begin{aligned}        \frac{1}{h}\left( \int_{\mathbb{R}^{d}}\varphi(\bm{X})\psi(t+h,\bm{X})\mathrm{d}\bm{X}-\int_{\mathbb{R}^{d}}^{} \varphi(\bm{X})\psi(t,\bm{X})\mathrm{d}\bm{X} \right)  \\        =\frac{1}{h}\int_{t}^{t+h} \int_{\mathbb{R}^{d}}^{}(\mathcal{L}\varphi)(\bm{X})\psi(s,\bm{X}) \mathrm{d}\bm{X} \mathrm{d}s.    \end{aligned}\]</span>  <span class="math inline">\(h\rightarrow0\)</span> <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}\left(\int_{\mathbb{R}^{d}}\varphi(\bm{X})\psi(t,\bm{X}) \mathrm{d}\bm{X}\right) = \int_{\mathbb{R}^{d}}^{}(\mathcal{L}\varphi)(\bm{X})\psi(t,\bm{X}) \mathrm{d}\bm{X}. \tag{11}\]</span> 1110. 510<span class="math inline">\(\mathcal{L}^{\dag}\psi=0\)</span> .</p><h3 id=""></h3><p> <span class="math inline">\(f\colon\mathbb{R}^{d}\rightarrow\mathbb{R}\)</span><span class="math display">\[    \mathrm{d}X_t=-\nabla f(X_t)\mathrm{d}t+\sigma \mathrm{d}W_t,\tag{12}\]</span>  Fokker-Planck  <span class="math display">\[    \partial_t m=\operatorname{div}(m\nabla f)+\frac{\sigma^{2}}{2}\Delta m, \tag{13}\]</span>  <span class="math inline">\(\sigma\)</span> <span class="math inline">\(W\)</span>  <spanclass="math inline">\(d\)</span> . 9<span class="math inline">\(m^{\sigma,*}\)</span><span class="math display">\[    m^{\sigma,*}(x)=\frac{1}{Z}\exp \left( -\frac{2}{\sigma^{2}}f(x)\right) , \quad \forall x\in \mathbb{R}^{d},\]</span>  <span class="math inline">\(Z\)</span> .</p><p><span class="math inline">\(m^{\sigma,*}\)</span> <span class="math display">\[    V^{\sigma}(m):=\int_{\mathbb{R}^{d}}^{}f(x)m(\mathrm{d}x)+\frac{\sigma^{2}}{2}H(m),\]</span>  <span class="math inline">\(H(m)\)</span>  <spanclass="math inline">\(m\)</span>  Lebesgue <span class="math display">\[    H(m):=\int_{\mathbb{R}^{d}}^{} m(x)\log \left( \frac{m(x)}{g(x)}\right)  \mathrm{d}x,\]</span>  <span class="math inline">\(g(x)=1\)</span> Lebesgue <span class="math inline">\(g(x)\)</span>.</p><h2 id=""></h2><p> <span class="math inline">\(\mathcal{P}(\mathbb{R}^{d})\)</span> <span class="math inline">\(\mathbb{R}^{d}\)</span><spanclass="math inline">\(\mathcal{P}_{p}(\mathbb{R}^{d})\)</span>  <spanclass="math inline">\(\mathbb{R}^{d}\)</span>  <spanclass="math inline">\(p\)</span> . <spanclass="math inline">\(F\)</span>  <spanclass="math inline">\(\mathcal{P}(\mathbb{R}^{d})\)</span> .<span class="math inline">\(U\in C^{\infty}(\mathbb{R}^{d})\)</span> <span class="math display">\[    \int_{\mathbb{R}^{d}}^{} \mathrm{e}^{-U(x)}  \mathrm{d}x=1,\]</span>  <span class="math inline">\(g(x)=\exp(-U(x))\)</span>.</p><p><strong>MFLD</strong> <span class="math display">\[\mathrm{d}X_t=-\left(D_{m}F(m_t,X_t)+\frac{\sigma^{2}}{2}\nablaU(X_t)\right)\mathrm{d}t+\sigma \mathrm{d}W_t, \tag{14}\]</span>  <span class="math inline">\(D_mF\)</span> <spanclass="math inline">\(D_mF(m,x):=\nabla \frac{\delta F}{\deltam}(m,x)\)</span>.</p><p><strong></strong>  <span class="math inline">\(f\inC^{1}(\mathbb{R}^{d},\mathbb{R})\)</span> <spanclass="math inline">\(F(m)=\int_{\mathbb{R}^{d}}^{}f(x)m(\mathrm{d}x)\)</span> <spanclass="math inline">\(D_mF(m,x)=\nabla f(x)\)</span>.  MFLD9.</p><p>MFLD  Fokker-Planck  MFLE</p><p><span class="math display">\[    \partial_{t}m=\nabla \cdot \left( \left(D_mF(m,\cdot)+\frac{\sigma^{2}}{2}\nabla U \right)m+\frac{\sigma^{2}}{2}\nabla m \right) , \tag{15}\]</span>  <span class="math inline">\(m_t\)</span>  <spanclass="math inline">\(X_t\)</span> .</p><p>10<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Hu, K., Ren, Z., SISKA, D. \&amp; SZPRUCH, L. (2020) Mean-Field Langevin Dynamics and Energy Landscape of Neural Networks. arXiv preprint at arXiv:1905.07769.">[3]</span></a></sup><span class="math display">\[    V^{\sigma}(m):=F(m)+\frac{\sigma^{2}}{2}H(m), \tag{16}\]</span></p><p> <span class="math inline">\(m^{*}\)</span> <spanclass="math inline">\(m^{*}\in \mathcal{P}_{2}(\mathbb{R}^{d})\)</span> <span class="math display">\[    \frac{\delta F}{\delta m}(m^{*},\cdot)+\frac{\sigma^{2}}{2}\log(m^{*})+\frac{\sigma^{2}}{2}U \ \text{} \tag{17}\]</span></p><p> <spanclass="math inline">\(\mathcal{W}_{2}\)</span>- <spanclass="math inline">\(m^{*}\)</span><spanclass="math inline">\(\lim_{t \to\infty}\mathcal{W}_{2}(m_t,m^{*})=0\)</span>.  Wasserstein.</p><h3id="noisy-particle-gradient-descent-npgd">NoisyParticle Gradient Descent, NPGD</h3><p>Noisy Particle Gradient Descent,NPGD.NPGD  MFLD .  <spanclass="math inline">\(G\)</span>  <spanclass="math inline">\(G\)</span><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Chizat, L. (2022) Mean-Field Langevin Dynamics: Exponential Convergence and Annealing. arXiv preprint at arXiv: 2202.01009v3.">[1]</span></a></sup>.</p><p><strong><span class="math inline">\(G\)</span></strong>  <span class="math inline">\(m \in\mathcal{P}_{2}(\mathbb{R}^{d})\)</span><spanclass="math inline">\(G\)</span>  <spanclass="math inline">\(m\)</span>  <spanclass="math inline">\(V[m]\in C^{1}(\mathbb{R}^{d})\)</span> <span class="math inline">\((m,x)\rightarrow \nabla V[m](x)\)</span> Lipschitz  <spanclass="math inline">\(L&gt;0\)</span>  <span class="math display">\[    \forall m,\nu \in \mathcal{P}_{2}(\mathbb{R}^{d}), \quad \forallx,y\in \mathbb{R}^{d}, \quad \left\| \nabla V[m](x)-\nabla V[\nu](y)\right\|_{2}\leqslant L(\left\| x-y \right\|_{2}+W_2(m,\nu)).\]</span>  <span class="math inline">\(W_{2}(\cdot,\cdot)\)</span> Wasserstein .</p><p>NPGD .  <spanclass="math inline">\(N\)</span>  <spanclass="math inline">\(X_1,\cdots ,X_N \in \mathbb{R}^{d}\)</span>. <span class="math inline">\(G\)</span><span class="math inline">\(V\)</span>.  <span class="math inline">\(X_{i,0}\sim\mu_0\in \mathcal{P}_{2}(\mathbb{R}^{d}), i \in [N]\)</span> NPGD <span class="math display">\[    \begin{cases}        X_{i,t+\mathrm{d}t}=X_{i,t}-\mathrm{d}t \nablaV[\hat{m}_{t}](X_{i,t})+\sigma\sqrt{\mathrm{d}t}Z_{i,t} \\        \hat{m}_{t}=\frac{1}{N}\sum_{i=1}^{N} \delta_{X_{i,t}},    \end{cases} \tag{18}\]</span>  <span class="math inline">\(\mathrm{d}t&gt;0\)</span><span class="math inline">\(Z_{i,t}\sim\mathcal{N}(0,1)\)</span> .</p><p> <span class="math inline">\(m_{t}\)</span> Fokker-Planck  <span class="math display">\[    \partial_{t} m=\nabla \cdot \left(m \nablaV[m]\right)+\frac{\sigma^{2}}{2} \Delta m, \tag{19}\]</span>  <span class="math display">\[    \partial_{t} m=\nabla \cdot\left(D_{m}G(m,\cdot)m+\frac{\sigma^{2}}{2} \nabla  m\right). \tag{20}\]</span>  <span class="math inline">\(G\)</span>19. 2015MFLD <span class="math inline">\(t\)</span>  <spanclass="math inline">\(X_{i}\)</span>  <spanclass="math inline">\(m_{t}\)</span>2015.</p><h2 id=""></h2><p> MFLD10.  <spanclass="math display">\[    \mathrm{d}x=-\frac{x}{50}\mathrm{d}t+\sigma\mathrm{d}W_t, \tag{21}\]</span>  <span class="math display">\[    m^{\sigma,*}(x)=\frac{1}{5\sqrt{2\pi}\sigma}\exp(-\frac{x^{2}}{50\sigma^{2}}). \tag{22}\]</span></p><p><strong></strong></p><p> KL  Wasserstein .. KL .<spanclass="math inline">\(p\)</span>-Wasserstein <spanclass="math inline">\(\mu_1,\mu_2\in\mathcal{P}_{p}(\mathbb{R})\)</span> <spanclass="math inline">\(F_1(x), F_2(x)\)</span> <spanclass="math inline">\(p\)</span>-Wasserstein  <spanclass="math display">\[    W_{p}(\mu_1,\mu_2)=\left( \int_{0}^{1} \lvert F_1 ^{-1}(q)-F_{2}^{-1}(q) \rvert ^{p} \mathrm{d}q \right) ^{1/p},\]</span>  <span class="math inline">\(F_1^{-1}\)</span>  <spanclass="math inline">\(F_{2}^{-1}\)</span> . <span class="math inline">\(p=1\)</span>  <spanclass="math display">\[    W_1(\mu_1,\mu_2)=\int_{\mathbb{R}}^{} \lvert F_1(x)-F_2(x)\rvert  \mathrm{d}x.\]</span></p><p>. <span class="math inline">\(N=10000\)</span> <span class="math inline">\(X^{1}_{0},\cdots ,X^{N}_{0}\)</span>.<span class="math inline">\(X^{i}_{0}\sim \mathcal{N}(0,1),i=1,\cdots,N\)</span>.  <spanclass="math inline">\(\mathrm{d}t=0.01\)</span>  <spanclass="math inline">\(10000\)</span>  <spanclass="math inline">\(t=100\)</span> .  <spanclass="math inline">\(\mathcal{N}(0,25\sigma^{2})\)</span> <span class="math inline">\(N=10000\)</span> <spanclass="math inline">\(\mathcal{N}(0,25\sigma^{2})\)</span>  KL Wasserstein . 1 <spanclass="math inline">\(\sigma=0.1,1,5\)</span> .KL  Wasserstein  <spanclass="math inline">\(\sigma\)</span> .  KL Wasserstein<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Chizat, L. (2022) Mean-Field Langevin Dynamics: Exponential Convergence and Annealing. arXiv preprint at arXiv: 2202.01009v3.">[1]</span></a></sup><sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="E, W., Li, T. \&amp; Vanden-Eijinden, E. 2019. Applied Stochastic Analysis. RI: AMS.">[2]</span></a></sup>.</p><h2id=""></h2><p>Chizat  NPGD  MFLD.  <spanclass="math inline">\(\mu,\nu\in\mathcal{P}(\mathcal{X})\)</span>kernel MaximumMean DiscrepancykMMD <span class="math display">\[    G^{2}(\mu,\nu)=\frac{1}{2}\int_{\mathcal{X}^{2}}^{} k(x,y)\mathrm{d}\mu(x)\mathrm{d}\mu(y)-\int_{\mathcal{X}^{2}}^{} k(x,y)\mathrm{d}\mu(x)\mathrm{d}\nu(y)+\frac{1}{2}\int_{\mathcal{X}^{2}}^{}k(x,y) \mathrm{d}\nu(x)\mathrm{d}\nu(y). \tag{23}   \]</span></p><p> <span class="math inline">\(k\in C^{2}(\mathcal{X} \times\mathcal{X})\)</span> .  <spanclass="math inline">\(\mathcal{P}(\mathcal{X})\)</span> . <span class="math inline">\(k(x,y)=k(y,x)\)</span> <span class="math inline">\(\mathcal{P}(\mathcal{X})\)</span>.</p><p>kMMD  Wasserstein  Kantorovich-Rubinstein . <spanclass="math inline">\(\mathcal{X}\)</span>  <spanclass="math inline">\(\mu,\nu \in\mathcal{P}_{1}(\mathcal{X})\)</span> <span class="math display">\[    W_{1}(\mu,\nu)=\sup_{\left\| f \right\|_{L}\leqslant 1} \left|\int_{}^{} f(\mu-\nu) \mathrm{d}x\right| \tag{24}\]</span>  <span class="math inline">\(G(\mu,\nu)\)</span> <span class="math display">\[    G^{2}(\mu,\nu)=\frac{1}{2}\left\| \mu-\nu\right\|^{2}_{\mathcal{H}}=\frac{1}{2}\int_{}^{} k(x,y)(\mu(x)-\nu(x))(\mu(y)-\nu(y))\mathrm{d}x\mathrm{d}y \tag{25}\]</span>  <span class="math inline">\(k(x,y)\)</span>  <spanclass="math inline">\(h(x)h(y)\)</span>  <spanclass="math inline">\(k(x,y)=xy\)</span> <spanclass="math display">\[    G(\mu,\nu)=\frac{\sqrt{2}}{2}\left| \int_{}^{} h(\mu-\nu)\mathrm{d}x \right| \tag{26}\]</span>  <span class="math inline">\(G(\mu,\nu)\)</span> <span class="math inline">\(W_1(\mu,\nu)\)</span> <span class="math inline">\(\mu_{n}\)</span>  <spanclass="math inline">\(\nu\)</span><spanclass="math inline">\(G(\mu_{n},\nu)=O_{p}(n^{-s})\LeftrightarrowW_1(\mu_{n},\nu)=O_{p}(n^{-s})\)</span>.</p><p> MFLD <span class="math inline">\(G\)</span> <span class="math inline">\(\mu\in\mathcal{P}(\mathcal{X})\)</span>  <span class="math inline">\(x\in\mathcal{X}\)</span> <span class="math display">\[    V[\mu](x)=\int_{}^{} k(x,y) \mathrm{d}(\mu-\nu)(y).\]</span></p><p>14 <spanclass="math inline">\(G\)</span>  <spanclass="math inline">\(F_{\tau}=G+\frac{\sigma^{2}}{2} H\)</span>.  <spanclass="math inline">\(\sigma=\sigma_{t}\)</span>  <spanclass="math inline">\(0\)</span> <spanclass="math inline">\(\sigma_{t}=\alpha/\sqrt{\log (t)}\)</span><span class="math inline">\(G(\mu_{t})\)</span>  <spanclass="math inline">\(F_{0}=G\)</span> .</p><p>18 <span class="math display">\[    \begin{cases}        X^{i}_{t+\mathrm{d}t}=X^{i}_{t}-\int_{}^{} k_{x}(x,y)\mathrm{d}(\mu_{t}-\nu)(y) +\sigma_{t} \sqrt{\mathrm{d}t}Z^{i}_{t}\\        \mu_{t}=\frac{1}{N}\sum_{i=1}^{N} \delta_{X^{i}_{t}},    \end{cases} \tag{27}\]</span>. <span class="math inline">\(N=100\)</span>.  <span class="math inline">\(N\)</span> <spanclass="math inline">\(\nu\)</span> . <span class="math display">\[    \nu=\frac{1}{10} \sum_{i=1}^{10} \delta_{2i-11}\]</span> 27.  <spanclass="math inline">\(\sigma_{t}\to 0 (t\to \infty)\)</span> <spanclass="math inline">\(\mu_{t}=\frac{1}{N}\sum_{i=1}^{N}\delta_{X^{i}_{t}}\)</span>  <spanclass="math inline">\(\nu\)</span> <span class="math display">\[    \lim_{t \to \infty}G(\mu_{t},\nu)=0.\]</span>  Chizat .  <spanclass="math inline">\(k\)</span>  <spanclass="math inline">\(\sigma_{k}=\sigma/\sqrt{\log k}\)</span>  <spanclass="math inline">\(\sigma\)</span>  <spanclass="math inline">\(\sigma^{*}\)</span> <span class="math inline">\(\mu_{t}\)</span>  <spanclass="math inline">\(G\)</span> Chizat, Theorem 4.1<span class="math display">\[    G(\mu_{t})-\inf G\leqslant C&#39; \frac{\log \log t}{\log t}.\]</span></p><p><strong></strong></p><p> <span class="math inline">\(X^{i}_{0}\sim\mathcal{N}(0,25)\)</span> <spanclass="math inline">\(\sigma=1\)</span> <spanclass="math inline">\(\mathrm{d}t=0.01\)</span> <spanclass="math inline">\(10000\)</span>27$k(x,y)k(x,y)=1+2_{k=1}^{5} (1+k)^{-1}(k(x-y)), k(x,y)=(-(x-y)^{2}/2),k(x,y)=xy $.  Chizat. <spanclass="math inline">\(k(x,y)=\tilde{k}(x-y)=\tilde{k}(y-x)\)</span>. . 2. <span class="math inline">\(N\)</span>  <spanclass="math inline">\(t\)</span> <spanclass="math inline">\(\sigma_{k}=\sigma/\sqrt{\log (k+1)}\)</span>. <spanclass="math inline">\(\sigma_{k}=\sigma/\sqrt{k}\)</span>  <spanclass="math inline">\(\sigma_{k}=\sigma/k\)</span>. <spanclass="math inline">\(\sigma_{k}=\sigma/k\)</span> . <spanclass="math inline">\(\sigma_{k}=\sigma/\sqrt{\log (k+1)}\)</span>Holley, Sec.3.  <spanclass="math inline">\(\sigma_{k}=\sigma/\sqrt{\log(k+1)}\)</span><span class="math inline">\(\sigma\)</span>.  <spanclass="math inline">\(\mu_{t}\)</span>  <spanclass="math inline">\(G\)</span> .</p><p>2 <span class="math inline">\(k(x,y)=1+2\sum_{k=1}^{5}(1+k)^{-1}\cos (k(x-y))\)</span>  <spanclass="math inline">\(k(x,y)=xy\)</span>.</p><p>. Chizat <spanclass="math inline">\((\mathbb{R}/2\pi\mathbb{Z})^{d}\)</span>. <spanclass="math inline">\(\sigma_{k}\)</span>  <spanclass="math inline">\(\sigma_{k}=\sigma/\sqrt{\log (k+1)}\)</span> <spanclass="math inline">\(\mu_{t}\)</span>  <spanclass="math inline">\(G\)</span> . <spanclass="math inline">\(\mathbb{R}\)</span>.3 <spanclass="math inline">\(X^{i}_{0}=0\)</span> <spanclass="math inline">\(\sigma=5\)</span>  <spanclass="math inline">\(G\)</span> . <span class="math inline">\(\mu_{t}\)</span>  <spanclass="math inline">\(-9,-7,-5,5,7,9\)</span> . <spanclass="math inline">\(\sigma_{k}=\sigma/\sqrt{\log (k+1)}, k(x,y)=\exp(-(x-y)^{2}/2)\)</span>  <span class="math inline">\(G\)</span> <spanclass="math inline">\(X^{i}_{0}=0\)</span>  <spanclass="math inline">\(\mu_{t}\)</span> . <span class="math inline">\(\mu_{t}\)</span>  <spanclass="math inline">\(\nu\)</span> <spanclass="math inline">\(\mu_{t}\)</span> <spanclass="math inline">\(G\)</span> . <spanclass="math inline">\(N\)</span>  <spanclass="math inline">\(t\)</span>.</p><h2 id=""></h2><p>MFLD.MFLD.. MFLD . <span class="math inline">\(G\)</span>.  Chizat  kMMD <spanclass="math inline">\(k(x,y)\)</span> <spanclass="math inline">\(\sigma_{k}\)</span> <spanclass="math inline">\(N\)</span> <spanclass="math inline">\(t\)</span> <spanclass="math inline">\(X^{i}_{0}\)</span>..</p><p>https://github.com/Newtonpula/Numecial-Simulation-of-MFLD .</p><p><strong></strong></p><h2 id="reference">Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Chizat, L. (2022) Mean-FieldLangevin Dynamics: Exponential Convergence and Annealing. arXiv preprintat arXiv: 2202.01009v3.<a href="#fnref:1" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:2" class="footnote-text"><span>E, W., Li, T. &amp;Vanden-Eijinden, E. 2019. Applied Stochastic Analysis. RI: AMS.<a href="#fnref:2" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Hu, K., Ren, Z., SISKA, D.&amp; SZPRUCH, L. (2020) Mean-Field Langevin Dynamics and EnergyLandscape of Neural Networks. arXiv preprint at arXiv:1905.07769.<a href="#fnref:3" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Lelivre, T. &amp; Stoltz,G. (2016) Partial differential equations and stochastic methods inmolecular dynamics. Acta Numerica. Cambridge University Press, 25, pp.681880. <a href="#fnref:4" rev="footnote" class="footnote-backref"></a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>numerical analysis</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fisher Information, AIC &amp; BIC</title>
    <link href="/2023/03/02/Fisher-Information/"/>
    <url>/2023/03/02/Fisher-Information/</url>
    
    <content type="html"><![CDATA[<h2 id="fisher-information">Fisher Information</h2><p> <span class="math inline">\(\theta\)</span><span class="math inline">\(x_1,\cdots ,x_n\)</span> <spanclass="math inline">\(x\sim D(\theta)\)</span>.</p><p> <span class="math display">\[    L(\theta)=\prod_{i=1}^{n} p(x_i|\theta)\]</span></p><p> normalization. <spanclass="math display">\[    \ln L(\theta)=\sum_{i=1}^{n} \ln p(x_i|\theta)\]</span></p><p> score  <span class="math display">\[    S(\theta)=\frac{\partial }{\partial \theta}\lnL(\theta)=\frac{\frac{\partial }{\partial \theta}L(\theta)}{L(\theta)}\]</span></p><p> normalization .</p><p> <span class="math display">\[    \mathbb{E}(S(\theta))=\int_{\mathbb{R}^{n}} \frac{\frac{\partial}{\partial \theta}L(\theta)}{L(\theta)}L(\theta)     \mathrm{d}x_1\cdots\mathrm{d}x_n=\frac{\partial }{\partial \theta}\int_{\mathbb{R}^{n}}L(\theta) \mathrm{d}x_1\cdots \mathrm{d}x_n=\frac{\partial }{\partial\theta}1=0\]</span></p><p> Fisher information <span class="math display">\[    I(\theta)=\operatorname{var}(S(\theta))\]</span></p><p><span class="math display">\[    I(\theta)=-\mathbb{E}\left(\frac{\partial ^{2}}{\partial\theta^{2}}\ln L(\theta)\right)\]</span></p><p> <span class="math inline">\(\theta\)</span>  <spanclass="math inline">\(\theta\)</span> <spanclass="math inline">\(\mathbb{E}(S(\theta))=0\)</span> <spanclass="math inline">\(I(\theta)\)</span> <span class="math inline">\(\lvertS(\theta) \rvert\)</span>  <spanclass="math inline">\(\{x_1,\cdots x_n\}\)</span>  <spanclass="math inline">\(I(\theta)\)</span> <span class="math inline">\(\lvertS(\theta) \rvert\)</span>  <spanclass="math inline">\(\{x_1,\cdots x_n\}\)</span><spanclass="math inline">\(L(\theta)\)</span>  <spanclass="math inline">\(\theta\)</span> <spanclass="math inline">\(\theta\)</span> .</p><p><span class="math inline">\(\displaystyle \frac{\frac{\partial}{\partial \theta}L(\theta)}{L(\theta)}\)</span> normalization <spanclass="math inline">\(L(\theta+\mathrm{d}\theta)\thickapproxL(\theta)+\frac{\partial }{\partial\theta}L(\theta)\mathrm{d}\theta\)</span> <spanclass="math inline">\(\displaystyle \frac{\frac{\partial }{\partial\theta}L(\theta)}{L(\theta)}\)</span> .</p><p> <spanclass="math inline">\(I(\theta)\)</span><spanclass="math inline">\(I(\theta)\)</span> .</p><p><strong>1</strong> <span class="math inline">\(X\sim\mathcal{N}(\theta,\sigma_0^{2})\)</span> <spanclass="math inline">\(\sigma_0\)</span>  <spanclass="math inline">\(n\)</span>  Fisher information.</p><p><span class="math display">\[    I(\theta)=\frac{n}{\sigma_0^{2}}\]</span></p><p><span class="math inline">\(I(\theta)\)</span>  <spanclass="math inline">\(\theta\)</span>.</p><p><strong>2</strong> <span class="math inline">\(X\sim\mathcal{N}(\mu_0,\theta^{2})\)</span> <spanclass="math inline">\(\mu_0\)</span>  <spanclass="math inline">\(n\)</span>  Fisher information. <spanclass="math display">\[    I(\theta)=\frac{2n}{\theta^{2}}\]</span></p><p><span class="math inline">\(I(\theta)\)</span>  <spanclass="math inline">\(\theta\)</span> .  <spanclass="math inline">\(\theta\to 0\)</span> <spanclass="math inline">\(\delta\)</span>  <spanclass="math inline">\(\mu_0\)</span> <spanclass="math inline">\(\theta=0\)</span>.  <spanclass="math inline">\(\theta\to\infty\)</span> <spanclass="math inline">\(x_i\)</span> <span class="math inline">\(\theta\)</span>  <spanclass="math inline">\(\theta\)</span> .</p><p><strong>3</strong> <span class="math inline">\(X\sim D\)</span> <span class="math inline">\(\theta\)</span><span class="math inline">\(I(\theta)\)</span> <span class="math inline">\(0\)</span>.</p><p><strong>4</strong> <span class="math inline">\(X\sim\mathcal{N}(1000+\exp (-\theta^{2}),\sigma_0^{2})\)</span> <spanclass="math inline">\(\sigma_0\)</span>  <spanclass="math inline">\(n\)</span> Fisher information. <spanclass="math display">\[    I(\theta)=\frac{4n\theta^{2}\exp (-2\theta^{2})}{\sigma_0^{2}}\]</span></p><p> <span class="math inline">\(1000\)</span>.  <span class="math inline">\(\exp (-\theta^{2})\)</span> <span class="math inline">\(1000\)</span> Fisher information.</p><h3 id="fisher-information-in-neural-activity---approach-i">Fisherinformation in neural activity - approach I</h3><p> <span class="math inline">\(\theta\)</span> neuralactivity <spanclass="math inline">\(\mathbf{r}\)</span>.<span class="math display">\[    p(\mathbf{r}|\theta)=g(\theta)\Phi(\mathbf{r})\exp(\mathbf{h}(\theta)^{\mathsf{T}}\mathbf{r}), \tag{1}\]</span></p><p> <span class="math display">\[    g(\theta)=\frac{1}{\int \Phi(\mathbf{r})\exp(\mathbf{h}(\theta)^{\mathsf{T}}\mathbf{r})\mathrm{d}\mathbf{r}},\tag{2}\]</span></p><p> <spanclass="math inline">\(g(\theta),\Phi(\mathbf{r}),\mathbf{h}(\theta)\)</span>. <span class="math inline">\(T(\mathbf{r}_1,\cdots,\mathbf{r}_n)=\mathbf{r}_1+\cdots +\mathbf{r}_n\)</span>.</p><p> - <span class="math inline">\(x_1,\cdots,x_n\)</span>  <spanclass="math inline">\(\theta\)</span> <span class="math inline">\(\mathbb{R}^{m}\)</span>  <spanclass="math inline">\(T(x_1,\cdots ,x_n)\)</span>  <spanclass="math inline">\(m\)</span>  <spanclass="math inline">\(n\)</span>. -  -</p><p> score  <span class="math display">\[    S(\theta)=\frac{\partial }{\partial \theta}\logp(\mathbf{r}|\theta)=\mathbf{h}&#39;(\theta)^{\mathsf{T}}(\mathbf{r}(\theta)-\mathbf{f}(\theta)),\tag{3}\]</span></p><p> <spanclass="math inline">\(\mathbf{f}(\theta)=\mathbb{E}(\mathbf{r}(\theta))\)</span> population activity vector.  <spanclass="math inline">\(\mathbf{\Sigma}(\theta)=\mathbb{E}[(\mathbf{r}(\theta)-\mathbf{f}(\theta))(\mathbf{r}(\theta)-\mathbf{f}(\theta))^{\mathsf{T}}]\)</span> neural activity.</p><p> Fisher information  <span class="math display">\[    I(\theta)=\mathbf{h}&#39;(\theta)^{\mathsf{T}}\mathbf{\Sigma}(\theta) \mathbf{h}&#39;(\theta), \tag{4}\]</span></p><p> <span class="math display">\[    \mathbf{f}&#39;(\theta)=\frac{\mathrm{d}}{\mathrm{d}\theta}\int_{}^{}\mathbf{r}p(\mathbf{r}|\theta)\mathrm{d}\mathbf{r}=\mathbf{\Sigma}(\theta)\mathbf{h}&#39;(\theta).\tag{5}\]</span></p><p>4 <span class="math display">\[    I(\theta)=\mathbf{f}&#39;(\theta)^{\mathsf{T}}\mathbf{\Sigma}^{-1}(\theta)\mathbf{f}&#39;(\theta).\tag{6}\]</span></p><h3 id="fisher-information-in-neural-activity---approach-ii">Fisherinformation in neural activity - approach II</h3><p> Fisher information. neural activity  <spanclass="math inline">\(\mathbf{w}\)</span> .</p><p><a href="https://www.nature.com/articles/nn1321">2004natureneuroscience</a> <spanclass="math inline">\(\theta_0\)</span>  <spanclass="math inline">\(\theta_1=\theta_0-\delta \theta,\theta_2=\theta_0+\delta \theta\)</span> <spanclass="math inline">\(\mathbf{w}\)</span>  <spanclass="math inline">\(b\)</span> neural activity <span class="math inline">\(\mathbf{r}\)</span> <span class="math inline">\(\theta_1\)</span>  <spanclass="math inline">\(\theta_2\)</span> <span class="math display">\[    \hat{\theta}=\mathbf{w}^{\mathsf{T}}\mathbf{r}+b.\]</span></p><p> <span class="math inline">\(\delta \theta\to 0\)</span> <span class="math inline">\(\hat{\theta}\)</span> <span class="math inline">\(\mathbf{r}\)</span>  <spanclass="math inline">\(\theta_0\)</span> <spanclass="math display">\[    \hat{\theta}=\mathbf{w}^{\mathsf{T}}(\mathbf{r}-\mathbf{f}(\theta_0))+\theta_0.\tag{7}\]</span></p><p>+ <spanclass="math inline">\(\mathbf{w}\)</span><span class="math inline">\(\mathbf{w}\)</span> <spanclass="math inline">\(\mathbf{w}\)</span>  <spanclass="math inline">\(\theta_0\)</span> <span class="math inline">\(\mathbf{r}\)</span> . unbiased locally linear estimator.</p><p><a href="https://www.jstor.org/stable/2984603">locally unbiasedestimation</a> -  <spanclass="math inline">\(\lim_{\theta_1 \to\theta_0}\mathbb{E}_{\theta_1}(\hat{\theta})=\mathbb{E}_{\theta_0}(\hat{\theta})\)</span>-  <span class="math inline">\(\lim_{\theta_1 \to\theta_0}\operatorname{var}(\hat{\theta}|\theta_1)=\operatorname{var}(\hat{\theta}|\theta_0)\)</span>.</p><p> <span class="math display">\[    \mathbb{E}_{\mathbf{r}}(\hat{\theta})=\theta_0\]</span></p><p> regularity <span class="math inline">\(\theta\)</span>  <spanclass="math display">\[    \frac{\mathrm{d}\mathbb{E}_{\mathbf{r}}(\hat{\theta})}{\mathrm{d}\theta}=1.\tag{8}\]</span></p><p> <spanclass="math inline">\(p(\mathbf{r}|\theta)\)</span>18 <span class="math display">\[    \mathbf{w}^{\mathsf{T}}\mathbf{f}&#39;(\theta)=1. \tag{9}\]</span></p><p>9 <spanclass="math inline">\(\mathbf{w}\)</span> <spanclass="math display">\[    \min _{\mathbf{w}}\mathbf{w}^{\mathsf{T}}\mathbf{\Sigma}\mathbf{w},\quad s.t.\ \mathbf{w}^{\mathsf{T}}\mathbf{f}&#39;(\theta)=1. \tag{10}\]</span></p><p> <span class="math display">\[    \mathbf{w}^{*}=\frac{\mathbf{\Sigma}^{-1}\mathbf{f}&#39;}{\mathbf{f}&#39;^{\mathsf{T}}\mathbf{\Sigma}^{-1}\mathbf{f}&#39;}\tag{11}\]</span></p><p> <span class="math display">\[    \operatorname{var}(\hat{\theta})=\frac{1}{\mathbf{f}&#39;^{\mathsf{T}}\mathbf{\Sigma}^{-1}\mathbf{f}&#39;}.\tag{12}\]</span></p><p> Fisher information <spanclass="math inline">\(\operatorname{var}(\hat{\theta})\)</span> <span class="math display">\[    I(\theta)=\mathbf{f}&#39;^{\mathsf{T}}\mathbf{\Sigma}^{-1}\mathbf{f}&#39;.\tag{13}\]</span></p><h2id="generalizing-fisher-information-beyond-fine-discrimination">GeneralizingFisher information beyond fine discrimination</h2><h3 id="approach-i">approach I</h3><p> <span class="math inline">\(C_1\)</span>  <spanclass="math inline">\(\theta_1\)</span>  <spanclass="math inline">\(C_2\)</span>  <spanclass="math inline">\(\theta_2\)</span> .neural activity  <spanclass="math display">\[    C_1\colonp(\mathbf{r}|\theta_1)=\mathcal{N}(\mathbf{r}|\mathbf{f}_1,\mathbf{\Sigma})\\    C_2\colonp(\mathbf{r}|\theta_2)=\mathcal{N}(\mathbf{r}|\mathbf{f}_2,\mathbf{\Sigma}),\tag{14}\]</span></p><p><span class="math inline">\(L_{ij}\)</span>  <spanclass="math inline">\(C_i\)</span>  <spanclass="math inline">\(C_j\)</span>  loss <spanclass="math inline">\(L_{12}=L_{21}, L_{11}=L_{22},L_{11}&lt;L_{12}\)</span> <spanclass="math inline">\(p(C_1)=p(C_2)=\frac{1}{2}\)</span>. expectedBayesian risk posterior expected loss/ Bayesian expectedloss <span class="math display">\[    \sum_{i\in \{1,2\}}^{} L_{iD(\mathbf{r})}p(C_i|\mathbf{r}).\]</span></p><p> <span class="math inline">\(D(\mathbf{r})\)</span> decision rule.  decision rule expected Bayesian risk <span class="math display">\[    D(\mathbf{r})=\begin{cases}        2, \quad\Lambda(\mathbf{r})=\log\frac{p(\mathbf{r}|\theta_2)}{p(\mathbf{r}|\theta_1)}&gt;0, \\        1, \quad \Lambda(\mathbf{r})\leqslant 0,    \end{cases}\]</span> (15)</p><p><span class="math inline">\(\Lambda(\mathbf{r})\)</span> log-likelihood ratio.  <span class="math display">\[    \Lambda(\mathbf{r})=(\mathbf{f}_2-\mathbf{f}_1)^{\mathsf{T}}\mathbf{\Sigma}^{-1}(\mathbf{r}-\mathbf{f}_0),\tag{16}\]</span></p><p> <spanclass="math inline">\(\mathbf{f}_0=\frac{1}{2}(\mathbf{f}_1+\mathbf{f}_2)\)</span>. <span class="math inline">\(\mathbf{w}=\mathbf{\Sigma}^{-1}\delta\mathbf{f}, \delta \mathbf{f}=\mathbf{f}_2-\mathbf{f}_1\)</span>. <spanclass="math display">\[    \Lambda(\mathbf{r})=\mathbf{w}^{\mathsf{T}}(\mathbf{r}-\mathbf{f}_0).\tag{17}\]</span></p><p> <span class="math display">\[    \Lambda(\mathbf{r})|C_1\sim \mathcal{N}\left(-\frac{1}{2}\mathbf{w}^{\mathsf{T}}\mathbf{\Sigma w} ,\mathbf{w}^{\mathsf{T}} \mathbf{\Sigma w}\right), \quad\Lambda(\mathbf{r})|C_2\sim \mathcal{N}\left(\frac{1}{2}\mathbf{w}^{\mathsf{T}}\mathbf{\Sigma w},\mathbf{w}^{\mathsf{T}}\mathbf{\Sigma w} \right). \tag{18}\]</span></p><p> <span class="math inline">\(D(\mathbf{r})\)</span> <span class="math display">\[    p(\text{correct})=\frac{1}{2}p(\Lambda(\mathbf{r})\leqslant0|C_1)+\frac{1}{2}p(\Lambda(\mathbf{r})&gt;0|C_2)=\Phi\left(\frac{1}{2}\sqrt{\mathbf{w}^{\mathsf{T}}\mathbf{\Sigmaw}}\right)=\Phi\left( \frac{1}{2}\sqrt{\delta\mathbf{f}^{\mathsf{T}}\mathbf{\Sigma}^{-1}\delta \mathbf{f}} \right)\]</span></p><p> <span class="math inline">\(\Phi(\cdot)\)</span> <span class="math display">\[    \Phi(y)=\frac{1}{\sqrt{2\pi}} \int_{-\infty}^{y} \exp(-\frac{x^{2}}{2}) \mathrm{d}x\]</span></p><p>6 <span class="math display">\[    I_{g}(\theta)=\frac{\delta\mathbf{f}^{\mathsf{T}}\mathbf{\Sigma}^{-1}\delta \mathbf{f}}{\delta\theta^{2}}, \tag{19}\]</span></p><h3 id="approach-ii">approach II</h3><p> generalized linear Fisher information optimal linear discriminator neural activity1.</p><p> <span class="math display">\[    \hat{\theta}=\mathbf{w}^{\mathsf{T}}\mathbf{r}. \tag{20}\]</span></p><p> <span class="math inline">\(\hat{\theta}\)</span> . <span class="math inline">\(\mathbf{w}\)</span><span class="math inline">\(\mathbf{r}\)</span> .  <spanclass="math display">\[    \max_{\mathbf{w}}\frac{\mathbf{w}^{\mathsf{T}}\delta \mathbf{f}\delta \mathbf{f}^{\mathsf{T}}\mathbf{w}}{\mathbf{w}^{\mathsf{T}}\mathbf{\Sigma w}}, \quad s.t. \ \left\| \mathbf{w} \right\|_{}^{2}=1,\tag{21}\]</span></p><p> <span class="math inline">\(\delta \mathbf{f} \delta\mathbf{f}^{\mathsf{T}}\)</span> <spanclass="math inline">\(\mathbf{\Sigma}\)</span> <span class="math display">\[    \mathbf{\Sigma}=\frac{\mathbf{\Sigma}_1+\mathbf{\Sigma}_2}{2}.\tag{22}\]</span></p><p> <span class="math display">\[    \mathbf{w}=\frac{\mathbf{\Sigma}^{-1}\delta \mathbf{f}}{\delta\mathbf{f}^{\mathsf{T}} \mathbf{\Sigma}^{-1}\delta \mathbf{f}}. \tag{23}\]</span></p><p> <span class="math inline">\(\hat{\theta}\)</span>. <span class="math display">\[    \hat{\theta}|C_1\sim\mathcal{N}(\mathbf{w}^{\mathsf{T}}\mathbf{f}_1, \mathbf{w}^{\mathsf{T}}\mathbf{\Sigma}_1 \mathbf{w}), \quad \hat{\theta}|C_2 \sim\mathcal{N}(\mathbf{w}^{\mathsf{T}}\mathbf{f}_2, \mathbf{w}^{\mathsf{T}}\mathbf{\Sigma}_2 \mathbf{w}). \tag{24}\]</span></p><p>.</p><h2 id="bic-bayesian-information-criterion">BIC (Bayesian informationcriterion)</h2><p><span class="math inline">\(BIC\)</span>.  <spanclass="math inline">\(r\)</span>  <spanclass="math inline">\(M_1,\cdots ,M_r\)</span>likelihood <spanclass="math inline">\(f_i(x|\mathbf{\theta}_{i})(\mathbf{\theta}_{i}\in\Theta_{i} \subset \mathbb{R}^{k_i})\)</span> <spanclass="math inline">\(\pi_i(\mathbf{\theta}_i)\)</span>  <spanclass="math inline">\(P(M_i)\)</span>.  <spanclass="math inline">\(n\)</span>  <spanclass="math inline">\(\mathbf{x}_{n}=\{x_1,\cdots,x_n\}\)</span>marginal likelihood  <spanclass="math display">\[    p(\mathbf{x}_n|M_i)=\int_{}^{}f_i(\mathbf{x}_n|\mathbf{\theta}_i)\pi_{i}(\mathbf{\theta}_{i})\mathrm{d}\mathbf{\theta}_{i}. \tag{25}\]</span></p><p> Bayes  <span class="math display">\[    P(M_i|\mathbf{x}_n)=\frac{p(\mathbf{x}_n|M_i)P(M_i)}{\sum_{j=1}^{r}p(\mathbf{x}_n|M_j)P(M_j)}, \quad i=1,\cdots ,r. \tag{26}\]</span></p><p> <span class="math inline">\(P(M_i|\mathbf{x}_n)\)</span> <span class="math inline">\(BIC\)</span>.</p><p> <spanclass="math inline">\(P(M_i)\)</span> . </p><p><span class="math display">\[    B_{12}=\frac{P(M_1|\mathbf{x}_n)}{P(M_2|\mathbf{x}_n)}=\frac{p(\mathbf{x}_n|M_1)}{p(\mathbf{x}_n|M_2)}\frac{P(M_1)}{P(M_2)}=\frac{\int_{}^{}f_1(\mathbf{x}_n|\mathbf{\theta}_1)\pi_{1}(\mathbf{\theta}_1)\mathrm{d}\mathbf{\theta}_{1}}{\int_{}^{}f_2(\mathbf{x}_n|\mathbf{\theta}_{2})\pi_{2}(\mathbf{\theta}_{2})\mathrm{d}\mathbf{\theta}_{2}} \tag{27}\]</span></p><p>Bayes factor  <span class="math inline">\(\displaystyle\frac{p(\mathbf{x}_n|M_1)}{p(\mathbf{x}_n|M_2)}\)</span>.</p><h3 id="section"></h3><h3 id="laplace-approximation">Laplace Approximation</h3><p>Laplace Approximation Bernsteinvon Mises.  <span class="math display">\[    \int_{}^{} \exp \{nq(\mathbf{\theta})\} \mathrm{d}\mathbf{\theta}\tag{28}\]</span></p><p> <span class="math inline">\(\mathbf{\theta}\)</span> <span class="math inline">\(p\)</span>  <spanclass="math inline">\(q(\mathbf{\theta})\)</span>  <spanclass="math inline">\(C_{c}^{2}(\mathbb{R}^{p})\)</span> <spanclass="math inline">\(\hat{\mathbf{\theta}}\)</span>.  <spanclass="math inline">\(\partial q(\mathbf{\theta})/\partial\mathbf{\theta}|_{\mathbf{\theta}=\hat{\mathbf{\theta}}}=\mathbf{0}\)</span>. <span class="math inline">\(\hat{\mathbf{\theta}}\)</span></p><p><span class="math display">\[    q(\mathbf{\theta})=q(\hat{\mathbf{\theta}})-\frac{1}{2}(\mathbf{\theta}-\hat{\mathbf{\theta}})^{\mathsf{T}}J_{q}(\hat{\mathbf{\theta}})(\mathbf{\theta}-\hat{\mathbf{\theta}})+\cdots, \tag{29}\]</span></p><p>Bernsteinvon Mises  <span class="math display">\[    \int_{}^{} \exp \{nq(\mathbf{\theta})\}\mathrm{d}\mathbf{\theta}\thickapprox \exp\{nq(\hat{\mathbf{\theta}})\}\int_{}^{} \exp\left\{-\frac{n}{2}(\mathbf{\theta}-\hat{\mathbf{\theta}})^{\mathsf{T}}J_q(\hat{\mathbf{\theta}})(\mathbf{\theta}-\hat{\mathbf{\theta}})\right\}\mathrm{d}\mathbf{\theta}=\exp\{nq(\hat{\mathbf{\theta}})\}\frac{(2\pi)^{p/2}}{n^{p/2}\lvertJ_q(\hat{\mathbf{\theta}}) \rvert ^{1/2}}\]</span></p><h3 id="derivation-of-the-bic">Derivation of the BIC</h3><p> <span class="math inline">\(M_i\)</span>marginal likelihood  <span class="math display">\[    p(\mathbf{x}_n)=\int_{}^{}f(\mathbf{x}_n|\mathbf{\theta})\pi(\mathbf{\theta}) \mathrm{d}\mathbf{\theta}=\int_{}^{} \exp \{\logf(\mathbf{x}_n|\mathbf{\theta})\}\pi(\mathbf{\theta})\mathrm{d}\mathbf{\theta}=\int_{}^{} \exp\{l(\mathbf{\theta})\}\pi(\mathbf{\theta}) \mathrm{d}\mathbf{\theta},\tag{30}\]</span></p><p> <span class="math display">\[    l(\theta)=\log f(\mathbf{x}_n|\theta)=\sum_{i=1}^{n} \logf(x_n|\theta)\]</span></p><p>MLE <spanclass="math inline">\(\hat{\mathbf{\theta}}\)</span>  <spanclass="math display">\[    l(\mathbf{\theta})=l(\hat{\mathbf{\theta}})-\frac{n}{2}(\mathbf{\theta}-\hat{\mathbf{\theta}})^{\mathsf{T}}J(\hat{\mathbf{\theta}})(\mathbf{\theta}-\hat{\mathbf{\theta}})+\cdots, \tag{31}\]</span></p><p> <span class="math display">\[    J(\hat{\mathbf{\theta}})=-\frac{1}{n}\frac{\partial^{2}l(\mathbf{\theta})}{\partial\mathbf{\theta} \partial\mathbf{\theta}^{\mathsf{T}}}\bigg|_{\mathbf{\theta}=\hat{\mathbf{\theta}}}\tag{32}\]</span></p><p> <span class="math inline">\(\pi(\mathbf{\theta})\)</span> <span class="math display">\[    \pi(\mathbf{\theta})=\pi(\hat{\mathbf{\theta}})+(\mathbf{\theta}-\hat{\mathbf{\theta}})^{\mathsf{T}}\frac{\partial\pi(\mathbf{\theta})}{\partial\mathbf{\theta}}\bigg|_{\theta=\hat{\mathbf{\theta}}}+\cdots  \tag{33}\]</span></p><p>313330 <spanclass="math inline">\(\hat{\mathbf{\theta}}-\mathbf{\theta}=O_p(n^{-1/2})\)</span><span class="math inline">\(n^{-1/2}\)</span> <span class="math display">\[    p(\mathbf{x}_n)\thickapprox \exp\{l(\hat{\mathbf{\theta}})\}\pi(\hat{\mathbf{\theta}})(2\pi)^{p/2}n^{-p/2}\lvert J(\hat{\mathbf{\theta}}) \rvert ^{-1/2}. \tag{34}\]</span></p><p> <span class="math display">\[    -2\log p(\mathbf{x}_n)\thickapprox -2l(\hat{\mathbf{\theta}})+p\logn + \log \lvert J(\hat{\mathbf{\theta}}) \rvert -p \log (2\pi)-2 \log\pi(\hat{\mathbf{\theta}}).\]</span></p><p> <span class="math inline">\(n\to \infty\)</span> dominate  <span class="math display">\[    BIC=-2\log f(\mathbf{x}_n|\hat{\mathbf{\theta}})+p\log n. \tag{35}\]</span></p><p><span class="math inline">\(BIC\)</span> regular  MLE  <spanclass="math inline">\(\hat{\mathbf{\theta}}\)</span>  Bayes freeenergy.</p><h2 id="aic-akaike-information-criterion">AIC (Akaike informationcriterion)</h2><p>AIC  K-L  MLE  <spanclass="math inline">\(f(x|\hat{\mathbf{\theta}})\)</span> . K-L  <span class="math inline">\(f\)</span> <span class="math inline">\(g\)</span>  <spanclass="math display">\[    I\{g(z);f(z|\hat{\mathbf{\theta}})\}=\mathbb{E}_{G}\left[ \log\left\{\frac{g(Z)}{f(Z|\hat{\mathbf{\theta}})}\right\}\right] \tag{36}\]</span></p><p> <spanclass="math inline">\(\hat{\mathbf{\theta}}=\hat{\mathbf{\theta}}(\mathbf{x}_n)\)</span>.</p><p> expected log-likelihood <span class="math display">\[    \mathbb{E}_{G}\left[ \logf(Z|\hat{\mathbf{\theta}})\right]=\int_{}^{} \logf(z|\hat{\mathbf{\theta}}) \mathrm{d}G(z)  \tag{37}\]</span></p><p>.  log-likelihood  <spanclass="math display">\[    \mathbb{E}_{\hat{G}}\left[ \logf(Z|\hat{\mathbf{\theta}})\right]=\int_{}^{} \logf(z|\hat{\mathbf{\theta}})\mathrm{d}\hat{G}(z)=\frac{1}{n}\sum_{\alpha=1}^{n} \logf(x_{\alpha}|\hat{\mathbf{\theta}})=\frac{1}{n}l(\hat{\mathbf{\theta}}).\tag{38}\]</span></p><h3 id="bias-of-the-log-likelihood">Bias of the Log-likelihood</h3><p> <spanclass="math inline">\(\mathbf{x}_n=\{x_1,\cdots x_n\}\)</span>MLE <spanclass="math inline">\(\hat{\mathbf{\theta}}\)</span>. <spanclass="math inline">\(\mathbf{x}_n\)</span>  <spanclass="math inline">\(\mathbb{E}_{G}\left[ \logf(Z|\hat{\mathbf{\theta}})\right]\)</span> ..</p><p><ahref="https://link.springer.com/book/10.1007/978-0-387-71887-3">3.4.3</a>bias</p><p><span class="math display">\[    b(G)=\mathbb{E}_{G(\mathbf{x}_n)}\left[ \logf(\mathbf{X}_n|\hat{\mathbf{\theta}}(\mathbf{X}_n))-n\mathbb{E}_{G(z)}[\log f(Z|\hat{\mathbf{\theta}}(\mathbf{X}_n))]\right]\tag{39}\]</span></p><p></p><p><span class="math display">\[    b(G)=\operatorname{tr}\{I(\mathbf{\theta}_0)J(\mathbf{\theta}_0)^{-1}\}\]</span></p><p> <span class="math display">\[    I(\mathbf{\theta}_0)=\mathbb{E}_{G(z)}\left[\frac{\partial \logf(Z|\mathbf{\theta})}{\partial \mathbf{\theta}} \frac{\partial \logf(Z|\mathbf{\theta})}{\partial\mathbf{\theta}^{\mathsf{T}}}\bigg|_{\mathbf{\theta}_0}\right]=\int_{}^{}g(z) \frac{\partial \log f(z|\mathbf{\theta})}{\partial \mathbf{\theta}}\frac{\partial \log f(z|\mathbf{\theta})}{\partial\mathbf{\theta}^{\mathsf{T}}}\bigg|_{\mathbf{\theta}_0} \mathrm{d}z\]</span></p><p><span class="math display">\[    J(\mathbf{\theta}_0)=-\mathbb{E}_{G(z)}\left[ \frac{\partial^{2}\log f(Z|\mathbf{\theta})}{\partial \mathbf{\theta} \partial\mathbf{\theta}^{\mathsf{T}}}\bigg|_{\mathbf{\theta}_0}\right]=-\int_{}^{}g(z) \frac{\partial ^{2}\log f(z|\mathbf{\theta})}{\partial\mathbf{\theta} \partial\mathbf{\theta}^{\mathsf{T}}}\bigg|_{\mathbf{\theta}_0} \mathrm{d}z\]</span></p><p> - <span class="math inline">\(b(G)\)</span> <span class="math inline">\(n\)</span>  - <spanclass="math inline">\(b(G)\)</span>  <spanclass="math inline">\(G(z)\)</span>38<span class="math inline">\(G(z)\)</span>. - <spanclass="math inline">\(b(G)\)</span>  <spanclass="math inline">\(g(x)\)</span>  ${f(x|);^{p}} $. <spanclass="math inline">\(I(\mathbf{\theta}_0)=J(\mathbf{\theta}_0)\)</span>.</p><h3 id="akaike-information-criterion-aic">Akaike Information Criterion(AIC)</h3><p> <span class="math inline">\(g(x)\)</span> ${f(x|);^{p}} $  <spanclass="math inline">\(\mathbf{\theta}_0\in \Theta\)</span>  <spanclass="math inline">\(g(x)=f(x|\mathbf{\theta}_0)\)</span>.  <spanclass="math display">\[    b(G)=\operatorname{tr}(I_p)=p, \tag{40}\]</span></p><p>. <spanclass="math display">\[    p=b(G)\thickapprox \sum_{\alpha=1}^{n} \logf(X_{\alpha}|\hat{\mathbf{\theta}})-n \mathbb{E}_{G(z)}\logf(Z|\hat{\mathbf{\theta}})\]</span></p><p> <span class="math inline">\(-2\)</span>  <spanclass="math inline">\(BIC\)</span>  <spanclass="math inline">\(AIC\)</span>  <span class="math display">\[    AIC= -2 \sum_{\alpha=1}^{n} \logf(X_{\alpha}|\hat{\mathbf{\theta}})+2p \tag{41}\]</span></p><h2 id="waic">WAIC</h2><p><strong>Fisher information matrix </strong> regular.  singular .singular  singular .</p><p><span class="math inline">\(WAIC\)</span>  <spanclass="math inline">\(AIC\)</span>  singular . singular MLE asymptotic normality .<span class="math inline">\(AIC\)</span>  averagegeneralization error<span class="math inline">\(BIC\)</span> Bayes marginal likelihood.</p><p> <span class="math inline">\(\mathbf{\Theta}\)</span> <spanclass="math inline">\(f(\mathbf{\theta})\)</span><span class="math inline">\(\mathbf{x}_n=\{x_1,\cdots,x_n\}\)</span><span class="math inline">\(f(\mathbf{\theta})\)</span></p><p><span class="math display">\[    \mathbb{E}_\mathbf{\theta}[f(\mathbf{\theta})]=\int_{}^{}f(\mathbf{\theta})p(\mathbf{\theta}|x_1,\cdots x_n)\mathrm{d}\mathbf{\theta}=\frac{\int_{}^{}f(\mathbf{\theta})\prod_{i=1}^{n}p(x_i|\mathbf{\theta})^{\beta}\pi(\mathbf{\theta})\mathrm{d}\mathbf{\theta}}{\int_{}^{} \prod_{i=1}^{n}p(x_i|\mathbf{\theta})^{\beta}\pi(\mathbf{\theta})\mathrm{d}\mathbf{\theta}}\]</span></p><p><span class="math inline">\(0&lt;\beta&lt;\infty\)</span> inverse temperature <span class="math inline">\(\beta=1\)</span> strict Bayes estimation. Watanabe  Bayes predictivedistribution <span class="math display">\[    p^{*}(x)\equiv \mathbb{E}_{\mathbf{\theta}}[p(x|\mathbf{\theta})].\]</span></p><p>Bayes generalization loss <spanclass="math inline">\(B_gL(n)\)</span> <span class="math display">\[    B_gL(n)=-\mathbb{E}_{X}[\log p^{*}(x)],\]</span></p><p>Bayes training loss <span class="math inline">\(B_{t}L(n)\)</span><span class="math display">\[    B_{t}L(n)=-\frac{1}{n} \sum_{i=1}^{n} \log p^{*}(x_{i}),\]</span></p><p>empirical variance <span class="math display">\[    V(n)=\sum_{i=1}^{n} \left\{ \mathbb{E}_{\mathbf{\theta}}[(\logp(x_i|\mathbf{\theta}))^{2}]-\mathbb{E}_{\mathbf{\theta}}[\logp(x_i|\mathbf{\theta})]^{2}\right\},\]</span></p><p>functional variance .  <spanclass="math display">\[    WAIC(n)\equiv B_{t}L(n)+\frac{\beta}{n}V(n),\]</span></p><p> <span class="math inline">\(\beta=1\)</span> <spanclass="math display">\[    WAIC(n)=-\frac{1}{n}\sum_{i=1}^{n} \log p^{*}(x_i)+\frac{1}{n}\sum_{i=1}^{n} \left\{ \mathbb{E}_{\mathbf{\theta}}[(\logp(x_i|\mathbf{\theta}))^{2}]-\mathbb{E}_{\mathbf{\theta}}[\logp(x_i|\mathbf{\theta})]^{2}\right\}.\]</span></p><p> <span class="math inline">\(2n\)</span>  <spanclass="math inline">\(AIC\)</span>  <spanclass="math inline">\(BIC\)</span>  <span class="math display">\[    WAIC=-2\sum_{i=1}^{n} \log p^{*}(x_i)+2\sum_{i=1}^{n} \left\{\mathbb{E}_{\mathbf{\theta}}[(\logp(x_i|\mathbf{\theta}))^{2}]-\mathbb{E}_{\mathbf{\theta}}[\logp(x_i|\mathbf{\theta})]^{2}\right\}\]</span></p><p> <span class="math inline">\(p^{*}(x)\)</span>.</p><h2 id="difference-among-information-criterions">Difference amonginformation criterions</h2><p><span class="math inline">\(BIC\)</span>  marginallikelihood posteriorChapter7.4Bayesian Data Analysis, third edition, Andrew Gelman, et al..</p>]]></content>
    
    
    <categories>
      
      <category>Statistics</category>
      
      <category>Information Theory</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The Circular Law (2)</title>
    <link href="/2023/01/28/The-Circular-Law-2/"/>
    <url>/2023/01/28/The-Circular-Law-2/</url>
    
    <content type="html"><![CDATA[<h1 id="the-circular-law-2">The Circular Law (2)</h1><h2 id="gaussian-case">Gaussian Case</h2><p>Let <span class="math inline">\(X:=(X_{ij})_{1\leqslant i,j\leqslantn}\)</span> be an i.i.d. random matrix on <spanclass="math inline">\(\mathbb{C}\)</span> with variance <spanclass="math inline">\(1\)</span>. We consider <spanclass="math inline">\(X\)</span> as a random variable in <spanclass="math inline">\(\mathcal{M}_{n}(\mathbb{C})\)</span>. Sometimes wedenote <span class="math inline">\(G\)</span> instead of <spanclass="math inline">\(X\)</span> in order to distinguish the Gaussiancase from the generalcase.<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Charles Bordenave and Djalil Chafa, The circular law">[1]</span></a></sup></p><h3 id="mean-circular-law">Mean Circular Law</h3><p>Recall that <spanclass="math inline">\(\mu_{G}=\frac{1}{n}\sum_{k=1}^{n}\delta_{\lambda_k(G)}\)</span>. The first result relies on the fact that<span class="math inline">\(\mathbb{E}\mu_{G}\)</span> has density <spanclass="math inline">\(\varphi_{n,1}\)</span>.</p><div class="note note-info">            <p><strong>Theorem</strong> <spanclass="math inline">\(\mathbb{E}\mu_{n^{-1/2}G}\rightsquigarrow\mathcal{C}_{1}\)</span> as <span class="math inline">\(n\to\infty\)</span>, where <spanclass="math inline">\(\mathcal{C}_{1}\)</span> is the uniform law on theunit disc of <span class="math inline">\(\mathbb{C}\)</span> withdensity <span class="math inline">\(z\mapsto \pi^{-1}\mathbf{1}_{\{z\in\mathbb{C}\colon \lvert z \rvert \leqslant 1\}}\)</span>.</p>          </div><p><strong>proof</strong> From (2.35) with <spanclass="math inline">\(k=1\)</span>, we get that the density of <spanclass="math inline">\(\mathbb{E}\mu_{G}\)</span> is <spanclass="math display">\[    \varphi_{n,1}\colon \mapsto \gamma(z)\left( \frac{1}{n}\sum_{l=0}^{n-1} \lvert H_l(z) \rvert ^{2}\right)=\frac{1}{n\pi}\mathrm{e}^{-\lvert z \rvert ^{2}}\sum_{l=0}^{n-1} \frac{\lvert z \rvert ^{2l}}{l!}.     \tag{3.1}  \]</span></p><p>For <span class="math inline">\(r^{2}&lt;n\)</span>, <spanclass="math display">\[    \mathrm{e}^{r^{2}} -\sum_{l=0}^{n-1}\frac{r^{2l}}{l!}=\sum_{l=n}^{\infty}\frac{r^{2l}}{l!}\leqslant\frac{r^{2n}}{n!}\sum_{l=0}^{\infty}\frac{r^{2l}}{(n+1)^{l}}=\frac{r^{2n}}{n!}\frac{n+1}{n+1-r^{2}}   \tag{3.2}\]</span></p><p>while for <span class="math inline">\(r^{2}&gt;n\)</span>, <spanclass="math display">\[    \sum_{l=0}^{n-1} \frac{r^{2l}}{l!}\leqslant\frac{r^{2(n-1)}}{(n-1)!}\sum_{l=0}^{n-1} \left( \frac{n-1}{r^{2}}\right) ^{l}\leqslant \frac{r^{2(n-1)}}{(n-1)!}\frac{r^{2}}{r^{2}-n+1}.\tag{3.3}\]</span></p><p>By taking <span class="math inline">\(r^{2}=\lvert \sqrt{n}z \rvert^{2}\)</span> we obtain that for every compact <spanclass="math inline">\(C\subset \mathbb{C}\)</span> <spanclass="math display">\[    \lim_{n \to \infty}\sup _{z\in \mathbb{C}} \lvertn\varphi_{n,1}(\sqrt{n}z)-\pi^{-1} \mathbf{1}_{[0,1]}(\lvert z \rvert )\rvert =0. \tag{3.4}\]</span></p><p>The <span class="math inline">\(n\)</span> in front of <spanclass="math inline">\(\varphi_{n,1}\)</span> is due to the fact that weare on <span class="math inline">\(\mathbb{C}\)</span>: <spanclass="math inline">\(\mathrm{d}\sqrt{n}x\mathrm{d}\sqrt{n}y=n\mathrm{d}x\mathrm{d}y\)</span>.</p><h3 id="strong-circular-law">Strong Circular Law</h3><div class="note note-info">            <p><strong>Theorem (Silverstein, 1984)</strong> a.s. <spanclass="math inline">\(\mu_{n^{-1/2}G}\rightsquigarrow\mathcal{C}_{1}\)</span> as <span class="math inline">\(n\to\infty\)</span>, where <spanclass="math inline">\(\mathcal{C}_{1}\)</span> is the uniform law on theunit disc of <span class="math inline">\(\mathbb{C}\)</span> withdensity <span class="math inline">\(z\mapsto \pi^{-1}\mathbf{1}_{\{z\in\mathbb{C}\colon \lvert z \rvert \leqslant 1\}}\)</span></p>          </div><p><strong>proof</strong> Let us pick a compactly supported continuousbounded function <span class="math inline">\(f\)</span> and set <spanclass="math display">\[    S_n:=\int_{\mathbb{C}}^{} f \mathrm{d}\mu_{n^{-1/2}G}, \quadS_{\infty}:=\pi^{-1}\int_{\lvert z \rvert \leqslant 1}^{} f(z)\mathrm{d}x\mathrm{d}y. \tag{3.5}\]</span></p><p>Suppose for now that we have <span class="math display">\[    \mathbb{E}[(S_n-\mathbb{E}S_n)^{4}]=O(n^{-2}). \tag{3.6}\]</span></p><p>By monotone convergence (or by the Fubini-Tonelli theorem), <spanclass="math display">\[    \mathbb{E}\sum_{n=1}^{\infty}(S_n-\mathbb{E}S_n)^{4}=\sum_{n=1}^{\infty}\mathbb{E}[(S_n-\mathbb{E}S_n)^{4}]&lt;\infty \tag{3.7}\]</span></p><p>and consequently <span class="math inline">\(\sum_{n=1}^{\infty}(S_n-\mathbb{E}S_n)^{4}&lt;\infty\)</span> a.s. which implies <spanclass="math inline">\(\lim_{n \to \infty}S_n-\mathbb{E}S_n=0\)</span>a.s. Since <span class="math inline">\(\lim_{n \to\infty}\mathbb{E}S_n=S_{\infty}\)</span> by the mean circular law, weget that a.s. <span class="math display">\[    \lim_{n \to \infty}S_n=S_{\infty}.\]</span></p><p>To establish (3.6), we set <span class="math display">\[    S_n-\mathbb{E}S_n=\frac{1}{n}\sum_{i=1}^{n} Z_i\]</span></p><p>with <span class="math display">\[    Z_i:=f(\lambda_i(n^{-1/2}G))-\mathbb{E}f(\lambda_i(n^{-1/2}G))\tag{3.8}\]</span></p><p>Now <span class="math display">\[    \begin{aligned}        \mathbb{E}[(S_n-\mathbb{E}S_{n})^{4}]&amp;=\frac{1}{n^{4}}\sum_{i_1}^{} \mathbb{E}[Z_{i_1}^{4}] \\        &amp;+\frac{4}{n^{4}}\sum_{i_1,i_2}^{}\mathbb{E}[Z_{i_1}Z_{i_2}^{3}] \\        &amp;+\frac{3}{n^{4}}\sum_{i_1,i_2}^{}\mathbb{E}[Z_{i_1}^{2}Z_{i_2}^{2}]\\        &amp;+\frac{6}{n^{4}}\sum_{i_1,i_2,i_3}^{}\mathbb{E}[Z_{i_1}Z_{i_2}Z_{i_3}^{2}] \\        &amp;+\frac{1}{n^{4}}\sum_{i_1,i_2,i_3,i_4}^{}\mathbb{E}[Z_{i_1}Z_{i_2}Z_{i_3}Z_{i_4}].    \end{aligned}\]</span></p><p>The first three terms of the RHS are <spanclass="math inline">\(O(n^{-2})\)</span> since <spanclass="math inline">\(\max_{1\leqslant i\leqslant n}\lvert Z_i \rvert\leqslant 2\left\| f \right\|_{\infty}\)</span>.</p><p>Note that the <span class="math inline">\(k\)</span>-correlation ofspectral density for <span class="math inline">\(n^{-1/2}G\)</span> is<span class="math display">\[    \tilde{\varphi}_{n,k}(z_1,\cdots,z_k)=\frac{(n-k)!}{n!}n^{k}\pi^{-k}\exp \left( -\sum_{i=1}^{k} n\lvertz_i \rvert^{2}  \right) \det \left[\sum_{l=0}^{n-1} n^{l}\frac{(z_i\bar{z_j})^{l}}{l!}\right]_{1\leqslant i,j\leqslant k}.  \tag{3.9}\]</span></p><p>For the fourth term, rewrite <span class="math display">\[    \tilde{\varphi}_{n,3}(z_1,z_2,z_3)=\frac{(n-3)!}{n!}n^{3}\prod_{i=1}^{3}\tilde{\varphi}_{n,1}(z_i)-\left(\frac{(n-3)!}{n!}n^{3}\prod_{i=1}^{3}\tilde{\varphi}_{n,1}(z_i)- \tilde{\varphi}_{n,3}(z_1,z_2,z_3) \right)\]</span></p><p>The function in the parenthesis is a density and has mass <spanclass="math inline">\(\displaystyle\frac{(n-3)!}{n!}n^{3}-1=\frac{3n-2}{(n-1)(n-2)}=O(n^{-1})\)</span>. And<span class="math inline">\(Z_1,Z_2,Z_3\)</span> are independent withrespect to <span class="math inline">\(\prod_{i=1}^{3}\tilde{\varphi}_{n,1}(z_i)\)</span>. Hence the contribution of thefourth term is <span class="math inline">\(O(n^{-2})\)</span>.</p><p>For the bound of the last term, note that <spanclass="math display">\[    \begin{aligned}        &amp;\exp \left( -\sum_{i=1}^{k} n\lvert z_i \rvert ^{2} \right)\det\left[ \sum_{l=0}^{n-1} \frac{n^{l}(z_i \bar{z_j})^{l}}{l!}\right]\\        &amp;=\det\left[ \exp (-\frac{n}{2}(\lvert z_i \rvert^{2}+\lvertz_j \rvert ^{2}))\sum_{l=0}^{n-1}\frac{n^{l}(z_i\bar{z_j})^{l}}{l!}\right] \\        &amp;=\det\left[\exp (n(-\frac{1}{2}\lvert z_i \rvert^{2}-\frac{1}{2}\lvert z_j \rvert ^{2}))\left( \exp (nz_i\bar{z_j})-\frac{n^{n}(z_i\bar{z_j})^{n}}{2\pi i}\int_{\xi=n\mathrm{e}^{i\theta}}^{} \frac{\mathrm{e}^{\xi}\mathrm{d}\xi}{(\xi-nz_i\bar{z_j})\xi^{n}}\right) \right].    \end{aligned}\]</span> (3.10)</p><p>Hence, if $S {z &lt;&lt;1} $, then <span class="math display">\[    \begin{aligned}        &amp;\operatorname{Pr}(z_1 \in S,\cdots ,z_k\in S)\\        &amp;= \frac{(n-k)!}{n!}n^{k}\pi^{-k}\int_{S^{k}}^{} \det \expn\left( -\frac{1}{2}\lvert z_i \rvert ^{2}-\frac{1}{2}\lvert z_j \rvert^{2}+z_i\bar{z_j} \right) \mathrm{d}z_1 \cdots\mathrm{d}z_k+O(\alpha^{n}). \\    \end{aligned}\]</span> (3.11)</p><p>The diagonal term in the determinant expansion gives <spanclass="math display">\[    \frac{(n-k)!}{n!}n^{k}\pi^{-k}\mu^{k}(S),\]</span></p><p>and this is the leading term. Let <span class="math display">\[    B=    \begin{cases}        0, \quad k=1 \\        -\frac{(n-k)!}{n!}n^{k}\pi^{-k}\tbinom{k}{2}\mu^{k-2}(S)\int_{S^{2}}^{}\exp \left( -\lvert z_1-z_2 \rvert ^{2}n\right)  \mathrm{d}z_1\mathrm{d}z_2, \quad k\geqslant 2.    \end{cases}\]</span></p><p>For <span class="math inline">\(k\geqslant 2\)</span>, <spanclass="math inline">\(B\)</span> is the contribution to (3.11) from allterms in the determinant containing all but two of the diagonalelements. The contributions from all other terms are <spanclass="math inline">\(O(n^{-2})\)</span>. (3.11) can be written as <spanclass="math display">\[    \frac{(n-k)!}{n!}n^{k}\left( \frac{\mu(S)}{\pi} \right)^{k}+B+O(n^{-2}).\]</span></p><p>Using this it is not hard to see the last term is also an <spanclass="math inline">\(O(n^{-2})\)</span>.</p><p>% note danger % I don't understand! % endnote %</p><p><strong>Theorem (Layers)</strong> We have the following equality indistribution: <span class="math display">\[    (\lvert \lambda_1(G)\rvert,\cdots ,\lvert \lambda_n(G)   \rvert)=(Z_{(1)},\cdots ,Z_{(n)})\]</span></p><p>where <span class="math inline">\(Z_{(1)},\cdots ,Z_{(n)}\)</span> isthe non-increasing reordering of a sequence <spanclass="math inline">\(Z_1,\cdots ,Z_n\)</span> of independent randomvariabes with <span class="math inline">\(Z_k^{2}\sim\Gamma(k,1)\)</span> for every <span class="math inline">\(1\leqslantk\leqslant n\)</span>. Here <spanclass="math inline">\(\Gamma(a,\lambda)\)</span> stands for the law on<span class="math inline">\(\mathbb{R}_{+}\)</span> with Lebesguedensity <span class="math inline">\(x\mapsto\lambda^{a}\Gamma(a)^{-1}x^{a-1}e^{-\lambda x}\)</span>.</p><p><strong>proof</strong> According to (2.24) and (3.9), for every <spanclass="math inline">\(r&gt;0\)</span>, <span class="math display">\[    \begin{aligned}        &amp;\operatorname{Pr}(\lvert \lambda_1(G) \rvert \leqslant\sqrt{n}r) \\        &amp;= \int_{\lvert z_1 \rvert \leqslant \sqrt{n}r}^{} \cdots\int_{\lvert z_n \rvert \leqslant \sqrt{n}r}^{} \frac{1}{n!\pi^{n}}\exp\left( -\sum_{i=1}^{n} \lvert z_i \rvert^{2}  \right) \det\left[\sum_{l=0}^{n-1} \frac{(z_i \bar{z_j})^{l}}{l!}\right]_{1\leqslanti,j\leqslant n}\mathrm{d}z_n^{r}\mathrm{d}z_n^{i}\cdots  \mathrm{d}z_1^{r}\mathrm{d}z_1^{i}\\        &amp;=\frac{1}{\pi^{n}}\det \left[ \int_{\lvert z \rvert\leqslant \sqrt{n}r}^{} \mathrm{e}^{-\lvert z \rvert ^{2}}z^{j}\bar{z}^{j} \mathrm{d}z^{r}\mathrm{d}z^{i} \right]=\prod_{j=1}^{n}\int_{0}^{nr^{2}} \mathrm{e}^{-t} t^{j} \mathrm{d}t=\prod_{j=1}^{n}\operatorname{Pr}\left( \frac{E_1+\cdots +E_j}{n}\leqslant r^{2} \right)    \end{aligned}\]</span></p><p>where <span class="math inline">\(E_1,\cdots ,E_k\)</span> are i.i.d.exponential random variables of unit mean.</p><p>The law of large numbers suggests that <spanclass="math inline">\(r=1\)</span> is a critical value. The centrallimit theorem suggests that <span class="math inline">\(\lvert\lambda_1(G) \rvert\)</span> behaves when <spanclass="math inline">\(n\gg 1\)</span> as the maximum of i.i.d.Gaussians, for which the fluctuations follow the Gumbel law.</p><h2 id="universal-case">Universal Case</h2><div class="note note-info">            <p><strong>Marchenko-Pastur quarter circular law</strong> a.s. <spanclass="math inline">\(\nu_{n^{-1/2}X}\rightsquigarrow\mathcal{Q}_{2}\)</span> as <span class="math inline">\(n\to\infty\)</span>, where <spanclass="math inline">\(\mathcal{Q}_{2}\)</span> is the quarter circularlaw on $[0,2]_{+} $ with density <span class="math display">\[    x\mapsto \frac{\sqrt{4-x^{2}}}{\pi}\mathbf{1}_{[0,2]}(x).\]</span></p>          </div><p>The <span class="math inline">\(n^{-1/2}\)</span> normalization iseasily understood from the law of large numbers:</p><p><span class="math display">\[    \int_{}^{} s^{2}\mathrm{d}\nu_{n^{-1/2}X}(s)=\frac{1}{n^{2}}\sum_{i=1}^{n}s_i(X)^{2}=\frac{1}{n^{2}}\operatorname{tr}(XX^{*})=\frac{1}{n^{2}}\sum_{i,j=1}^{n} \lvert X_{i,j} \rvert^{2}\rightarrow \mathbb{E}(\lvert X_{1,1} \rvert )^{2}. \tag{2.1}\]</span></p><div class="note note-info">            <p><strong>Girko circular law</strong> a.s. <spanclass="math inline">\(\mu_{n^{-1/2}X}\rightsquigarrow\mathcal{C}_{1}\)</span> as <span class="math inline">\(n\to\infty\)</span>, where <spanclass="math inline">\(\mathcal{C}_{1}\)</span> is the circular law whichis the uniform law on the unit disc of <spanclass="math inline">\(\mathbb{C}\)</span> with density <spanclass="math display">\[    z\mapsto \frac{1}{\pi}\mathbf{1}_{\{z\in \mathbb{C}\colon \lvert z\rvert \leqslant 1\}}.\]</span></p>          </div><p>The a.s. tightness of <spanclass="math inline">\(\mu_{n^{-1/2}X}\)</span> is easily understoodsince Weyl's inequality give <span class="math display">\[    \int_{}^{} \lvert \lambda^{2}\rvert  \mathrm{d}\mu_{n^{-1/2}X}(\lambda)=\frac{1}{n^{2}}\sum_{i=1}^{n}\lvert \lambda_i(X) \rvert ^{2}\leqslant \frac{1}{n^{2}}\sum_{i=1}^{n}s_i(X)^{2}=\int_{}^{} s^{2} \mathrm{d}\nu_{n^{-1/2}X}(s)\]</span></p><h2 id="reference">Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Charles Bordenave and DjalilChafa, The circular law<a href="#fnref:1" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:2" class="footnote-text"><span>R. A. Horn and Ch. R.Johnson, Topics in matrix analysis, Cambridge University Press,Cambridge, 1994, Corrected reprint of the 1991 original.<a href="#fnref:2" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Madan Lal Mehta, Randommatrices, third ed. Pure and Applied Mathematics, vol.142, Acad. Press,2004. <a href="#fnref:3" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Rider, B. A limit theorem atthe edge of a non-Hermitian random matrix ensemble. Random matrixtheory. J. Phys. A 36 (2003), no. 12, 34013409.<a href="#fnref:4" rev="footnote" class="footnote-backref"></a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The Circular Law (1)</title>
    <link href="/2023/01/22/The-Circular-Law-1/"/>
    <url>/2023/01/22/The-Circular-Law-1/</url>
    
    <content type="html"><![CDATA[<h1 id="the-circular-law-1">The Circular Law (1)</h1><p>This page gives some basic results and prove the spectral law of anGaussian i.i.d randommatrix.<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Charles Bordenave and Djalil Chafa, The circular law">[1]</span></a></sup></p><p>All random variables are defined on a unique common probability space<span class="math inline">\((\Omega,\mathcal{A},\mathbb{P})\)</span>. Atypical element of <span class="math inline">\(\Omega\)</span> isdenoted <span class="math inline">\(\omega\)</span>. We write <spanclass="math inline">\(a.s.\)</span>, <spanclass="math inline">\(a.a.\)</span>, and <spanclass="math inline">\(a.e.\)</span> for almost surely, Lebesgue almostall, and Lebesgue alomost everwhere respectively.</p><h2 id="two-kinds-of-spectra">Two kinds of spectra</h2><p>Label the eigenvalues of <span class="math inline">\(A \in\mathcal{M}_{n}(\mathbb{C})\)</span> as <spanclass="math inline">\(\lambda_1(A),\cdots ,\lambda_{n}(A)\)</span> sothat $<em>1(A) </em>{n}(A) $. The singular values of <spanclass="math inline">\(A\)</span> are defined by <spanclass="math display">\[    s_k(A):=\lambda_{k}(\sqrt{AA^{*}}).\]</span></p><p>We have <span class="math display">\[    s_1(A)\geqslant \cdots \geqslant s_{n}(A)\geqslant 0.\]</span></p><p>The matrices <spanclass="math inline">\(A,A^{\mathsf{T}},A^{*}\)</span> have the samesingular values. The Hermitian matrix <span class="math display">\[    H_{A}:=    \begin{pmatrix}        0 &amp; A \\        A^{*} &amp; 0 \\    \end{pmatrix}\]</span></p><p>has eigenvalues <span class="math inline">\(s_1(A),-s_1(A),\cdots,s_n(A),-s_n(A)\)</span>. Notice the mapping <spanclass="math inline">\(A \mapsto H_{A}\)</span> is linear in <spanclass="math inline">\(A\)</span>, in contrast with the mapping <spanclass="math inline">\(A \mapsto \sqrt{AA^{*}}\)</span>.</p><p>Define the operator norm or spectral norm of <spanclass="math inline">\(A\)</span> as <span class="math display">\[    \left\| A \right\|_{2\rightarrow 2}:=\max _{\left\| x\right\|_{2}=1}\left\| Ax \right\|_{2}=s_1(A)\]</span></p><p>Notice that if <span class="math inline">\(A\)</span> is non-singularthen <span class="math inline">\(s_i(A ^{-1})=s_{n-i}(A) ^{-1}\)</span>for <span class="math inline">\(1\leqslant i\leqslant n\)</span> and<span class="math inline">\(s_n(A)=s_1(A ^{-1}) ^{-1}=\left\| A^{-1}\right\|_{2\rightarrow 2} ^{-1}\)</span>.</p><div class="note note-info">            <p><strong>Courant-Fischer variational formulas</strong> Denoting <spanclass="math inline">\(\mathcal{G}_{n,i}\)</span> the Grassmannian of all<span class="math inline">\(i\)</span>-dimensional subspaces, we have<span class="math display">\[      \begin{aligned}        s_i(A)&amp;=\min _{E\in \mathcal{G}_{n,i-1}} \max _{\left\| x\right\|_{2}=1,x\perp E} \left\| Ax \right\|_{2}\\        &amp;=\max_{E \in \mathcal{G}_{n,n-i}} \min _{\left\| x\right\|_{2}=1,x\perp E}\left\| Ax \right\|_{2} \\        &amp;=\min _{E\in \mathcal{G}_{n,n-i+1}} \max_{\left\| x\right\|_{2}=1,x\in E}\left\| Ax \right\|_{2} \\        &amp;=\max _{E\in \mathcal{G}_{n,i}} \min _{\left\| x\right\|_{2}=1,x \in E} \left\| Ax \right\|_{2}    \end{aligned}\]</span></p>          </div><div class="note note-info">            <p><strong>Corollary</strong> Let <span class="math inline">\(A \in\mathcal{m,n}\)</span> be given, and let <spanclass="math inline">\(A_r\)</span> denote a submatrix of <spanclass="math inline">\(A\)</span> obtained by deleting a total of <spanclass="math inline">\(r\)</span> rows and/or columns from <spanclass="math inline">\(A\)</span>. Then <span class="math display">\[    s_k(A)\geqslant s_k(A_r)\geqslant s_{k+r}(A), k=1,\cdots,\min\{m,n\} \tag{1.1}\]</span></p><p>where for <span class="math inline">\(X \in\mathcal{M}_{p,q}\)</span> we set <spanclass="math inline">\(s_j(X)\equiv 0\)</span> if <spanclass="math inline">\(j&gt;\min\{p,q\}\)</span>.</p>          </div><p><strong>proof</strong> It suffices to consider the case <spanclass="math inline">\(r=1\)</span>, i.e., <spanclass="math inline">\(s_k(A)\geqslant s_{k}(A_1)\geqslants_{k+1}(A)\)</span>. If <span class="math inline">\(A_1\)</span> isformed from <span class="math inline">\(A\)</span> by deleting column<span class="math inline">\(s\)</span>, denote by <spanclass="math inline">\(e_s\)</span> the standard unit basis vector with a<span class="math inline">\(1\)</span> in position <spanclass="math inline">\(s\)</span>. If <span class="math inline">\(x\in\mathbb{C}^{n}\)</span>, denote by <span class="math inline">\(\xi \in\mathbb{C}^{n-1}\)</span> the vector obtained by deleting entry <spanclass="math inline">\(s\)</span> from <spanclass="math inline">\(x\)</span>. <span class="math display">\[    \begin{aligned}        s_k(A)&amp;=\min_{E\in \mathcal{G}_{n,k-1}} \max_{x\perpE,\left\| x \right\|_{2}=1}\left\| Ax \right\|_{2} \\        &amp;\geqslant \min_{E\in \mathcal{G}_{n,k-1}} \max_{x\perpE,x\perp e_s,\left\| x \right\|_{2}=1}\left\| Ax \right\|_{2}\\        &amp;= \min_{E\in \mathcal{G}_{n-1,k-1}} \max_{\xi\perpE,\left\| x \right\|_{2}=1}\left\| A_1\xi \right\|_{2}=s_k(A_1)    \end{aligned}\]</span></p><p><span class="math display">\[    \begin{aligned}        s_{k+1}(A)&amp;=\max _{\mathcal{G}_{n,n-k-1}}\min _{\left\| x\right\|_{2}=1,x\perp E} \left\| Ax \right\|_{2} \\        &amp;\leqslant \max _{\mathcal{G}_{n,n-k-1}}\min _{x\perpE,x\perp e_s,\left\| x \right\|_{2}=1} \left\| Ax \right\|_{2} \\        &amp;= \max _{\mathcal{G}_{n-1,n-k-1}}\min _{\xi\perp E,\left\|\xi \right\|_{2}=1} \left\| A_1\xi \right\|_{2}=s_k(A_1)    \end{aligned}\]</span></p><p>If a row of <span class="math inline">\(A\)</span> is deleted, applythe same argument to <span class="math inline">\(A^{*}\)</span>, whichhas the same singular values as <spanclass="math inline">\(A\)</span>.</p><div class="note note-info">            <p><strong>Lemma</strong> Let <span class="math inline">\(C \in\mathcal{M}_{m,n}\)</span>, <span class="math inline">\(V_{k} \in\mathcal{M}_{m,k}\)</span> and <span class="math inline">\(W_{k} \in\mathcal{M}_{n,k}\)</span> be given, where <spanclass="math inline">\(k\leqslant \min\{m,n\}\)</span> and <spanclass="math inline">\(V_k,W_k\)</span> have orthonormal columns. Then -<span class="math inline">\(s_i(V_k^{*}CW_{k})\leqslant s_i(C)\)</span>,<span class="math inline">\(i=1,\cdots,k\)</span> - <spanclass="math inline">\(\lvert \det V_k^{*}CW_{k} \rvert \leqslants_1(C)\cdots s_k(C)\)</span>.</p>          </div><p><strong>proof</strong> There are unitary matrices <spanclass="math inline">\(V\in \mathcal{M}_{m}\)</span> and <spanclass="math inline">\(W \in \mathcal{M}_{n}\)</span> such that <spanclass="math inline">\(V=[V_k \ *]\)</span> and <spanclass="math inline">\(W=[W_k \ *]\)</span>. Since <spanclass="math inline">\(V_k^{*}CW_{k}\)</span> is the upper left <spanclass="math inline">\(k\)</span>-by-<spanclass="math inline">\(k\)</span> submatrix of <spanclass="math inline">\(V^{*}CW\)</span>, (1.1) and unitary invariance ofsingular values ensure that <spanclass="math inline">\(s_i(V_k^{*}CW_{k})\leqslants_i(V^{*}CW)=s_i(C),i=1,\cdots ,k\)</span>, and hence <spanclass="math inline">\(\lvert \det V_k^{*}CW_{k} \rvert=s_1(V_k^{*}CW_{k})\cdots s_k(V_{k}^{*}CW_{k})\leqslant s_1(C)\cdotss_k(C)\)</span>.</p><div class="note note-info">            <p><strong>Weyl inequalities</strong> For every <spanclass="math inline">\(A \in \mathcal{M}_{n}(\mathbb{C})\)</span> and<span class="math inline">\(1\leqslant k\leqslant n\)</span>, <spanclass="math display">\[    \prod_{i=1}^{k} \lvert \lambda_{i}(A) \rvert \leqslant\prod_{i=1}^{k} s_i(A). \tag{1.2}\]</span></p>          </div><p><strong>proof</strong> By the Schur triangularization theorem, thereis a unitary <span class="math inline">\(U \in\mathcal{M}_{n}(\mathbb{C})\)</span> such that <spanclass="math inline">\(U^{*}AU=\Delta\)</span> is upper triangular and<span class="math inline">\(diag \Delta=(\lambda_1,\cdots,\lambda_n)\)</span>. Let <span class="math inline">\(U_k \in\mathcal{M}_{n,k}\)</span> denote the first <spanclass="math inline">\(k\)</span> columns of <spanclass="math inline">\(U\)</span>. It's easy to know that <spanclass="math inline">\(U_k^{*}AU_{k}=\Delta_{k}\)</span> is the upperleft <span class="math inline">\(k\)</span>-by-<spanclass="math inline">\(k\)</span> principal submatrix of <spanclass="math inline">\(\Delta\)</span>, and <spanclass="math inline">\(diag \Delta_k=(\lambda_1,\cdots,\lambda_n)\)</span>. Apply the lemma with <spanclass="math inline">\(C=A\)</span> and <spanclass="math inline">\(V_k=W_k=U_k\)</span> to conclude that <spanclass="math display">\[    \lvert \lambda_1(A)\cdots \lambda_k(A) \rvert =\lvert \det \Delta_k\rvert =\lvert \det U_k^{*}AU_{k} \rvert \leqslant s_1(A)\cdotss_k(A)      \]</span></p><p>It's easy to know that (1.2) holds quality when <spanclass="math inline">\(k=n\)</span>.</p><p>The reversed form $<em>{i=n-k+1}^{n} s_i(A)</em>{i=n-k+1}^{n} _i(A) $for every <span class="math inline">\(1\leqslant k\leqslant n\)</span>can be deduced easily.</p><div class="note note-info">            <p><strong>Theorem</strong> Let <span class="math inline">\(A\in\mathcal{M}_{m,p}\)</span> and <span class="math inline">\(B\in\mathcal{M}_{p,n}\)</span> be given, let <spanclass="math inline">\(q\equiv \min\{n,p,m\}\)</span>. Then <spanclass="math display">\[    \prod_{i=1}^{k} s_i(AB)\leqslant \prod_{i=1}^{k} s_i(A)s_i(B), \k=1,\cdots ,q \tag{1.3}\]</span></p><p>If <span class="math inline">\(n=p=m\)</span>, then equality holds in(1.3) for <span class="math inline">\(k=n\)</span>.</p>          </div><p><strong>proof</strong> Let <span class="math inline">\(AB=V\SigmaW^{*}\)</span>, and let <span class="math inline">\(V_k \in\mathcal{M}_{m,k}\)</span> and <span class="math inline">\(W_k\in\mathcal{M}_{n,k}\)</span> denote the first <spanclass="math inline">\(k\)</span> columns of <spanclass="math inline">\(V\)</span> and <spanclass="math inline">\(W\)</span>, respectively. Then <spanclass="math inline">\(V_k^{*}(AB)W_k=diag(s_1(AB),\cdots,s_k(AB))\)</span>. Since <span class="math inline">\(p\geqslantk\)</span>, use the polar decomposition to write the product <spanclass="math inline">\(BW\in \mathcal{M}_{p,k}\)</span> as <spanclass="math inline">\(BW_{k}=X_{k}Q\)</span>, where <spanclass="math inline">\(X_k\in \mathcal{M}_{p,k}\)</span> has orthonormalcolumns, <span class="math inline">\(Q\in \mathcal{M}_{k}\)</span> ispositive semidefinite, <span class="math inline">\(\det Q^{2}=\detW_k^{*}(B^{*}B)W_k\leqslant s_1(B^{*}B)\cdotss_k(B^{*}B)=s_1(B)^{2}\cdots s_k(B)^{2}\)</span> by lemma. Use lemmaagain to compute <span class="math display">\[    s_1(AB)\cdots s_k(AB)= \lvert \det V_k^{*}(AB)W_k \rvert = \lvert\det V_k^{*}AX_{k} \det Q \rvert \leqslant (s_1(A)\cdotss_k(A))(s_1(B)\cdots s_k(B))\]</span></p><p>If <span class="math inline">\(n=p=m\)</span>, then <spanclass="math inline">\(s_1(AB)\cdots s_n(AB)=\lvert \det AB \rvert=\lvert \det A \rvert \lvert \det B \rvert =s_1(A)\cdotss_k(A)s_1(B)\cdots s_k(B)\)</span>.</p><div class="note note-info">            <p><strong>(Strong) majorization inequalities</strong> If <spanclass="math inline">\(A\)</span> is nonsingular, then</p><p><span class="math display">\[    \sum_{i=1}^{k} \log \lvert \lambda_i(A) \rvert \leqslant\sum_{i=1}^{k} \log s_i(A), k=1,\cdots ,n,\]</span></p><p>with equality for <span class="math inline">\(k=n\)</span>.</p>          </div><p><strong>proof</strong> It's just a corollary of the previoustheorem.</p><p>Using some classic inequality trick, we can introduce Schur-convex orisotone functions and prove weak majorization inequality.</p><div class="note note-warning">            <p><strong>Definition</strong> Let <spanclass="math inline">\(x=[x_i],y=[y_i]\in \mathbb{R}^{n}\)</span> begiven vectors, <span class="math inline">\(x_{[1]}\geqslant \cdots\geqslant x_{[n]}\)</span> and <spanclass="math inline">\(y_{[1]}\geqslant \cdots \geqslanty_{[n]}\)</span>. We say that <span class="math inline">\(y\)</span>weakly majorizes <span class="math inline">\(x\)</span> if <spanclass="math display">\[    \sum_{i=1}^{k} x_{[i]}\leqslant \sum_{i=1}^{k} y_{[i]}, \quadk=1,2,\cdots,n \tag{1.4}\]</span></p><p>If (1.4) holds equality when <spanclass="math inline">\(k=n\)</span>, then we say <spanclass="math inline">\(y\)</span> majorizes <spanclass="math inline">\(x\)</span>, denoted as <spanclass="math inline">\(x \prec y\)</span>.</p>          </div><div class="note note-info">            <p><strong>Schur</strong> Suppose <spanclass="math inline">\(A=(a_{ij})\in \mathbb{C}^{n\times n}\)</span> isHermitian with eigenvalues <spanclass="math inline">\(\{\lambda_i\}_{1\leqslant i\leqslant n}\)</span>,then <span class="math inline">\(\{a_{ii}\}\prec\{\lambda_{i}\}\)</span>.</p>          </div><p><strong>proof</strong> <span class="math inline">\(\sum_{i=1}^{r}\lambda_{[i]}=\sup _{E\in\mathcal{G}_{n,r}}\operatorname{tr}(A|_{E})\geqslant \sum_{i=1}^{r}a_{[ii]}\)</span>.</p><div class="note note-info">            <p><strong>Horn</strong> Suppose <spanclass="math inline">\(\{d_i\}\prec \{\mu_i\}\)</span>, then there existsa symmetric matrix with primary diagonal entries <spanclass="math inline">\(\{d_i\}\)</span>, eigenvalues <spanclass="math inline">\(\{\mu_i\}\)</span>.</p>          </div><p><strong>proof</strong> We construct orthogonal matrix <spanclass="math inline">\(Q\)</span>, such that <spanclass="math inline">\(Q^{\mathsf{T}}diag(\mu_1,\cdots ,\mu_n)Q\)</span>has primary diagonal entries <spanclass="math inline">\(\{d_i\}\)</span>. Use induction to prove it, referto https://zhuanlan.zhihu.com/p/76422480.</p><div class="note note-info">            <p><strong>Hardy-Littlewood-Polya</strong> Suppose <spanclass="math inline">\(x,y\in \mathbb{R}^{n}\)</span>, then <spanclass="math inline">\(x\prec y\)</span> if and only if there is a doublysubstochastic <span class="math inline">\(A\in\mathcal{M}_{n}(\mathbb{R})\)</span> such that <spanclass="math inline">\(x=Ay\)</span>.</p>          </div><p><strong>proof</strong> If <span class="math inline">\(x\precy\)</span>, then the Horn theorem gives an orthogonal matrix <spanclass="math inline">\(Q=(q_{ij})\)</span> such that <spanclass="math inline">\(x_i=\sum_{j=1}^{n} q_{ij}^{2}y_j,\forall1\leqslant i\leqslant n\)</span>. <spanclass="math inline">\(A=(q_{ij}^{2})\)</span> satisfies the condition.For the rest, refer to https://zhuanlan.zhihu.com/p/76422480 aswell.</p><div class="note note-info">            <p><strong>Theorem</strong> <span class="math inline">\(x\)</span> and<span class="math inline">\(y\)</span> are two given vectors withnonnegative entries. Then <span class="math inline">\(y\)</span> weaklymajorizes <span class="math inline">\(x\)</span> if and only if there isa doubly substochastic <span class="math inline">\(Q\in\mathcal{M}_{n}(\mathbb{R})\)</span> such that <spanclass="math inline">\(x=Qy\)</span>.</p>          </div><p><strong>proof</strong> If <span class="math inline">\(Q\in\mathcal{M}_{n}(\mathbb{R})\)</span> is doubly substochastic and <spanclass="math inline">\(Qy=x\)</span> with <spanclass="math inline">\(x,y\geqslant 0\)</span>, let <spanclass="math inline">\(S\in \mathcal{M}_{n}(\mathbb{R})\)</span> bedoubly stochastic and such that <span class="math inline">\(0\leqslantQ\leqslant S\)</span>. If <spanclass="math inline">\((Qy)_{i_1}=(Qy)_{[1]},\cdots,(Qy)_{i_k}=(Qy)_{[k]}\)</span>, then <span class="math display">\[    \sum_{i=1}^{k} (Qy)_{[i]}=\sum_{j=1}^{k} (Qy)_{i_j}\leqslant\sum_{j=1}^{k} (Sy)_{i_j}\leqslant \sum_{i=1}^{k} (Sy)_{[i]}\leqslant\sum_{i=1}^{k} y_{[i]}\]</span></p><p>for <span class="math inline">\(k=1,2,\cdots,n\)</span>. Thus, <spanclass="math inline">\(y\)</span> weakly majorizes <spanclass="math inline">\(Qy=x\)</span>.</p><p>Conversely, suppose <span class="math inline">\(y\)</span> weaklymajorizes <span class="math inline">\(x\)</span> and <spanclass="math inline">\(x,y\geqslant 0\)</span>. If <spanclass="math inline">\(x=0\)</span>, let <spanclass="math inline">\(Q\equiv 0\)</span>. If <spanclass="math inline">\(x\neq 0\)</span>, let <spanclass="math inline">\(\varepsilon\)</span> denote the smallest positiveentry of <span class="math inline">\(x\)</span>, set <spanclass="math inline">\(\delta\equiv (y_1-x_1)+\cdots +(y_n-x_n)\geqslant0\)</span>, and let <span class="math inline">\(m\)</span> be anypositive integer such that <spanclass="math inline">\(\varepsilon\geqslant \delta/m\)</span>. Let <spanclass="math display">\[    \xi\equiv [x_1,\cdots ,x_n,\delta/m,\cdots,\delta/m]^{\mathsf{T}}\in \mathbb{R}^{n+m}\]</span></p><p>and let <span class="math display">\[    \eta=[y_1,\cdots ,y_n,0,\cdots ,0]^{\mathsf{T}}\in \mathbb{R}^{n+m}\]</span></p><p>Then <span class="math display">\[    \sum_{i=1}^{k} \xi_{[i]}\leqslant \sum_{i=1}^{k} \eta_{[i]}, \quadk=1,2,\cdots,m+n\]</span></p><p>with equality for <span class="math inline">\(k=m+n\)</span>. Thus,there is strong majorization relationship between <spanclass="math inline">\(\xi\)</span> and <spanclass="math inline">\(\eta\)</span>. By Hardy-Littlewood-Polya theorem,there is a doubly stochastic <span class="math inline">\(S\in\mathcal{M}_{m+n}(\mathbb{R})\)</span> such that <spanclass="math inline">\(\xi=S\eta\)</span>. If we let <spanclass="math inline">\(Q\)</span> denote the upper left <spanclass="math inline">\(n\)</span>-by-<spanclass="math inline">\(n\)</span> principal submatrix of <spanclass="math inline">\(S\)</span>, <span class="math inline">\(Q\)</span>is doubly substochastic and <spanclass="math inline">\(x=Qy\)</span>.</p><div class="note note-info">            <p><strong>Lemma</strong> Let <span class="math inline">\(x_1,\cdots,x_n\)</span>, <span class="math inline">\(y_1,\cdots ,y_n\)</span> be<span class="math inline">\(2n\)</span> given real numbers such that<span class="math inline">\(x_1\geqslant x_2\geqslant \cdots \geqslantx_n\)</span>,<span class="math inline">\(y_1\geqslant y_2\geqslant\cdots \geqslant y_n\)</span>, and <span class="math display">\[    \sum_{i=1}^{k} x_i\leqslant \sum_{i=1}^{k} y_i, \quad k=1,2,\cdots,n\tag{1.5}\]</span></p><p>If <span class="math inline">\(f(t)\)</span> is a given real-valuedincreasing convex function on the interval <spanclass="math inline">\([\min\{x_n,y_n\},y_1]\)</span>, then <spanclass="math inline">\(f(x_1)\geqslant \cdots \geqslant f(x_n)\)</span>,<span class="math inline">\(f(y_1)\geqslant \cdots f(y_n)\)</span>, and<span class="math display">\[    \sum_{i=1}^{k} f(x_i)\leqslant \sum_{i=1}^{k} f(y_i), \quadk=1,2,\cdots,n\]</span></p><p>If equality holds for <span class="math inline">\(k=n\)</span> in(1.4) and if <span class="math inline">\(f(\cdot)\)</span> is convex(but not necessarily increasing) on the interval <spanclass="math inline">\([y_n,y_1]\)</span>, then <spanclass="math display">\[    \sum_{i=1}^{n} f(x_i)\leqslant \sum_{i=1}^{n} f(y_i) \tag{1.6}\]</span></p>          </div><p><strong>proof</strong> The asserted inequality is trivial for <spanclass="math inline">\(k=1\)</span>. Let <spanclass="math inline">\(k\)</span> be a given integer with <spanclass="math inline">\(2\leqslant k\leqslant n\)</span>. Since <spanclass="math inline">\(x\)</span> is weakly majorized by <spanclass="math inline">\(y\)</span>, there is a doubly stochastic <spanclass="math inline">\(S\in \mathcal{M}_{k}(\mathbb{R})\)</span> suchthat <span class="math inline">\(x\leqslant Sy\)</span>.</p><p>Notice that the entries of <span class="math inline">\(Sy\)</span>lie in the interval <span class="math inline">\([y_n,y_1]\)</span> andhence are in the domain of <spanclass="math inline">\(f(\cdot)\)</span>. Using the monotonicity andconvexity of <span class="math inline">\(f(\cdot)\)</span>, we have</p><p><span class="math display">\[    \sum_{i=1}^{k} f(x_i)\leqslant \sum_{i=1}^{k} f\left( \sum_{j=1}^{k}s_{ij}y_j \right) \leqslant \sum_{i=1}^{k} \sum_{j=1}^{k}s_{ij}f(y_j)=\sum_{j=1}^{k} \left( \sum_{i=1}^{k} s_{ij} \right)f(y_j)=\sum_{j=1}^{k} f(y_j)\]</span></p><p>If equality holds for <span class="math inline">\(k=n\)</span>, thereis a doubly stochastic <span class="math inline">\(S\in\mathcal{M}_{n}(\mathbb{R})\)</span> such that <spanclass="math inline">\(x=Sy\)</span> and hence convexity of <spanclass="math inline">\(f(\cdot)\)</span> gives <spanclass="math display">\[    \sum_{i=1}^{n} f(x_i)=\sum_{i=1}^{n} f\left( \sum_{j=1}^{n}s_{ij}y_j \right) \leqslant \sum_{i=1}^{n} \sum_{j=1}^{n}s_{ij}f(y_j)=\sum_{j=1}^{n} f(y_j)\]</span></p><div class="note note-info">            <p><strong>Theorem</strong> Let <span class="math inline">\(A\in\mathcal{M}_{n}(\mathbb{C})\)</span>, then <span class="math display">\[    \sum_{i=1}^{k} \lvert \lambda_{i}(A) \rvert^{p}\leqslant\sum_{i=1}^{k} s_i(A)^{p} \ \text{for} \ k=1,\cdots ,n, \forall p&gt;0\]</span></p><p><span class="math display">\[    \lvert \operatorname{tr} \ A \rvert \leqslant \sum_{i=1}^{n} s_i(A)\]</span></p><p>We have a more general version of this theorem, please refer to [HJ,Theorem3.3.13].<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="R. A. Horn and Ch. R. Johnson, Topics in matrix analysis, Cambridge University Press, Cambridge, 1994, Corrected reprint of the 1991 original.">[2]</span></a></sup></p>          </div><p>We can deduce from Weyl's inequalities that <spanclass="math display">\[    \sum_{i=1}^{n} \lvert \lambda_i(A) \rvert ^{2}\leqslant\sum_{i=1}^{n} s_i(A)^{2}=\operatorname{tr}(AA^{*})=\sum_{i,j=1}^{n}\lvert A_{i,j} \rvert ^{2}. \tag{1.7}\]</span></p><p>Since <span class="math inline">\(s_1(\cdot)=\left\| \cdot\right\|_{2\rightarrow 2}\)</span> we have for any <spanclass="math inline">\(A,B\in \mathcal{M}_{n}(\mathbb{C})\)</span> that<span class="math display">\[    s_1(AB)\leqslant s_1(A)s_1(B), \quad s_1(A+B)\leqslants_1(A)+s_1(B). \tag{1.8}\]</span></p><p>We define tha empirical eigenvalues and singular values measures by<span class="math display">\[    \mu_{A}:=\frac{1}{n}\sum_{k=1}^{n} \delta_{\lambda_k(A)}, \quad\nu_{A}:=\frac{1}{n}\sum_{k=1}^{n} \delta_{s_k(A)}.\]</span></p><p><span class="math inline">\(\mu_{A}\)</span> ad <spanclass="math inline">\(\nu_{A}\)</span> are supported respectively in<span class="math inline">\(\mathbb{C}\)</span> and <spanclass="math inline">\(\mathbb{R}_{+}\)</span>. From (1.2) we get <spanclass="math display">\[    \begin{aligned}        \int_{}^{} \log \lvert \lambda\rvert  \mathrm{d}\mu_{A}(\lambda)&amp;= \frac{1}{n}\sum_{i=1}^{n} \log\lvert \lambda_i(A) \rvert \\        &amp;=\frac{1}{n}\sum_{i=1}^{n} \log (s_i(A)) \\        &amp;=\int_{}^{} \log (s) \mathrm{d}\nu_{A}(s).    \end{aligned}\]</span></p><p>The map <span class="math inline">\(A\mapsto (s_1(A),\cdotss_n(A))\)</span> is 1-Lipschitz: for any <spanclass="math inline">\(A,B\in \mathcal{M}_{n}(\mathbb{C})\)</span>,</p><p><span class="math display">\[    \max_{1\leqslant i\leqslant n}\lvert s_i(A)-s_i(B) \rvert \leqslants_1(A-B).\]</span></p><p>The Hilbert-Schimidt norm/ Schur norm/ Frobenius norm, is</p><p><span class="math display">\[    \left\| A \right\|_{2}^{2}=\operatorname{tr}(AA^{*})=\sum_{i=1}^{n}s_i(A)^{2}=n \int_{}^{} s^{2} \mathrm{d}\nu_{A}(s).\]</span></p><p>In the sequel, we say that a sequence of (possibly signed) measures<span class="math inline">\((\eta_{n})_{n\geqslant 1}\)</span> on <spanclass="math inline">\(\mathbb{C}\)</span> (respectively on <spanclass="math inline">\(\mathbb{R}\)</span>) tends weakly to a (possiblysigned) measure <span class="math inline">\(\eta\)</span>, and we denote<span class="math display">\[    \eta_{n} \rightsquigarrow \eta,\]</span></p><p>when for all continuous and bounded function <spanclass="math inline">\(f\colon \mathbb{C}\rightarrow \mathbb{R}\)</span>(respectively <span class="math inline">\(f\colon \mathbb{R}\rightarrow\mathbb{R}\)</span>), <span class="math display">\[    \lim_{n \to \infty}\int_{}^{} f \mathrm{d}\eta_n=\int_{}^{} f\mathrm{d}\eta.\]</span></p><h2 id="spectral-law">Spectral Law</h2><p>Since <span class="math inline">\(\mathbb{C}\equiv\mathbb{R}^{2}\)</span>, we now consider the case where <spanclass="math inline">\(X_{11}\sim \mathcal{N}(0,\frac{1}{2}I_2)\)</span>.We denote <span class="math inline">\(G\)</span> instead of <spanclass="math inline">\(X\)</span> in order to distinguish the Gaussiancase from the general case. We say <spanclass="math inline">\(G\)</span> belongs to the <strong>Complex GinibreEnsemble</strong>.</p><p>The Lebesgue density of the <span class="math inline">\(n\timesn\)</span> random matrix <span class="math inline">\(G=(G_{ij})\)</span>in <span class="math inline">\(\mathcal{M}_{n}(\mathbb{C})\equiv\mathbb{C}^{n\times n}\)</span> is <span class="math display">\[    A\in \mathcal{M}_{n}(\mathbb{C})\mapsto\pi^{-n^{2}}\mathrm{e}^{-\sum_{i,j=1}^{n} \lvert A_{ij} \rvert ^{2}} .\tag{2.1}\]</span></p><p>Notice that it's a Boltzmann distribution with energy <spanclass="math display">\[    A\mapsto \sum_{i,j=1}^{n} \lvert A_{ij} \rvert^{2}=\operatorname{tr}(AA^{*})=\left\| A \right\|_{2}^{2}=\sum_{i=1}^{n}s_i^{2}(A).\]</span></p><p>So this law is unitary invariant, in the sense that if <spanclass="math inline">\(U\)</span> and <spanclass="math inline">\(V\)</span> are <span class="math inline">\(n\timesn\)</span> unitary matrices then <spanclass="math inline">\(UGV\)</span> and <spanclass="math inline">\(G\)</span> are equally distributed.</p><p>We set <span class="math display">\[    \Delta_n:=\{(z_1,\cdots ,z_n)\in \mathbb{C}^{n}\colon \lvert z_1\rvert \geqslant \cdots \geqslant \lvert z_n \rvert \}.\]</span></p><p>Recall the Lebesgue density of the <spanclass="math inline">\(n\times n\)</span> random matrix <spanclass="math inline">\(G=(G_{ij})\)</span> in <spanclass="math inline">\(\mathcal{M}_{n}(\mathbb{C})\equiv\mathbb{C}^{n\times n}\)</span> is <span class="math display">\[    G\in \mathcal{M}_{n}(\mathbb{C})\mapsto\pi^{-n^{2}}\mathrm{e}^{-\sum_{i,j=1}^{n} \lvert G_{ij} \rvert ^{2}} .\]</span></p><div class="note note-info">            <p><strong>Theorem</strong> <spanclass="math inline">\((\lambda_1(G),\cdots ,\lambda_n(G))\)</span> hasdensity <span class="math inline">\(n!\varphi_{n}\mathbf{1}_{\Delta_n}\)</span> where <span class="math display">\[    \varphi_{n}(z_1,\cdots ,z_n)=\frac{\pi^{-n}}{1!2!\cdots n!}\exp\left( -\sum_{k=1}^{n} \lvert z_k \rvert ^{2} \right) \prod_{1\leqslanti&lt;j\leqslant n}^{} \lvert z_i-z_j \rvert ^{2}.\]</span></p><p>In particular, for every symmetric Borel function <spanclass="math inline">\(F\colon \mathbb{C}^{n}\rightarrow\mathbb{R}\)</span>, <span class="math display">\[    \mathbb{E}[F(\lambda_1(G),\cdots,\lambda_n(G))]=\int_{\mathbb{C}^{n}}^{} F(z_1,\cdots,z_n)\varphi_{n}(z_1,\cdots ,z_n) \mathrm{d}z_1\cdots \mathrm{d}z_n\]</span></p>          </div><div class="note note-danger">            <p>There are two ways to get <spanclass="math inline">\(\varphi_n\)</span>, using diagonalization or Schurdecompositionrespectively.<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Madan Lal Mehta, Random matrices, third ed. Pure and Applied Mathematics, vol.142, Acad. Press, 2004.">[3]</span></a></sup>We apply the first one since it is more understandable.</p>          </div><p>We may suppose that <span class="math inline">\(G\)</span> hasdistinct eigenvalues since matrices with multiple eigenvalues havemeasure <span class="math inline">\(0\)</span>. Let <spanclass="math inline">\(X\)</span> be the <spanclass="math inline">\(N\times N\)</span> matrix whose columns are theeigenvectors of <span class="math inline">\(G\)</span> so that <spanclass="math inline">\(G=XEX^{-1}\)</span>. By differentiation, <spanclass="math display">\[    X^{-1}\mathrm{d}GX=\mathrm{d}E+\mathrm{d}BE-E\mathrm{d}B \tag{2.2}\]</span></p><p>with <span class="math display">\[    \mathrm{d}B=X^{-1}\mathrm{d}X.\]</span></p><p><span class="math display">\[    (X^{-1}\mathrm{d}GX)_{jj}^{r}=\mathrm{d}z_j^{r}=\mathrm{d}x_j, \quad(X^{-1}\mathrm{d}GX)_{jj}^{i}=\mathrm{d}z_j^{i}=\mathrm{d}y_j,\]</span></p><p><span class="math display">\[    (X^{-1}\mathrm{d}GX)_{jk}^{r}=(x_k-x_j)\mathrm{d}B_{jk}^{r}-(y_k-y_j)\mathrm{d}B_{jk}^{i},\quad j\neq k\]</span></p><p><span class="math display">\[    (X^{-1}\mathrm{d}GX)_{jk}^{i}=(y_k-y_j)\mathrm{d}B_{jk}^{r}+(x_k-x_j)\mathrm{d}B_{jk}^{i},\quadj\neq k\]</span></p><p>where <span class="math inline">\(x_j,y_j\)</span> are the real andimaginary parts of <span class="math inline">\(z_j\)</span>, thediagonal elements of <span class="math inline">\(E\)</span>. Denote<spanclass="math inline">\(\mathrm{d}\hat{G}=X^{-1}\mathrm{d}GX\)</span>.</p><p>The Jacobian <span class="math display">\[    \frac{\partial(\mathrm{d}\hat{G}_{jj}^{r},\mathrm{d}\hat{G}_{jj}^{i})}{\partial(x_j,y_j)}=1\]</span></p><p><span class="math display">\[    \frac{\partial (\mathrm{d}G_{jk}^{r},\mathrm{d}G_{jk}^{i})}{\partial(\mathrm{d}B_{jk}^{r},\mathrm{d}B_{jk}^{i})}=(x_k-x_j)^{2}+(y_k-y_j)^{2}=\lvertz_k-z_j \rvert ^{2}\]</span></p><p>The volume element is therefore given by <spanclass="math display">\[    \mu(\mathrm{d}G)=\mu(\mathrm{d}\hat{G})=\prod_{j\neq k}^{} \lvertz_k-z_j \rvert ^{2}\mathrm{d}B_{jk}^{r}\mathrm{d}B_{jk}^{i}\prod_{i}^{}\mathrm{d}x_i\mathrm{d}y_i. \tag{2.3}\]</span></p><p>We now evaluate the integral <span class="math display">\[    J=\int_{}^{} \exp [-\operatorname{tr}(G^{*}G)]\prod_{j\neqk}^{}  \mathrm{d}B_{jk}^{r}\mathrm{d}B_{jk}^{i}. \tag{2.4}\]</span></p><h3 id="evaluation-of-2.4">Evaluation of (2.4)</h3><p>Similar to QR decomposition, any complex nonsingular <spanclass="math inline">\(N\times N\)</span> matrix <spanclass="math inline">\(X\)</span> can be expressed in one and only oneway as <span class="math display">\[    X=UYV \tag{2.5}\]</span></p><p>where <span class="math inline">\(U\)</span> is a unitary matrix,<span class="math inline">\(Y\)</span> is a triangular matrix with alldiagonal elements equal to unity, <spanclass="math inline">\(y_{ij}=0,i&gt;j\)</span>, <spanclass="math inline">\(y_{ii}=1\)</span>, and <spanclass="math inline">\(V\)</span> is a diagonal matrix with real positivediagonal elements.</p><p>Using the fact that <span class="math inline">\(G=XEX^{-1}\)</span>,<span class="math inline">\(U^{*}U=I\)</span>, and <spanclass="math inline">\(EV=VE\)</span>, we can write <spanclass="math display">\[    \operatorname{tr}(G^{*}G)=\operatorname{tr}[E^{*}Y^{*}YE(Y^{*}Y)^{-1}],\tag{2.6}\]</span></p><p><span class="math display">\[    \mathrm{d}B=X^{-1}\mathrm{d}X=V^{-1}Y^{-1}(U^{-1}\mathrm{d}U)YV+V^{-1}Y^{-1}\mathrm{d}YV+V^{-1}\mathrm{d}V.\tag{2.7}\]</span></p><p>From (3.1.6) and the structure of <spanclass="math inline">\(Y\)</span> and <spanclass="math inline">\(V\)</span> we see that <spanclass="math display">\[    \prod_{j\neq k}^{}\mathrm{d}B_{jk}^{r}\mathrm{d}B_{jk}^{i}=\prod_{j&lt;k}^{}(Y^{-1}\mathrm{d}Y)_{jk}^{r}(Y^{-1}\mathrm{d}Y)_{jk}^{i}a, \tag{2.8}\]</span></p><p>where <span class="math inline">\(a\)</span> depends only on <spanclass="math inline">\(U\)</span> and <spanclass="math inline">\(V\)</span>.</p><p><span class="math display">\[    \int_{}^{} \exp [-\operatorname{tr}(E^{*}HEH^{-1})]\prod_{j&lt;k}^{}(Y^{-1}\mathrm{d}Y)_{jk}^{r}(Y^{-1}\mathrm{d}Y)_{jk}^{i}, \tag{2.9}\]</span></p><p>where <span class="math display">\[    H=Y^{*}Y. \tag{2.10}\]</span></p><p>The matrix <span class="math inline">\(H\)</span> is Hermitian. Anyof its upper left diagonal block of size <spanclass="math inline">\(m\)</span> is obtained from the upper leftdiagonal block of <span class="math inline">\(Y\)</span> of the samesize: <span class="math inline">\(H_n=Y_n^{*}Y_n\)</span>. Therefore,for every <span class="math inline">\(n\)</span>, <spanclass="math inline">\(\det H_n=1\)</span>.</p><p>We make a change of variables. First, because <spanclass="math inline">\(\det Y=1\)</span>, <span class="math display">\[    \prod_{j&lt;k}^{}(Y^{-1}\mathrm{d}Y)_{jk}^{r}(Y^{-1}\mathrm{d}Y)_{jk}^{i}=\prod_{j&lt;k}^{}\mathrm{d}Y_{jk}^{r}\mathrm{d}Y_{jk}^{i}. \tag{2.11}\]</span></p><p>Next, we take <span class="math inline">\(H_{jk}^{r}\)</span>, <spanclass="math inline">\(H_{jk}^{i}\)</span> for <spanclass="math inline">\(j&lt;k\)</span> as independent variables from<span class="math display">\[    H_{jk}=Y_{jk}+\sum_{l&gt;j}^{} Y_{jl}^{*}Y_{lk}, \quad j&lt;k,  \]</span></p><p>The Jacobian of the transformation from <spanclass="math inline">\(Y\)</span> to <spanclass="math inline">\(H\)</span> is <spanclass="math inline">\(1\)</span>. So <span class="math display">\[    J=C \int_{}^{} \exp[-\operatorname{tr}(E^{*}HEH^{-1})]\prod_{j&lt;k}^{}\mathrm{d}H_{jk}^{r} \mathrm{d}H_{jk}^{i}, \tag{2.12}\]</span></p><p>where <span class="math inline">\(C\)</span> is a constant.</p><p>The integration over <span class="math inline">\(H\)</span> is donein <span class="math inline">\(N\)</span> steps. At every step weintegrate over the variables of the last column and thus decrease by onethe size of the matrix, whose structure remains the same.</p><p>Let <span class="math inline">\(H&#39;=Y_n^{*}Y_n\)</span>, <spanclass="math inline">\(E&#39;=[z_i\delta_{jk}]_{j,k=1,2,\cdots,n}\)</span>, be the relevant matrices of order <spanclass="math inline">\(m\)</span> and <spanclass="math inline">\(H\)</span>, <span class="math inline">\(E\)</span>be those obtained from <span class="math inline">\(H&#39;\)</span>,<span class="math inline">\(E&#39;\)</span> by removing the last row andlast column. Let the Greek indices run from <spanclass="math inline">\(1\)</span> to <spanclass="math inline">\(n\)</span>, and the Latin indices from <spanclass="math inline">\(1\)</span> to <spanclass="math inline">\(n-1\)</span>. Let <spanclass="math inline">\(\Delta_{\alpha\beta}&#39;\)</span> be the cofactorof <span class="math inline">\(H_{\alpha\beta}&#39;\)</span> in <spanclass="math inline">\(H&#39;\)</span> and <spanclass="math inline">\(\Delta_{jk}\)</span>, the cofactor of <spanclass="math inline">\(H_{jk}\)</span> in <spanclass="math inline">\(H\)</span>. Let <spanclass="math inline">\(g_i=H_{in}&#39;\)</span>. Because <spanclass="math inline">\(\det H&#39;=\det H=1\)</span>, we have <spanclass="math display">\[    \Delta_{\alpha\beta}&#39;=(H&#39;^{-1})_{\beta\alpha}, \quad\Delta_{jk}=(H^{-1})_{kj}. \tag{2.13}\]</span></p><p>Expanding <span class="math inline">\(\det H&#39;\)</span>, <spanclass="math inline">\(\Delta_{in}&#39;\)</span>,<spanclass="math inline">\(\Delta_{jk}&#39;\)</span> by the last row and lastcolumn, we have <span class="math display">\[    1=H_{nn}&#39;-\sum_{j,k}^{} g_{j}^{*}g_{k}\Delta_{kj}, \tag{2.14}\]</span></p><p><span class="math display">\[    \Delta_{in}&#39;=-\sum_{l}^{} \Delta_{il}g_{l}^{*}, \tag{2.15}\]</span></p><p><span class="math display">\[    \Delta_{ij}&#39;=H_{nn}&#39;\Delta_{ij}-\sum_{l,k}^{}g_k^{*}g_l\Delta_{ij}^{lk}, \tag{2.16}\]</span></p><p>where <span class="math inline">\(\Delta_{ij}^{lk}\)</span> is thecofactor obtained from <span class="math inline">\(H\)</span> byremoving the <span class="math inline">\(i\)</span>th and <spanclass="math inline">\(l\)</span>th rows and the <spanclass="math inline">\(j\)</span>th and <spanclass="math inline">\(k\)</span>th columns. Sylvester's theorem (alsoknown as DesnanotJacobi identity) expresses <spanclass="math inline">\(\Delta_{ij}^{lk}\)</span> in terms of <spanclass="math inline">\(\Delta_{rs}\)</span> <span class="math display">\[    \Delta_{ij}^{lk}=\Delta_{ij}\Delta_{lk}-\Delta_{ik}\Delta_{lj}.\tag{2.17}\]</span></p><p>In writing (2.17), we have replaced <span class="math inline">\(\detH\)</span> by unity on the LHS. Let <span class="math display">\[    \phi_{n}=\operatorname{tr}(E&#39;^{*}H&#39;E&#39;H&#39;^{-1})=\sum_{\alpha,\beta}^{}z_{\alpha}^{*}z_{\beta}H_{\alpha\beta}&#39;\Delta_{\alpha\beta}&#39;.\tag{2.18}\]</span></p><p>Separating the last row and last column and making use of (2.13) to(2.17), we get <span class="math display">\[    \phi_{n}=\lvert z_n \rvert ^{2}+\phi_{n-1}+\langle g^{*} |H^{-1}(E^{*}-z_n^{*}I)H(E-z_nI)H^{-1}|g \rangle , \tag{2.19}\]</span></p><p>where <span class="math display">\[    \langle g^{*}|B|g \rangle =\sum_{i,j}^{} g_{i}^{*}B_{ij}g_j.\tag{2.20}\]</span></p><p>Substituting (2.19) for <span class="math inline">\(n=N\)</span> in(2.12), we get <span class="math display">\[    \begin{aligned}        J&amp;=C\mathrm{e}^{-\lvert z_{N} \rvert ^{2}} \int_{}^{}\mathrm{e}^{-\phi_{N-1}} \prod_{1\leqslant i&lt;j\leqslant N-1}^{}\mathrm{d}H_{ij}^{r}\mathrm{d}H_{ij}^{i} \\        &amp;\times \int_{}^{} \exp [-\langle g^{*}|H^{-1}(E^{*}-z_{N}^{*}I)H(E-z_{N}I)H^{-1}|g \rangle ]\prod_{1\leqslanti\leqslant N-1}^{}  \mathrm{d}g_i^{r}\mathrm{d}g_i^{i}.    \end{aligned}\]</span></p><div class="note note-info">            <p><span class="math inline">\(B\in\mathcal{M}_{N}(\mathbb{C})\)</span>, <spanclass="math inline">\(B\)</span> is Hermitian. Then <spanclass="math display">\[    \int_{}^{} \exp [-\langle g^{*}| B |g \rangle ] \prod_{j=1}^{N}\mathrm{d}g_{j}^{r}\mathrm{d}g_{j}^{i}=\pi^{N}(\det B)^{-1}\]</span></p>          </div><p>The last integral is immediate and gives <spanclass="math display">\[    \pi^{N-1}\{\det[H^{-1}(E^{*}-z_{N}^{*}I)H(E-z_{N}I)H^{-1}]\}^{-1}=\pi^{N-1}\prod_{i=1}^{N-1}\lvert z_i-z_{N} \rvert ^{-2}. \tag{2.21}\]</span></p><p>The process can be repeated <span class="math inline">\(N\)</span>times and we finally get <span class="math display">\[    J=C\exp \left( -\sum_{i=1}^{N} \lvert z_i \rvert ^{2} \right)\prod_{1\leqslant i&lt;j\leqslant N}\lvert z_i-z_j \rvert ^{-2},\tag{2.22}\]</span></p><p>where <span class="math inline">\(C\)</span> is a new constant.</p><h3 id="the-joint-probability-density-for-g">The joint probabilitydensity for <span class="math inline">\(G\)</span></h3><p>We have proved the joint probability density for the eigenvalues of<span class="math inline">\(G\)</span> belonging to the Complex GinibreEnsemble is given by <span class="math display">\[    P_c(z_1,\cdots ,z_{N})=\mu(\mathrm{d}G)=C\exp \left( -\sum_{i=1}^{N}\lvert z_i \rvert ^{2} \right) \prod_{1\leqslant i&lt;j\leqslantN}\lvert z_i-z_j \rvert ^{2}, \tag{2.23}\]</span></p><p>where <span class="math inline">\(C\)</span> is the normalizationconstant. We now compute <span class="math inline">\(C\)</span>.</p><p>The probability that all the eigenvalues <spanclass="math inline">\(z_i\)</span> will lie outside a circle of radius<span class="math inline">\(\alpha\)</span> centered at <spanclass="math inline">\(z=0\)</span> is <span class="math display">\[    E_{N_c}(\alpha)=\int_{\lvert z_i \rvert \geqslant \alpha}^{}P_c(z_1,\cdots ,z_{N})\prod_{i}^{} \mathrm{d}x_i\mathrm{d}y_i.\]</span></p><p>By writing (Here we don't require $z_1 z_2 z_{N} $) <spanclass="math display">\[    \begin{aligned}        \prod_{i&lt;j}^{} \lvert z_i-z_j \rvert^{2}&amp;=\prod_{i&lt;j}^{} (z_i-z_j)(z_i^{*}-z_j^{*}) \\        &amp;=\det        \begin{bmatrix}        1 &amp; \cdots &amp; 1 \\        z_1 &amp; \cdots &amp; z_{N} \\        \vdots &amp; &amp; \vdots \\        z_1^{N-1} &amp; \cdots &amp; z_{N}^{N-1} \\        \end{bmatrix}        \det        \begin{bmatrix}        1 &amp; \cdots &amp; 1 \\        z_1^{*} &amp; \cdots &amp; z_{N}^{*} \\        \vdots &amp; &amp; \vdots \\        z_1^{*N-1} &amp; \cdots &amp; z_{N}^{*N-1} \\            \end{bmatrix} \\        &amp;=\det        \begin{bmatrix}        N &amp; \sum_{i}^{} z_i &amp; \cdots &amp; \sum_{i}^{} z_i^{N-1}\\        \sum_{i}^{} z_i^{*} &amp; \sum_{i}^{} z_i^{*}z_i &amp; \cdots&amp; \sum_{i}^{} z_i^{*}z_i^{N-1} \\        \vdots &amp; &amp; &amp; \vdots \\        \sum_{i}^{} z_i^{*N-1} &amp; \sum_{i}^{} z_i^{*N-1}z_i &amp;\cdots &amp; \sum_{i}^{} z_i^{*N-1}z_i^{N-1}        \end{bmatrix}    \end{aligned}\]</span></p><p>The integrand is symmetric in all the <spanclass="math inline">\(z_i\)</span>, we can replace the first row with<span class="math inline">\(1,z_1,z_1^{2},\cdots ,z_1^{N-1}\)</span> andmultiply the result by <span class="math inline">\(N\)</span>; <spanclass="math inline">\(z_1\)</span> can be eliminated from the other rowsby subtracting a suitable multiple of the first row. The resultingdeterminant is symmetric in the <span class="math inline">\(N-1\)</span>variables <span class="math inline">\(z_2,z_3,\cdots ,z_{N}\)</span>;therefore we replace the second row with <spanclass="math inline">\(z_2^{*},z_2^{*}z_2,\cdots,z_2^{*}z_2^{N-1}\)</span> and multiply the result by <spanclass="math inline">\(N-1\)</span>. The process can be repeated and weget <span class="math display">\[    \begin{aligned}        E_{N_c}(\alpha)&amp;=CN!\int_{\lvert z_i \rvert \geqslant\alpha}^{} \left\{ \prod_{i}^{} \mathrm{d}x_i\mathrm{d}y_i\right\}\exp\left( -\sum_{i}^{N} \lvert z_i \rvert ^{2} \right) \\        &amp;\times\det        \begin{bmatrix}        1 &amp; z_1 &amp; \cdots &amp; z_1^{N-1} \\        z_2^{*} &amp; z_2^{*}z_2 &amp; \cdots &amp; z_2^{*}z_2^{N-1} \\        \vdots &amp; &amp; &amp;\vdots \\        z_{N}^{*N-1} &amp; z_{N}^{*N-1}z_{N} &amp; \cdots &amp;z_{N}^{*N-1}z_{N}^{N-1} \\        \end{bmatrix}    \end{aligned}\]</span></p><p>By changing to polar coordinates and performing the angularintegrations first we see that <span class="math display">\[    \int_{\lvert z \rvert \geqslant \alpha}^{} \mathrm{e}^{-\lvert z\rvert ^{2}} z^{*j}z^{k} \mathrm{d}x\mathrm{d}y=\pi \delta_{jk}\Gamma(j+1,\alpha^{2}), \tag{2.24}\]</span></p><p>so that <span class="math display">\[    E_{N_c}(\alpha)=CN!\pi^{N}\prod_{j=1}^{N} \Gamma(j,\alpha^{2}),\tag{2.25}\]</span></p><p>where <span class="math inline">\(\Gamma(j,\alpha^{2})\)</span> isthe incomplete gamma function <span class="math display">\[    \Gamma(j,\alpha^{2})=\int_{\alpha^{2}}^{\infty} \mathrm{e}^{-x}x^{j-1} \mathrm{d}x=\Gamma(j)\mathrm{e}^{-\alpha^{2}} \sum_{l=0}^{j-1}\frac{\alpha^{2l}}{l!}. \tag{2.26}\]</span></p><p>Since <span class="math inline">\(E_{N_c}(0)=1\)</span>, the constant<span class="math inline">\(C\)</span> can be determined from (3.1.23)as <span class="math display">\[    C^{-1}=\pi^{N}\prod_{j=1}^{N} j!, \tag{2.27}\]</span></p><p>and therefore <span class="math display">\[    E_{N_c}(\alpha)=\prod_{j=1}^{N} \left( \mathrm{e}^{-\alpha^{2}}\sum_{l=0}^{j-1} \frac{\alpha^{2l}}{l!} \right). \tag{2.28}   \]</span></p><p>We will use the spectral law with symmetric functions of the form<span class="math display">\[    F(z_1,\cdots ,z_n)=\sum_{i_1,\cdots i_k\ \text{distinct}}^{}f(z_{i_1})\cdots f(z_{i_k}).\]</span></p><h2 id="k-points-correlations"><spanclass="math inline">\(k\)</span>-points correlations</h2><p>Let <span class="math inline">\(z\in \mathbb{C}\mapsto\gamma(z)=\pi^{-1}\mathrm{e}^{-\lvert z \rvert ^{2}}\)</span> be thedensity of the standard Gaussian <spanclass="math inline">\(\mathcal{N}(0,\frac{1}{2}I_2)\)</span> on <spanclass="math inline">\(\mathbb{C}\)</span>. For every <spanclass="math inline">\(1\leqslant k\leqslant n\)</span>, the <spanclass="math inline">\(k\)</span>-point correlation is <spanclass="math display">\[    \varphi_{n,k}(z_1,\cdots ,z_k):=\int_{\mathbb{C}^{n-k}}^{}\varphi_{n}(z_1,\cdots ,z_n)\mathrm{d}z_{k+1}^{r}\mathrm{d}z_{k+1}^{i}\cdots\mathrm{d}z_{n}^{r}\mathrm{d}z_n^{i}.  \tag{2.29}\]</span></p><div class="note note-info">            <p><strong>Theorem (Dyson, 1970)</strong> Let <spanclass="math inline">\(K(x,y)\)</span> be a function with complex values,such that <span class="math display">\[    \bar{K}(x,y)=K(y,x), \tag{2.30}\]</span></p><p>Assume that <span class="math display">\[    \int K(x,y)K(y,z) \mathrm{d}y=K(x,z), \tag{2.31}\]</span></p><p>Let <span class="math inline">\([K(z_i,z_j)]_{N}\)</span> denote the<span class="math inline">\(N\times N\)</span> matrix with its <spanclass="math inline">\((i,j)\)</span> element equal to <spanclass="math inline">\(K(z_i,z_j)\)</span>. Then <spanclass="math display">\[    \int_{}^{} \det[K(z_i,z_j)]_{N}\mathrm{d}z_{N}^{r}\mathrm{d}z_{N}^{i}=(c-N+1)\det[K(z_i,z_j)]_{N-1},\tag{2.32}\]</span></p><p>where <span class="math display">\[    c=\int_{}^{} K(z,z) \mathrm{d}z^{r}\mathrm{d}z^{i} \tag{2.33}\]</span></p><p>Note that (2.30) and (2.31) mean that the linear operator defined bythe kernel <span class="math inline">\(K(x,y)\)</span> is a projector,and the constant <span class="math inline">\(c\)</span> is its trace(hence a nonnegative integer).</p>          </div><p><strong>proof</strong> From the definition of the determinant, <spanclass="math display">\[    \det[K(z_i,z_j)]_{N}=\sum_{P}^{} \sigma(P)\prod_{1}^{l}(K(\xi,\eta)K(\eta,\zeta)\cdots K(\theta,\xi)), \tag{2.34}\]</span></p><p>where the permutation <span class="math inline">\(P\)</span> consistsof <span class="math inline">\(l\)</span> cycles of the form <spanclass="math inline">\((\xi \rightarrow \eta \rightarrow \zeta\rightarrow\cdots \rightarrow \theta\rightarrow \xi)\)</span>. Now theindex <span class="math inline">\(N\)</span> occurs somewhere. - <spanclass="math inline">\(N\)</span> forms a cycle by itself, and <spanclass="math inline">\(K(z_{N},z_{N})\)</span> gives on integration thescalar constant <span class="math inline">\(c\)</span>; the remainingfactor is by definition <spanclass="math inline">\(\det[K(z_i,z_j)]_{N-1}\)</span>; - <spanclass="math inline">\(N\)</span> occurs in a longer cycle andintegration on <span class="math inline">\(x_{N}\)</span> reduces thelength of this cycle by one. This can happen in <spanclass="math inline">\((N-1)\)</span> ways since the index <spanclass="math inline">\(N\)</span> can be inserted between any two indicesof the cyclic sequence <span class="math inline">\(1,2,\cdots,N-1\)</span>. Alos the resulting permutation over <spanclass="math inline">\(N-1\)</span> indices has an opposite sign. Theremaining expression is by definition <spanclass="math inline">\(\det[K(z_i,z_j)]_{N-1}\)</span>.</p><p>Adding the two contributions we get the result.</p><div class="note note-info">            <p><strong>Theorem (<span class="math inline">\(k\)</span>-pointscorrelations)</strong> For every <span class="math inline">\(1\leqslantk\leqslant n\)</span>, <span class="math display">\[    \varphi_{n,k}(z_1,\cdots ,z_k)=\frac{(n-k)!}{n!}\gamma(z_1)\cdots\gamma(z_k)\det [K(z_i,z_j)]_{1\leqslant i,j\leqslant k} \tag{2.35}\]</span></p><p>where <span class="math display">\[    K(z_i,z_j):=\sum_{l=0}^{n-1}\frac{(z_iz_j^{*})^{l}}{l!}=\sum_{l=0}^{n-1} H_l(z_i)H_l(z_j)^{*}\tag{2.36}\]</span></p><p>with <span class="math display">\[    H_{l}(z):=\frac{1}{\sqrt{l!}}z^{l}. \tag{2.37}\]</span></p><p>In particular, by taking <span class="math inline">\(k=n\)</span> weget <span class="math display">\[    \varphi_{n,n}=\varphi_{n}(z_1,\cdots,z_n)=\frac{1}{n!}\gamma(z_1)\cdots\gamma(z_n)\det[K(z_i,z_j)]_{1\leqslant i,j\leqslant n}.       \]</span></p>          </div><p><strong>proof</strong></p><p>It's easy to check the <span class="math inline">\(k=n\)</span>situation <span class="math display">\[    \varphi_{n}(z_1,\cdots ,z_n)=\frac{\pi^{-n}}{n!}\exp \left(-\sum_{i=1}^{n} \lvert z_i \rvert ^{2} \right)\det[K(z_i,z_j)]_{1\leqslant i,j\leqslant n}.\]</span></p><p>By the previous theorem, setting <spanclass="math inline">\(K(x,y)=\sum_{l=0}^{n-1} H_l(x)H_l(y)^{*}\)</span>,we get</p><p><span class="math display">\[    \int_{}^{} \mathrm{e}^{-\lvert y \rvert ^{2}} K(x,y)K(y,z)\mathrm{d}y^{r}\mathrm{d}y^{i}=\pi K(x,z)\]</span></p><p>and <span class="math display">\[    \int_{}^{} \mathrm{e}^{-\lvert z \rvert ^{2}} K(z,z)\mathrm{d}z^{r}\mathrm{d}z^{i}=n\pi\]</span></p><p>So we have <span class="math display">\[    \begin{aligned}        \varphi_{n,n-1}&amp;=\int_{\mathbb{C}}^{}\frac{\pi^{-n}}{n!}\exp \left( -\sum_{i=1}^{n} \lvert z_i \rvert ^{2}\right) \det[K(z_i,z_j)]_{1\leqslant i,j\leqslant n}\mathrm{d}z_n^{r}\mathrm{d}z_n^{i} \\        &amp;=\frac{\pi^{-n+1}}{n!}\exp \left( -\sum_{i=1}^{n-1} \lvertz_i \rvert ^{2} \right) \det[K(z_i,z_j)]_{1\leqslant i,j\leqslant n-1}\\    \end{aligned}\]</span></p><p>By induction, (2.35) holds.</p><h2 id="reference">Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Charles Bordenave and DjalilChafa, The circular law<a href="#fnref:1" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:2" class="footnote-text"><span>R. A. Horn and Ch. R.Johnson, Topics in matrix analysis, Cambridge University Press,Cambridge, 1994, Corrected reprint of the 1991 original.<a href="#fnref:2" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Madan Lal Mehta, Randommatrices, third ed. Pure and Applied Mathematics, vol.142, Acad. Press,2004. <a href="#fnref:3" rev="footnote" class="footnote-backref"></a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2023/01/19/%E5%87%A0%E4%B8%AA%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E7%9B%B8%E5%85%B3%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <url>/2023/01/19/%E5%87%A0%E4%B8%AA%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E7%9B%B8%E5%85%B3%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><h3 id="-n-1"> <spanclass="math inline">\(n-1\)</span> </h3><p> <span class="math inline">\(X\)</span> <span class="math inline">\(\mu\)</span> <spanclass="math inline">\(\sigma^{2}=E[(X-\mu)^{2}]\)</span>.</p><p> <span class="math display">\[    S^{2}=\frac{1}{n}\sum_{i=1}^{n} (X_i-\mu)^{2}\]</span></p><p> <span class="math inline">\(\sigma^{2}\)</span>.</p><p> <span class="math inline">\(\mu\)</span> <span class="math display">\[    \bar{X}=\frac{1}{n}\sum_{i=1}^{n} X_{i}.\]</span></p><p> <span class="math display">\[    S^{2}=\frac{1}{n-1}\sum_{i=1}^{n} (X_{i}-\bar{X})^{2}\]</span></p><p> <span class="math inline">\(\sigma^{2}\)</span>.</p><p> <span class="math display">\[    E[\frac{1}{n}\sum_{i=1}^{n} (X_{i}-\mu)^{2}]=\sigma^{2}\]</span></p><p><span class="math inline">\(S^{2}\)</span> <span class="math inline">\(\mu=\sigma^{2}\)</span>. .</p><p> <span class="math display">\[    \sum_{i=1}^{n} (X_{i}-\bar{X})^{2}= \sum_{i=1}^{n}(X_{i}-\mu)^{2}-n(\mu-\bar{X})^{2}\leqslant \sum_{i=1}^{n}(X_{i}-\mu)^{2}\]</span></p><p></p><p><span class="math display">\[    \tilde{S}^{2}=\frac{1}{n}\sum_{i=1}^{n} (X_{i}-\bar{X})^{2}\]</span></p><p> <span class="math display">\[    \begin{aligned}        E[\tilde{S}^{2}]&amp;=E[\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\bar{X})^{2}]=E[\frac{1}{n}\sum_{i=1}^{n}((X_{i}-\mu)-(\bar{X}-\mu))^{2}] \\        &amp;=E[\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\mu)^{2}-\frac{2}{n}(\bar{X}-\mu)\sum_{i=1}^{n}(X_{i}-\mu)+(\bar{X}-\mu)^{2}]\\        &amp;=\sigma^{2}-E[(\bar{X}-\mu)^{2}]    \end{aligned}\]</span></p><p> <span class="math display">\[    \begin{aligned}        E[(\bar{X}-\mu)^{2}]=E(\bar{X}-E[\bar{X}])^{2}&amp;=var(\bar{X})\\        &amp;=var\left(\frac{\sum_{i=1}^{n} X_{i}}{n}\right)\\        &amp;=\frac{1}{n^{2}}\sum_{i=1}^{n} var(X_{i}) \\        &amp;=\frac{\sigma^{2}}{n}    \end{aligned}\]</span></p><p> <span class="math display">\[    E[\tilde{S}^{2}]=\frac{n-1}{n}\sigma^{2}\]</span></p><p> <span class="math display">\[    S^{2}=\frac{1}{n-1}\sum_{i=1}^{n} (X_{i}-\bar{X})^{2}\]</span></p><p>.</p><h3id=""></h3><p> <span class="math inline">\(M\)</span>  <spanclass="math inline">\(N\)</span><span class="math display">\[    M=\mu_{M}+\Sigma_{M}\sqrt{1-\rho}X_1+\Sigma_{M}\sqrt{\rho}Y,\]</span></p><p><span class="math display">\[    N=\mu_{N}+\Sigma_{N}\sqrt{1-\rho}X_2+\Sigma_{M}\sqrt{\rho}Y.\]</span></p><p> <span class="math inline">\(X_1,X_2,Y\)</span> .</p><p><span class="math display">\[    E[M]=\mu_{M},E[N]=\mu_{N}.\]</span></p><p><span class="math display">\[    var(M)=\Sigma_{M}, var(N)=\Sigma_{N}.\]</span></p><p><span class="math display">\[    cov(M,N)=\rho\Sigma_{M}\Sigma_{N}.\]</span></p><p>.</p><h3 id=""></h3><p> <spanclass="math inline">\(\kappa(t)\)</span> <spanclass="math inline">\(\tilde{\kappa}(t)\)</span>  <spanclass="math display">\[    (1+\frac{\mathrm{d}}{\mathrm{d}t})\tilde{\kappa}(t)=\kappa(t)\]</span></p><p> <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}\tilde{\kappa}(t)=-\tilde{\kappa}(t)+\kappa(t)\]</span></p><p>.</p><h3 id=""></h3><p> Markov chain <span class="math inline">\(Z \rightarrowX_1,X_2\)</span> <span class="math display">\[    \mathbb{E}_{X_1}(X_1)=\mathbb{E}_{Z}(\mathbb{E}_{X_1|Z}(X_1)),  \]</span></p><p><span class="math display">\[    \operatorname{var}_{X_1}(X_1)=\mathbb{E}_{Z}(\operatorname{var}_{X_1|Z}(X_1))+\operatorname{var}_{Z}(\mathbb{E}_{X_1|Z}(X_1)),\]</span></p><p><span class="math display">\[    \operatorname{cov}_{X_1,X_2}(X_1,X_2)=\mathbb{E}_{Z}(\operatorname{cov}_{X_1,X_2|Z}(X_1,X_2))+\operatorname{cov}_{Z}(\mathbb{E}_{X_1|Z}(X_1),\mathbb{E}_{X_2|Z}(X_2)).\]</span></p><h2 id="pca-ica-cca-pls">PCA, ICA, CCA, PLS</h2><h3 id="pca-ica">PCA, ICA</h3><p>PCA  ICA .  PCAICA.</p><p>PCA . ICA. By the central limittheorem, any linear mixture of independent variables will be more"Gaussian" than the original variables. Thus, ICA seeks to create a newset of axes. The axes are oriented such that the projection of datapoints onto the axes is maximally non-Gaussian. We can use kurtois,negentropy, or mutual information to measure the non-Gaussianity.</p><p>Independent components can be interpreted as the dominant functionalnetworks or modes of activity that contribute to the observedneuroimagingdata<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="McIntosh, Anthony R., &amp; Bratislav Mii. Multivariate Statistical Analyses for Neuroimaging Data. Annual Review of Psychology 64, 1 (2013): 499525. https://doi.org/10.1146/annurev-psych-113011-143804.">[1]</span></a></sup>.</p><h4 id="cvpca">cvPCA</h4><p>cross-validated PCA is a method to derive unbiased estimation of the(major part) of the eigenspectrum of the (sampled) covariancematrix.</p><p>Assume we have several trials of neural activity recordings <spanclass="math inline">\(\mathbf{F}^{r}\in \mathbb{R}^{N_c\times N_s},r=1,2,\cdots ,p\)</span>. <spanclass="math inline">\(\mathbf{F}^{r}_{i,j} = f(c_i,s_j) +\nu_{r}(c_{i},s_{j})\)</span> is the activity of the <spanclass="math inline">\(i\)</span>-th neuron/ROI under the <spanclass="math inline">\(j\)</span>-th stimulus (e.g., position) in the<span class="math inline">\(r\)</span>-th trial. The 'signal' <spanclass="math inline">\(f(c_i,s_j)\)</span> is the expected response. The'noise' <span class="math inline">\(\nu_{r}(c_{i},s_{j})\)</span> is theresidual and is assumed to be i.i.d. across trials. It has zeroexpectation: <spanclass="math inline">\(\mathbb{E}_{\nu_{r}}[\nu_{r}(c,s)|c,s]=0\)</span>.</p><p>Theorem 1 and 2 in [SI, Stringer et al.,2019<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Stringer, Carsen, Marius Pachitariu, Nicholas Steinmetz, Matteo Carandini &amp; Kenneth D. Harris. High-Dimensional Geometry of Population Responses in Visual Cortex. Nature 571, 7765 (2019): 36165. https://doi.org/10.1038/s41586-019-1346-5.">[2]</span></a></sup>]tell us that how the cvPCA method is implemented. Note that therequirement in theorem 2 that one source of noise has dimensionsorthogonal to the signal dimensions cannot be satisfied in some cases,e.g., the sample correlation matrix is full rank. However, we often have<span class="math inline">\(N_c &gt; N_{s}\)</span>, so the samplecorrelation matrix is not full rank.</p><p>Let's now describe how cvPCA is implemented. Suppose we have atraining recording <spanclass="math inline">\(\mathbf{F}_{train}\)</span>. We also have anothertest/cross-validating recording <spanclass="math inline">\(\mathbf{F}_{test}\)</span>.</p><p>First, we can do PCA on the training recording <spanclass="math inline">\(\mathbf{F}_{train}\)</span>: <spanclass="math display">\[    \mathbf{F}_{train} = PC_{train} S_{train},\]</span></p><p>where <span class="math inline">\(PC_{train}\in \mathbb{R}^{N_c\timesN_{s}}, S_{train}\in \mathbb{R}^{N_{s}\times N_{s}}\)</span>. So thetrain scores are <span class="math display">\[    S_{train} = PC_{train}^{\mathsf{T}}\mathbf{F}_{train}.\]</span></p><p>Next, we project the test recording <spanclass="math inline">\(F_{test}\)</span> onto the principal components ofthe training recording to get the cross-validated test scores: <spanclass="math display">\[S_{test} = PC_{train}^{\mathsf{T}}\mathbf{F}_{test}.\]</span></p><p>The estimated <span class="math inline">\(n\)</span>-th eigenvalue ofthe covariance matrix is <span class="math display">\[    \lambda_{n} =PC^{\mathsf{T}}_{train,n}\mathbf{F}_{train}\mathbf{F}_{test}^{\mathsf{T}}PC_{train,n}^{\mathsf{T}}, n =1,2,\cdots ,N_{s}.\]</span></p><h3 id="canonical-correlation-analysis-cca">Canonical CorrelationAnalysis (CCA)</h3><p>The goal of CCA is to relate two sets of data, <spanclass="math inline">\(\mathbf{X}_{n \times p}\)</span> and <spanclass="math inline">\(\mathbf{Y}_{n \times q}\)</span>, with <spanclass="math inline">\(p\)</span> and <spanclass="math inline">\(q\)</span> variables in their respective columnsand <span class="math inline">\(n\)</span> observations in the row.</p><p>CVA (Friston et al. 1996, Strother et al. 2002), linear discriminantanalysis (LDA), and multivariate analysis of variance are special caseof CCA.</p><p>We want to create pairs of new variable that are linear combinationsof the original variables in <spanclass="math inline">\(\mathbf{X}(\mathbf{XU}(i)_{n \times 1})\)</span>and the variables in <spanclass="math inline">\(\mathbf{Y}(\mathbf{YV}(i)_{n \times 1})\)</span>and that have the maximum correlation with each other. In addition,pairs of canonical variates are mutually orthogonal with all the otherpairs. Factorize the adjusted correlation matrix <spanclass="math display">\[    (\mathbf{X}^{\mathsf{T}}\mathbf{X})^{-1/2}\mathbf{X}^{\mathsf{T}}\mathbf{Y}(\mathbf{Y}^{\mathsf{T}}\mathbf{Y})^{-1/2}=\mathbf{USV}^{\mathsf{T}}.\]</span></p><h2 id="reference">Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>McIntosh, Anthony R., &amp;Bratislav Mii. Multivariate Statistical Analyses for NeuroimagingData. Annual Review of Psychology 64, 1 (2013): 499525.https://doi.org/10.1146/annurev-psych-113011-143804.<a href="#fnref:1" rev="footnote" class="footnote-backref"></a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Stringer, Carsen, MariusPachitariu, Nicholas Steinmetz, Matteo Carandini &amp; Kenneth D.Harris. High-Dimensional Geometry of Population Responses in VisualCortex. Nature 571, 7765 (2019): 36165.https://doi.org/10.1038/s41586-019-1346-5.<a href="#fnref:2" rev="footnote" class="footnote-backref"></a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Statistics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/12/26/%E5%BA%9E%E5%8A%A0%E8%8E%B1%E5%AE%9A%E7%90%86%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/"/>
    <url>/2022/12/26/%E5%BA%9E%E5%8A%A0%E8%8E%B1%E5%AE%9A%E7%90%86%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><p>..De Rham..</p><h2 id=""></h2><p> <spanclass="math inline">\(\mathrm{d}\)</span> ..  <spanclass="math inline">\(\Omega^{p}(M)\)</span>  <spanclass="math inline">\(M\)</span>  <spanclass="math inline">\(p\)</span>  <spanclass="math inline">\(\Omega(M)\)</span></p><p><strong>1.1</strong>  <span class="math inline">\(\omega\in \Omega^{p}(M)\)</span>  <spanclass="math inline">\(\mathrm{d}\omega=0\)</span>.</p><p><strong>1.2</strong>  <span class="math inline">\(\omega\in \Omega^{p}(M)(p&gt;0)\)</span> <spanclass="math inline">\(\omega=\mathrm{d}\alpha\)</span>  <spanclass="math inline">\(\alpha \in \Omega^{p-1}(M)\)</span> .</p><p> <span class="math inline">\(M\)</span>  <spanclass="math inline">\(p\)</span>  <spanclass="math inline">\(Z^{p}(M)\)</span> <spanclass="math inline">\(M\)</span>  <spanclass="math inline">\(p\)</span>  <spanclass="math inline">\(B^{p}(M)\)</span>.</p><p> <span class="math inline">\(\mathrm{d}\)</span> <span class="math inline">\(\mathrm{d}^{2}=0\)</span>$B<sup>{p}(M)Z</sup>{p}(M) $. .</p><p> <spanclass="math inline">\(\omega\)</span>  <spanclass="math inline">\(\mathrm{d}\omega=0\)</span>  <spanclass="math inline">\(\mathrm{d}\alpha=\omega\)</span> . <span class="math inline">\(M\)</span>.</p><h2 id=""></h2><p></p><p><strong>2.1</strong>  <span class="math inline">\(M\)</span> <span class="math inline">\(x_0\inM\)</span> <spanclass="math inline">\(h\colon M\times I\rightarrow M\)</span><span class="math inline">\(I=[0,1]\)</span> <spanclass="math inline">\(h(x,1)=x\)</span><spanclass="math inline">\(h(x,0)=x_0\)</span>.</p><p><strong>2.2.</strong>  <spanclass="math inline">\(M\)</span>  <spanclass="math inline">\(p+1\)</span>  <spanclass="math inline">\((p\geqslant 0)\)</span> .</p><p> <span class="math inline">\(M\times I\)</span> <spanclass="math inline">\(M\)</span>  <spanclass="math inline">\(I\)</span>  <spanclass="math inline">\(j_{i}\colon M\rightarrow M\times I\)</span><spanclass="math inline">\(j_{i}(x)=(x,i),i=0,1\)</span> <spanclass="math inline">\(M\)</span>  <spanclass="math inline">\(M\times I\)</span> . <span class="math inline">\(j_{i}^{*}\colon\Omega^{p}(M\times I)\rightarrow \Omega^{p}(M)\)</span><span class="math inline">\(\Omega^{p}(M\times I)\)</span> <span class="math inline">\(t\)</span>  <spanclass="math inline">\(i\)</span> .  <spanclass="math inline">\(\mathrm{d}t\)</span>  <spanclass="math inline">\(0\)</span>.</p><h3 id=""></h3><p> <span class="math inline">\(K\colon\Omega^{p+1}(M\times I)\rightarrow\Omega^{p}(M)\)</span> <spanclass="math display">\[    K(a(x,t)\mathrm{d}x^{i_1}\wedge \cdots \wedge\mathrm{d}x^{i_{p+1}}):=0,\]</span></p><p><span class="math display">\[    K(a(x,t)\mathrm{d}t \wedge \mathrm{d}x^{i_1}\wedge \cdots\wedge\mathrm{d}x^{i_p}):=\left( \int_{0}^{1} a(x,t) \mathrm{d}r \right)\mathrm{d}x^{i_1}\wedge \cdots \wedge \mathrm{d}x^{i_p}.\]</span></p><p> <span class="math inline">\(K\)</span> <spanclass="math inline">\(\omega \in \Omega^{p+1}(M\timesI)\)</span> <span class="math display">\[    K(\mathrm{d}\omega)+\mathrm{d}(K\omega)=j_1^{*}\omega-j_0^{*}\omega.\tag{1}\]</span></p><p> <span class="math inline">\(K,d,j_1^{*},j_0^{*}\)</span>.  <spanclass="math inline">\(\omega=a(x,t)\mathrm{d}x^{i_1}\wedge\cdots \wedge\mathrm{d}x^{i_{p+1}}\)</span> <spanclass="math inline">\(K\omega=0\)</span><spanclass="math inline">\(\mathrm{d}(K\omega)=0\)</span> <spanclass="math display">\[    \mathrm{d}\omega=\frac{\partial a}{\partialt}\mathrm{d}t\wedge\mathrm{d}x^{i_1}\wedge\cdots\wedge\mathrm{d}x^{i_{p+1}}+\text{ $\mathrm{d}t$ },\]</span></p><p><span class="math display">\[\begin{aligned}    K(\mathrm{d}\omega)&amp;=\left( \int_{0}^{1} \frac{\partiala}{\partial t} \mathrm{d}t \right) \mathrm{d}x^{i_1} \wedge\cdots \wedge\mathrm{d}x^{i_{p+1}} \\    &amp;=(a(x,1)-a(x,0))\mathrm{d}x^{i_1}\wedge\cdots\wedge\mathrm{d}x^{i_{p+1}}=j_1^{*}(\omega)-j_0^{*}(\omega),\end{aligned}\]</span></p><p>1.</p><p> <span class="math inline">\(\omega=a(x,t)\mathrm{d}t\wedge\mathrm{d}x^{i_1}\wedge\cdots \wedge \mathrm{d}x^{i_p}\)</span><span class="math inline">\(j_1^{*}\omega=j_0^{*}\omega=0\)</span>. <span class="math display">\[\begin{aligned}    K(\mathrm{d}\omega)&amp;=K\left(-\sum_{i_0}^{} \frac{\partiala}{\partialx^{i_0}}\mathrm{d}t\wedge\mathrm{d}x^{i_0}\wedge\mathrm{d}x^{i_1}\wedge\cdots\wedge \mathrm{d}x^{i_p} \right)\\    &amp;=-\sum_{i_0}^{} \left( \int_{0}^{1} \frac{\partial a}{\partialx^{i_0}}\mathrm{d}x \right) \mathrm{d}x^{i_0}\wedge\mathrm{d}x^{i_1}\wedge\cdots \wedge \mathrm{d}x^{i_p},\end{aligned}\]</span></p><p><span class="math display">\[\begin{aligned}    \mathrm{d}(K\omega)&amp;=\mathrm{d}\left( \left( \int_{0}^{1} a(x,t)\mathrm{d}t \right) \mathrm{d}x^{i_1}\wedge \cdots \wedge\mathrm{d}x^{i_p} \right) \\    &amp;=\sum_{i_0}^{} \frac{\partial }{\partial x^{i_0}}\left(\int_{0}^{1} a(x,t) \mathrm{d}t \right) \mathrm{d}x^{i_0}\wedge\mathrm{d}x^{i_1}\wedge\cdots \wedge \mathrm{d}x^{i_p}\\    &amp;=\sum_{i_0}^{} \left( \int_{0}^{1} \frac{\partial a}{\partialx^{i_0}} \mathrm{d}t \right)\mathrm{d}x^{i_0}\wedge\mathrm{d}x^{i_1}\wedge\cdots \wedge\mathrm{d}x^{i_p}\end{aligned}\]</span></p><p>1.</p><p> <span class="math inline">\(M\)</span>  <spanclass="math inline">\(x_0 \in M\)</span> <spanclass="math inline">\(h\colon M\times I\rightarrow M\)</span>3<span class="math inline">\(\omega\)</span>  <spanclass="math inline">\(M\)</span>  <spanclass="math inline">\(p+1\)</span> .  <spanclass="math inline">\(h \circ j_1\colon M\rightarrow M\)</span><span class="math inline">\(h\circ j_0\colon M\rightarrowx_0\)</span>  <span class="math inline">\(M\)</span>  <spanclass="math inline">\(x_0\)</span>  <spanclass="math inline">\((j_1^{*}\circ h^{*})\omega=\omega\)</span><spanclass="math inline">\((j_0^{*}\circ h^{*})\omega=0\)</span>.1 <span class="math display">\[    K(\mathrm{d}(h^{*}\omega))+\mathrm{d}(K(h^{*}\omega))=\omega.\tag{2}   \]</span>  <span class="math inline">\(\omega\)</span>  <spanclass="math inline">\(M\)</span>  <spanclass="math inline">\(\mathrm{d}(h^{*}\omega)=h^{*}(\mathrm{d}\omega)=0\)</span>2<span class="math display">\[    \mathrm{d}(K(h^{*}\omega))=\omega.\]</span></p><p> <span class="math inline">\(\omega\)</span> <span class="math inline">\(\alpha=K(h^{*}\omega)\in\Omega^{p}(M)\)</span>  <spanclass="math inline">\(\omega\)</span>  <spanclass="math inline">\(M\)</span> </p><h3 id=""></h3><p> <span class="math inline">\(X\)</span>  <spanclass="math inline">\(\omega\)</span> .</p><p><strong>4.</strong>  <span class="math inline">\(X\)</span> <span class="math inline">\(M\)</span> <spanclass="math inline">\(\omega\)</span>  <spanclass="math inline">\(M\)</span>  <spanclass="math inline">\(k\)</span> .  <spanclass="math inline">\((i_{X}\omega)(X_1,\cdots,X_{k-1}):=\omega(X,X_1,\cdots ,X_{k-1})\)</span> <spanclass="math inline">\(X_1,\cdots ,X_{k-1}\)</span>  <spanclass="math inline">\(M\)</span>  <spanclass="math inline">\(k-1\)</span>  <spanclass="math inline">\(i_{X}\omega\)</span>  <spanclass="math inline">\(X\)</span>  <spanclass="math inline">\(\omega\)</span> .  <spanclass="math inline">\(0\)</span>  <spanclass="math inline">\(M\)</span>  <spanclass="math inline">\(i_{X}f=0\)</span>.</p><p></p><p><strong>1.</strong>  <span class="math inline">\(t\mapsto h_t\in C^{\infty}(M,N)\)</span>  $t I $ <span class="math inline">\(M\)</span>  <spanclass="math inline">\(N\)</span> .  <spanclass="math inline">\(\omega \in \Omega(N)\)</span><span class="math display">\[    \frac{\partial }{\partialt}(h_t^{*}\omega)(x)=\mathrm{d}h_t^{*}(i_{X}\omega)(x)+h_t^{*}(i_{X}\mathrm{d}\omega)(x), \tag{3}\]</span></p><p> <span class="math inline">\(x \in M\)</span><spanclass="math inline">\(X\)</span>  <spanclass="math inline">\(N\)</span>  <spanclass="math inline">\(X(x,t)\in TN_{h_t(x)}\)</span> <spanclass="math inline">\(t&#39;\mapsto h_{t&#39;}(x)\)</span><spanclass="math inline">\(X(x,t)\)</span>  <spanclass="math inline">\(t&#39;=t\)</span> .</p><p><strong></strong>  $^{n}U M $  <spanclass="math inline">\(x^{1},\cdots ,x^{n}\)</span>  <spanclass="math inline">\(\omega|_{U}\)</span>  <spanclass="math display">\[    \sum_{1\leqslant i_1&lt;\cdots &lt;i_{k}\leqslant n}^{} a_{i_1\cdotsi_k}(x)\mathrm{d}x^{i_1}\wedge\cdots \wedge\mathrm{d}x^{i_k}=\frac{1}{k!}a_{i_1\cdotsi_k}\mathrm{d}x^{i_1}\wedge\cdots \wedge \mathrm{d}x^{i_k},\]</span></p><p> <span class="math inline">\(X=X^{i}\frac{\partial }{\partialx^{i}}\)</span></p><p><span class="math display">\[\begin{aligned}    (i_{X}\omega)\left(\frac{\partial }{\partial x^{\alpha_2}},\cdots,\frac{\partial }{\partial x^{\alpha_{k}}}\right)&amp;=\omega \left(X^{i}\frac{\partial }{\partial x^{i}},\frac{\partial }{\partialx^{\alpha_2}},\cdots ,\frac{\partial }{\partial x^{\alpha_k}}\right)  \\    &amp;=X^{i}\omega \left( \frac{\partial }{\partialx^{i}},\frac{\partial }{\partial x^{\alpha_2}},\cdots ,\frac{\partial}{\partial x^{\alpha_k}} \right) \\    &amp;=\frac{1}{k!}X^{i}a_{i_1\cdotsi_k}\mathrm{d}x^{i_1}\wedge\cdots \wedge \mathrm{d}x^{i_k}\left(\frac{\partial }{\partial x^{i}},\frac{\partial }{\partialx^{\alpha_2}},\cdots ,\frac{\partial }{\partial x^{\alpha_k}} \right) \\    &amp;=\frac{1}{(k-1)!}X^{i}a_{ii_2\cdots i_k}\delta_{\alpha_2\cdots\alpha_k}^{i_2\cdots i_k},\end{aligned}\]</span></p><p> <span class="math display">\[    i_{X}\omega=\frac{1}{(k-1)!}X^{i}a_{ii_2\cdotsi_k}\mathrm{d}x^{i_2}\wedge \cdots \wedge \mathrm{d}x^{i_k}.\]</span></p><p> <span class="math inline">\(\omega\)</span> 3. <span class="math inline">\(\omega\)</span>  <spanclass="math inline">\(N\)</span>  <spanclass="math inline">\(\omega=f\)</span> <spanclass="math inline">\(i_{X}f=0\)</span> <spanclass="math display">\[    \frac{\partial }{\partial t}(h_t^{*}\omega)=\frac{\partial}{\partial t}(h_t^{*}f)=\frac{\partial f}{\partial y^{j}}\frac{\partialh_t^{j}}{\partial t}=X^{j}\frac{\partial f}{\partialy^{j}}=h_t^{*}(i_{X}\frac{\partial f}{\partialy^{j}}\mathrm{d}y^{j})=h_t^*(i_{X}\mathrm{d}\omega) \tag{4}\]</span></p><p> <span class="math inline">\(\omega=f\)</span>3.</p><p> <spanclass="math inline">\(\omega=a(y)\mathrm{d}y^{j_1}\wedge\cdots \wedge\mathrm{d}y^{j_k}\)</span><spanclass="math inline">\(X(x,t)=X^{j}(x,t)\frac{\partial }{\partialy^{j}}=\frac{\partial h_t^{j}}{\partial y^{j}}\frac{\partial }{\partialy^{j}}\)</span>.  <span class="math display">\[    i_{X}\omega=\sum_{p=1}^{k} (-1)^{p-1}X^{j_p}a(y)\mathrm{d}y^{j_1}\wedge\cdots \wedge\widehat{\mathrm{d}y^{j_p}}\wedge\cdots \wedge\mathrm{d}y^{j_k}\]</span></p><p> <span class="math display">\[    \begin{aligned}        \frac{\partial }{\partial t}(h_t^{*}\omega)&amp;=\frac{\partial}{\partial t}\left( a\circ h_t \frac{\partial y^{j_1}}{\partialx^{i_1}}\cdots \frac{\partial y^{j_k}}{\partialx^{i_k}}\mathrm{d}x^{i_1}\wedge\cdots \wedge\mathrm{d}x^{i_k}\right)  \\        &amp;=\frac{\partial a}{\partial y^{j_0}} \frac{\partialh_t^{j_0}}{\partial t}\frac{\partial y^{j_1}}{\partial x^{i_1}}\cdots\frac{\partial y^{j_k}}{\partial x^{i_k}}\mathrm{d}x^{i_1}\wedge\cdots\wedge \mathrm{d}x^{i_k}        \end{aligned}\]</span> (5)</p><p> <spanclass="math inline">\(\mathrm{d}h_t^{*}(i_{X}\omega)=h_t^{*}(\mathrm{d}i_{X}\omega)\)</span><spanclass="math inline">\(=h_t^{*}(\mathrm{d}i_{X}\omega+i_{X}\mathrm{d}\omega)\)</span><span class="math display">\[    \begin{aligned}    i_{X}\mathrm{d}\omega&amp;=i_{X}\left( \frac{\partial a}{\partialy^{j_0}}\mathrm{d}y^{j_0}\wedge\mathrm{d}y^{j_1}\wedge\cdots \wedge\mathrm{d}y^{j_k} \right)  \\    &amp;=\sum_{p=0}^{k} (-1)^{p}X^{j_p}\frac{\partial a}{\partialy^{j_0}}\mathrm{d}y^{i_0}\wedge\cdots \wedge\widehat{\mathrm{d}y^{j_p}}\wedge\cdots \wedge \mathrm{d}y^{j_k}\\    &amp;=\sum_{p=0}^{k} (-1)^{p}\frac{\partial h_t^{j_p}}{\partialy^{j_p}}\frac{\partial a}{\partial y^{j_0}}\mathrm{d}y^{i_0}\wedge\cdots\wedge \widehat{\mathrm{d}y^{j_p}}\wedge\cdots \wedge \mathrm{d}y^{j_k}    \end{aligned}\]</span> (6)</p><p><span class="math display">\[    \begin{aligned}        \mathrm{d}i_{X}\omega&amp;=\mathrm{d}\left( \sum_{p=1}^{k}(-1)^{p-1} X^{j_p}a(y)\mathrm{d}y^{j_1}\wedge \cdots \wedge\widehat{\mathrm{d}y^{j_p}}\wedge\cdots \wedge\mathrm{d}y^{j_k}\right)  \\        &amp;=\mathrm{d}\left( \sum_{p=1}^{k} (-1)^{p-1}\frac{\partialh_t^{j_p}}{\partial t}a(y)\mathrm{d}y^{j_1}\wedge \cdots \wedge\widehat{\mathrm{d}y^{j_p}}\wedge\cdots \wedge\mathrm{d}y^{j_k} \right)\\        &amp;=\sum_{p=1}^{k} (-1)^{p-1}\frac{\partialh_t^{j_p}}{\partial t}\frac{\partial a}{\partialy^{j_0}}\mathrm{d}y^{j_0}\wedge\mathrm{d}y^{j_1}\wedge \cdots \wedge\widehat{\mathrm{d}y^{j_p}}\wedge\cdots \wedge\mathrm{d}y^{j_k}    \end{aligned}\]</span> (7)</p><p> <span class="math display">\[    \begin{aligned}        h_t^{*}(\mathrm{d}i_{X}\omega+i_{X}\mathrm{d}\omega)&amp;=h_t^{*}\left( \frac{\partial h_t^{j_0}}{\partial t}\frac{\partial a}{\partialy^{j_0}}\mathrm{d}y^{j_1}\wedge\cdots \wedge \mathrm{d}y^{j_k}\right) \\        &amp;=\frac{\partial a}{\partial y^{j_0}} \frac{\partialh_t^{j_0}}{\partial t}\frac{\partial y^{j_1}}{\partial x^{i_1}}\cdots\frac{\partial y^{j_k}}{\partial x^{i_k}}\mathrm{d}x^{i_1}\wedge\cdots\wedge \mathrm{d}x^{i_k}    \end{aligned}\]</span> (8)</p><p>583.  <spanclass="math inline">\(\omega\)</span>3.</p><p> <span class="math inline">\(M=N\)</span>. <span class="math inline">\(M\)</span>  <spanclass="math inline">\(\omega\)</span> <span class="math display">\[    \frac{\partial }{\partialt}(h_t^{*}\omega)(x)=\mathrm{d}h_t^{*}(i_{X}\omega)(x). \tag{9}\]</span></p><p> <span class="math inline">\(M\)</span>  <spanclass="math inline">\(x_0\)</span> <spanclass="math inline">\(t \in [0,1]\)</span>  <spanclass="math inline">\(M\)</span>  <spanclass="math inline">\(M\)</span>  <spanclass="math inline">\(x \in M\)</span> <spanclass="math inline">\(h_1(x)=x\)</span><spanclass="math inline">\(h_0(x)=x_0\)</span>. 9 <spanclass="math inline">\(t\)</span> <spanclass="math inline">\(0\)</span><spanclass="math inline">\(1\)</span> <span class="math display">\[    \omega=h_1^{*}(\omega)-h_0^{*}(\omega)=\int_{0}^{1} \frac{\partial}{\partial t}(h_t^{*}\omega) \mathrm{d}t=\int_{0}^{1}\mathrm{d}h_t^{*}(i_{X}\omega) \mathrm{d}t=\mathrm{d} \int_{0}^{1}h_t^{*}(i_{X}\omega) \mathrm{d}t. \tag{10}\]</span></p><p> <span class="math inline">\(\omega\)</span>  <spanclass="math inline">\(M\)</span> .</p><p>.. <span class="math inline">\(\mathbb{R}^{2}\backslash0\)</span>  <span class="math inline">\(\omega=\displaystyle\frac{-y\mathrm{d}x+x\mathrm{d}y}{x^{2}+y^{2}}\)</span>.</p><h2 id="de-rham-">De Rham </h2><p><strong>5.</strong>  <span class="math display">\[    H^{p}(M):=Z^{p}(M)/ B^{p}(M) \tag{11}\]</span></p><p> <span class="math inline">\(M\)</span>  <spanclass="math inline">\(p\)</span> .</p><p>De Rham.  <spanclass="math inline">\(\omega_1,\omega_2 \in Z^{p}(M)\)</span>  <spanclass="math inline">\(\omega_1-\omega_2 \inB^{p}(M)\)</span>. <span class="math inline">\([\omega]\)</span>  <spanclass="math inline">\(\omega \in Z^{p}(M)\)</span> . <span class="math inline">\(Z^{p}(M)\)</span>  <spanclass="math inline">\(\mathrm{d}^{p}\colon \Omega^{p}(M)\rightarrow\Omega^{p+1}(M)\)</span>  <spanclass="math inline">\(B^{p}(M)\)</span>  <spanclass="math inline">\(\mathrm{d}^{p-1}\colon \Omega^{p-1}(M)\rightarrow\Omega^{p}(M)\)</span> 11 <spanclass="math display">\[    H^{p}(M)=\text{Ker}\ \mathrm{d}^{p}/\text{Im}\ \mathrm{d}^{p-1}.\]</span></p><p> <span class="math inline">\(M\)</span> <spanclass="math inline">\(H^{p}(M)\)</span>  <spanclass="math inline">\(M\)</span>  <spanclass="math inline">\(p\)</span> .</p><p><strong></strong> 13.1. 2.3 <span class="math inline">\(f\colonM\rightarrow \mathbb{R}\)</span>  <spanclass="math inline">\(\mathrm{d}f=0\)</span> <spanclass="math inline">\(f\)</span> .</p><p> <spanclass="math inline">\(h\colon M\times I\rightarrow M\)</span> <span class="math inline">\(t \in I\)</span> <span class="math inline">\(h_t\colon M\rightarrow M\)</span><span class="math inline">\(M\)</span>  <spanclass="math inline">\(\omega\)</span> <span class="math display">\[    h_{t}^{*}(\omega)=\mathrm{d}\int_{0}^{t} h_t^{*}(i_{X}\omega)\mathrm{d}t\]</span>  <span class="math inline">\(h_t^{*}\omega(t\in I)\)</span> ..</p><p> <span class="math inline">\(K\)</span> <spanclass="math inline">\(M\)</span>  <spanclass="math inline">\(p\)</span> <spanclass="math inline">\(H^{p}(K\times M)=H^{p}(M)\)</span> .</p><p><strong></strong>  <span class="math inline">\(K\)</span> <spanclass="math inline">\(M\)</span> <spanclass="math inline">\(t \in I=[0,1]\)</span>  <spanclass="math inline">\(M\times K\)</span>  <spanclass="math inline">\(x \in M,p \in K\)</span> <spanclass="math display">\[    h_1(x,p)=(x,p),\ h_0(x,p)=(x,p_0)\]</span></p><p> <span class="math inline">\(M\times K\)</span> <span class="math inline">\(\omega\)</span> <spanclass="math inline">\(h_1^{*}\omega\)</span>  <spanclass="math inline">\(h_0^{*}\omega\)</span> <span class="math inline">\(H^{p}(K\times M)=H^{p}(M\times{p_0})\)</span>.  <spanclass="math inline">\(H^{p}(M\times{p_0})=H^{p}(M)\)</span><span class="math inline">\(H^{p}(K\times M)=H^{p}(M)\)</span> .</p><p><span class="math inline">\(M\times K\)</span>  <spanclass="math inline">\(M\)</span> . De Rham..</p><p>De Rham DeRham . Re RhamReRham .</p><p> <span class="math inline">\(\varphi\colon M\rightarrowN\)</span>  <span class="math display">\[    \varphi^{*}\colon H^{p}(N)\rightarrow H^{p}(M)\]</span> .</p><p><strong>6.</strong> <span class="math inline">\([\omega]\inH^{k}(M)\)</span>  <span class="math inline">\([\eta]\inH^{l}(M)\)</span>  <span class="math display">\[    [\omega]\cup [\eta]:=[\omega \wedge \eta] \in H^{k+l}(M).    \]</span></p><p>.  <span class="math inline">\(\omega \inZ^{k}(M)\)</span><span class="math inline">\(\eta \inZ^{l}(M)\)</span> <span class="math display">\[    \mathrm{d}(\omega\wedge \eta)=\mathrm{d}\omega\wedge\eta+(-1)^{k}\omega \wedge \mathrm{d}\eta=0,\]</span>  <span class="math inline">\(\omega\wedge \eta \inZ^{k+l}(M)\)</span>.  <span class="math inline">\(\xi_1\in \Omega^{k-1}(M)\)</span>  <span class="math inline">\(\xi_2 \in\Omega^{l-1}(M)\)</span> <span class="math display">\[    (\omega+\mathrm{d}\xi_1)\wedge (\eta+\mathrm{d}\xi_2)=\omega\wedge\eta +\mathrm{d}[(-1)^{k}\omega\wedge\xi_2+(-1)^{k-1}\xi_1 \wedge\eta+(-1)^{k-1}\xi_1\wedge\xi_2].\]</span> .</p><p> <span class="math inline">\(\varphi \colon M\rightarrowN\)</span> .  <spanclass="math inline">\(\mathrm{d}\varphi^{*}=\varphi^{*}\mathrm{d}\)</span> <span class="math display">\[    \varphi^{*}(Z^{k}(N))\subset Z^{k}(M), \quad\varphi^{*}(B^{k}(N))\subset B^{k}(M).\]</span>  <span class="math inline">\(\varphi^{*}\colon\Omega^{k}(N)\rightarrow \Omega^{k}(M)\)</span> <span class="math inline">\(\varphi^{*}\colonH^{k}(N)\rightarrow H^{k}(M)\)</span> <span class="math display">\[    \varphi^{*}([\omega]):=[\varphi^{*}\omega].\]</span>  <span class="math inline">\(\varphi^{*}\)</span>.  - <span class="math inline">\((\psi\circ\varphi)^{*}=\varphi^{*}\circ \psi^{*}\)</span>. - <spanclass="math inline">\(\text{Id}^{*}=\text{Id}\)</span>.</p><p> <spanclass="math inline">\(\psi=\varphi^{-1}\)</span>3.4.</p><h2 id=""></h2><p>Cartan..De RhamStokes .</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hartman-Grobman</title>
    <link href="/2022/12/12/%E6%B5%85%E8%B0%88Hartman-Grobman%E5%AE%9A%E7%90%86/"/>
    <url>/2022/12/12/%E6%B5%85%E8%B0%88Hartman-Grobman%E5%AE%9A%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="hartman-grobman">Hartman-Grobman</h1><p>ODE..Hartman-Grobman..</p><h2 id="banach">Banach</h2><p>..</p><p> <span class="math inline">\(\mathbb{R}\)</span> <span class="math inline">\(\mathbb{C}\)</span>  <spanclass="math inline">\(E\)</span>  <spanclass="math inline">\(A:E \to E\)</span> <spanclass="math inline">\(A\)</span> 1 <spanclass="math inline">\(E\)</span>  <spanclass="math inline">\(E^{s}\)</span>  <spanclass="math inline">\(E^{u}\)</span>  - <spanclass="math inline">\(E=E^{s}\oplus E^{u}\)</span>; - <spanclass="math inline">\(A^{n}x\to 0\)</span> as <spanclass="math inline">\(n\to +\infty\)</span> for all <spanclass="math inline">\(x \in E^{s}\)</span>; - <spanclass="math inline">\(A^{-n}x \to 0\)</span> as <spanclass="math inline">\(n\to +\infty\)</span> for all <spanclass="math inline">\(x \in E^{u}\)</span>.</p><p> <span class="math inline">\(E^{s}\)</span>  <spanclass="math inline">\(A\)</span>  <spanclass="math inline">\(E^{u}\)</span>  <spanclass="math inline">\(A\)</span> .Banach.### </p><p>Banach.</p><p><strong>1.1.1</strong>  <span class="math inline">\(E\)</span>Banach.  <span class="math inline">\(T \in\mathscr{L}(E)\)</span>  <span class="math display">\[    \sigma(T)=\{ \lambda \in \mathbb{C}\colon \lambda I-T\text{}\}.\]</span></p><p> <span class="math inline">\(T\)</span>  <spanclass="math display">\[    \rho(T)=\mathbb{C} \backslash \sigma (T).\]</span></p><p> <span class="math inline">\(\lambda \in \rho(T)\)</span><span class="math inline">\(T\)</span>  <spanclass="math inline">\(\lambda\)</span>  <spanclass="math display">\[    R_{\lambda}(T)=(\lambda I-T)^{-1}.\]</span></p><p><strong>1.1.2.</strong>  <span class="math inline">\(E\)</span>Banach<span class="math inline">\(A,B\colon E \toE\)</span>  - <span class="math inline">\(\sigma(A^{-1})=\{1/\lambda\colon \lambda \in \sigma(A)\}\)</span>; - <spanclass="math inline">\(\sigma(BAB ^{-1})=\sigma(A)\)</span>.</p><p><strong></strong>  <span class="math inline">\(A\)</span></p><h3 id=""></h3><p> <span class="math inline">\(E\)</span> <spanclass="math display">\[    E_{\mathbb{C}}=\{x+iy\colon x,y \in E\}.\]</span></p><p> <span class="math inline">\(E_{\mathbb{C}}\)</span> <span class="math inline">\(x+iy,u+iv \inE_{\mathbb{C}}\)</span> <span class="math display">\[    (x+iy)+(u+iv)=(x+u)+i(y+v).\]</span></p><p> <span class="math inline">\(\alpha +i \beta \in\mathbb{C}\)</span>  <span class="math inline">\(x+iy \inE_{\mathbb{C}}\)</span> <span class="math display">\[    (\alpha+i\beta)(x+iy)=(\alpha x-\beta y)+i(\alpha y+\beta x).\]</span></p><p> <span class="math inline">\(E_{\mathbb{C}}\)</span> <span class="math inline">\(E\)</span>.</p><p> <span class="math inline">\(E_{\mathbb{C}}\)</span> <span class="math inline">\(E\)</span>Banach<span class="math inline">\(E_{\mathbb{C}}\)</span>Banach. A.E.Taylor<span class="math inline">\(x+iy \in E_{\mathbb{C}}\)</span> <spanclass="math display">\[    \left\| x+iy \right\|_{\mathbb{C}}=\sup_{\theta \in [0,2\pi]}\left\|x\cos \theta- y\sin \theta \right\|_{}\]</span></p><p><span class="math inline">\(\left\| \cdot\right\|_{\mathbb{C}}\)</span>  <spanclass="math inline">\(E_{\mathbb{C}}\)</span> <span class="math inline">\(x,y \in E\)</span> <spanclass="math display">\[    \max \{\left\| x \right\|_{},\left\| y \right\|_{}\}\leqslant\left\| x+iy \right\|_{\mathbb{C}}\leqslant \left\| x\right\|_{}+\left\| y \right\|_{}. \tag{2}\]</span></p><p> <span class="math inline">\(E\)</span>.</p><p><strong>1.2.1.</strong>  <spanclass="math inline">\(E\)</span>  <spanclass="math inline">\(\left\| \cdot \right\|_{}\)</span>Banach <span class="math inline">\(E_{\mathbb{C}}\)</span> <span class="math inline">\(\left\| \cdot\right\|_{\mathbb{C}}\)</span> Banach.</p><p><strong></strong>  <spanclass="math inline">\(E_{\mathbb{C}}\)</span> Cauchy <spanclass="math inline">\(\{x_{n}+iy_{n}\}\)</span>2 <spanclass="math inline">\(\{x_n\}\)</span>  <spanclass="math inline">\(\{y_n\}\)</span>  <spanclass="math inline">\(E\)</span> Cauchy.  <spanclass="math inline">\(E\)</span>  <spanclass="math inline">\(x \in E\)</span><span class="math inline">\(y\in E\)</span> <span class="math inline">\(x_n \tox\)</span><span class="math inline">\(y_n\to y\)</span>.  <spanclass="math inline">\(x_n+iy_n \to x+iy\)</span> <spanclass="math inline">\(E_{\mathbb{C}}\)</span> .</p><p>.  <spanclass="math inline">\(E\)</span>  <spanclass="math inline">\(T\colon E \to E\)</span> <spanclass="math inline">\(T\)</span>  <span class="math display">\[    \begin{aligned}        T_{\mathbb{C}}\colon E_{\mathbb{C}} &amp;\to E_{\mathbb{C}}\\        x+iy&amp;\mapsto Tx+iTy.    \end{aligned}\]</span>  <span class="math inline">\(T_{\mathbb{C}}\)</span> <span class="math inline">\(E_{\mathbb{C}}\)</span> ..</p><p><strong>1.2.2.</strong>  <span class="math inline">\(E\)</span>Banach.  <span class="math inline">\(T\colon E \toE\)</span>  <spanclass="math inline">\(T_{\mathbb{C}}\colon E_{\mathbb{C}} \toE_{\mathbb{C}}\)</span>  <spanclass="math inline">\(\left\| T_{\mathbb{C}}\right\|_{\mathbb{C}}=\left\| T \right\|_{}\)</span>.</p><p><strong></strong>  <span class="math inline">\(x+iy \inE_{\mathbb{C}}\)</span> <span class="math display">\[    \begin{aligned}        \left\| T_{\mathbb{C}}(x+iy) \right\|_{\mathbb{C}} &amp;=\left\| Tx+iTy \right\|_{\mathbb{C}}\\        &amp;=\sup_{\theta \in [0,2\pi]} \left\| T_x \cos \theta-Ty \sin\theta \right\|_{} \\        &amp;=\sup_{\theta \in [0,2\pi]} \left\| T(x \cos \theta-y\sint\eta) \right\|_{} \\        &amp; \leqslant  \left\| T \right\|_{} \sup_{\theta \in[0,2\pi]} \left\| x\cos \theta-y\sin \theta \right\|_{} \\        &amp;= \left\| T \right\|_{} \left\| x+iy \right\|_{\mathbb{C}},    \end{aligned}\]</span></p><p> <span class="math inline">\(\left\| T_{\mathbb{C}}\right\|_{\mathbb{C}}\leqslant \left\| T \right\|_{}\)</span>. <span class="math inline">\(x \in E\)</span> <span class="math inline">\(\left\| x\right\|_{\mathbb{C}}=\left\| x \right\|_{}\)</span> <spanclass="math display">\[    \left\| T_{\mathbb{C}} \right\|_{\mathbb{C}}\geqslant \sup _{x\neq0} \frac{\left\| T_{\mathbb{C}}(x+i 0) \right\|_{\mathbb{C}}}{\left\|x+i0 \right\|_{\mathbb{C}}}=\sup_{x\neq 0} \frac{\left\| Tx\right\|_{\mathbb{C}}}{\left\| x \right\|_{\mathbb{C}}}=\sup_{x\neq0}\frac{\left\| Tx \right\|_{}}{\left\| x \right\|_{}}=\left\| T\right\|_{},\]</span></p><p>.</p><p><strong>1.2.3.</strong>  <span class="math inline">\(E\)</span>Banach.  <span class="math inline">\(\mathscr{L}(E)\ni T\mapsto T_{\mathbb{C}} \in \mathscr{L}(E_{\mathbb{C}})\)</span>.</p><p> <span class="math inline">\(E\)</span> <span class="math inline">\(T\colon E\toE\)</span> <span class="math inline">\(\sigma(T)\)</span> <span class="math display">\[    \sigma(T)=\sigma(T_{\mathbb{C}}).\]</span></p><h3 id=""></h3><p>Banach.</p><p><strong>1.3.1.</strong> <spanclass="math inline">\(E\)</span> Banach<spanclass="math inline">\(T \in \mathscr{L}(E)\)</span>.  <spanclass="math inline">\(\sigma(T)=\sigma_1 \cup \sigma_2\)</span><span class="math inline">\(\sigma_1,\sigma_2\)</span> <spanclass="math inline">\(E_1,E_2\)</span>  - <spanclass="math inline">\(E=E_1 \oplus E_2\)</span> - <spanclass="math inline">\(T(E_1) \subset E_1\)</span>  $T(E_2) E_2 $ -<span class="math inline">\(\sigma(T|_{E_1})=\sigma_1\)</span>  <spanclass="math inline">\(\sigma(T|_{E_2})=\sigma_2\)</span>.</p><p><strong>1.3.2</strong>  <span class="math inline">\(E\)</span>Banach <span class="math inline">\(\left\| \cdot\right\|_{}\)</span>.  <span class="math inline">\(T \in\mathscr{L}(E)\)</span> - <spanclass="math inline">\(T\)</span> 1 -  <spanclass="math inline">\(\left\| \cdot \right\|_{}\)</span>  $$ <span class="math inline">\(\lvert T \rvert &lt;1\)</span> - <span class="math inline">\(C&gt;0\)</span>  <spanclass="math inline">\(0&lt;\lambda&lt;1\)</span>  <spanclass="math inline">\(x \in E\)</span><span class="math inline">\(n\in \mathbb{N}\)</span> <span class="math inline">\(\left\| T^{n}x\right\|_{}\leqslant C \lambda^{n}\left\| x \right\|_{}\)</span>.</p><h3 id=""></h3><p> <span class="math inline">\(E\)</span> <spanclass="math inline">\(GL(E)\)</span>  <spanclass="math inline">\(\mathscr{L}(E)\)</span> . <span class="math inline">\(E\)</span>Banach <span class="math inline">\(A \inGL(E)\)</span> <span class="math inline">\(A ^{-1} \inGL(E)\)</span>.</p><p><strong>1.4.1.</strong>  <span class="math inline">\(A\in GL(E)\)</span>  $S^{1} $ <spanclass="math inline">\(A\)</span> .  <spanclass="math inline">\(H(E)=\{A \in GL(E)\colon A\text{}\}\)</span>.</p><p><strong>1.4.2.</strong>  <span class="math inline">\(E\)</span>Banach <span class="math inline">\(H(E)\)</span>  <spanclass="math inline">\(GL(E)\)</span> .</p><p><strong></strong>  <span class="math inline">\(E\)</span>Banach.  <span class="math inline">\(A \inH(E)\)</span> $S^{1} (A) $.  <span class="math display">\[    M=\max_{\lambda \in  S^{1}} \left\| (\lambda I-A)^{-1} \right\|_{}\]</span></p><p> <span class="math inline">\(B \in GL(E)\)</span><spanclass="math inline">\(\left\| B-A \right\|_{}&lt;M ^{-1}\)</span> <span class="math inline">\(\lambda \in S^{1}\)</span><span class="math display">\[    \lambda I-B=(\lambda I-A)[I+(\lambda I-A)^{-1}(A-B)].\]</span></p><p> <span class="math inline">\(\left\| (\lambda I-A) ^{-1}(A-B)\right\|_{}&lt;1\)</span> <span class="math inline">\(\lambdaI-B\)</span> .  <span class="math inline">\(B \inH(E)\)</span> <span class="math inline">\(H(E)\)</span>  <spanclass="math inline">\(GL(E)\)</span> .</p><p> <span class="math inline">\(E\)</span> .<spanclass="math inline">\(\sigma(A)=\sigma(A_{\mathbb{C}})\)</span><span class="math inline">\(A \in GL(E)\)</span>  <spanclass="math inline">\(A_{\mathbb{C}}\in GL(E_{\mathbb{C}})\)</span>.  <span class="math inline">\(A \mapstoA_{\mathbb{C}}\)</span>  <spanclass="math inline">\(H(E_{\mathbb{C}})\)</span>  <spanclass="math inline">\(GL(E_{\mathbb{C}})\)</span>  <spanclass="math inline">\(H(E)\)</span>  <spanclass="math inline">\(GL(E)\)</span> .</p><p><strong>1.4.3.</strong>  <span class="math inline">\(A \inGL(E)\)</span>  <span class="math display">\[    E^{s}=\{x \in E \colon \lim_{n \to +\infty}A^{n}x=0\}\]</span></p><p><span class="math inline">\(A\)</span>  <spanclass="math display">\[    E^{u}=\{x \in E \colon \lim_{n \to +\infty} A^{-n}x=0\}.\]</span></p><p> <span class="math inline">\(E^{s}\)</span>  <spanclass="math inline">\(E^{u}\)</span>  <spanclass="math inline">\(E\)</span>  <spanclass="math display">\[    A(E^{s})=E^{s}\ \text{} \ A(E^{u})=E^{u}.\]</span></p><p> <span class="math inline">\(C\geqslant 1\)</span> <span class="math inline">\(0&lt;\lambda&lt;1\)</span> <spanclass="math display">\[    \left\| A^{n}x \right\|_{}\leqslant C \lambda^{n}\left\| x\right\|_{}, \quad \forall x \in E^{s}, n\geqslant 0,\]</span></p><p><span class="math display">\[    \left\| A^{-n}x \right\|_{}\leqslant C \lambda^{n}\left\| x\right\|_{}, \quad \forall x \in E^{u}, n\geqslant 0.\]</span></p><p> <span class="math inline">\(E^{s}\)</span>  <spanclass="math inline">\(A\)</span>  <spanclass="math inline">\(\left\| \cdot\right\|_{}\)</span><spanclass="math inline">\(E^{u}\)</span>  <spanclass="math inline">\(A\)</span>  <spanclass="math inline">\(\left\| \cdot \right\|_{}\)</span>. <span class="math inline">\(\dim E^{s}\)</span>  <spanclass="math inline">\(A\)</span> .  <spanclass="math inline">\(E\)</span> <spanclass="math inline">\(A\)</span> .  <spanclass="math inline">\(E\)</span>Banach.</p><p>..</p><p><strong>1.4.4.</strong>  <span class="math inline">\(E\)</span>Banach<span class="math inline">\(A \in H(E)\)</span><span class="math inline">\(E=E^{s} \oplus E^{u}\)</span>.</p><p><strong></strong>  <span class="math inline">\(E\)</span>Banach.  <spanclass="math inline">\(E\)</span>  <spanclass="math inline">\(E=E_1 \oplus E_2\)</span> <spanclass="math inline">\(E_1\)</span>  <spanclass="math inline">\(E_2\)</span>  <spanclass="math inline">\(E\)</span>  <spanclass="math inline">\(A\)</span>  <spanclass="math display">\[    \sigma(A|_{E_1})=\sigma(A) \cap \{\lvert z \rvert &lt;1\}\ \text{}\ \sigma(A|_{E_2})=\sigma(A) \cap \{\lvert z \rvert &gt;1\}. \tag{3}\]</span></p><p> <span class="math inline">\(E^{s}=E_1\)</span>  <spanclass="math inline">\(E^{u}=E_2\)</span>. 3 <spanclass="math inline">\(A|_{E_1}\)</span>  <spanclass="math inline">\(r(A|_{E_1})\)</span>  <spanclass="math display">\[    r(A|_{E_1})&lt;1, \tag{4}\]</span></p><p>1.3.2</p><h2 id="hartman-grobman-">Hartman-Grobman </h2><p> <span class="math inline">\(E\)</span> Banach<spanclass="math inline">\(L\colon E\to E\)</span> . <span class="math inline">\(E=E^{u}\oplusE^{s}\)</span><span class="math inline">\(E^{u}\)</span>  <spanclass="math inline">\(E^{s}\)</span>  <spanclass="math inline">\(E\)</span>  <spanclass="math inline">\(L\)</span> . <spanclass="math inline">\(L_{u}=L|_{E^{u}}\)</span> <spanclass="math inline">\(L_{s}=L|_{E^{s}}\)</span> <spanclass="math inline">\(\left\| L_{u}^{-1} \right\|_{}&lt;1\)</span> <span class="math inline">\(\left\| L_{s} \right\|_{}&lt;1\)</span>. <span class="math inline">\(E\)</span> <spanclass="math inline">\(E\)</span> .</p><p> <span class="math inline">\(a=\max (\left\|L_u^{-1} \right\|_{},\left\| L_{s} \right\|_{})&lt;1\)</span>. <spanclass="math inline">\(a\)</span>  <spanclass="math inline">\(L\)</span>  <spanclass="math inline">\(\left\| \cdot \right\|_{}\)</span> . <spanclass="math inline">\(E\)</span>  <spanclass="math inline">\(\lvert x+y \rvert =\max(\lvert x \rvert ,\lvert y\rvert )\)</span> <span class="math inline">\(x\in E^{u}, y\inE^{s}\)</span>.  <span class="math inline">\(\mu&gt;0\)</span> <span class="math display">\[    C_{*}^{0}(E,E)=\{\text{ $E$ $E$ }\}\]</span></p><p><span class="math display">\[    \mathscr{L}_{\mu}(L)=\{\Lambda=L+\lambda\colon \lambda \inC_{*}^{0}(E,E) \text{Lipschitz $\mu$$\leqslant\mu$Lipschitz}\}\]</span></p><p><span class="math display">\[    \mathscr{H}=\{h=\mathbf{1}+g\colon g \in C_{*}^{0}(E,E)\}\]</span></p><p> <span class="math inline">\(\mathbf{1}\)</span>  <spanclass="math inline">\(E\)</span> .  <spanclass="math inline">\(C_{*}^{0}(E,E)\)</span>  <spanclass="math inline">\(C^{0}\)</span>  <span class="math display">\[    \left\| \phi \right\|_{}=\sup_{x \in E} \lvert \phi(x) \rvert .\]</span></p><p>Banach <spanclass="math inline">\(\mathscr{L}_{\mu}(L)\)</span><spanclass="math inline">\(\mathscr{H}\)</span> .</p><h3 id="-hartman-grobman-"> Hartman-Grobman </h3><p> <span class="math inline">\(\mu\)</span> <span class="math inline">\(\Lambda \in\mathscr{L}_{\mu}(L)\)</span> <spanclass="math inline">\(h=h_{\Lambda} \in \mathscr{H}\)</span>  <spanclass="math inline">\(h\Lambda=Lh\)</span>. <spanclass="math inline">\(h_{\Lambda}\)</span>  <spanclass="math inline">\(\Lambda \in \mathscr{L}_{\mu}(L)\)</span>.</p><p> <span class="math inline">\(\Lambda \in\mathscr{L}_{\mu}(L)\)</span> .</p><p><strong>2.1.1.</strong>  <spanclass="math inline">\(\mu\)</span>  <spanclass="math inline">\(\Lambda \in \mathscr{L}_{\mu}(L)\)</span>Lipschitz. Lipschitz.</p><p><strong>2.1.2</strong>  <span class="math inline">\(P\)</span><span class="math inline">\(Y\)</span><span class="math inline">\(F\colon P \times Y \toY\)</span> .  <span class="math inline">\(F_p=F(p,\cdot)\colon Y\to Y\)</span>  <spanclass="math inline">\(k_{p}&lt;1\)</span> .  <spanclass="math inline">\(k_p\)</span> 1 <spanclass="math inline">\(F_p\)</span>  <spanclass="math inline">\(p\)</span> .</p><p><strong> Hartman-Grobman </strong>  <spanclass="math inline">\(\Lambda,\Lambda&#39;\in\mathscr{L}_{\mu}(L)\)</span> <spanclass="math inline">\(h \in \mathscr{H}\)</span>  <spanclass="math inline">\(h\Lambda=\Lambda&#39;h\)</span>. <spanclass="math inline">\(h\)</span>  <spanclass="math inline">\(\Lambda,\Lambda&#39;\)</span> .</p><p> <span class="math inline">\(h\Lambda=\Lambda&#39;h\)</span> <span class="math inline">\(g \inC_{*}^{0}(E,E)\)</span> <span class="math display">\[    (\mathbf{1}+g)(L+\lambda)=(L+\lambda&#39;)(\mathbf{1}+g),\]</span></p><p> <span class="math display">\[    \lambda+g\Lambda=Lg+\lambda&#39;(\mathbf{1}+g), \tag{5}\]</span></p><p>2.1.1 <span class="math inline">\(\Lambda ^{-1}\)</span><spanclass="math inline">\(g=[Lg+\lambda&#39;(\mathbf{1}+g)-\lambda]\Lambda^{-1}\)</span>.  <span class="math inline">\(\forall \phi \inC_{*}^{0}(E,E)\)</span> <span class="math inline">\(\phi_s=\pi_s\circ\phi\)</span><span class="math inline">\(\phi_u=\pi_u \circ\phi\)</span> <span class="math inline">\(\pi_s\)</span>  <spanclass="math inline">\(\pi_u\)</span> .  <spanclass="math display">\[    g_{u}=[L_u g_u+\lambda_u&#39;(\mathbf{1}+g)-\lambda_u]\Lambda^{-1}\tag{6a}\]</span></p><p><span class="math display">\[    g_{s}=[L_s g_s+\lambda_s&#39;(\mathbf{1}+g)-\lambda_s]\Lambda^{-1}.\tag{6b}\]</span> 5 <span class="math inline">\(g=L^{-1}[g\Lambda+\lambda-\lambda&#39;(\mathbf{1}+g)]\)</span> <spanclass="math display">\[    g_u=L_u^{-1}[g_u\Lambda+\lambda_u-\lambda_u&#39;(\mathbf{1}+g)]\tag{7a}\]</span></p><p><span class="math display">\[    g_s=L_s^{-1}[g_s\Lambda+\lambda_s-\lambda_s&#39;(\mathbf{1}+g)].\tag{7b}\]</span> 6a,b7a,b. 6b,7a. <spanclass="math inline">\(\mu&gt;0\)</span>6b,7a <spanclass="math inline">\(K\colon C_{*}^{0}(E,E)\toC_{*}^{0}(E,E)\)</span> <span class="math display">\[    g=(g_u,g_s)\mapsto(L_u^{-1}[g_u\Lambda+\lambda_u-\lambda_u&#39;(\mathbf{1}+g)],[L_sg_s+\lambda_s&#39;(\mathbf{1}+g)-\lambda_s]\Lambda ^{-1})\]</span></p><p> <span class="math inline">\(\lambda,\lambda&#39;,g\)</span> <span class="math inline">\(C_{*}^{0}(E,E)\)</span> <spanclass="math inline">\(K(g)\)</span>  <spanclass="math inline">\(C_{*}^{0}(E,E)\)</span>.  <spanclass="math inline">\(K\)</span>  <spanclass="math inline">\(g,g&#39; \in C_{*}^{0}(E,E)\)</span> <spanclass="math display">\[    \begin{aligned}        &amp;\left\| K_s(g)-K_s(g&#39;) \right\|_{} \\        &amp;=\left\|[L_s(g_s-g_s&#39;)+\lambda_s&#39;(g-g&#39;)]\Lambda ^{-1} \right\|_{} \\        &amp;=\sup _{x \in E} \lvert[L_s(g_s-g_s&#39;)+\lambda_s&#39;(g-g&#39;)]\Lambda ^{-1}(x) \rvert \\        &amp;=\sup _{y \in E} \lvert(L_s(g_s-g_s&#39;)+\lambda_s&#39;(g-g&#39;))(y) \rvert \\        &amp;\leqslant \sup_{y \in E}(a\cdot \lvert g_s(y)-g_s&#39;(y)\rvert +\mu\cdot \lvert g(y)-g&#39;(y) \rvert ) \\        &amp;\leqslant (a+\mu) \left\| g-g&#39; \right\|_{}.    \end{aligned}\]</span></p><p> <span class="math display">\[    \begin{aligned}        \left\| K_u(g)-K_u(g&#39;) \right\|_{} &amp;\leqslant(a+a\mu)\left\| g-g&#39; \right\|_{} \\        &amp;\leqslant (a+\mu)\left\| g-g&#39; \right\|_{}.    \end{aligned}\]</span></p><p> <span class="math inline">\(K\)</span> <spanclass="math inline">\(g=g_{\Lambda,\Lambda&#39;} \inC_{*}^{0}(E,E)\)</span> <spanclass="math inline">\(\Lambda,\Lambda&#39; \in\mathscr{L}_{\mu}(L)\)</span>  <spanclass="math inline">\(h_{\Lambda,\Lambda&#39;}=\mathbf{1}+g_{\Lambda,\Lambda&#39;}\)</span>.</p><p> <span class="math display">\[    h_{\Lambda,\Lambda&#39;}\Lambda=\Lambda&#39;h_{\Lambda,\Lambda&#39;}.\tag{8}\]</span></p><p> <span class="math inline">\(\Lambda\)</span>  <spanclass="math inline">\(\Lambda&#39;\)</span>  <spanclass="math display">\[    h_{\Lambda&#39;,\Lambda}\Lambda&#39;=\Lambdah_{\Lambda&#39;,\Lambda}. \tag{9}\]</span></p><p>89 <span class="math display">\[    h_{\Lambda&#39;,\Lambda}h_{\Lambda,\Lambda&#39;}\Lambda=\Lambdah_{\Lambda&#39;,\Lambda}h_{\Lambda,\Lambda&#39;}.\]</span></p><p> <span class="math inline">\(\mathbf{1}\)</span>  <spanclass="math inline">\(\Lambda\)</span>  <spanclass="math inline">\(\mathbf{1}+C_{*}^{0}(E,E)\)</span> <span class="math display">\[    h_{\Lambda,\Lambda&#39;}h_{\Lambda&#39;,\Lambda}=h_{\Lambda&#39;,\Lambda}h_{\Lambda,\Lambda&#39;}=\mathbf{1}.\]</span></p><p> <span class="math inline">\(h_{\Lambda,\Lambda&#39;}\)</span>.</p><h3 id="-hartman-"> Hartman </h3><p> <span class="math inline">\(\Lambda=L+\lambda\)</span>  <spanclass="math inline">\(E\)</span> $^{L} $ <span class="math inline">\(E=E^{u}\oplus E^{s}\)</span><span class="math inline">\(\nu\)</span> <span class="math inline">\(\Lambda \in \mathscr{L}_{\mu}(L)\)</span> <span class="math inline">\(H=H_{\Lambda}\in\mathscr{H}\)</span>  <span class="math inline">\(x \in E, t\in\mathbb{R}\)</span> <span class="math inline">\(H\phi_{\Lambda}(t,x)\equiv \phi_{L}(t,Hx)\)</span>  <spanclass="math inline">\(\phi_{L}, \phi_{\Lambda}\)</span>  <spanclass="math inline">\(L\)</span>-  <spanclass="math inline">\(\Lambda\)</span>- . <spanclass="math inline">\(H_{\Lambda}\)</span>  <spanclass="math inline">\(\Lambda \in \mathscr{L}_{\nu}(L)\)</span>.</p><p> $^{L} $  <spanclass="math inline">\(L\)</span> .</p><p><strong></strong>  <spanclass="math inline">\(\tilde{\phi}_{L}=\phi_{L}(1,\cdot)=\mathrm{e}^{L}\)</span>  <spanclass="math inline">\(\tilde{\phi}_{\Lambda}=\phi_{\Lambda}(1,\cdot)\)</span>.  <span class="math inline">\(\Lambda \mapsto\tilde{\phi}_{\Lambda}\)</span>  <spanclass="math inline">\(\nu&gt;0\)</span> Hartman <span class="math inline">\(\mu\)</span><span class="math inline">\(\Lambda \in \mathscr{L}_{\nu}(L) \Rightarrow\tilde{\phi}_{\Lambda}\in \mathscr{L}_{\mu}(\mathrm{e}^{L} )\)</span>. <span class="math inline">\(h \in \mathscr{H}\)</span> <span class="math inline">\(\tilde{\phi}_{L}h=h\tilde{\phi}_{\Lambda}\)</span>.</p><p> <span class="math inline">\(h\)</span>  <spanclass="math inline">\(h \phi_{\Lambda}(t,x)\equiv\phi_{L}(t,hx)\)</span>  <span class="math inline">\(h=H\)</span>.  <span class="math display">\[\phi_{L}(t,h\phi_{\Lambda}(-t,\cdot ))\equiv h. \tag{10}\]</span></p><p> <spanclass="math inline">\(\phi_{L}(t,h\phi_{\Lambda}(-t,\cdot ))\in\mathscr{H}\)</span> <spanclass="math inline">\(\tilde{\phi}_{L}()=()\tilde{\phi}_{\Lambda}\)</span> Hartman 10.  <spanclass="math inline">\(\phi_{L}\)</span>  <spanclass="math inline">\(\phi_{\Lambda}\)</span> <span class="math inline">\(\tilde{\phi}_{L}\)</span>  <spanclass="math inline">\(\tilde{\phi}_{\Lambda}\)</span> . <span class="math inline">\(H=h\)</span> <span class="math inline">\(\Lambda \in \mathscr{L}_{\nu}(L)\)</span>.</p><p>.  Hartman-Grobman. 4<spanclass="math inline">\(Df(\mathbf{0})\)</span>  <spanclass="math inline">\(\mathrm{e}^{Df(\mathbf{0})}\)</span>.  <spanclass="math inline">\(Df(\mathbf{0})\)</span> Hartman-Grobman <span class="math inline">\(\nu\)</span><span class="math inline">\(E\)</span>  <spanclass="math inline">\(U\)</span><spanclass="math inline">\(f-Df(\mathbf{0}) \in\mathscr{L}_{\nu}(L)\)</span>. Hartman-Grobman <span class="math inline">\(H \in\mathscr{H}\)</span> <span class="math inline">\(H\phi_{\Lambda}(t,x)\equiv \phi_{L}(t,Hx)\)</span>35.</p><p> Hartman-Grobman  <spanclass="math inline">\(h\)</span>  Lipschitz .</p><h4 id=""></h4><p> <span class="math inline">\(A_{\alpha}\colon \mathbb{R}\to\mathbb{R}\)</span>  <span class="math display">\[    A_{\alpha}(x)=\alpha x.\]</span></p><p> <span class="math inline">\(0&lt;\alpha&lt;1,0&lt;\beta&lt;1\)</span> <span class="math inline">\(h(x)=x^{\ln\beta/\ln \alpha},x\geqslant 0\)</span> <spanclass="math inline">\(h\)</span> .  <spanclass="math inline">\(h(x)\)</span>  <spanclass="math inline">\(\mathbb{R}\)</span>  <spanclass="math inline">\(hA_{\alpha}=A_{\beta}h\)</span>.  <spanclass="math inline">\(\alpha \neq \beta\)</span> Lipschitz <span class="math inline">\(h\colon \mathbb{R}\to\mathbb{R}\)</span>  <span class="math inline">\(hA_{\alpha}=A_{\beta} h\)</span>.</p><p> <span class="math inline">\(h\colon\mathbb{R}\to \mathbb{R}\)</span>.  <spanclass="math inline">\(h(0)\neq 0\)</span> <spanclass="math inline">\(h(\alpha x)=\beta h(x)\)</span>  <spanclass="math inline">\(\beta=1\)</span> <spanclass="math inline">\(h\)</span>  <spanclass="math inline">\(\alpha=1\)</span>.  <spanclass="math inline">\(\alpha\neq \beta\)</span> .</p><p> <span class="math inline">\(h(0)=0\)</span> <spanclass="math inline">\(\alpha \beta\neq 0\)</span>.  -<spanclass="math inline">\(h(\alpha^{n}x)/(\alpha^{n}x)=(\beta^{n}h(x))/(\alpha^{n}x)\)</span>- <span class="math inline">\(h ^{-1}(\beta^{n}y)/(\beta^{n}y)=(\alpha^{n}h ^{-1}(y))/(\beta^{n}y)\)</span> -$h(<sup>{-n}x)/(</sup>{-n}x)=(<sup>{n}h</sup>{-1}(x))/(^{n}x) $ -<span class="math inline">\(h^{-1}(\beta^{-n}y)/(\beta^{-n}y)=(\beta^{n} h^{-1}(y))/(\alpha^{n}y)\)</span>.</p><p> <span class="math inline">\(h\)</span>  <spanclass="math inline">\(h ^{-1}\)</span>  Lipschitz  <spanclass="math inline">\(h\)</span> Lipschitz .</p>]]></content>
    
    
    <categories>
      
      <category>analysis</category>
      
      <category>differential equation</category>
      
      <category>dynamical system</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (14)</title>
    <link href="/2022/12/11/Neuronal-Dynamics-14/"/>
    <url>/2022/12/11/Neuronal-Dynamics-14/</url>
    
    <content type="html"><![CDATA[<h1id="quasi-renewal-theory-and-the-integral-equation-approach">Quasi-RenewalTheory and the Integral-equation Approach</h1><p>The online version of this chapter:</p><hr /><p>Chapter 14 Quasi-Renewal Theory and the Integral-equation Approachhttps://neuronaldynamics.epfl.ch/online/Ch14.html</p><hr /><p>For neuron models that include biophysical phenomena such asrefractoriness and adaptation on multiple time-scales, the resultingPDEs are situated in more than 2D and therefore difficult to solveanalytically. We indicate an alternative to describing the populationactivity in networks of model neurons.</p><h2 id="population-activity-equations">Population activityequations</h2><h3 id="assumptions-of-time-dependent-renewal-theory">Assumptions oftime-dependent renewal theory</h3><p>The state of a neuron <span class="math inline">\(i\)</span> at time<span class="math inline">\(t\)</span> is described by - its last firingtime <span class="math inline">\(\hat{t}_i\)</span>; - the input <spanclass="math inline">\(I(t&#39;)\)</span> it received for times <spanclass="math inline">\(t&#39;&lt;t\)</span>; - the characteristics ofpotential noise sources, be it noise in the input, noise in the neuronalparameters, or noise in the output.</p><h3 id="integral-equations-for-non-adaptive-neurons">Integral equationsfor non-adaptive neurons</h3><p>The integral equation for activity dynamics with time-dependentrenewal theory states that <span class="math display">\[    A(t)=\int_{-\infty}^{t} P_{I}(t|\hat{t})A(\hat{t})\mathrm{d}\hat{t}. \tag{14.5}\]</span></p><p><span class="math inline">\(A(t)\)</span> on the LHS is the expectedactivity at time <span class="math inline">\(t\)</span> while <spanclass="math inline">\(A(\hat{t})\)</span> on the RHS is the observedactivity in the past. (14.5) becomes exact in the limit of <spanclass="math inline">\(N \to \infty\)</span>. Finite-size effects arediscussed later.</p><ul><li>(14.5) is linear in the variable <spanclass="math inline">\(A\)</span>. Instead of defining the activity by aspike count divided by <span class="math inline">\(N\)</span>, we couldhave chosen to work directly with the spike count per unit of time orany other normalization.</li><li>(14.5) is a highly nonlinear equation in the drive because thekernel <span class="math inline">\(P_{I}(t|\hat{t})\)</span> dependsnonlinearly on the input <span class="math inline">\(I(t)\)</span>.</li></ul><h3id="normalization-and-derivation-of-the-integral-equation">Normalizationand derivation of the integral equation</h3><p>Recap <span class="math display">\[    S_{I}(t|\hat{t})=1-\int_{\hat{t}}^{t} P_{I}(s|\hat{t}) \mathrm{d}s,\tag{14.6}\]</span></p><p>We now return to the homogeneous population of neurons in the limitof <span class="math inline">\(N \to \infty\)</span> and assume that thefiring of different neurons at time <spanclass="math inline">\(t\)</span> is independent, given that we know thehistory of each neuron. ('conditional independence')</p><p>Define the proportion of neurons at time <spanclass="math inline">\(t\)</span> which have fired their last spikebetween <span class="math inline">\(t_0\)</span> and <spanclass="math inline">\(t_1&lt;t\)</span> (and have not firied since) as<span class="math display">\[    \left\langle \frac{\text{number of neurons at $t$ with last spikein}\ [t_0,t_1]}{\text{total number of neurons}} \right\rangle=\int_{t_0}^{t_1} S_{I}(t|\hat{t})A(\hat{t}) \mathrm{d}\hat{t}.\tag{14.7}  \]</span></p><p>We use the fact that the total number of neurons remains constant.<span class="math display">\[    1=\int_{-\infty}^{t} S_{I}(t|\hat{t})A(\hat{t}) \mathrm{d}\hat{t},\tag{14.8}\]</span> The normalization of (14.8) must hold at arbitrary times <spanclass="math inline">\(t\)</span>.</p><p>Take the derivative of (14.8) w.r.t <spanclass="math inline">\(t\)</span>, <span class="math display">\[    0=S_{I}(t|t)A(t)+\int_{-\infty}^{t} \frac{\partialS_{I}(t|\hat{t})}{\partial t}A(\hat{t}) \mathrm{d}\hat{t}. \tag{14.9}\]</span></p><p>Use <span class="math inline">\(P_{I}(t|\hat{t})=-\frac{\partial}{\partial t}S_{I}(t|\hat{t})\)</span> and <spanclass="math inline">\(S_{I}(t|t)=1\)</span> and we yield (14.5).</p><h4id="example-absolute-refractoriness-and-the-wilson-cowan-integral-equation">Example:Absolute refractoriness and the Wilson-Cowan integral equation</h4><p>Consider a population of Poisson neurons with an absolute refractoryperiod <span class="math inline">\(\Delta^{abs}\)</span>.</p><p>The population activity of a homogeneous group of Poisson neuronswith absolute refractoriness is <span class="math display">\[    A(t)=f[h(t)] \left\{ 1-\int_{t-\Delta^{abs}}^{t} A(t&#39;)\mathrm{d}t&#39; \right\}. \tag{14.10}\]</span> where <span class="math inline">\(h(t)=\int_{0}^{\infty}\kappa(s)I(t-s) \mathrm{d}s\)</span>. <spanclass="math inline">\(f\)</span> is the stochastic intensity of aninhomogeneous Poisson process describing neurons in a homogeneouspopulation.</p><p>For constant input current, <span class="math inline">\(h(t)=h_0=I_0\int_{0}^{\infty} \kappa(s) \mathrm{d}s\)</span>. The populationactivity has a stationary solution <span class="math display">\[    A_0=\frac{f(h_0)}{1+\Delta^{abs}f(h_0)}=g(h_0). \tag{14.11}\]</span></p><h3 id="integral-equation-for-adaptive-neurons">Integral equation foradaptive neurons</h3><p>For an isolated adaptive neuron in the presence of noise, theprobability density of firing around time <spanclass="math inline">\(t\)</span> will depend on its past firing times<span class="math inline">\(\hat{t}_{n}&lt;\cdots&lt;\hat{t}_2&lt;\hat{t}_1=\hat{t}&lt;t\)</span> where <spanclass="math inline">\(\hat{t}=\hat{t}_1\)</span> denotes the most recentspike time.</p><p>In a population of neurons, we can approximate the past firing timesby the population activity <span class="math inline">\(A(t)\)</span>.Let <span class="math inline">\(P_{I,A}(t|\hat{t})\)</span> be theprobability of observing a spike at time <spanclass="math inline">\(t\)</span> given the last spike at time <spanclass="math inline">\(\hat{t}\)</span>, the input-current and theactivity history <span class="math inline">\(A(t)\)</span> until time<span class="math inline">\(t\)</span>, then <spanclass="math display">\[    A(t)=\int_{-\infty}^{t} P_{I,A}(t|\hat{t})A(\hat{t})\mathrm{d}\hat{t}. \tag{14.12}\]</span></p><h3 id="numerical-methods-for-integral-equations">Numerical methods forintegral equations</h3><p>We take as an example the quasi-renewal equivalent of (14.8) <spanclass="math display">\[    1=\int_{-\infty}^{t} S_{I,A}(t|\hat{t})A(\hat{t}) \mathrm{d}\hat{t}.\tag{14.13}\]</span></p><p>Let <span class="math inline">\(\tau_c\)</span> be a period time suchthat the survivor <spanclass="math inline">\(S_{I,A}(t|\hat{t-\tau_c})\)</span> is very small.Then <span class="math display">\[    1=\int_{t-\tau_c}^{t} S_{I,A}(t|\hat{t})A(\hat{t})\mathrm{d}\hat{t}. \tag{14.14}\]</span></p><p>Discretize the integral on small bins of size <spanclass="math inline">\(\Delta t\)</span>. Let <spanclass="math inline">\(\mathbf{m}^{(t)}\)</span> be the vector made ofthe fraction of neurons at <span class="math inline">\(t\)</span> withlast spike within <span class="math inline">\(\hat{t}\)</span> and <spanclass="math inline">\(\hat{t}+\Delta t\)</span>, which means that the<span class="math inline">\(k\)</span>th element is <spanclass="math inline">\(m_k^{(t)}=S(t|t-k\Delta t)A(t-k\Delta t)\Deltat\)</span>, <span class="math inline">\(m_0^{(t)}=A_t\Delta t\)</span>since <span class="math inline">\(S(t|t)=1\)</span>. Therefore <spanclass="math display">\[    A_t \Delta t=1-\sum_{k=1}^{K} m_k^{(t)}. \tag{14.15}\]</span></p><p>Because of <span class="math inline">\(S(t|\hat{t})=\exp[-\int_{\hat{t}}^{t} \rho(t&#39;|\hat{t}) \mathrm{d}t&#39;]=\exp[-\int_{t-\Delta t}^{t} \rho(t&#39;|\hat{t}) \mathrm{d}t&#39;]S(t-\Deltat|\hat{t})\)</span>, we find for sufficiently small <spanclass="math inline">\(\Delta t\)</span>, <span class="math display">\[    m_k^{(t)}=m_{k-1}^{(t-\Delta t)}\exp [-\rho(t|t-k\Delta t)\Deltat]  \quad\text{for}\ k\geqslant 1 \tag{14.16}\]</span></p><p>Note that <span class="math inline">\(m_k^{(t)}\)</span> and <spanclass="math inline">\(m_{k-1}^{(t-\Delta t)}\)</span> refer to the samegroup of neurons, i.e., those that have fired their last spike aroundtime <span class="math inline">\(\hat{t}=t-k\Delta t\)</span>. Together,(14.15) and (14.16) can be used to solve <spanclass="math inline">\(A_t\)</span> iteratively.</p><h2 id="recurrent-networks-and-interacting-populations">RecurrentNetworks and Interacting Populations</h2>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/11/30/%E6%B0%B8%E4%B9%85%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%83%EF%BC%89/"/>
    <url>/2022/11/30/%E6%B0%B8%E4%B9%85%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%83%EF%BC%89/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (13)</title>
    <link href="/2022/10/24/Neuronal-Dynamics-13/"/>
    <url>/2022/10/24/Neuronal-Dynamics-13/</url>
    
    <content type="html"><![CDATA[<h1 id="continuity-equation-and-the-fokker-planck-approach">ContinuityEquation and the Fokker-Planck Approach</h1><p>The online version of this chapter:</p><hr /><p>Chapter 13 Continuity Equation and the Fokker-Planck Approachhttps://neuronaldynamics.epfl.ch/online/Ch13.html</p><hr /><p>In this chapter we present a formulation of population activityequations that can account for the temporal aspects of populationdynamics.</p><h2 id="continuity-equation">Continuity equation</h2><p>In this chapter and the next, the population activity is the expectedpopulation activity <span class="math inline">\(A(t)\equiv \langleA(t)\rangle\)</span>.</p><h3 id="distribution-of-membrane-potentials">Distribution of membranepotentials</h3><p>The aim of this section is to describe the evolution of the density<span class="math inline">\(p(u,t)\)</span> as a function of time. <spanclass="math display">\[    \lim_{N \to \infty}\left\{\frac{\text{neurons with }u_0&lt;u_i(t)\leqslant u_0+\Delta u}{N}\right\}=\int_{u_0}^{u_0+\Deltau} p(u,t) \mathrm{d}u, \tag{13.2}\]</span> <span class="math display">\[    \int_{-\infty}^{\theta_{reset}} p(u,t) \mathrm{d}u=1. \tag{13.3}\]</span></p><h3 id="flux-and-continuity-equation">Flux and continuity equation</h3><p>Consider the portion of neurons with a membrane potential between<span class="math inline">\(u_0\)</span> and <spanclass="math inline">\(u_1\)</span>. The <strong>flux</strong> <spanclass="math inline">\(J(u,t)\)</span> is the net fraction oftrajectories per unit time that crosses the value <spanclass="math inline">\(u\)</span>. A positive flux <spanclass="math inline">\(J(u,t)&gt;0\)</span> is defined as a flux towardincreasing values of <span class="math inline">\(u\)</span>.</p><p>We have the conservation law <span class="math display">\[    \frac{\partial }{\partial t}\int_{u_0}^{u_1} p(u&#39;,t)\mathrm{d}u&#39;=J(u_0,t)-J(u_1,t). \tag{13.5}\]</span> The continuity equation, <span class="math display">\[    \frac{\partial }{\partial t}p(u,t)=-\frac{\partial }{\partialu}J(u,t) \quad u \neq u_r, u\neq \theta_{reset}, \tag{13.6}\]</span></p><p>Since neurons that have fired start a new trajectory at <spanclass="math inline">\(u_r\)</span>, we have a 'source of newtrajectories' at <span class="math inline">\(u=u_r\)</span>, i.e., newtrajectories appear in the interval <spanclass="math inline">\([u_r-\varepsilon, u_r+\varepsilon]\)</span> thathave not entered the interval through one of the borders. Adding a term<span class="math inline">\(A(t)\delta(u-u_r)\)</span> on the RHS of(13.6) accounts for this source of trajectories. <spanclass="math display">\[    \frac{\partial }{\partial t}p(u,t)=-\frac{\partial }{\partialu}J(u,t)+A(t)\delta(u-u_r)-A(t)\delta(u-\theta_{reset}) \tag{13.7}\]</span> The density <span class="math inline">\(p(u,t)\)</span>vanishes for all values <spanclass="math inline">\(u&gt;\theta_{reset}\)</span>.</p><p>The population activity <span class="math inline">\(A(t)\)</span> isthe fraction of neurons that fire, <span class="math display">\[    A(t)=J(\theta_{reset},t). \tag{13.8}\]</span></p><h2 id="stochastic-spike-arrival">Stochastic spike arrival</h2><p>We consider the flux <span class="math inline">\(J(u,t)\)</span> in ahomogeneous population of integrate-and-fire neurons with voltageequation (13.1). An input spike at a synapse of type <spanclass="math inline">\(k\)</span> causes a jump of the membrane potentialby an amount <span class="math inline">\(w_k\)</span>. The effectivespike arrival rate (summed over all synapses of the same type <spanclass="math inline">\(k\)</span>) is denoted as <spanclass="math inline">\(\nu_k\)</span>.</p><p>A finite input current <spanclass="math inline">\(I^{ext}(t)\)</span> generates a smooth drift ofthe membrane potential trajectories. The flux <spanclass="math inline">\(J(u,t)\)</span> can be generated through a 'jump'or a 'drift' of trajectories. <span class="math display">\[    J(u_0,t)=J_{drift}(u_0,t)+J_{jump}(u_0,t), \tag{13.9}\]</span></p><h3id="jumps-of-membrane-potential-due-to-stochastic-spike-arrival">Jumpsof membrane potential due to stochastic spike arrival</h3><p><span class="math display">\[    J_{jump}(u_0,t)=\sum_{k}^{} \nu_k(t) \int_{u_0-w_k}^{u_0} p(u,t)\mathrm{d}u. \tag{13.10}\]</span></p><h3 id="drift-of-membrane-potential">Drift of membrane potential</h3><p><span class="math display">\[    J_{drift}(u_0,t)=\frac{\mathrm{d}u}{\mathrm{d}t}\bigg|_{u_0}p(u_0,t)=\frac{1}{\tau_m}[f(u_0)+RI^{ext}(t)]p(u_0,t),\tag{13.11}\]</span></p><p>Synaptic <span class="math inline">\(\delta\)</span>-curent pulsescause a jump of the membrane potential and therefore contribute only to<span class="math inline">\(J_{jump}\)</span></p><div class="note note-info">            <p>(13.7)  (13.11)  <spanclass="math inline">\(p(u,t)\)</span>  <spanclass="math inline">\(t\)</span> <span class="math inline">\(A(t)\)</span> spikepostneuron <spanclass="math inline">\(p(u,t)\)</span>  <spanclass="math inline">\(t\)</span>  (13.11)  <spanclass="math inline">\(J_{drift}\)</span> </p>          </div><h3 id="population-activity">Population activity</h3><p>A positive flux through the threshold <spanclass="math inline">\(\theta_{reset}\)</span> yields the populationactivity <span class="math inline">\(A(t)\)</span>. The total flux atthe threshold is <span class="math display">\[    A(t)=\frac{1}{\tau_m}[f(\theta_{reset}+RI^{ext}(t)]p(\theta_{reset},t)+\sum_{k}^{}\nu_k \int_{\theta_{reset}-w_k}^{\theta_{reset}} p(u,t) \mathrm{d}u.\tag{13.13}\]</span> Since the probability density vanishes for <spanclass="math inline">\(u&gt;\theta_{reset}\)</span>, the sum over thesynapses <span class="math inline">\(k\)</span> can be restricted to allexcitatory synapses.</p><p><span class="math display">\[\begin{aligned}    \frac{\partial }{\partialt}p(u,t)&amp;=-\frac{1}{\tau_m}\frac{\partial }{\partialu}{[f(u)+RI^{ext}(t)]p(u,t)}\\    &amp;+\sum_{k}^{} \nu_k(t)[p(u-w_k,t)-p(u,t)] \\    &amp;+A(t)\delta(u-u_r)-A(t)\delta(u-\theta_{reset}).\end{aligned}\]</span> (13.14)</p><p>we have <span class="math inline">\(p(u,t)=0\)</span> for <spanclass="math inline">\(u&gt;\theta_{reset}\)</span>.</p><h2 id="fokker-planck-equation">Fokker-Planck equation</h2><p>We expand the RHS of (13.14) into a Taylor series up to second orderin <span class="math inline">\(w_k\)</span>. <spanclass="math display">\[    \begin{aligned}        \tau_m \frac{\partial }{\partial t}p(u,t)=&amp;-\frac{\partial}{\partial u} \left\{ \left[ f(u)+RI^{ext}(t)+\tau_m\sum_{k}^{}\nu_k(t)w_k\right]p(u,t) \right\} \\        &amp;+\frac{1}{2}\left[ \tau_m \sum_{k}^{}\nu_k(t)w_k^{2}\right] \frac{\partial ^{2}}{\partial u^{2}}p(u,t) \\        &amp;+ \tau_m A(t)\delta(u-u_r)-\tau_mA(t)\delta(u-\theta_{reset})+o(w_k^{3})    \end{aligned}\]</span> (13.16)</p><p>The term with the second derivative describes a 'diffusion' in termsof the membrane potential.</p><p>We define the total 'drive' in voltage units as <spanclass="math display">\[    \mu(t)=RI^{ext}(t)+\tau_m \sum_{k}^{} \nu_k(t)w_k \tag{13.17}\]</span> and the amount of diffusive noise (again in voltage units) as<span class="math display">\[    \sigma^{2}(t)=\tau_m \sum_{k}^{} \nu_k(t)w_k^{2}. \tag{13.18}\]</span></p><p>The firing threshold acts as an absorbing boundary so that thedensity at threshold vanishes <span class="math display">\[    p(\theta_{reset},t)=0. \tag{13.19}\]</span></p><p>We expand (13.13) in <span class="math inline">\(w_k\)</span> about<span class="math inline">\(u=\theta_{reset}\)</span> and obtain <spanclass="math display">\[    A(t)=-\frac{\sigma^{2}(t)}{2\tau_m}\frac{\partial p(u,t)}{\partialu} \bigg |_{u=\theta_{reset}}, \tag{13.20}\]</span></p><h4 id="example-flux-in-the-diffusion-limit">Example: Flux in thediffusion limit</h4><p>Compare (13.7) with (13.16), we can identify the flux caused bystochastic spike arrival and external current in the diffusion limit<span class="math display">\[    J^{diff}(u,t)=\frac{f(u)+\mu(t)}{\tau_m}p(u,t)-\frac{1}{2}\frac{\sigma^{2}(t)}{\tau_m}\frac{\partial}{\partial u}p(u,t). \tag{13.21}\]</span></p><p>Stochastic spike arrival contributes to the mean drive <spanclass="math inline">\(\mu(t)=RI^{ext}(t)+\tau_m\sum_{k}^{}\nu_k(t)w_k\)</span> as well as to the diffusive noise <spanclass="math inline">\(\sigma^{2}(t)=\tau_m \sum_{k}^{}\nu_k(t)w_k^{2}\)</span>.</p><h3id="stationary-solution-for-leaky-integrate-and-fire-neurons">Stationarysolution for leaky integrate-and-fire neurons</h3><p>We now derive the stationary solution <spanclass="math inline">\(p(u,t)\equiv p(u)\)</span> of (13.16) for apopulation of leaky integrate-and-fire neurons (<spanclass="math inline">\(f(u)=-u\)</span>, the voltage scale is shifted sothat theh equilibrium potential is at zero). The reset threshold is thesame as the rheobase firing threshold and will be denoted by <spanclass="math inline">\(\theta=\theta_{reset}=\theta_{rh}\)</span>.</p><p>We assume that the total input <spanclass="math inline">\(h_0=RI^{ext}+\tau_m \sum_{k}^{} \nu_k w_k\)</span>is constant. For <span class="math inline">\(u&lt;\theta\)</span>, <spanclass="math display">\[    0=-\frac{\partial }{\partial u}J(u)+A_0 \delta(u-u_r), \tag{13.22}\]</span> where <span class="math inline">\(A_0\)</span> is thepopulation activity (or mean firing rate) in the stationary state and<span class="math display">\[    J(u)=\frac{-u+h_0}{\tau_m}p(u)-\frac{1}{2}\frac{\sigma^{2}}{\tau_m}\frac{\partial}{\partial u}p(u) \tag{13.23}\]</span></p><p>is the total flux. The meaning of (13.22) is that the flux isconstant except at <span class="math inline">\(u=u_r\)</span> where itjumps by an amount <span class="math inline">\(A_0\)</span>.</p><p>The boundary condition <spanclass="math inline">\(p(\theta,t)=0\)</span> implies a seconddiscontinuity of the flux at <spanclass="math inline">\(u=\theta\)</span>.</p><p>For any constant <span class="math inline">\(c_1\)</span>, <spanclass="math display">\[    p(u)=\frac{c_1}{\sigma}\exp \left[-\frac{(u-h_0)^{2}}{\sigma^{2}}\right]\]</span> for <span class="math inline">\(u\leqslant u_r\)</span> is asolution of (13.22) with <spanclass="math inline">\(J(u)=0\)</span>.</p><p>The solution to (13.22) with <spanclass="math inline">\(p(\theta)=0\)</span> is a modified Gaussian, <spanclass="math display">\[    p(u)=\frac{c_2}{\sigma^{2}}\exp\left[-\frac{(u-h_0)^{2}}{\sigma^{2}}\right] \cdot \int_{u}^{\theta}\exp \left[\frac{(x-h_0)^{2}}{\sigma^{2}} \right] \mathrm{d}x\tag{13.25}\]</span> for <span class="math inline">\(u_r&lt;u\leqslant\theta\)</span> with some constant <spanclass="math inline">\(c_2\)</span>. The constant <spanclass="math inline">\(c_2\)</span> is proportional to the flux, <spanclass="math display">\[    c_2=2\tau_m J(u) \tag{13.26}        \]</span> for <span class="math inline">\(u_r&lt;u\leqslant\theta\)</span>.</p><p>The solution defined by (13.24) and (13.25) must be continuous at<span class="math inline">\(u=u_r\)</span>. Hence <spanclass="math display">\[    c_1=\frac{c_2}{\sigma}\int_{u_r}^{\theta} \exp\left[\frac{(x-h_0)^{2}}{\sigma^{2}}\right] \mathrm{d}x. \tag{13.27}\]</span></p><p>The constant <span class="math inline">\(c_2\)</span> is determinedby (13.3). <span class="math display">\[    \frac{1}{c_2}=\int_{u_r}^{\theta} \int_{-\infty}^{x} f(x,u)\mathrm{d}u \mathrm{d}x, \tag{13.28}\]</span> with <span class="math display">\[    f(x,u)=\frac{1}{\sigma^{2}}\exp\left[-\frac{(u-h_0)^{2}}{\sigma^{2}}\right]\exp\left[\frac{(x-h_0)^{2}}{\sigma^{2}}\right]. \tag{13.29}\]</span> <span class="math display">\[    A_0^{-1}=\tau_m \sqrt{\pi}\int_{\frac{u_r-h_0}{\sigma}}^{\frac{\theta-h_0}{\sigma}} \exp(x^{2})[1+\text{erf}(x)] \mathrm{d}x, \tag{13.30}\]</span></p><h2 id="networks-of-leaky-integrate-and-fire-neurons">Networks of leakyintegrate-and-fire neurons</h2><h3 id="multiple-populations">Multiple populations</h3><p><span class="math display">\[    \begin{aligned}        \tau_n \frac{\partial }{\partial t}p_n(u,t)=&amp;-\frac{\partial}{\partial u} \left\{ \left[ -u+R_nI_n^{ext}(t)+\tau_n\sum_{k}^{}C_{nk}A_k(t)w_{nk}\right]p_n(u,t) \right\} \\        &amp;+\frac{1}{2}\left[ \tau_n \sum_{k}^{}C_{nk}A_k(t)w_{nk}^{2}\right] \frac{\partial ^{2}}{\partialu^{2}}p_n(u,t) \\        &amp;+ \tau_n A_n(t)\delta(u-u_r^{n})-\tau_nA_n(t)\delta(u-\theta_{n})    \end{aligned}\]</span> (13.31)</p><p><span class="math display">\[    A_n(t)=-\frac{1}{2}\left[ \sum_{k}^{} C_{nk}A_k(t)w_{nk}^{2}\right]\left( \frac{\partial p_n(u,t)}{\partial u}\right)_{u=\theta_n}.\tag{13.32}\]</span></p><p>Thus populations interact with each other via the variable <spanclass="math inline">\(A_k(t)\)</span>.</p><h3 id="synchrony-oscillations-and-irregularity">Synchrony,oscillations, and irregularity</h3><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x334.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x335.png" /></div></div></div><ul><li>Asynchronous irregular (AI):</li><li>Synchronous regular (SR):</li><li>Synchronous irregular (SI):</li></ul><h2 id="networks-of-nonlinear-integrate-and-fire-neurons">Networks ofNonlinear Integrate-and-Fire Neurons</h2><p>Now we determine, for arbitrary nonlinear integrate-and-fire modelsdriven by a diffusive input with constant mean <spanclass="math inline">\(\mu=RI_0\)</span> and noise <spanclass="math inline">\(\sigma^{2}\)</span>, the distribution of membranepotentials <span class="math inline">\(p_0(u)\)</span> as well as thelinear response of the population activity <span class="math display">\[    A(t)=A_0+A_1(t)=A_0+\int_{0}^{\infty} G(s)I_1(t-s) \mathrm{d}s,\tag{13.37}\]</span> to a drive <spanclass="math inline">\(\mu(t)=R[I_0+I_1(t)]\)</span>.</p><h3 id="steady-state-population-activity">Steady state populationactivity</h3><p>We start with the continuity equation (13.7) <spanclass="math display">\[    \frac{\partial }{\partial t}p(u,t)=-\frac{\partial }{\partialu}J(u,t)+A(t)\delta(u-u_r)-A(t)\delta(u-\theta_{reset}). \tag{13.38}\]</span> In the stationary state, <span class="math display">\[    \frac{\partial }{\partialu}J(u,t)=A(t)\delta(u-u_r)-A(t)\delta(u-\theta_{reset}). \tag{13.39}\]</span></p><p>The flux takes a constant value except at <spanclass="math inline">\(\theta_{reset}\)</span> and <spanclass="math inline">\(u_r\)</span>. For <spanclass="math inline">\(u_r&lt;u&lt;\theta_{reset}\)</span> the constantvalue <span class="math inline">\(J(u,t)=c&gt;0\)</span>.</p><p>In the diffusion limit the flux according to (13.21) is <spanclass="math display">\[    J(u,t)=\frac{1}{\tau_m}\left[f(u)+\mu(t)-\frac{1}{2}\sigma^{2}(t)\frac{\partial }{\partialu}\right]p(u,t). \tag{13.40}    \]</span></p><p>In the stationary state, <spanclass="math inline">\(p(u,t)=p_0(u)\)</span> and <spanclass="math inline">\(J(u,t)=c\)</span> for <spanclass="math inline">\(u_r&lt;u&lt;\theta_{reset}\)</span>. Hence <spanclass="math display">\[    \frac{\mathrm{d}p_0(u)}{\mathrm{d}u}=\frac{2\tau_m}{\sigma^{2}}\left[\frac{f(u)+\mu}{\tau_m}p_0(u)-c\right]. \tag{13.41}   \]</span></p><p>With initial condition <spanclass="math inline">\(p_0(\theta_{reset})=0\)</span> and <spanclass="math inline">\(\mathrm{d} p_0/\mathrm{d}u|_{\theta_{reset}}=-2c\tau_m/\sigma^{2}\)</span>, one can integrate(13.41) from <span class="math inline">\(u=\theta_{reset}\)</span>. Whenthe integration passes <span class="math inline">\(u=u_r\)</span> theconstant switches from <span class="math inline">\(c\)</span> to <spanclass="math inline">\(0\)</span>. The integration is stopped at a lowerbound <span class="math inline">\(u_{low}\)</span> when <spanclass="math inline">\(p_0\)</span> has approached <spanclass="math inline">\(0\)</span>.</p><h3 id="response-to-modulated-input">Response to modulated input</h3><p>Recall the exponential integrate-and-fire model <spanclass="math display">\[    \tau \frac{\mathrm{d}}{\mathrm{d}t}u=-(u-u_{r})+\Delta_{T}\exp\left(\frac{u-\theta_{rh}}{\Delta_{T}}\right)+\mu, \tag{13.42}\]</span></p><p>We now add a small periodic perturbation to the input <spanclass="math display">\[    I(t)=I_0+\varepsilon \cos (\omega t), \tag{13.43}\]</span> where <span class="math inline">\(\omega=2\pi/T\)</span>. Weexpect the periodic drive to lead to a small periodic change in thepopulation activity <span class="math display">\[    A(t)=A_0+A_1(\omega)\cos (\omega t+\phi_{A}(\omega)). \tag{13.44}\]</span></p><p>We are to calculate <span class="math inline">\(\hat{G}\)</span> thatcharacterize the linear response or 'gain' at frequency <spanclass="math inline">\(\omega\)</span> <span class="math display">\[    \hat{G}(\omega)=\frac{A_1(\omega)}{\varepsilon}\mathrm{e}^{i\phi_{A}(\omega)} . \tag{13.45}\]</span></p><p>The small periodic drive at frequency <spanclass="math inline">\(\omega\)</span> leads to a small periodic changein <span class="math inline">\(p(u,t)\)</span> <spanclass="math display">\[    p(u,t)=p_0(u)+p_1(u)\cos (\omega t +\phi_{p}(u)) \tag{13.48}\]</span> We assume that <spanclass="math inline">\(\varepsilon\)</span> is small so that <spanclass="math inline">\(p_1(u)\ll p_0(u)\)</span> for most values of <spanclass="math inline">\(u\)</span>. We say that the change is at most of'order <span class="math inline">\(\varepsilon\)</span>'.</p><p><span class="math inline">\(J(u,t)\)</span> will also exhibit a smallperturbation of order <span class="math inline">\(\varepsilon\)</span>.For (13.42) with <span class="math inline">\(u_{r}=0\)</span> the fluxis <span class="math display">\[    J(u,t)=\left[ \frac{-u+RI(t)}{\tau_m}+\frac{\Delta_{T}}{\tau_m}\exp\left(\frac{u-\theta_{rh}}{\Delta_{T}}\right)-\frac{\sigma^{2}(t)}{2\tau_m}\frac{\partial}{\partial u}\right]p(u,t)=Q(u,t)p(u,t),\]</span> (13.49)</p><p>The stationary state under the assumption of constant input <spanclass="math inline">\(I(t)=I_0\)</span> has a flux <spanclass="math inline">\(J_0(u)=Q_0(u)p_0(u)\)</span>. In the presence ofthe periodic perturbation, the flux is <span class="math display">\[    J(u,t)=J_0(u)+J_1(u)\cos (\omega t+\phi_{J}(u)) \tag{13.50}\]</span></p><p>with <span class="math display">\[    J_1(u)\cos (\omega t+\phi_{J}(u))=Q_0(u)p_1(u)\cos (\omegat+\phi_{p}(u))+Q_1(u,t)p_0(u)+O(\varepsilon^{2}),\]</span> (13.51)</p><p>where <span class="math inline">\(Q_1(u,t)=\varepsilon R\cos (\omegat)/\tau_m\)</span> is the change of the operator <spanclass="math inline">\(Q\)</span> to order <spanclass="math inline">\(\varepsilon\)</span>.</p><p>Note that the flux through the threshold <spanclass="math inline">\(\theta_{reset}\)</span> gives the periodicmodulation of the population activity.</p><p>We insert <span class="math inline">\(A,p,J\)</span> into (13.38),include the phase into the definition of <spanclass="math inline">\(A_1,p_1(u),J_1(u)\)</span>, e.g. <spanclass="math inline">\(\hat{A}_1=A_1 \exp (i\phi_{A})\)</span>; the hatindicates the complex number. If we take the Fourier transform overtime, (13.38) becomes <span class="math display">\[    -\frac{\partial }{\partial u}\hat{J_1}(u)=i \omega\hat{p_1}(u)+\hat{A_1}[\delta(u-\theta_{reset})-\delta(u-u_r)].\tag{13.52}\]</span></p><ul><li>We have quite arbitrarily normalized the membrane potential densityto an integral of unity.</li><li>The flux <span class="math inline">\(J_1\)</span> in (13.51) can bequite naturally separated into two components. The first contribution tothe flux is proportional to the perturbation <spanclass="math inline">\(p_1\)</span> of the membrane potential density.The second component is caused by the direct action of the externalcurrent <span class="math inline">\(Q_1(u,t)=\varepsilon R\cos (\omegat)/\tau_m\)</span>.</li><li>The explicit expression for <span class="math inline">\(Q_0\)</span>and <span class="math inline">\(Q_1\)</span> can be inserted into(13.51) <span class="math display">\[  \frac{\partial }{\partial u} \hat{p_1}(u)=\frac{2}{\sigma^{2}}\left[-u+RI_0+\Delta_{T}\exp\left(\frac{u-\theta_{rh}}{\Delta_{T}}\right)\right]\hat{p_1}(u)+\frac{2R\varepsilon}{\sigma^{2}}p_0(u)-\frac{2\tau}{\sigma^{2}}\hat{J_1}(u).  \]</span> (13.53)</li></ul><p>We therefore have two first-order differential equations (13.52) and(13.53) which are coupled to each other.</p><p>(13.52) and (13.53) are two first-order differential equations whichare coupled to each other. We drop the hats on <spanclass="math inline">\(J_1\)</span>, <spanclass="math inline">\(p_1\)</span>, <spanclass="math inline">\(A_1\)</span> to lighten the notation. <spanclass="math inline">\(\varepsilon\)</span> and <spanclass="math inline">\(A_1\)</span> are parameters.</p><ul><li>We find the 'free' component by integrating (13.53) with <spanclass="math inline">\(\varepsilon=0\)</span> in parallel with (13.52)with <span class="math inline">\(A_1=1\)</span>. The integration startsat the initial condition <spanclass="math inline">\(p_1(\theta_{reset})=0\)</span> and <spanclass="math inline">\(J_1^{free}(\theta_{reset})=A_1=1\)</span> andcontinues toward decreasing voltage values. Integration stops ar a lowerbound <span class="math inline">\(u_{low}\)</span> which we place at anarbitrary large negative value.</li><li>We find the 'driven' component by integrating (13.53) with <spanclass="math inline">\(\varepsilon&gt;0\)</span> in parallel with (13.52)with parameter <span class="math inline">\(A_1=0\)</span> starting atthe initial condition <spanclass="math inline">\(p_1(\theta_{reset})=0\)</span> and <spanclass="math inline">\(J_1^{\varepsilon}(\theta_{reset})=A_1=0\)</span>and continue toward decreasing voltage values. Integration stops ar alower bound <span class="math inline">\(u_{low}\)</span></li></ul><p>Then we combinate them <span class="math display">\[    J_1(u)=a_1 J_1^{free}(u)+a_2 J_1^{\varepsilon}(u)\]</span> and <span class="math display">\[    p_1(u)=a_1 p_1^{free}(u)+a_2 p_1^{\varepsilon}(u)\]</span> We require a boundary condition <span class="math display">\[    0=J(u_{low})=a_1J_1^{free}(u_{low})+a_2 J_1^{\varepsilon}(u_{low}).\tag{13.54}\]</span></p><p>Recall that <span class="math inline">\(J_1^{\varepsilon}\)</span> isproportional to the drive <spanclass="math inline">\(\varepsilon\)</span>. <strong>The factor <spanclass="math inline">\(a_1\)</span> is the populationresponse.</strong></p><p>The gain factor is <span class="math display">\[    \hat{G}(\omega)=\frac{A_1}{\varepsilon}=\frac{a_1}{\varepsilon}=-\frac{a_2}{\varepsilon}\frac{J_1^{\varepsilon}(u_{low})}{J_1^{free}(u_{low})}.\tag{13.55}\]</span> which has an amplitude $() $ and a phase <spanclass="math inline">\(\phi_{G}\)</span>.</p><h2 id="neuronal-adaptation-and-synaptic-conductance">Neuronaladaptation and synaptic conductance</h2><h3 id="adaptation-currents">Adaptation currents</h3><p>Suppose that the stochastic spike arrival can be modeled by a mean<span class="math inline">\(\mu(t)=R\langle I(t)\rangle\)</span> plus awhite-noise term <span class="math inline">\(\xi\)</span>. <spanclass="math display">\[    \tau_m \frac{\mathrm{d}u}{\mathrm{d}t}=-(u-u_{rest})+\Delta_{T}\exp\left(\frac{u-\theta_{rh}}{\Delta _{T}}\right)-Rw+\mu(t)+\xi(t)\tag{13.59}\]</span> <span class="math display">\[    \tau_w \frac{\mathrm{d}w}{\mathrm{d}t}=a(u-u_{rest})-w+b\tau_w\sum_{t^{(f)}}^{} \delta(t-t^{(f)}). \tag{13.60}     \]</span></p><p>Suppose furthermore that the time constant <spanclass="math inline">\(\tau_w\)</span> of the adaptation variable <spanclass="math inline">\(w\)</span> is larger than <spanclass="math inline">\(\tau_m\)</span> and <spanclass="math inline">\(b\ll 1\)</span> so the fluctuations of theadaptation variable <span class="math inline">\(w\)</span> around $w $are small. Therefore for the solution of the membrane potential densityequations <span class="math inline">\(p_0(u)\)</span> the adaptationvariable can be approximated by a constant $w_0=w $.</p><h3 id="embedding-in-a-network">Embedding in a network</h3><p>The mean input <span class="math inline">\(\mu(t)\)</span> to neuron<span class="math inline">\(i\)</span> arriving at time <spanclass="math inline">\(t\)</span> from population <spanclass="math inline">\(k\)</span> is proportional to its activity <spanclass="math inline">\(A_k(t)\)</span>. The contribution of population<span class="math inline">\(k\)</span> to the variance <spanclass="math inline">\(\sigma^{2}\)</span> of <spanclass="math inline">\(\mu(t)\)</span> is also proportional to <spanclass="math inline">\(A_k(t)\)</span>; cd. Eq.(13.31).</p><p>We can analyze the stationary states and their stability in a networkof adaptive model neurons as</p><hr /><p>13.6 Neuronal adaptation and synaptic conductancehttps://neuronaldynamics.epfl.ch/online/Ch13.S6.html#Ch13.E61</p><hr /><h3 id="conductance-input-vs.-current-input">Conductance input vs.current input</h3><p>Synaptic input is more accurately described as a change inconductance <span class="math inline">\(g(t-t_j^{(f)})\)</span>, ratherthan as current injection. A time dependent synaptic conductance leadsto a total synaptic current into neuron <spanclass="math inline">\(i\)</span><br /><span class="math display">\[    I_i(t)=\sum_{j}^{} \sum_{f}^{}w_{ij}g_{ij}(t-t_j^{(f)})(u_i(t)-E_{syn}), \tag{13.63}   \]</span></p><p>We will now show that, in the state of stationary asynchronousactivity, conductance-based input can be approximated by an effectivecurrent input. The main effect of conductance-based input is that themembrane time constant of the stochastically driven neuron is shorterthan the 'raw' passive membrane time constant.</p><p>We consider <span class="math inline">\(N_{E}\)</span> excitatory and<span class="math inline">\(N_{I}\)</span> inhibitory LIF neurons in thesubthreshold regime <span class="math display">\[    C\frac{\mathrm{d}u}{\mathrm{d}t}=-g_{L}(u-E_{L})-g_{E}(t)(u-E_{E})-g_{I}(t)(u-E_{I}),\tag{13.64}\]</span> where <span class="math inline">\(C\)</span> is the membranecapacity, <span class="math inline">\(g_{L}\)</span> the leakconductance and <span class="math inline">\(E_{L}\)</span>, <spanclass="math inline">\(E_{E}\)</span>,<spanclass="math inline">\(E_{I}\)</span> are the reversal potentials forleak, excitation, and inhibition, respectively. Input spikes atexcitatory synapse lead to an increased conductance <spanclass="math display">\[    g_{E}(t)=\Delta g_{E}\sum_{j}^{} \sum_{f}^{} \exp[-(t-t_j^{(f)})/\tau_{E}] \mathscr{H}(t-t_j^{(f)}). \tag{13.65}   \]</span></p><p>The sum over <span class="math inline">\(j\)</span> runs over allexcitatory synapses. We assume that excitatory and inhibitory inputspikes arrive with a total rate <spanclass="math inline">\(\nu_{E}\)</span> and <spanclass="math inline">\(\nu_{I}\)</span>.</p><p>Using the methods from Chapter 8, we can calculate the meanexcitatory conductance <span class="math display">\[    g_{E,0}=\Delta g_{E}\nu_{E}\tau_{E}, \tag{13.66}\]</span> where <span class="math inline">\(\nu_{E}\)</span> is thetotal spike arrival rate at excitatory synapses.</p><p>The variance of the conductance is <span class="math display">\[    \sigma_{E}^{2}=\frac{1}{2}(\Delta g_{E})^{2}\nu_{E} \tau_{E}.\tag{13.67}\]</span></p><p>Write the conductance as the mean plus a fluctuating component <spanclass="math display">\[    g_{E,f}(t)=g_{E}(t)-g_{E,0}.\]</span> This turns (13.64) into <span class="math display">\[    C\frac{\mathrm{d}u}{\mathrm{d}t}=-g_0(u-\mu)-g_{E,f}(t)(u-E_{E})-g_{I,f}(t)(u-E_{I}),\tag{13.68}   \]</span> with a total conductance <spanclass="math inline">\(g_0=g_{L}+g_{E,0}+g_{I,0}\)</span> and aninput-dependent equilibrium potential <span class="math display">\[    \mu=\frac{g_{L}E_{L}+g_{E,0}E_{E}+g_{I,0}E_{I}}{g_0}. \tag{13.69}\]</span></p><p>In (13.64) the dynamics is characterized by a raw membrane timeconstant <span class="math inline">\(C/g_{L}\)</span> whereas (13.68) iscontrolled by an effective membrane time constant <spanclass="math display">\[    \tau_{eff}=\frac{C}{g_0}= \frac{C}{g_{L}+g_{E,0}+g_{I,0}},\tag{13.70}  \]</span></p><p>and a mean depolarization <span class="math inline">\(\mu\)</span>which acts as an effective equilibrium potential.</p><p>Then, we compare the momentary voltage <spanclass="math inline">\(u(t)\)</span> with the effective equilibriumpotential <span class="math inline">\(\mu\)</span>. The fluctuating partof the conductance in (13.68) can be written as <spanclass="math display">\[    g_{E,f}(t)(u-E_{E})=g_{E,f}(t)(\mu-E_{E})+g_{E,f}(t)(u-\mu),\tag{13.71}\]</span></p><p>The second term on the RHS of (13.71) is small compared to the firstterm and needs to be dropped to arrive at a consistent diffusionapproximation. The first term on the RHS of (13.71) can be interpretedas the summed effects of postsynaptic current pulses.</p><h3 id="colored-noise">Colored Noise</h3><p>Some synapse types, such as the NMDA component of excitatorysynapses, are rather slow. A spike that has arrived at an NMDA synapseat time <span class="math inline">\(t_0\)</span> generates afluctuation, which cause the input to exhibit temporal correlations.</p><p>We have two approaches to colored noise in the membrane potentialdensity equations.</p><p>The first approach is to approximate colored noise by white noise andreplace the temporal smoothing by a broad distribution of delays.Problem set: current-based. The mean input to a neuron <spanclass="math inline">\(i\)</span> in population <spanclass="math inline">\(n\)</span> arising from other populations <spanclass="math inline">\(k\)</span> is <span class="math display">\[    I_i(t)=\sum_{k}^{} C_{nk}w_{nk}\int_{0}^{\infty}\alpha_{nk}(s)A(t-s) \mathrm{d}s, \tag{13.72}      \]</span></p><p>Suppose <span class="math inline">\(C_{nk}\)</span> is a largenumber, but the population <span class="math inline">\(k\)</span> itselfis larger so that the connectivity <spanclass="math inline">\(C_{nk}/N_k \ll 1\)</span>. We now replace <spanclass="math inline">\(\alpha(s)\)</span> by <spanclass="math inline">\(q \delta(s-\Delta)\)</span> such that <spanclass="math inline">\(q=\int_{0}^{\infty} \alpha(s)\mathrm{d}s\)</span>. For each of the <spanclass="math inline">\(C_{nk}\)</span> connections we randomly draw thetransmission delay <span class="math inline">\(\Delta\)</span> from adistribution <spanclass="math inline">\(p(\Delta)=\alpha(\Delta)/q\)</span>. Because ofthe low connectivity, we may assume that the firing of different neuronsis uncorrelated. The broad distribution of delays stabilizes thestationary state of asynchronous firing.</p><p>The second approach consists in an explicit model of the synapticcurrent variables. We focus on a single population coupled to itself andsuppose that the synaptic current pulses are exponential <spanclass="math inline">\(\alpha(s)=(q/\tau_q)\exp (-s/\tau_q)\)</span>. Thedriving current of a neuron <span class="math inline">\(i\)</span> in apopulation <span class="math inline">\(n\)</span> is then <spanclass="math display">\[    \frac{\mathrm{d}I_i}{\mathrm{d}t}=-\frac{I_i}{\tau_q}+C_{nn}w_{nn}\frac{q}{\tau_q}A_n(t)\tag{13.73}\]</span> which we can verify by taking the temporal derivative of(13.72). <span class="math inline">\(A(t)\)</span> has a mean <spanclass="math inline">\(\mu(t)\)</span> (which is the same for allneurons) and a fluctuating part <spanclass="math inline">\(\xi_{i}(t)\)</span> with white-noisecharacteristics: <span class="math display">\[    \frac{\mathrm{d}I_i}{\mathrm{d}t}=-\frac{I_i}{\tau_q}+\mu(t)+\xi_{i}(t).\tag{13.74}\]</span> (13.74) needs to be combined with the differential equationfor the voltage <span class="math display">\[    \tau_m \frac{\mathrm{d}u_i}{\mathrm{d}t}=f(u_i)+RI_{i}(t),\tag{13.75}\]</span> We now have two coupled differential equations, the momentarystate of a population of <span class="math inline">\(N\)</span> neuronsis described by a two-dimensional density <spanclass="math inline">\(p(u,I)\)</span>.</p><p>We recall that, in the case of white noise, the membrane potentialdensity at threshold vanishes. The main insight for the mathematicaltreatment of the membrane potential density equations in two dimensionsis that the density at threshold <spanclass="math inline">\(p(\theta_{reset},I(t))\)</span> is finite,whenever the momentary slope of the voltage <spanclass="math inline">\(\mathrm{d}u/\mathrm{d}t \proptoRI(t)+f(\theta_{reset})\)</span> is positive.</p><h2 id="summary">Summary</h2><p>The momentary state of a population of one-dimensionalintegrate-and-and fire neurons can be characterized by the membranepotential density <span class="math inline">\(p(u,t)\)</span>. Thecontinuity equation describes the evolution of <spanclass="math inline">\(p(u,t)\)</span> over time. In the special casethat neurons in the population receive many inputs that each cause asmall change of the membrane potential, the continuity equation has theform of a Fokker-Planck equation. Several populations ofintegrate-and-fire neurons interact via the population activity <spanclass="math inline">\(A(t)\)</span> which is identified with the fluxacross the threshold.</p><p>The stationary state of the Fokker-Planck equation and the stabilityof the stationary solution can be calculated by a mix of analytical andnumerical methods, be it for a population of independent orinterconnected neurons. The mathematical and numerical methods developedfor membrane potential density equations apply to leaky as well as toarbitrary nonlinear one-dimensional integrate-and-fire model. A slowadaptation variable such as in the adaptive exponentialintegrate-and-fire model can be treated as quasi-stationary in theproximity of the stationary solution. Conductance input can beapproximated by an equivalent current-based model.</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (12)</title>
    <link href="/2022/10/05/Neuronal-Dynamics-12/"/>
    <url>/2022/10/05/Neuronal-Dynamics-12/</url>
    
    <content type="html"><![CDATA[<h1 id="neuronal-populations">Neuronal Populations</h1><p>The online version of this chapter:</p><hr /><p>Chapter 12 Neuronal Populationshttps://neuronaldynamics.epfl.ch/online/Ch12.html</p><hr /><p>The aim of this chapter is to provide the foundation of the notionsof 'neuronal population' and 'population activity'.</p><h2 id="columnar-organization">Columnar organization</h2><p>We present in this section a short introduction into the structuralorganization and functional characterization of cortex.</p><h3 id="receptive-fields">Receptive fields</h3><p>Simple cells in visual cortex are sensitive to the orientationn of alight bar.</p><p>In this and the following chapters, we exploit the fact thatneighboring neurons in visual cortex have similar receptive fields.</p><h4 id="example-cortical-maps">Example: Cortical Maps</h4><p>Neighboring neurons have similar receptive fields, but the exactcharacteristics of the receptive fields change slightly as one movesparallel to the cortical surface.</p><h3 id="how-many-populations">How many populations?</h3><p>Inside a column neurons are organized in different layers. Each layercontains one or several types of neurons. From shallow to deep, neuronsbecomes indistinguishable. Besides, the number of populations that atheoretician takes into account depends on the level of'coarse-graining' that he is ready to accept, as well as on the amountof information that is available from experiments.</p><h3 id="distributed-assemblies">Distributed assemblies</h3><p>The mathematical notion of population does not require that neuronsneed to form local groups to qualify as a homogeneous populaton.</p><p>Donald Hebb introduced the notion of neuronal assemblies, i.e.,groups of cells which get activated together so as to represent a mentalconcept. An assembly can be a group of neurons which are distributedacross one or several areas. However, such an assignment of a neuron toa population is not fixed, but can depend on the stimulus.</p><h2 id="identical-neurons-a-mathematical-abstraction">Identical Neurons:A Mathematical Abstraction</h2><p>In a population of <span class="math inline">\(N\)</span> neurons,the population activity is <span class="math display">\[    A(t)=\lim_{\Delta t \to 0}\frac{1}{\Delta t}\frac{n_{act}(t;t+\Deltat)}{N}=\frac{1}{N}\sum_{j=1}^{N} \sum_{f}^{} \delta(t-t_j^{(f)}),\tag{12.1}\]</span></p><p>For the sake of notational simplicity, we do not distinguish theobserved activity from its expectation value and denote in the followingthe expected activity by <span class="math inline">\(A(t)\)</span>.</p><h3 id="homogeneous-networks">Homogeneous networks</h3><p>By homogeneous we mean that - all neurons <spanclass="math inline">\(1\leqslant i\leqslant N\)</span> are identical; -all neurons receive the same external input <spanclass="math inline">\(I^{ext}_i(t)=I^{ext}(t)\)</span>; - theinteraction strength <span class="math inline">\(w_{ij}\)</span> for theconnection between any pair <span class="math inline">\(j,i\)</span> ofpre- and postsynaptic neurons is 'statistically uniform'.</p><h4id="homogeneous-population-of-integrate-and-fire-neurons">Homogeneouspopulation of integrate-and-fire neurons</h4><p>We assume that a neuron is coupled to all others as well as to itselfwith coupling strength <span class="math inline">\(w_{ij}=w_0\)</span>.The input current <span class="math inline">\(I_i\)</span> takes care ofboth the external drive and synaptic coupling <spanclass="math display">\[    I_i=\sum_{j=1}^{N} \sum_{f}^{} w_{ij}\alpha(t-t_j^{(f)})+I^{ext}(t). \tag{12.3}\]</span> Here we have assumed that each input spike generates apostsynaptic current with some generic time course <spanclass="math inline">\(\alpha(t-t_j^{(f)})\)</span>.</p><p>Using (12.1), we find a total input current, <spanclass="math display">\[    I(t)=w_0 N \int_{0}^{\infty} \alpha(s)A(t-s) \mathrm{d}s+I^{ext}(t),\tag{12.4}\]</span> which is independent of the neuronal index <spanclass="math inline">\(i\)</span>. Thus, the input current at time <spanclass="math inline">\(t\)</span> depends on the past population activityand is the same for all neurons.</p><h2 id="connectivity-schemes">Connectivity Schemes</h2><p>In the following we discuss some schemes with a special focus on thescaling behavior induced by each choice of coupling scheme. Here,scaling behavior refers to a change in the number <spanclass="math inline">\(N\)</span> of neurons that participate in thepopulation.</p><h3 id="full-connectivity">Full connectivity</h3><p>All-to-all connectivity, all connections have the same strength. Anappropriate scaling law is <span class="math display">\[    w_{ij}=\frac{J_0}{N}. \tag{12.6}\]</span></p><p>A slightly more intricate all-to-all coupling scheme is thefollowing: weights <span class="math inline">\(w_{ij}\)</span> are drawnfrom a Gaussian distribution with mean <spanclass="math inline">\(J_0/N\)</span> and standard deviation <spanclass="math inline">\(\sigma/\sqrt{N}\)</span>. The fluctuations of themembrane potential are of the order <spanclass="math inline">\(\sigma_0\)</span> even in the limit of large <spanclass="math inline">\(N\)</span>.</p><h3 id="random-coupling-fixed-coupling-probability">Random coupling:Fixed coupling probability</h3><p>Experimentally the probability <span class="math inline">\(p\)</span>that a neuron inside a cortical column makes a functional connection toanother neuron in the same column is in the range of 10%, but variesacross layers.</p><p>The number of presynaptic input links <spanclass="math inline">\(C_j\)</span> to a postsynaptic neuron <spanclass="math inline">\(j\)</span> has a mean value of <spanclass="math inline">\(\langle C_j \rangle=pN\)</span>, but fluctuatesbetween one neuron and the next with variance <spanclass="math inline">\(p(1-p)N\)</span>.</p><p>Alternatively, we can take one model neuron <spanclass="math inline">\(j=1,2,3,\cdots N\)</span> after the other andchoose randomly <span class="math inline">\(C=pN\)</span> presynapticpartners for it.</p><p>It is useful to scale the strengh of the connections as <spanclass="math display">\[    w_{ij}=\frac{J_0}{C}=\frac{J_0}{pN}, \tag{12.7}\]</span></p><h3 id="random-coupling-fixed-number-of-presynaptic-partners">Randomcoupling: Fixed number of presynaptic partners</h3><p>We pick one model neuron <span class="math inline">\(j=1,2,3,\cdotsN\)</span> after the other and choose randomly its <spanclass="math inline">\(C\)</span> presynaptic partners. Whenever thenetwork size <span class="math inline">\(N\)</span> is much bigger than<span class="math inline">\(C\)</span>, the inputs to a given neuron canbe thought of as random samples from the current network activity. Noscaling of the connections with the population size <spanclass="math inline">\(N\)</span> is necessary.</p><h3 id="balanced-excitation-and-inhibition">Balanced excitation andinhibition</h3><p>If the total amount of excitation and inhibition cancel each other,so that excitation and inhibition are 'balanced'. The resulting networkis called a balanced network or a population with balanced excitationand inhibition.</p><p>We can scale synaptic weights so as to control specifically theamount of fluctuations of the input current around zero. An appropriatechoice is <span class="math display">\[    w_{ij}=\frac{J_0}{\sqrt{C}}=\frac{J_0}{\sqrt{pN}}. \tag{12.8}\]</span></p><h3 id="interacting-populations">Interacting Populations</h3><p>We assume that neurons are homogeneous within each pool. The activityof neurons in pool <span class="math inline">\(n\)</span> is <spanclass="math display">\[    A_n(t)=\frac{1}{N_n}\sum_{j \in \Gamma_n}^{} \sum_{f}^{}\delta(t-t_j^{(f)}), \tag{12.9}\]</span> where <span class="math inline">\(N_n\)</span> is the numberof neurons in pool <span class="math inline">\(n\)</span> and <spanclass="math inline">\(\Gamma_n\)</span> denotes the set of neurons thatbelong to pool <span class="math inline">\(n\)</span>. Each neuron <spanclass="math inline">\(i\)</span> in pool <spanclass="math inline">\(n\)</span> receives input from all neuons <spanclass="math inline">\(j\)</span> in pool <spanclass="math inline">\(m\)</span> with strength <spanclass="math inline">\(w_{ij}=J_{nm}/N_m\)</span>. The time course <spanclass="math inline">\(\alpha_{ij}(s)\)</span> caused by a spike of apresynaptic neuron <span class="math inline">\(j\)</span> may depend onthe synapse type. The input current to a neuron <spanclass="math inline">\(i\)</span> in group <spanclass="math inline">\(\Gamma_n\)</span> is generated by the spikes ofall neurons in the network, <span class="math display">\[    I_{i,n}=\sum_{j}^{} \sum_{f}^{}w_{ij}\alpha_{ij}(t-t_j^{(f)})=\sum_{m}^{} J_{nm}\int_{0}^{\infty}\alpha_{nm}\sum_{j\in \Gamma_m}^{} \sum_{f}^{}\frac{\delta(t-t_j^{(f)}-s)}{N_m} \mathrm{d}s,\]</span> (12.10)</p><p>where <span class="math inline">\(\alpha_{nm}(t-t_j^{(f)})\)</span>denotes thhe time course of a postsynaptic current caused by spikefiring at time <span class="math inline">\(t_j^{(f)}\)</span> of thepresynaptic neuron <span class="math inline">\(j\)</span> which is partof population <span class="math inline">\(m\)</span>. So <spanclass="math display">\[    I_n=\sum_{m}^{} J_{nm}\int_{0}^{\infty} \alpha(s)A_m(t-s)\mathrm{d}s. \tag{12.11}      \]</span> We have dropped the index <spanclass="math inline">\(i\)</span> since the input current is the same forall neurons in pool <span class="math inline">\(n\)</span>.</p><h3 id="distance-dependent-connectivity">Distance dependentconnectivity</h3><p>For models of distance-dependent connectivity it is necessary toassign to each model neuron <span class="math inline">\(i\)</span> alocation <span class="math inline">\(x(i)\)</span> on thetwo-dimensional cortical sheet.</p><p>Two different algorithmic procedures can be used to assigndistance-dependent connectivity. The first one assumes full connectivitywith a strength <span class="math inline">\(w_{ij}\)</span> which fallsoff with distance</p><p><span class="math display">\[    w_{ij}=w(\lvert x(i)-x(j) \rvert ), \tag{12.12}\]</span></p><p>One may assume finite support so that <spanclass="math inline">\(w\)</span> vanishes for distances <spanclass="math inline">\(\lvert x(i)-x(j) \rvert &gt;d\)</span>.</p><p>The second alternative is to give all connections the same weight,but to assume that the probability <spanclass="math inline">\(P\)</span> of forming a connection depends on thedistance <span class="math display">\[    \operatorname{Pr}(w_{ij}=1)=P(\lvert x(i)-x(j) \rvert ), \tag{12.13}\]</span></p><h3 id="spatial-continuum-limite">Spatial Continuum Limite</h3><p>For neurons organized in a spatially extended multidimensionalnetwork, a description by discrete pool does not seem appropriate.However, a transition from discrete pools to a continuous population ispossible.</p><p>We consider a population of neurons that extends along aone-dimensional axis and discretize space in segments of size <spanclass="math inline">\(d\)</span>. The number of neurons in the interval<span class="math inline">\([nd,(n+1)d]\)</span> is <spanclass="math inline">\(N_n=\rho d\)</span> where <spanclass="math inline">\(\rho\)</span> is the spatial density. Neurons inthat interval form the group <spanclass="math inline">\(\Gamma_n\)</span>.</p><p>We replace our notation <span class="math display">\[    A_m(t) \longrightarrow A(md,t) =A(y,t). \tag{12.14}\]</span></p><p>We have <span class="math inline">\(J_{nm}=\rho d w(nd,md)\)</span>.Use (12.11) and find <span class="math display">\[    I(nd,t)=\rho \sum_{m}^{} d w(nd,md) \int_{0}^{\infty}\alpha(s)A(md,t-s) \mathrm{d}s, \tag{12.15}\]</span> where <span class="math inline">\(\alpha(s)\)</span> describesthe time course of the postsynaptic current caused by spike firing inone of the presynaptic neurons. For <span class="math inline">\(d \to0\)</span>, we arrive at <span class="math display">\[    I(x,t)=\rho \int_{}^{} w(x,y)\int_{0}^{\infty} \alpha(s)A(y,t-s)\mathrm{d}s \mathrm{d}y, \tag{12.16}\]</span></p><p>To rephrase (12.16) in words, the input to neurons at location <spanclass="math inline">\(x\)</span> depends on the spatial distribution ofthe population activity convolved with the spatial coupling filter <spanclass="math inline">\(w(x,y)\)</span> and the temporal filter <spanclass="math inline">\(\alpha(s)\)</span>. The population activity <spanclass="math inline">\(A(y,t-s)\Delta s\)</span> is the number of spikesin a short interval <span class="math inline">\(\Delta s\)</span> summedacross neurons in the neighborhood around <spanclass="math inline">\(y\)</span> normalized by the number of neurons inthat neighborhood.</p><h2 id="from-microscopic-to-macroscopic">From Microscopic toMacroscopic</h2><p>We now make the transition from the properties of single spikingneurons to the population activity in a homogeneous group ofneurons.</p><h3 id="stationary-activity-and-asynchronous-firing">Stationary activityand asynchronous firing</h3><p>We define asynchronous firing of a neuronal population as amacroscopic firing state with constant activity <spanclass="math inline">\(A(t)=A_0\)</span>. We will see that the onlyrelevant single-neuron property is its gain function, i.e. its meanfiring rate as a function of input.</p><p>If the filter is kept fixed, while the population size is increased,the population activity in the stationary state of asynchronous firingapproaches the constant value <spanclass="math inline">\(A_0\)</span>.</p><h3 id="stationary-activity-as-single-neuron-firing-rate">StationaryActivity as Single-Neuron Firing Rate</h3><p>In a finite population, the empirical activity fluctuates and we canpredict the expectation value <span class="math display">\[    \langle A_0\rangle =\nu_i. \tag{12.18}\]</span> The mean firing rate is given by the gain function <spanclass="math display">\[    \nu_i=g_{\sigma}(I_0), \tag{12.19}\]</span> where the subscript <spanclass="math inline">\(\sigma\)</span> is intended to remind the readerthat the shape of the gain function depends on the level of noise.</p><h3 id="activity-of-a-fully-connected-network">Activity of a fullyconnected network</h3><p>We know <span class="math display">\[    \langle A_0\rangle =g_{\sigma}(I). \tag{12.21}\]</span> The gain function in the absence of any noise (fluctuationamplitude <span class="math inline">\(\sigma=0\)</span>) will be denotedby <span class="math inline">\(g_0\)</span>.</p><p>We can impose a normalization <spanclass="math inline">\(\int_{0}^{\infty} \alpha(s) \mathrm{d}s=1\)</span>and set <span class="math inline">\(\int_{0}^{\infty} \alpha(s)A(t-s)\mathrm{d}s=A_0\)</span>.</p><p>Therefore, the assumption of stationarity activity <spanclass="math inline">\(A_0\)</span> combined with the assumption ofconstant external input <spanclass="math inline">\(I^{ext}(t)=I_0^{ext}\)</span> yields a constanttotal driving current <span class="math display">\[    I_0=w_0 NA_0+I_0^{ext}. \tag{12.23}\]</span></p><p>Together with (12.21) we arrive at an implicit equation for thepopulation activity <span class="math inline">\(A_0\)</span>, <spanclass="math display">\[    A_0=g_0(J_0A_0+I_0^{ext}). \tag{12.24}\]</span> where <span class="math inline">\(g_0\)</span> is thenoise-free gain function of single neurons and <spanclass="math inline">\(J_0=w_0N\)</span>.</p><h4id="example-leaky-integrate-and-fire-model-with-diffusive-noise">Example:Leaky integrate-and-fire model with diffusive noise</h4><p>We consider a large and fully connected network of identical leakyintegrate-and-fire neurons with homogeneous coupling <spanclass="math inline">\(w_{ij}=J_0/N\)</span> and normalized postsynapticcurrents <span class="math inline">\(\int_{0}^{\infty} \alpha(s)\mathrm{d}s=1\)</span>. In the state of asynchronous firing, the totalinput current driving a typical neuron of the network is then</p><p><span class="math display">\[    I_0=I^{ext}+J_0A_0. \tag{12.25}\]</span> In addition, each neuron receives individual diffusive noiseof variance <span class="math inline">\(\sigma^{2}\)</span> that couldrepresent spike arrival from other populations. The single-neuron gainfunction in the presence of diffusive noise has been stated in (8.54).We use the formula of the gain function to calculate the populationacitvity <span class="math display">\[    A_0=g_{\sigma}(I_0)=\left\{ \tau_m\sqrt{\pi}\int_{\frac{u_r-RI_0}{\sigma}}^{\frac{\theta-RI_0}{\sigma}}\exp (u^{2})[1+\text{erf}(u)]  \mathrm{d}u\right\}^{-1}, \tag{12.26}\]</span></p><p><strong>(Siegert-formula)</strong></p><h3 id="activity-of-a-randomly-connected-network">Activity of a randomlyconnected network</h3><p>In this subsection, we discuss how to mathematically treat theadditional noise arising from the network.</p><p>If all neurons fire at a rate <spanclass="math inline">\(\nu\)</span> then the mean input current to neuron<span class="math inline">\(i\)</span> generated by its <spanclass="math inline">\(C_{pre}\)</span> presynaptic partners is <spanclass="math display">\[    \langle I_0\rangle =C_{pre}qw\nu+I_0^{ext}, \tag{12.27}\]</span> where <span class="math inline">\(q=\int_{0}^{\infty}\alpha(s) \mathrm{d}s\)</span> denotes the integral over thepostsynaptic current and can be interpreted as the total electric chargedelivered by a single input spike.</p><p>The input current is not constant but fluctuates with a variance<span class="math inline">\(\sigma_{I}^{2}\)</span> given by <spanclass="math display">\[    \sigma_{I}^{2}=C_{pre} w^{2} q_2 \nu, \tag{12.28}\]</span> where <span class="math inline">\(q_2=\int_{0}^{\infty}\alpha^{2}(s) \mathrm{d}s\)</span>.</p><h4 id="brunel-network-excitatory-and-inhibitory-populations">Brunelnetwork: excitatory and inhibitory populations</h4><p>We assume that excitatory and inhibitory neurons have the sameparameters <span class="math inline">\(\theta, \tau_m, R\)</span> and<span class="math inline">\(u_r\)</span>. All neurons are driven by acommon external current <span class="math inline">\(I^{ext}\)</span>.Each neuron in the population receives <spanclass="math inline">\(C_{E}\)</span> synapses from excitatory neuronswith weight <span class="math inline">\(w_{E}&gt;0\)</span> and <spanclass="math inline">\(C_{I}\)</span> synapses from inhibitory neuronswith weight <span class="math inline">\(w_{I}&lt;0\)</span>.</p><p>If an input spike arrives at the synapses of neuron <spanclass="math inline">\(i\)</span> from a presynaptic neuron <spanclass="math inline">\(j\)</span>, its membrane potential changes by anamount <span class="math inline">\(\Delta u_{E}=w_{E}qR/\tau_m\)</span>if <span class="math inline">\(j\)</span> is excitatory and <spanclass="math inline">\(\Delta u_{I}=\Delta u_{E} w_{I}/w_{E}\)</span> if<span class="math inline">\(j\)</span> is inhibitory. We set <spanclass="math display">\[    \gamma=\frac{C_{I}}{C_{E}}, \quadg=-\frac{w_{I}}{w_{E}}=-\frac{\Delta u_{I}}{\Delta u_{E}}.\tag{12.30}    \]</span></p><p>Since excitatory and inhibitory neurons receive the same number ofinput connections in our model, we assume that they fire with a commonfiring rate <span class="math inline">\(\nu\)</span>. The total inputcurrent generated by the external current and by the lateral couplingsis <span class="math display">\[    I_0=I^{ext}+q\sum_{j}^{} \nu_j w_j=I_0^{ext}+q\nuw_{E}C_{E}[1-\gamma g]. \tag{12.31}\]</span></p><p>We measure the noise strength by the variance <spanclass="math inline">\(\sigma_{u}^{2}\)</span> of the membrane potential(as opposed to the variance <spanclass="math inline">\(\sigma_{I}^{2}\)</span> of the input). FromChapter 8, we set <spanclass="math inline">\(\sigma_{u}^{2}=\frac{1}{2}\sigma^{2}\)</span>where <span class="math display">\[    \sigma^{2}=\sum_{j}^{} \nu_j \tau(\Delta u_j^{2})=\nu(\Deltau_{E})^{2} C_{E}[1+\gamma g^{2}]. \tag{12.32}\]</span> The stationary firing rate <spanclass="math inline">\(A_0\)</span> of the population with mean input<span class="math inline">\(I_0\)</span> and variance <spanclass="math inline">\(\sigma\)</span> is copied from (12.26) andrepeated here for convenience</p><p><span class="math display">\[    A_0=\nu=g_{\sigma}(I_0)=\frac{1}{\tau_m}\left\{\sqrt{\pi}\int_{\frac{u_r-RI_0}{\sigma}}^{\frac{\theta-RI_0}{\sigma}}\exp (u^{2})[1+\text{erf}(u)]  \mathrm{d}u\right\}^{-1}, \tag{12.33}\]</span></p><p>Numerical solutions of (12.31)-(12.33) have been obtained by Amit andBrunel.</p><h4 id="example-inhibition-dominated-network">Example: Inhibitiondominated network</h4><p>Suppose the mean feedback is dominated by inhibition. The effectivecoupling <span class="math inline">\(J^{eff}=\tau C_{E}\Deltau_{E}(1-\gamma g)\)</span>. In this case (12.31) is to be replaced by<span class="math display">\[    h_0=\tau_m \nu \Delta u_{E}C_{E}[1-\gamma g]+\tau_m \nu_{ext}\Deltau_{ext} C_{ext}, \tag{12.34}\]</span> with <span class="math inline">\(C_{ext}\)</span> the numberof connections that a neuron receives from neurons outside thepopulation, <span class="math inline">\(\Delta u_{ext}\)</span> theirtypical coupling strength characterized by the amplitude of the voltagejump, and <span class="math inline">\(\nu_{ext}\)</span> their spikearrival rate. Due to the extra stochasticity in the input, the variance<span class="math inline">\(\sigma_u^{2}\)</span> of the membranevoltage is larger <span class="math display">\[    \sigma_u^{2}=\frac{1}{2}\sigma^{2}=\frac{1}{2}\tau_m \nu(\Deltau_{E})^{2}C_{E}[1+\gamma g^{2}]+\frac{1}{2}\tau_m \nu_{ext}(\Deltau_{ext})^{2} C_{ext} \tag{12.35}\]</span></p><p>(12.33)-(12.35) can be solved numerically.</p><h4 id="example-vogels-abbott-network">Example: Vogels-Abbottnetwork</h4><p>Excitatory and inhibitory model neurons have the same parameters andare connected with the same probability <spanclass="math inline">\(p\)</span> within and across the twosub-populations. The two difference to the Brunel network are - thechoice of random connectivity in the Vogels-Abbott network does notpreserve the number of presynaptic partners per neuron so that someneurons receive more and others less than <spanclass="math inline">\(pN\)</span> connections - neurons in theVogels-Abbott network communicates with each other by conductance-basedsynapses. A spike fired at time <spanclass="math inline">\(t_j^{(f)}\)</span> causes a change in conductance<span class="math display">\[    \tau_g \frac{\mathrm{d}g}{\mathrm{d}t}=-g+\tau_g \Delta g\sum_{f}^{} \delta(t-t_j^{(f)}). \tag{12.36}\]</span> Thus, a synaptic input causes for <spanclass="math inline">\(t&gt;t_j^{(f)}\)</span> a contribution to theconductance <span class="math inline">\(g(t)=\Delta g \exp[-(t-t_j^{(f)})/\tau_g]\)</span>.</p><p>The dominant effect of conductance based input is a decrease of theeffective membrane time constant. The mean input current <spanclass="math inline">\(I_0\)</span> and the fluctuations <spanclass="math inline">\(\sigma\)</span> of the membrane voltage also enterinto the time constant <spanclass="math inline">\(\tau_{eff}\)</span>.</p><p>The Siegert formula holds only for short time constants for theconductances (<span class="math inline">\(\tau_{E}\to 0\)</span> and<span class="math inline">\(\tau_{I}\to 0\)</span>).</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (11)</title>
    <link href="/2022/10/04/Neuronal-Dynamics-11/"/>
    <url>/2022/10/04/Neuronal-Dynamics-11/</url>
    
    <content type="html"><![CDATA[<h1 id="encoding-and-decoding-with-stochastic-neuron-models">Encodingand Decoding with Stochastic Neuron models</h1><p>The online version of this chapter:</p><hr /><p>Chapter 11 Encoding and Decoding with Stochastic Neuron modelshttps://neuronaldynamics.epfl.ch/online/Ch11.html</p><hr /><h2 id="encoding-models-for-intracellular-recordings">Encoding Modelsfor Intracellular Recordings</h2><p>We focus on GLMs with escape noise, also called soft-thresholdintegrate-and-fire models.</p><h3 id="predicting-membrane-potential">Predicting MembranePotential</h3><p>The SRM model <span class="math display">\[    u(t)=\sum_{f}^{} \eta(t-t^{(f)})+\int_{0}^{\infty}\kappa(s)I^{ext}(t-s) \mathrm{d}s +u_{rest}. \tag{11.1}\]</span></p><p>For both the main type of excitatory neurons and the main type oninhibitory neurons, the membrane filter <spanclass="math inline">\(\kappa(t)\)</span> is well described by a singleexponential. Different cell types have different amplitudes and timeconstants. The inhibitory neurons are typically faster, with a smallertime constant than the excitatory neurons, suggesting we coulddiscriminate between excitatory and inhibitory neurons in terms of theshape of <span class="math inline">\(\kappa(t)\)</span>. When we takeinto account the spike-afterpotential, discrimination of cell types ismuch improved. The shape of <span class="math inline">\(\eta(t)\)</span>in inhibitory cells is very different than that in excitatory ones.</p><p>While the spike-afterpotential is a monotonically decreasing functionin the excitatory cells, in the inhibitory cells the function <spanclass="math inline">\(\eta(t)\)</span> is better fitted by twoexponentials of opposite polarity. Spike afterpotential of inhibitoryneurons has an oscillatory component.</p><p>If we set <span class="math inline">\(\kappa(s)=(1/C)\exp(-s/\tau_m)\)</span>, we can take the derivative of (11.1) and write itin the form of <span class="math display">\[    C\frac{\mathrm{d}u(t)}{\mathrm{d}t}=-\frac{1}{R}(u-u_{rest})+\sum_{f}^{}\tilde{\eta}(t-t^{(f)})+I^{ext}(t) \tag{11.2}\]</span> where <span class="math inline">\(\tilde{\eta}\)</span> is thetime course of the net current triggered after a spike.</p><p>After every spike the total current that can charge the membranecapacitance is <span class="math display">\[    C\frac{\mathrm{d}u}{\mathrm{d}t} \propto\eta_{C}(t-\hat{t})(u-E_{rev}) \tag{11.3}\]</span> where <span class="math inline">\(\eta_{C}\)</span> is thespike triggered change in conductance and <spanclass="math inline">\(E_{rev}\)</span> its reversal potential. Theprediction performance based on conductance change instead of spikeafter-effects in terms of potential is not significantly improved.</p><h3 id="predicting-spikes">Predicting Spikes</h3><p>Assuming a moving threshold that can undergo a stereotypical changeat every spike <spanclass="math inline">\(\theta(t)=\theta_0+\sum_{f}^{}\theta_1(t-t^{(f)})\)</span> we can model the conditional firingintensity as (c.f., 9.27) <span class="math display">\[    p(t|S)=\frac{1}{\tau_0}\exp \left[\beta\left(u(t)-\theta_0-\sum_{t^{(f)}\in S}^{} \theta_1(t-t^{(f)})\right)\right] \tag{11.4}\]</span></p><p>Since the parameters regulating <spanclass="math inline">\(u(t)\)</span> were optimized using thesubthreshold membrane potential in 11.1.1, the only free parameters leftare those of the threshold, that is, <spanclass="math inline">\(\theta_0\)</span>,<spanclass="math inline">\(\beta\)</span> and the function <spanclass="math inline">\(\theta_1(t)\)</span>. Once the function <spanclass="math inline">\(\theta_1\)</span> is expanded in a linearcombination of basis functions, maximizing the likelihood (10.40) can bedone through a convex gradient descent.</p><p>Dynamic threshold is not necessary for some neurons. The threshold inthose cells is constant in time. However, the excitatory cells have astrongly moving threshold which is characterized by at least two decaytime constants. Inactivation of sodium channels is a likely candidatefor the biophysical causes of a moving threshold.</p><p>GLMs predict more than 80 percent of the 'predictable' spikes. Somecells were predicted better than others such that the <spanclass="math inline">\(M\)</span> reached up to 95%. Other optimizationmethods but with similar models could improve the spike timingprediction of inhibitory neurons, reaching up to <spanclass="math inline">\(M\)</span>=100% for some cells. However,optimizing a GLM model with refractory effects but no adaptation reducesthe prediction performance by 20-30%, for both the excitatory andinhibitory cortical cells. A single spike has a measurable effect morethan 10 seconds after the action potential has occurred. Thus,adaptation is not characterized by a single time scale and shows up as apower-law decay in both spike-triggered current and threshold.</p><h2 id="encoding-models-in-systems-neuronscience">Encoding Models inSystems Neuronscience</h2><h3 id="receptive-fields-and-linear-nonlinear-poisson-model">Receptivefields and Linear-Nonlinear Poisson Model</h3><p>For a two-dimensional image, we label all pixels with a single index<span class="math inline">\(k\)</span>. A full image corresponds to avector <span class="math inline">\(\mathbf{x}=(x_1,\cdots,x_{K})\)</span> while a single spot of light corresponds to a vectorwith all components equal to zero except one.</p><p>The spatial receptive field of a neuron is a vector <spanclass="math inline">\(\mathbf{k}\)</span> of the same dimensionality as<span class="math inline">\(\mathbf{x}\)</span>. The response of theneuron to an arbitrary spatial stimulus <spanclass="math inline">\(\mathbf{x}\)</span> depends on the total drive<span class="math inline">\(\mathbf{k}\cdot \mathbf{x}_t\)</span>, i.e.,the similarity between the stimulus and the spatial filter.</p><p>More generally, the receptive field filter <spanclass="math inline">\(\mathbf{k}\)</span> can be described not only by aspatial component, but also by a temporal component. The scalar product<span class="math inline">\(\mathbf{k}\cdot \mathbf{x}_t\)</span> is ashorthand notation for integration over space as well as over time. Sucha filter <span class="math inline">\(\mathbf{k}\)</span> is called aspatio-temporal receptive field.</p><p>In the linear-nonlinear-Poisson (LNP) model, one assumes that spiketrains are produced by an inhomogeneous Poisson process with rate <spanclass="math display">\[    \rho(t)=f(\mathbf{k}\cdot \mathbf{x}_t) \tag{11.5}\]</span> Note that the LNP model neglects the spike history effectsthat are the hallmark of the SRM and the GLM - otherwise the two modelsare suprisingly similar.</p><h4id="example-detour-on-reverse-correlation-for-receptive-field-estimation">Example:Detour on reverse correlation for receptive field estimation</h4><p>Reverse correlation measurements are an experimental procedure basedon spike-triggered averaging. Stimuli <spanclass="math inline">\(\mathbf{x}\)</span> are drawn from somestatistical ensemble and presented on after the other. Each time theneuron elicits a spike, the stimulus <spanclass="math inline">\(\mathbf{x}\)</span> presented just before thefiring is recorded. The reverse correlation filter is the mean of allinputs that have triggered a spike <span class="math display">\[    \mathbf{x}_{RevCorr}=\langle \mathbf{x}\rangle_{spike}=\frac{\sum_{t}^{} n_t \mathbf{x}_t}{\sum_{t}^{} n_t},\tag{11.6}\]</span> where <span class="math inline">\(n_t\)</span> is the spikecount in trial <span class="math inline">\(t\)</span>. The reversecorrelation technique finds the typical stimulus that causes aspike.</p><p>Consider an ensemble <spanclass="math inline">\(p(\mathbf{x})\)</span> of stimuli <spanclass="math inline">\(\mathbf{x}\)</span> with a 'power' constraint<span class="math inline">\(\lvert \mathbf{x} \rvert ^{2}&lt;c\)</span>.In this case, the stimulus that is most likely to generate a spike underthe linear receptive field model is the one which is aligned with thereceptive field <span class="math display">\[    \mathbf{x}_{opt}\propto \mathbf{k}  \tag{11.7}  \]</span> The receptive field vector <spanclass="math inline">\(\mathbf{k}\)</span> can be interpreted as theoptimal stimulus to cause a spike.</p><p>Then, consider an ensemble of stimuli <spanclass="math inline">\(\mathbf{x}\)</span> with a radially-symmetricdistribution, where the probability of a possibly multidimensional <spanclass="math inline">\(\mathbf{x}\)</span> is equal to the probability ofobserving its norm <span class="math inline">\(\lvert \mathbf{x} \rvert\colon p(\mathbf{x})=p_c(\lvert \mathbf{x} \rvert )\)</span>. Animportant result is that the experimental reverse correlation techniqueyields an unbiased estimator of the filter <spanclass="math inline">\(\mathbf{k}\)</span>, i.e., <spanclass="math display">\[    \langle \mathbf{x}_{RevCorr}\rangle =\mathbf{k}. \tag{11.8}\]</span></p><h3 id="multiple-neurons">Multiple Neurons</h3><p>An SRM-like model of the membrane potential of a neuron <spanclass="math inline">\(i\)</span> surrounded by <spanclass="math inline">\(n\)</span> other neurons is <spanclass="math display">\[    u_i(t)=\sum_{f}^{} \eta_i(t-t_i^{(f)})+\mathbf{k}_i \cdot\mathbf{x}(t)+\sum_{j \neq i}^{} \sum_{f}^{}\varepsilon_{ij}(t-t_j^{(f)})+u_{rest}. \tag{11.12}\]</span></p><h2 id="decoding">Decoding</h2><p>In this section, we apply 'Bayes' rule to obtain the posteriorprobability of the stimulus, conditional on the observed response: <spanclass="math display">\[    p(\mathbf{x}|D)\propto p(D|\mathbf{x})p(\mathbf{x}), \tag{11.13}\]</span></p><h3 id="maximum-a-posteriori-decoding">Maximum a posterioridecoding</h3><p>The <strong>Maximum A Posteriori</strong> (MAP) estimate is thestimulus <span class="math inline">\(\mathbf{x}\)</span> that is mostprobable given the observed spike response <spanclass="math inline">\(D\)</span>, i.e., the <spanclass="math inline">\(\mathbf{x}\)</span> that maximizes <spanclass="math inline">\(p(\mathbf{x}|D)\)</span>.</p><p>The log-posterior, <span class="math display">\[    \log p(\mathbf{x}|D)=\log p(D|\mathbf{x})+\log p(\mathbf{x})+c\tag{11.14}\]</span> is concave as long as the stimulus log-prior <spanclass="math inline">\(\log p(\mathbf{x})\)</span> is itself a concavefunction of <span class="math inline">\(\mathbf{x}\)</span> (e.g. <spanclass="math inline">\(p\)</span> is Gaussian). In this case, again, wemay easily compute <span class="math inline">\(\hat{x}_{MAP}\)</span> bynumerically ascending the function <span class="math inline">\(\logp(\mathbf{x}|D)\)</span>.</p><p>The MAP estimate of the stimulus is, in general, a nonlinear functionof the observed spiking data <span class="math inline">\(D\)</span>.</p><h4 id="example-linear-stimulus-reconstruction">Example: Linear stimulusReconstruction</h4><p>We predict the stimulus <spanclass="math inline">\(\mathbf{x}_t\)</span> by linear filtering of theobserved spike times <span class="math inline">\(t^{1},t^{2},\cdots,t^{F}&lt;t\)</span>, <span class="math display">\[    x(t)=x_0+\sum_{f}^{} k(t-t^{f}) \tag{11.15}\]</span></p><p>The aim is to find the shape of the filter <spanclass="math inline">\(k\)</span>, i.e., the optimal linear estimator(OLE) of the stimulus. It can be obtained using standard least-squaresregression of the spiking data onto the stimulus <spanclass="math inline">\(\mathbf{x}\)</span>.</p><h3 id="assessing-decoding-uncertainty">Assessing decodinguncertainty</h3><p>In addition to providing a reliable estimate of the stimulusunderlying a set of spike responses, computing the MAP estimate <spanclass="math inline">\(\hat{x}_{MAP}\)</span> gives us easy access to thevariance of the posterior distribution around <spanclass="math inline">\(\hat{x}_{MAP}\)</span>. It tells us somethingabout which stimulus features are best encoded by the response <spanclass="math inline">\(D\)</span>.</p><p>For example, along stimulus axes where the posterior has smallvariance (i.e. the posterior declines rapidly as we move away from <spanclass="math inline">\(\hat{x}_{MAP}\)</span>), we have relatively highcertainty that the true <span class="math inline">\(\mathbf{x}\)</span>is close to <span class="math inline">\(\hat{x}_{MAP}\)</span>.Conversely, we have relatively low certainty about any feature axisalong which the posterior variance is large.</p><p>We measure the curvature at <spanclass="math inline">\(\hat{x}_{MAP}\)</span> by computing the 'Hessian'matrix <span class="math inline">\(A\)</span> of second-derivatives ofthe log-posterior, <span class="math display">\[    A_{ij}=-\frac{\partial ^{2}}{\partial x_i \partial x_j}\logp(\mathbf{x}|D). \tag{11.16}\]</span></p><p>Moreover, the eigendecomposition of this matrix <spanclass="math inline">\(A\)</span> tells us exactly which axes of stimulusspace correspond to the 'best' and 'worst' encoded feature of the neuralresponse: small eigenvalues of <span class="math inline">\(A\)</span>correspond to directions of small curvature, where the observed data<span class="math inline">\(D\)</span> poorly constrains the posteriordistribution <span class="math inline">\(p(\mathbf{x}|D)\)</span> (andtherefore the posterior variance will be relatively large in thisdirection), while conversely large eigenvalues in <spanclass="math inline">\(A\)</span> imply relatively precise knowledge of<span class="math inline">\(\mathbf{x}\)</span>, i.e., small posteriorvariance (for this reason the Hessian of the log-likelihood <spanclass="math inline">\(p(D|x)\)</span> is referred to as the 'observedFisher information matrix' in the statistic literature).</p><p>We can use this Hessian to construct a useful approximation to theposterior <span class="math inline">\(p(\mathbf{x}|D)\)</span>. The ideais simply to approximate this log-concave bump with a Gaussian function,where the parameters of the Gaussian are chosen to exactly match thepeak and curvature of the true posterior. <span class="math display">\[    p(\mathbf{x}|D)\thickapprox (2\pi)^{-d/2}\lvert A \rvert^{1/2}\mathrm{e}^{-(\mathbf{x}-\hat{x}_{MAP})^{\mathsf{T}}A(\mathbf{x}-\hat{x}_{MAP})^{2}}, \tag{11.17}\]</span> with <spanclass="math inline">\(d=\text{dim}(\mathbf{x})\)</span>. The approximateposterior entropy of variance of <spanclass="math inline">\(x_i\)</span> is <spanclass="math inline">\(var(x_i|D)\thickapprox [A^{-1}]_{ii}\)</span>.</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> </title>
    <link href="/2022/09/18/%E6%B0%B8%E4%B9%85%E8%AE%B0%E5%BD%95%EF%BC%88%E4%BA%94%EF%BC%89/"/>
    <url>/2022/09/18/%E6%B0%B8%E4%B9%85%E8%AE%B0%E5%BD%95%EF%BC%88%E4%BA%94%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>91891820</p><p>20229121230A300A3004:30</p><p>2022913</p><p>91622:55</p><p>2491723:38</p><p>9181:11471:22N95</p><p>2:3880%26%4:004:08</p><p>4:284:41</p><p>7:33LycorisRecoil20</p><p>11:30</p><p>51683BUP@VIgot smoke</p><p></p><p></p><p></p><p></p><p></p>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (10)</title>
    <link href="/2022/09/16/Neuronal-Dynamics-10/"/>
    <url>/2022/09/16/Neuronal-Dynamics-10/</url>
    
    <content type="html"><![CDATA[<h1 id="estimating-parameters-of-probabilistic-neuron-models">EstimatingParameters of Probabilistic Neuron Models</h1><p>The online version of this chapter:</p><hr /><p>Chapter 10 Estimating Parameters of Probabilistic Neuron Modelshttps://neuronaldynamics.epfl.ch/online/Ch10.html</p><hr /><h2 id="parameter-optimization-in-linear-and-nonlinear-models">Parameteroptimization in linear and nonlinear models</h2><h3 id="linear-models">Linear Models</h3><p>Suppose the maximal amplitude of the input current has been chosensmall enough for the neuron to stay in the subthreshold regime. <spanclass="math display">\[    u(t)=\int_{0}^{\infty} \kappa(s)I(t-s) \mathrm{d}s+u_{rest}\tag{10.1}\]</span> vector <span class="math display">\[    \mathbf{k}=(\kappa(\mathrm{d}t),\kappa(2 \mathrm{d}t),\cdots,\kappa(K \mathrm{d}t)). \tag{10.2}        \]</span> which describes the time course <spanclass="math inline">\(\kappa\)</span> in discrete time. The inputcurrent <span class="math inline">\(I\)</span> during the last <spanclass="math inline">\(k\)</span> time steps is given by <spanclass="math display">\[    \mathbf{x}_{t}=(I_{t-1},\cdots ,I_{t-K})\mathrm{d}t \tag{10.3}\]</span></p><p>The discrete-time version of (10.1) is <span class="math display">\[    u_t=\sum_{l=1}^{K} k_lI_{t-l}\mathrm{d}t+u_{rest}=\mathbf{k}\cdot\mathbf{x}_t+u_{rest}. \tag{10.4}\]</span></p><p>Prediction <span class="math inline">\(u_t\)</span> of the modelequation (10.4) with the experimental measurement <spanclass="math inline">\(u_t^{exp}\)</span>. The components of the vector<span class="math inline">\(\mathbf{k}\)</span> will be chosen such that<span class="math display">\[    E(\mathbf{k})=\sum_{t=K+1}^{T} [u_t^{exp}-u_t]^{2} \tag{10.5}\]</span> is minimal.</p><h4 id="example-analytical-solution">Example: Analytical solution</h4><p>Let</p><p><span class="math display">\[    X=    \begin{pmatrix}    I_{K} &amp; I_{K-1} &amp; \cdots &amp; I_1 \\    I_{K+1} &amp; I_{K} &amp; \cdots &amp; I_2 \\    \vdots &amp; \vdots &amp; &amp; \vdots \\    I_{T} &amp; I_{T-1} &amp; \cdots &amp; I_{T-K+1} \\    \end{pmatrix} \mathrm{d}t\]</span></p><p><span class="math inline">\(\mathbf{u}^{exp}=(u_{K+1}^{exp},\cdots,u_{T}^{exp})^{\mathsf{T}}\)</span>,<spanclass="math inline">\(\mathbf{u}_{rest}\)</span> is a vector with allcomponents equal to <span class="math inline">\(u_{rest}\)</span>.</p><p>(10.4) can be written as <span class="math display">\[    \mathbf{u}=X \mathbf{k}^{\mathsf{T}}+\mathbf{u}_{rest}\tag{10.6}       \]</span> (10.5) indicates <span class="math display">\[    \nabla _{\mathbf{k}} E=0\]</span></p><p>Since <span class="math display">\[    E(\mathbf{k})=[u^{exp}-X\mathbf{k}-\mathbf{u}_{rest}]^{\mathsf{T}}\cdot [u^{exp}-X\mathbf{k}-\mathbf{u}_{rest}] \tag{10.7}\]</span></p><p>The solution is the parameter vector <span class="math display">\[    \hat{\mathbf{k}}_{LS}=(X^{\mathsf{T}}X)^{-1}X^{\mathsf{T}}(\mathbf{u}^{exp}-\mathbf{u}_{rest})\tag{10.8}\]</span></p><p>assuming the matrix <spanclass="math inline">\(X^{\mathsf{T}}X\)</span> is invertible. (If thismatrix is non-invertible, then a unique minimum does not exist.)</p><h3 id="generalized-linear-models">Generalized Linear Models</h3><p>Deterministic formulation of the SRM, <span class="math display">\[    u(t)=\int_{0}^{\infty} \eta(s)S(t-s) \mathrm{d}s+\int_{0}^{\infty}\kappa(s)I(t-s) \mathrm{d}s+u_{rest}. \tag{10.9}\]</span></p><p>Suppose that the spike history filter <spanclass="math inline">\(\eta\)</span> extends over a maximum of <spanclass="math inline">\(J\)</span> time steps. Then we can introduce a newparameter vector. <span class="math display">\[    \mathbf{k}=(\kappa(\mathrm{d}t),\kappa(2 \mathrm{d}t),\cdots,\kappa(K \mathrm{d}t),\eta(\mathrm{d}t),\eta(2 \mathrm{d}t),\cdots,\eta(J \mathrm{d}t),u_{rest})\tag{10.10}\]</span></p><p>The spike train in the last <span class="math inline">\(J\)</span>time steps is represented by the spike count sequence <spanclass="math inline">\(n_{t-1},n_{t-2},\cdots ,n_{t-J}\)</span>, where<span class="math inline">\(n_t \in \{0,1\}\)</span>, and included intothe 'input' vector <span class="math display">\[    \mathbf{x}_t=(I_{t-1}\mathrm{d}t,I_{t-2}\mathrm{d}t,\cdots,I_{t-K}\mathrm{d}t,n_{t-1},n_{t-2},\cdots ,n_{t-J},1). \tag{10.11}\]</span> So the discrete-time version <span class="math display">\[    u_t=\sum_{j=1}^{J} k_{K+j}n_{t-j}+\sum_{k=1}^{K}k_{k}I_{t-k}\mathrm{d}t+u_{rest}=\mathbf{k}\cdot \mathbf{x}_t\tag{10.12}\]</span></p><p>We have the firing intensity <span class="math display">\[    \rho(t)=f(u(t)-\theta)=f(\mathbf{k}\cdot \mathbf{x}_t-\theta),\tag{10.13}\]</span></p><h2 id="statistical-formulation-of-encoding-models">StatisticalFormulation of Encoding Models</h2><p>A neural 'encoding model' is a model that assigns a conditionalprobability, <span class="math inline">\(p(D|\mathbf{x})\)</span>, toany possible neural response <span class="math inline">\(D\)</span>given a stimulus <span class="math inline">\(\mathbf{x}\)</span>. Wehypothesize some encoding model, <span class="math display">\[    p(D|\mathbf{x},\theta). \tag{10.16}\]</span> Here <span class="math inline">\(\theta\)</span> is ashort-hand notation for the set of all model parameters. In the exampleof the previous section, the model parameters are <spanclass="math inline">\(\theta=\{\mathbf{k}\}\)</span></p><h3 id="parameter-estimation">Parameter estimation</h3><p>Find a good estimate for <span class="math inline">\(\theta\)</span>for a chosen model class: - Introduce a model that makes sensebiophysically, and incorporates our prior knowledge in a tractablemanner. - Write down the likelihood of the observed data given the modelparameters, along with a prior distribution that encodes our priorbeliefs about the model parameters. - Compute the posterior distributionof the model parameters given the observed data, using Bayes'rule, <spanclass="math display">\[    p(\theta|D)\propto p(D|\theta)p(\theta);\tag{10.18}\]</span></p><p>The maximum likelihood (ML): <span class="math display">\[    ML:\hat{k}_{ML}=\mathop{\arg\max}_\mathbf{k}\{p(D|X,\mathbf{k})\}\]</span></p><p>maximum a posteriori (MAP): <span class="math display">\[    MAP:\hat{k}_{MAP}=\mathop{\arg\max}_\mathbf{k}\{p(D|X,\mathbf{k})p(\mathbf{k})\}\]</span></p><p>where the maximization runs over all possible parameter choices.</p><p>Assume that spike counts per bin follows <spanclass="math display">\[    n_t \sim Poiss[\rho(t)\mathrm{d}t]\]</span></p><p>With the rate parameter of the Poisson distribution given by GLM orSRM model <span class="math inline">\(\rho(t)=f(\mathbf{k}\cdot\mathbf{x}_t)\)</span>, we have <span class="math display">\[    p(D|X,\mathbf{k})=\prod_{t}^{} \left\{ \frac{[f(\mathbf{k}\cdot\mathbf{x}_t) \mathrm{d}t]^{n_t}}{(n_t)!}\exp [-f(\mathbf{x}_t\cdot\mathbf{k})\mathrm{d}t]\right\}\]</span></p><p>For a given observed spike train, the spike count numbers <spanclass="math inline">\(n_t\)</span> are fixed, so we treat them asconstants. We reshuffle the terms and consider the logarithm, <spanclass="math display">\[    \log p(D|X,\mathbf{k})=c_0+\sum_{t} \{ n_t \log f(\mathbf{k}\cdot\mathbf{x}_t)-f(\mathbf{x}_t\cdot \mathbf{k})\mathrm{d}t\} \tag{10.23}\]</span></p><p>If we assume that - <span class="math inline">\(f(u)\)</span> is aconvex (upward-curving) function of its scalar argument <spanclass="math inline">\(u\)</span> - <span class="math inline">\(\logf(u)\)</span> is concave (downward-curving) in <spanclass="math inline">\(u\)</span>, <span class="math display">\[    \mathbf{x}_t=(1,I_{t-1}\mathrm{d}t,I_{t-2}\mathrm{d}t,\cdots,I_{t-K}\mathrm{d}t,n_{t-1},n_{t-2},\cdots ,n_{t-J}). \tag{10.24}\]</span></p><p>the parameter vector <span class="math display">\[    \mathbf{k}=(b,\kappa(\mathrm{d}t),\kappa(2\mathrm{d}t),\cdots,\kappa(K\mathrm{d}t),\eta(\mathrm{d}t),\eta(2\mathrm{d}t),\cdots,\eta(J\mathrm{d}t)); \tag{10.25}\]</span></p><p>here <span class="math inline">\(b=u_{rest}-\theta\)</span> is aconstant offset term which we want to optimize.</p><p>then (10.23) is guaranteed to be a concave function of <spanclass="math inline">\(\mathbf{k}\)</span>.</p><p>Note that, <span class="math inline">\(\rho(t)\)</span> depends onthe past spike trains, therefore <spanclass="math inline">\(D=\{n_t\}\)</span> is no longer a Poissonprocess.</p><p>Finally, we expand the definition of <spanclass="math inline">\(X\)</span> to include observations of other spiketrains. Spike counts are conditionally Poisson distributed given <spanclass="math inline">\(\rho_i(t)\)</span>. <spanclass="math inline">\(n_{i,t}\sim Poiss(\rho_i(t)\mathrm{d}t)\)</span>with a firing rate <span class="math display">\[    \rho_i(t)=f\left(\mathbf{k}_i\cdot \mathbf{x}_t+\sum_{i&#39;\neqi}^{}\sum_{j}^{}  \varepsilon_{i&#39;,j}n_{i&#39;,t-j}\right)\]</span></p><p>these terms are summed over all past spike activity <spanclass="math inline">\(n_{i&#39;,i-j}\)</span> in the population ofcells.</p><p><span class="math inline">\(\rho_i(t)\)</span>: the instantaneousfiring rate of the <span class="math inline">\(i\)</span>-th cell attime <span class="math inline">\(t\)</span> <spanclass="math inline">\(\mathbf{k}_i\)</span>: the cell's linear receptivefield including spike-history effects. <spanclass="math inline">\(\varepsilon_{i&#39;,j}\)</span>: the net effect ofa spike of neuron <span class="math inline">\(i&#39;\)</span> onto themembrane potential of neuron <span class="math inline">\(i\)</span>. Ifwe record from all neurons in the population, <spanclass="math inline">\(\varepsilon_{i&#39;,j}\)</span> can be interpretedas the excitatory or inhibitory postsynaptic potential caused by a spikeof neuron <span class="math inline">\(i&#39;\)</span> a time <spanclass="math inline">\(j \mathrm{d}t\)</span> earlier.</p><h4 id="example-linear-regression-and-voltage-estimation">Example:Linear regression and voltage estimation</h4><p>We want to show that the standard procedure of least-squareminimization can be linked to statitical parameter estimation under theassumption of Gaussian noise.</p><p>Set <span class="math inline">\(\mathbf{x}_t=(I_{t-1},\cdots,I_{t-K})\mathrm{d}t\)</span> and <spanclass="math inline">\(\mathbf{k}=(\kappa(\mathrm{d}t),\cdots,\kappa(K\mathrm{d}t))\)</span>.</p><p>If we assume that the discrete-time voltage measurements have aGaussian distribution around the mean predicted by (10.4), then we needto maximize the likelihood. <span class="math display">\[    \log p(D|X,\mathbf{k})=c_1-c_2 \sum_{t}^{} (u_t-(\mathbf{k}\cdot\mathbf{x}_t))^{2}, \tag{10.28}\]</span> where <span class="math display">\[    X=\begin{pmatrix}    \mathbf{x}_1/\mathrm{d}t \\    \vdots \\    \mathbf{x}_T/\mathrm{d}t    \end{pmatrix}\]</span></p><p><span class="math inline">\(c_1,c_2\)</span> are constants that donot depend on the parameter <spanclass="math inline">\(\mathbf{k}\)</span>. Maximization yields <spanclass="math inline">\(\mathbf{k}_{opt}=(X^{\mathsf{T}}X)^{-1}(\sum_{t}^{}u_t \mathbf{x}_t/\mathrm{d}t)\)</span> which determines the time courseof the filter <span class="math inline">\(\kappa(s)\)</span>. The resultis identical to (10.8): <span class="math display">\[    \hat{\mathbf{k}}_{opt}=\hat{\mathbf{k}}_{LS}\]</span></p><h3 id="regularization-maximum-penalized-likelihood">Regularization:maximum penalized likelihood</h3><p>We want to maximize the posterior <span class="math display">\[    p(\mathbf{k}|X,D)\propto p(D|X,\mathbf{k})p(\mathbf{k}) \tag{10.30}\]</span> here <span class="math inline">\(p(\mathbf{k})\)</span>encodes our a priori beliefs about the true underlying <spanclass="math inline">\(\mathbf{k}\)</span>.</p><p><span class="math display">\[    \log p(k|X,\mathbf{D})=c+\log p(\mathbf{k})+\sum_{t}^{} (n_t \logf(\mathbf{x}_t\cdot \mathbf{k})-f(\mathbf{x}_t\cdot\mathbf{k})\mathrm{d}t). \tag{10.32}\]</span></p><h4 id="example-linear-regression-and-gaussian-prior">Example: Linearregression and Gaussian prior</h4><p>Consider a zero-mean Gaussian <span class="math display">\[    \log p(\mathbf{k})=c-\mathbf{k}^{\mathsf{T}}A \mathbf{k}/2.\]</span> where <span class="math inline">\(A\)</span> is a positivedefinite matrix (the inverse covariance matrix). Combining with (10.28),maximizing the corresponding posterior leads directly to the regularizedleast-square estimator <span class="math display">\[    \hat{\mathbf{k}}_{RLS}=(X^{\mathsf{T}}X+A)^{-1}\left(\sum_{t}^{} u_t\mathbf{x}_t/\mathrm{d}t\right)\]</span></p><h2 id="evaluating-goodness-of-fit">Evaluating Goodness-of-fit</h2><p>In the following we assume that the goodness of fit quantities arecomputed using 'cross-validation': parameters are estimated using thetraining set, and then the goodness of fit quantification is performedon the test set.</p><h3 id="comparing-spiking-membrane-potential-recordings">ComparingSpiking Membrane Potential Recordings</h3><div class="note note-info">            <p>threshold0</p>          </div><p>[1]</p><hr /><p>The use of the least squares method for extracting information fromimperfect observations assumes a specific a priori probabilitydistribution for the errors, viz. the Gauss distribution. The sameassumption, however, cannot be true for all variable that might be usedto measure the observed quantity (but at most for one variable, and allthose that are linearly connected with it). The method of least squaresapplied in the frequency scale does not lead to the same result as whenapplied to the same observations plotted according to wavelengths. The'best value' for the brightness of a star depends on whether one appliesthe method of least squares to the magnitude or to its intensity inenergy measure. The redeeming feature is, as long as the errors aresmall, that any reasonable transformation is practically linear in therelevant range. But there is no logical foundation for applying it towidely scattered data.</p><hr /><p>The goodness-of-fit in terms of subthreshold membrane potential awayfrom spikes is considered separately from the goodness-of-fit in termsof the spike times only.</p><p>We compute the squared error between the recorded membrane potential<span class="math inline">\(u_t^{exp}\)</span> and model membranepotential <span class="math inline">\(u_t^{mod}\)</span> with forcedspikes at the times of the observed ones. All voltage traces start atthe same point and repeated <span class="math inline">\(N_{rep}\)</span>times. For subthreshold membrane potential, we can average the squarederror over all recorded times <span class="math inline">\(t\)</span>that are not too close to an action potential: <spanclass="math display">\[    RMSE_{nm}=\sqrt{\frac{1}{T_{\Omega_1}N_{rep}}\sum_{i=1}^{N_{rep}}\int_{\Omega_1}^{} (u_i^{exp}(t)-u_i^{mod}(t))^{2} \mathrm{d}t}\tag{10.37}\]</span> where <span class="math inline">\(\Omega_1\)</span> refers tothe ensemble of time bins at least 5 ms before or after any spikes and<span class="math inline">\(T_{\Omega_1}\)</span> is the total time in<span class="math inline">\(\Omega_1\)</span>. <spanclass="math inline">\(RMSE_{nm}\)</span> has index <spanclass="math inline">\(n\)</span> for 'neuron' and index <spanclass="math inline">\(m\)</span> for 'model'.</p><p>For spike times, we find times which are sufficiently away from aspike in any repetition and compute the average squared error <spanclass="math display">\[    RMSE_{nn}=\sqrt{\frac{2}{T_{\Omega_2}N_{rep}(N_{rep}-1)}\sum_{i=1}^{N_{rep}}\sum_{j=1}^{i-1} \int_{\Omega_2}^{} (u_j^{exp}j(t)-u_i^{exp}(t))^{2}\mathrm{d}t} \tag{10.38}\]</span> where <span class="math inline">\(\Omega_2\)</span> refers tothe ensemble of time bins far from the spike times in any repetition and<span class="math inline">\(T_{\Omega_2}\)</span> is the total time in<span class="math inline">\(\Omega_2\)</span>. Typically, 20 ms beforeand 200 ms after the spike is considered sufficiently far (spikeafterpotentials can extend for longer, so its a rather badapproximation). Because the earlier spiking history will affect themembrane potential, the <span class="math inline">\(RMSE_{nn}\)</span>calculated in (10.38) is an overestimate.</p><p>We compute the model error with the intrinsic error by taking theratio <span class="math display">\[    RMSER=\frac{RMSE_{nn}}{RMSE_{nm}} \tag{10.39}\]</span></p><p>The root-mean-squared-error ratio (RMSER) reaches one if the modelprecision is matched with intrinsic error. When smaller than one, theRMSER indicates that the model could be improved. Values larger than oneare possible because <span class="math inline">\(RMSE_{nn}\)</span> isan overestimate of the true intrinsic error.</p><h3 id="spike-train-likelihood">Spike Train Likelihood</h3><p><span class="math display">\[    L^{n}(S|\theta)=\prod_{t^{(i)}\in S}^{} \rho(t^{(i)}|S,\theta)\exp\left[ -\int_{0}^{T} \rho(s|S,\theta) \mathrm{d}s\right] \tag{10.40}\]</span> where we used <spanclass="math inline">\(\rho(t^{(i)}|S,\theta)\)</span> to emphasize thatthe firing intensity of a spike at <spanclass="math inline">\(t^{(i)}\)</span> depends on both the stimulus andspike history as well as the model parameter <spanclass="math inline">\(\theta\)</span>.</p><p>We compare <span class="math inline">\(L^{n}\)</span> with thelikelihood of a homogeneous Poisson model with a constant firingintensity <span class="math inline">\(\rho_0=n/T\)</span>. Thedifference in log-likelihood between the Poisson model and the neuronmodel is finally divided by the total number <spanclass="math inline">\(n\)</span> of observed spikes in order to obtain aquantity with units of 'bits per spike': <span class="math display">\[    \frac{1}{n}\log_{2}\frac{L^{n}(S|\theta)}{\rho_0^{n}\mathrm{e}^{-\rho T} } \tag{10.41}\]</span></p><p>This quantity can be interpreted as an instantaneous mutualinformation between the spike count in a single time bin and thestimulus given the parameters.</p><h3 id="time-rescaling-theorem">Time-rescaling Theorem</h3><p>For a spike train with spikes at <spanclass="math inline">\(t^{(1)}&lt;t^{(2)}&lt;\cdots &lt;t^{(n)}\)</span>and with firing intensity <spanclass="math inline">\(\rho(t|S,\theta)\)</span>, the time-rescalingtransformation <span class="math inline">\(t \to \Lambda(t)\)</span> isdefined as <span class="math display">\[    \Lambda(t)=\int_{0}^{t} \rho(x|S,\theta) \mathrm{d}x. \tag{10.42}\]</span></p><p>Somewhat suprisingly, <spanclass="math inline">\(\Lambda(t^{(k)})\)</span> (evaluated at themeasured firing times) is a Poisson process with unit rate. A correlateof this time-rescaling theorem is that the time intervals <spanclass="math display">\[    \Lambda(t^{(k)})-\Lambda(t^{(k-1)}) \tag{10.43}\]</span> are independent random variables with an exponentialdistribution. Re-scaling again the time axis with the transformation<span class="math display">\[    z_k=1-\exp [-(\Lambda(t^{(k)})-\Lambda(t^{(k-1)}))] \tag{10.44}\]</span> forms independent uniform random variables on the intervalzero to one.</p><p>To verify that the <span class="math inline">\(z_k\)</span>'s areindependent, we can look at the serial correlation of the interspikeintervals or use a scatter plot <spanclass="math inline">\(z_{k+1}\)</span> against <spanclass="math inline">\(z_k\)</span>. Testing whether the <spanclass="math inline">\(z_k\)</span>'s are uniformly distributed can bedone with a <strong>Kolmogorov-Smirnov</strong> (K-S) test. In our case,the reference function is the uniform distribution, so that itscumulative is simply <span class="math inline">\(z\)</span>. Thus, <spanclass="math display">\[    D=\text{sup}_{z}\lvert P(z)-z \rvert . \tag{10.45}\]</span></p><p>The time-rescaling theorem along with the K-S test provide a usefulgoodness-of-fit measure for spike train data with confidence intervalsthat does not require multiple repetitions.</p><h3 id="spike-train-metric">Spike Train Metric</h3><p>Evaluating the goodness-of-fit in terms of log-likelihood or thetime-rescaling theorem requires that we know that the conditional firingintensity <span class="math inline">\(\rho(t|S,\theta)\)</span>accurately.</p><p>Another approach for comparing spike trains involves defining ametric between spike trains.</p><p>Let us consider spike trains as vectors in an abstract vector space,with these vectors denoted with boldface: <spanclass="math inline">\(\bold{S}\)</span>. For now, consider the generalform <span class="math display">\[    (\bold{S}_{i}, \bold{S}_{j})=\int_{0}^{T} \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} K_{\Delta}(s,s&#39;)S_i(t-s)S_j(t-s&#39;)\mathrm{d}s \mathrm{d}s&#39; \mathrm{d}t, \tag{10.46}\]</span> where <span class="math inline">\(K_{\Delta}\)</span> is atwo-dimensional coincidence kernel with a scaling parameter <spanclass="math inline">\(\Delta\)</span>, and <spanclass="math inline">\(T\)</span> is the maximum length of the spiketrains. <span class="math inline">\(K_{\Delta}\)</span> is required tobe a non-negative function with a global maximum at <spanclass="math inline">\(s=s&#39;=0\)</span>. Moreover, <spanclass="math inline">\(K_{\Delta}(s,s&#39;)\)</span> should fall offrapidly so that <spanclass="math inline">\(K_{\Delta}(s,s&#39;)\thickapprox 0\)</span> forall <span class="math inline">\(s,s&#39;&gt;\Delta\)</span>. Examples ofkernels include <spanclass="math inline">\(K_{\Delta}(s,s&#39;)=k_1(s)k_2(s&#39;)=\frac{1}{\Delta^{2}}\exp[-(s+s&#39;)/\Delta]\Theta(s)\Theta(s&#39;)\)</span>. The scalingparameter <span class="math inline">\(\Delta\)</span> must be small,much smaller than the length <span class="math inline">\(T\)</span> ofthe spike train.</p><p>With <spanclass="math inline">\(K_{\Delta}(s,s&#39;)=\delta(s)\delta(s&#39;)\)</span>,we observe that <span class="math inline">\((\bold{S}_{i},\bold{S}_{i})=\int_{0}^{T} S_i^{2}(t) \mathrm{d}t=n_i\)</span> where<span class="math inline">\(n_i\)</span> is the number of spikes in<span class="math inline">\(\bold{S}_i\)</span>.</p><p>Define distance <span class="math inline">\(D_{ij}\)</span>, betweentwo spike-trains <span class="math display">\[    D_{ij}=\left\| \bold{S}_i-\bold{S}_j \right\|_{} \tag{10.47}\]</span> Consider <spanclass="math inline">\(K_{\Delta}(s,s&#39;)=\delta(s)\delta(s&#39;)\)</span>,then <span class="math inline">\(D_{ij}\)</span> is the total number ofspikes in both <span class="math inline">\(\bold{S}_i\)</span> and <spanclass="math inline">\(\bold{S}_j\)</span> reduced by 2 for each spike in<span class="math inline">\(\bold{S}_i\)</span> that coincided with onein <span class="math inline">\(\mathbf{S}_j\)</span>. For the following,it is useful to think of a distance between spike trains as a number ofnon-coincident spikes.</p><p><span class="math display">\[    \cos \theta_{ij}=\frac{(\mathbf{S}_i,\mathbf{S}_j)}{\left\|\mathbf{S}_i \right\|_{}\left\| \mathbf{S}_j \right\|_{}}. \tag{10.48}\]</span></p><h3 id="comparing-sets-of-spike-trains">Comparing Sets of SpikeTrains</h3><p>Let the two sets of spike trains be denoted by <spanclass="math inline">\(X\)</span> and <spanclass="math inline">\(Y\)</span>, containing <spanclass="math inline">\(N_{X}\)</span> and <spanclass="math inline">\(N_{Y}\)</span> spike trains, respectively. Define<span class="math display">\[    \hat{L}_{X}=\frac{1}{N_X}\sum_{i=1}^{N_{X}} \left\|\mathbf{S}_i^{(x)} \right\|_{}^{2}. \tag{10.49}     \]</span> where we have used '^' to denote that the quantity is anexperimental estimate. <span class="math inline">\(\hat{L}_{X}\)</span>is related to the averaged spike count. <spanclass="math inline">\(L_{X}\)</span> is exactly the averaged spike countif the inner product satisfies i)<span class="math inline">\(\int_{}^{}\int_{}^{} K_{\Delta}(s,s&#39;) \mathrm{d}s \mathrm{d}s&#39;=1\)</span>and ii) <span class="math inline">\(K_{\Delta}(s,s&#39;)=0\)</span>whenever $s-s' $ is greater than the minimum interspike interval of anyof the spike trains considered.</p><p>The vector of averaged spike trains <span class="math display">\[    \hat{\nu}_{X}=\frac{1}{N_X}\sum_{i=1}^{N_{X}} \mathbf{S}_i^{(x)}.\tag{10.50}       \]</span> is another occurrence of the spike density. It defines theinstantaneous firing rate of the spiking process, <spanclass="math inline">\(\nu(t)=\langle \hat{\nu}\rangle\)</span>.</p><p>The variability is defined as the variance <spanclass="math display">\[    \hat{V}_{X}=\frac{1}{N_{X}}\sum_{i=1}^{N_{X}} \left\|\mathbf{S}_i^{(x)}-\hat{\nu}_{X} \right\|_{}^{2}. \tag{10.51}\]</span></p><p>Variability relates to reliability. While variability is a positivequantity that cannot exceed <span class="math inline">\(L_X\)</span>,reliabiliy is usually defined between zero and one where one meansperfectly reliable spike timing: <spanclass="math inline">\(\hat{R}_{X}=1-\hat{V}_{X}/\hat{L}_{X}\)</span>.</p><p>Finally, we come to a measure of match between the set of spiketrains <span class="math inline">\(X\)</span> and <spanclass="math inline">\(Y\)</span>. To reproduce the detailed timestructure of the PSTH, we define <span class="math display">\[    \hat{M}=\frac{2(\hat{\nu}_{X},\hat{\nu}_{Y})}{\hat{R}_{X}\hat{L}_{X}+\hat{R}_{Y}\hat{L}_{Y}}.\tag{10.52}\]</span></p><p>We have <span class="math inline">\(M\)</span> (for match) equal toone if <span class="math inline">\(X\)</span> and <spanclass="math inline">\(Y\)</span> have the same instantaneous firingrate. The smaller <span class="math inline">\(M\)</span> the greater themismatch between the spiking processes. The quantity <spanclass="math inline">\(R_{X}L_{X}\)</span> can be interpreted as a numberof reliable spikes. Since <spanclass="math inline">\((\hat{\nu}_{X},\hat{\nu}_{Y})\)</span> isinterpreted as a number of coincident spikes between <spanclass="math inline">\(X\)</span> and <spanclass="math inline">\(Y\)</span>, we can still regard <spanclass="math inline">\(M\)</span> as a factor counting the fraction ofcoincident spikes.</p><p>If the kernel <spanclass="math inline">\(K_{\Delta}(s,s&#39;)\)</span> is chosen to be<span class="math inline">\(k_g(s)k_g(s&#39;)\)</span> and <spanclass="math inline">\(k_g\)</span> is a Gaussian distribution of width<span class="math inline">\(\Delta\)</span>, then <spanclass="math inline">\(M\)</span> relates to a mean square error betweenPSTHs that were filtered with <span class="math inline">\(k_g\)</span>.Therefore, the kernel used in the definition of the inner product(10.46) can be related to the smoothing filter of the PSTH.</p><h2 id="closed-loop-stimulus-design">Closed-loop stimulus design</h2><p>Here we describe how to take advantage of the properties of the GLMto optimize our experiments: the objective is to select, in an online,closed-loop manner, the stimuli that will most efficiently characterizethe neuron's response properties.</p><p>A property of GLMs: not all stimuli will provide the same amount ofinformation about the unknown coefficients <spanclass="math inline">\(\mathbf{k}\)</span>. We need a well-definedobjective function that will rank any given stimuli according to itspotential informativeness.</p><p>When the goal is estimating the unknown parameters of a model, given<span class="math inline">\(D=\{ \mathbf{x}(s),n_s\}_{s&lt;t}\)</span>,the observed data up to the current trial. The posterior uncertainty in<span class="math inline">\(\theta\)</span> can be quantified using theinformation-theoretic notion of 'entropy'.</p><h2 id="summary">Summary</h2><p>For a suitable chosen model class, the likelihood of the data beinggenerated by the model is a concave function of the model parameters,i.e., there are no local maxima.</p><p>Once neuron models are phrased in the language of statistics, theproblems of coding and stimulus design can be formulated in a singleunified framework.</p><h3 id="comparing-psths-and-spike-train-similarity-measures">ComparingPSTHs and spike train similarity measures</h3><p>Experimentally the PSTH is constructed from a set of <spanclass="math inline">\(N_{rep}\)</span> spike trains, <spanclass="math inline">\(S_i(t)\)</span>, measured from repeatedpresentations of the same stimulus. The ensemble average of the recordedspike trains: <span class="math display">\[    \frac{1}{N_{rep}}\sum_{i=1}^{N_{rep}} S_i(t)\]</span> is typically convolved with a Gaussian function <spanclass="math display">\[    h_g(x)=(2\pi \sigma^{2})^{-1/2}\exp (-x^{2}/2\sigma^{2})\]</span> with <span class="math inline">\(\sigma\)</span> around 5 ms,such that <span class="math inline">\(A_1(t)=(h_g *\frac{1}{N_{rep}}\sum_{}^{} S_i)\)</span> is a smoothed PSTH.</p><p>With the kernel <spanclass="math inline">\(K(t,t&#39;)=h_g(t)h_g(t&#39;)\)</span>, We have<span class="math display">\[    \int_{0}^{T} (A_1(t)-A_2(t))^{2} \mathrm{d}t=\left\|\frac{1}{N_{rep}}\sum_{i=1}^{N_{rep}} (S_1(t)-S_2(t)) \right\|_{}^{2}\]</span></p><p>The correlation coefficient between the two smoothed PSTHs can bewritten as a angular separation between the sets of spike trains withkernel <spanclass="math inline">\(K(t,t&#39;)=h_g(t)h_g(t&#39;)\)</span>.</p><h3 id="victor-and-purpura-metric">Victor and Purpura metric</h3><p>Consider the minimum cost <span class="math inline">\(C\)</span>required to transform a spike train <spanclass="math inline">\(S_i\)</span> into another spike train <spanclass="math inline">\(S_j\)</span> if the only transformations availableare - Removing a spike has a cost of one - Adding a spike has a cost ofone - Shifting a spike by a distance <spanclass="math inline">\(d\)</span> has a cost <spanclass="math inline">\(qd\)</span> where <spanclass="math inline">\(q\)</span> is a parameter defining temporalprecision.</p><p>The <span class="math inline">\(C\)</span> defines a metric thatmeasures the dissimilarity between spike train <spanclass="math inline">\(S_i\)</span> and spike train <spanclass="math inline">\(S_j\)</span>.</p><p>For <span class="math inline">\(q=0\)</span> units of cost perseconds, <span class="math inline">\(C\)</span> becomes the differencein number of spikes in spike trains <spanclass="math inline">\(S_i\)</span> and <spanclass="math inline">\(S_j\)</span>.</p><p>For <span class="math inline">\(q&gt;0\)</span>, <spanclass="math inline">\(C\)</span> can be written as a distance <spanclass="math inline">\(D_{ij}^{2}\)</span> with kernel <spanclass="math inline">\(K(t,t&#39;)=h_t(t)\delta(t&#39;)\)</span> andtriangular function <span class="math display">\[    h_t(t)=(1-\lvert t \rvert q/2)\Theta(1-\lvert t \rvert q/2)\]</span> as follows <span class="math display">\[    C(S_i,S_j)=D_{ij}^{2}=\int_{0}^{T} \int_{-\frac{2}{q}}^{\frac{2}{q}}(1-\lvert s \rvert \frac{q}{2})[S_i(t-s)-S_j(t-s)][S_i(t)-S_j(t)]\mathrm{d}s \mathrm{d}t\]</span></p><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>N.G.van Kampen (2007)Stochastic Processes in Physics and Chemistry.<a href="#fnref:1" rev="footnote" class="footnote-backref"></a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (9)</title>
    <link href="/2022/09/12/Neuronal-Dynamics-9/"/>
    <url>/2022/09/12/Neuronal-Dynamics-9/</url>
    
    <content type="html"><![CDATA[<h1 id="noisy-output-escape-rate-and-soft-threshold">Noisy Output:Escape Rate and Soft Threshold</h1><p>The online version of this chapter:</p><hr /><p>Chapter 9 Noisy Output: Escape Rate and Soft Thresholdhttps://neuronaldynamics.epfl.ch/online/Ch9.html</p><hr /><h2 id="escape-noise">Escape noise</h2><p>In this section the notion of escape noise is introduced.</p><h3 id="escape-rate">Escape rate</h3><p><span class="math inline">\(\theta\)</span>: formal threshold</p><p>In Ch6, the value of the membrane potential of a SRM can be expressedas <span class="math display">\[    u(t)=\sum_{f}^{} \eta(t-t^{(f)})+\int_{0}^{\infty}\kappa(s)I^{det}(t-s) \mathrm{d}s +u_{rest}, \tag{9.1}\]</span></p><p>where <span class="math inline">\(I^{det}\)</span> is the knowndriving current and <span class="math inline">\(\kappa\)</span> and<span class="math inline">\(\eta\)</span> are filters that describe theresponse of the membrane to an incoming pulse or an outgoing spike.</p><p>Replace the strict threshold by a stochastic firing criterion. In thenoisy threshold model, spikes can occur at any time with a probabilitydensity <span class="math display">\[    \rho(t)=f(u(t)-\theta), \tag{9.2}\]</span></p><p>In the mathematical theory of point processes, the quantity <spanclass="math inline">\(\rho\)</span> is called a 'stochastic intensity'.We refer to <span class="math inline">\(\rho\)</span> as a firingintensity.</p><p>A common choice for <span class="math inline">\(f\)</span> is theexponential, <span class="math display">\[    f(u-\theta)=\frac{1}{\tau_0}\exp [\beta(u_0-\theta)] \tag{9.3}\]</span> where <span class="math inline">\(\beta\)</span> and <spanclass="math inline">\(\tau_0\)</span> are parameters. For <spanclass="math inline">\(\beta \to \infty\)</span>, the soft thresholdturns into a sharp one.</p><p>More generally, the spike trigger process could also depend on theslope <spanclass="math inline">\(\dot{u}=\mathrm{d}u/\mathrm{d}t\)</span>. <spanclass="math display">\[    \rho(t)=f[u(t)-\theta,\dot{u}]. \tag{9.4}\]</span></p><h3 id="transition-from-continuous-time-to-discrete-time">Transitionfrom continuous time to discrete time</h3><p>We consider the probability <spanclass="math inline">\(P_{F}(u)\)</span> of firing in a finite time stepgiven the neurons has membrane potential <spanclass="math inline">\(u\)</span>.</p><p>In a straightforward discretization scheme, <spanclass="math inline">\(\int_{t}^{t+\Delta t} \rho(t&#39;)\mathrm{d}t&#39;\thickapprox \rho(t)\Delta t\)</span>. <spanclass="math inline">\(\Delta t\)</span> must be taken extremely short soas to guarantee <span class="math inline">\(\rho(t)\Deltat&lt;1\)</span>.</p><p>In an improved discretization scheme, <span class="math display">\[    P_{F}(u)=\operatorname{Pr}\{\text{spike in} \ [t,t+\Deltat]|u(t)\}\thickapprox 1-\exp \{-\Delta t f[u(t)-\theta] \}.\]</span> (9.8)</p><p>For small <span class="math inline">\(\Delta t\)</span>, theprobability <span class="math inline">\(P_{F}\)</span> scales as <spanclass="math inline">\(f \Delta t\)</span>.</p><p>For an exponential escape rate, an increase in the discretization<span class="math inline">\(\Delta t\)</span> mainly shifts the firingcurve to the left.</p><h2 id="likelihood-of-a-spike-train">Likelihood of a spike train</h2><p>In this section we determine the likelihood that a specific spiketrain is generated by a neuron model with escape noise.</p><p>Assume a spike train which is generated by an escape noiseprocess</p><p><span class="math display">\[    \rho(t)=f(u(t)-\theta) \tag{9.9}\]</span></p><p>and the membrane potential <span class="math inline">\(u(t)\)</span>arises from the dynamics of one of the dynamics of one of thegeneralized integrate-and-fire models such as the SRM.</p><p>The likelihood <span class="math inline">\(L^{n}\)</span> that spikesoccur at the times <span class="math inline">\(t^{(1)},\cdots,t^{(f)},\cdots ,t^{(n)}\)</span> is <span class="math display">\[    L^{n}(\{t^{(1)},t^{(2)},\cdots ,t^{(n)}\})=\rho(t^{(1)})\cdot\rho(t^{(2)})\cdots \rho(t^{(n)}) \exp \left[ -\int_{0}^{T} \rho(s)\mathrm{d}s\right], \tag{9.10}      \]</span> where <span class="math inline">\([0,T]\)</span> is theobservation interval. The product on the right-hand side contains themomentary firing intensity <spanclass="math inline">\(\rho(t^{(f)})\)</span> at the firing times <spanclass="math inline">\(t^{(1)},t^{(2)},\cdots ,t^{(n)}\)</span>. Theexponential factor takes into account that the neuron needs to 'survive'without firing in the intervals between the spikes.</p><p>Actually, <span class="math display">\[    \begin{aligned}        L^{n}(\{t^{(1)},t^{(2)},\cdots ,t^{(n)}\})= \exp\left[-\int_{0}^{t^{(1)}} \rho(s) \mathrm{d}s\right]\cdot \\        \rho(t^{(1)})\exp \left[-\int_{t^{(1)}}^{t^{(2)}} \rho(s)\mathrm{d}s\right]\cdot \ldots\\        \rho(t^{(n)})\exp \left[-\int_{t^{(n)}}^{T} \rho(s)\mathrm{d}s\right].    \end{aligned}\]</span> (9.11)</p><p>Sometimes it is more convenient to work with the logarithm of thelikelihood, called the <strong>log-likelihood</strong> <spanclass="math display">\[    \log L^{n}(\{t^{(1)},\cdots ,t_i^{(f)},t^{(n)}\})=-\int_{0}^{T}\rho(s) \mathrm{d}s+\sum_{f=1}^{n} \log \rho(t_i^{(f)}). \tag{9.12}\]</span></p><h4id="example-discrete-time-version-of-likelihood-and-generative-model">Example:Discrete-time version of likelihood and generative model</h4><p>For a discrete-time simulation, the probability of finding an emptytime bin (spike count <span class="math inline">\(n_t=0\)</span>) is<span class="math display">\[    \operatorname{Pr} \{\text{silent in} \ [t,t+\Delta t]\}=1-P_{t}=\exp\{-\Delta t \rho(t)\} \tag{9.13}\]</span></p><p>With sufficiently small <span class="math inline">\(\Deltat\)</span>, it is impossible to have two spikes in a time bin <spanclass="math inline">\(\Delta t\)</span>. This reflects the fact that,because of neuronal refractoriness, neurons cannot emit two spikes in atime bin shorter than, say, half the duration of an actionpotential.</p><p><span class="math display">\[    P_{total}=\prod_{bins \ with \ spike}^{} [P_t] \cdot \prod_{empty \bins}^{} [1-P_t]=\prod_{t}^{} \{[P_t]^{n_t}\cdot[1-P_t]^{1-n_t}\},       \]</span> where the product runs over all time bins and <spanclass="math inline">\(n_t \in \{0,1\}\)</span> is the spike count numberin each bin.</p><p>The observed spike trains has <span class="math display">\[    P_{total}=\prod_{k=1}^{n} [P_{t_k}]\cdot \prod_{k&#39;}^{}[1-P_{t_{k&#39;}}]. \tag{9.16}\]</span></p><p>All time bins that fall into the interval between two spikes can beregrouped as follows <span class="math display">\[    \prod_{\{k&#39;|t_k&lt;t_{k&#39;}&lt;t_{k+1}\}}^{}[1-P_{t_{k&#39;}}]\exp\{-\Delta t \rho(t_{k&#39;})\}=\exp\left\{-\sum_{t_k&lt;t_{k&#39;}&lt;t_{k+1}}^{} \Delta t\rho(t_{k&#39;})\right\}.  \]</span> (9.17)</p><p>The Riemann-sum on the right-hand side of (9.17) then turns into anintegral. Combine the limit in (9.16), we have <spanclass="math display">\[    P_{total}=L^{n}(\{t^{(1)},t^{(2)},\cdots ,t^{(n)}\})(\Delta t)^{n},\tag{9.18}\]</span></p><h2 id="renewal-approximation-of-the-spike-response-model">RenewalApproximation of the Spike Response Model</h2><p>In this section we apply the escape noise formalism to the SRM andshow an interesting link to the renewal statistics encountered inCh7.</p><p>We focus a SRM with escape noise. If the firing rate is low, so thatthe interspike interval is much longer than the decay time of therefractory kernel <span class="math inline">\(\eta\)</span>, then we cantruncate the sum over past firing times and keep track only of theeffect of the most recent spike <span class="math display">\[    u(t)=\eta(t-\hat{t})+\int_{0}^{\infty} \kappa(s)I^{det}(t-s)\mathrm{d}s +u_{rest}, \tag{9.19}\]</span> where <span class="math inline">\(\hat{t}\)</span> denotes thelast firing time <span class="math inline">\(t^{(f)}&lt;t\)</span>.</p><p>(9.19) is called the 'short-term momory' approximation of the SRM andabbreviated as SRM<span class="math inline">\(_0\)</span>. Sometimes wewrite <span class="math inline">\(u(t|\hat{t})\)</span> intstead of<span class="math inline">\(u(t)\)</span> to emphasize that the value ofthe membrane potential depends only on the most recent spike.</p><p>Input potential: <span class="math display">\[    h(t)=\int_{0}^{\infty} \kappa(s)I^{det}(t-s) \mathrm{d}s \tag{9.20}\]</span> which allows us to rewrite (9.19) as <spanclass="math display">\[    u(t|\hat{t})=\eta(t-\hat{t})+h(t)+u_{rest}. \tag{9.21}\]</span></p><p>The escape rate <span class="math display">\[    \rho(t|\hat{t})=f(u(t|\hat{t})) \tag{9.22}\]</span> depends on the time since the last spike and, implicitly, onthe stimulating current <span class="math inline">\(I^{det}(t)\)</span>.Given the last spike time <span class="math inline">\(\hat{t}\)</span>and the input <span class="math inline">\(I^{det}(t&#39;)\)</span> for<span class="math inline">\(t&#39;&lt;t\)</span>, we can calculate theprobability density that the next spike occurs at time <spanclass="math inline">\(t&gt;\hat{t}\)</span> <spanclass="math display">\[    P_{I}(t|\hat{t})=\rho(t|\hat{t})\exp \left[-\int_{\hat{t}}^{t}\rho(t&#39;|\hat{t}) \mathrm{d}t&#39;\right]. \tag{9.23}\]</span></p><h4id="example-interval-distribution-with-exponential-escape-noise">Example:Interval distribution with exponential escape noise</h4><p>We study a model SRM<span class="math inline">\(_0\)</span> withmembrane potential <spanclass="math inline">\(u(t|\hat{t})=\eta(t-\hat{t})+h(t)\)</span> andchoose a refractory kernel with absolute and relative refractorinessdefined as <span class="math display">\[    \eta(s)=    \begin{cases}        -\infty, s&lt;\Delta^{abs} \\        -\eta_0 \exp \left(-\frac{s-\Delta^{abs}}{\tau}\right),s&gt;\Delta^{abs}      \end{cases}\]</span> (9.24)</p><p>and the exponential escape rate (9.3).</p><p><img src="/img/neu_dyn/x649.png" /></p><p>The above figure shows the interval distribution for constant inputcurrent <span class="math inline">\(I_0\)</span> as a function of <spanclass="math inline">\(s=t-\hat{t}\)</span>. With the normalization <spanclass="math inline">\(\int_{0}^{\infty} \kappa(s)\mathrm{d}s=1\)</span>, we have <spanclass="math inline">\(h_0=I_0\)</span>. The gain function <spanclass="math inline">\(\nu=g(I_0)\)</span> of a noisy SRM<spanclass="math inline">\(_0\)</span> neuron is shown on the right.</p><p>We now study the same model with periodic input <spanclass="math inline">\(I^{det}(t)=I_0+I_1 \cos (\Omega t)\)</span>. Thisleads to an input potential <span class="math inline">\(h(t)=h_0+h_1\cos (\Omega t+\varphi_1)\)</span> with bias <spanclass="math inline">\(h_0=I_0\)</span> and a periodic component with acertain amplitude <span class="math inline">\(h_1\)</span> and phase<span class="math inline">\(\varphi_1\)</span>.</p><p>The result of <span class="math inline">\(P_{I}(t|0)\)</span> isshown in the following figure on the left.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x244.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x245.png" /></div></div></div><p>The periodic component of the input is well represented in theresponse of the neuron. This example illustrates how neurons in theauditory system can transmit stimuli of frequencies higher than the meanfiring rate of the neuron.</p><h2 id="from-noisy-inputs-to-escape-noise">From noisy inputs to escapenoise</h2><p>Sometimes without noise there would be no output spike. On the otherhand, at very high noise levels, the modulation of the intervaldistribution would be much weaker. Thus a certain amount of noise isbeneficial for signal transmission.</p><p>In this section we show that the noisy, unknown part <spanclass="math inline">\(\xi(t)\)</span> in the input can be approximatedby an appropriately chosen escape function.</p><h4id="example-adaptive-exponential-integrate-and-fire-model-with-noisy-input">Example:Adaptive exponential integrate-and-fire model with noisy input</h4><p>Suppose that the AdEx model is driven by an input as <spanclass="math inline">\(I(t)=I^{det}(t)+\xi(t)\)</span> containing aripidly moving deterministic signal <spanclass="math inline">\(I^{det}(t)\)</span> as well as a white noisecomponent <span class="math inline">\(\xi(t)\)</span>.</p><p>We approximate the voltage of the AdEx model by a linear model <spanclass="math display">\[    u(t)=\sum_{f}^{} \eta(t-t_i^{(f)})+\int_{0}^{\infty}\kappa(s)I^{det}(t-s) \mathrm{d}s+u_{rest}. \tag{9.26}\]</span></p><p>In the subthreshold regime <spanclass="math inline">\(u&lt;\theta-\Delta_{T}\)</span>, the filters <spanclass="math inline">\(\kappa\)</span> and <spanclass="math inline">\(\eta\)</span> can be calculated analytically.(9.26) combined with the exponential escape rate <spanclass="math display">\[    f(u-\theta)=\frac{1}{\tau_0}\exp [\beta(u-\theta)]\]</span></p><p>describes the activity of the AdEx with noisy input surprisinglywell. <strong>In the presence of an input noise <spanclass="math inline">\(\xi(t)\)</span> the exponential term in thevoltage equation of the AdEx model can be replaced by an exponentialescape rate.</strong></p><h3 id="leaky-integrate-and-fire-model-with-noisy-input">Leakyintegrate-and-fire model with noisy input</h3><p>Though the probability density at <spanclass="math inline">\(u=\theta\)</span> vanishes in the presence ofthreshold, we can approximate the probability density near <spanclass="math inline">\(u=\theta\)</span> by the 'free' distribution(i.e., without the threshold) <span class="math display">\[    \operatorname{Pr} \{u \ \text{reaches} \ \theta \ \text{in} \[t,t+\Delta t]\}\propto \Delta t \exp\left\{-\frac{[u_0(t)-\theta]^{2}}{2\langle \Deltau^{2}(t)\rangle}\right\} \tag{9.28}\]</span></p><blockquote><p>Fig 9.10<span class="math inline">\([t,t+\Deltat]\)</span> threshold</p></blockquote><p>We can replace the time dependent variance <spanclass="math inline">\(2\langle \Delta u(t)^{2}\rangle\)</span> by itsstationary value <span class="math inline">\(\sigma^{2}\)</span>(see(8.13)). So <span class="math display">\[    f(u_0-\theta)=\frac{c_1}{\tau_m}\exp\left\{-\frac{[u_0(t)-\theta]^{2}}{\sigma^{2}}\right\} \tag{9.29}\]</span> (Arrhenius formula)</p><p><span class="math inline">\(c_1/\tau_m\)</span> shows that the escaperate has units of one over time.</p><p>If at <span class="math inline">\(t=t_0\)</span>, an input currentpulse causes a jump of the membrane trajectory by amount <spanclass="math inline">\(\Delta u&gt;0\)</span>, then there is a nonzeroprobability that the neuron fires exactly at <spanclass="math inline">\(t_0\)</span>. We expect an increase of theinstantaneous rate proportional to <spanclass="math inline">\(\dot{u}_{0}\)</span>, so we study <spanclass="math display">\[    f(u_0,\dot{u}_0)=\left(\frac{c_1}{\tau_m}+\frac{c_2}{\sigma}[\dot{u}_0]_{+}\right)\exp\left\{-\frac{[u_0(t)-\theta]^{2}}{\sigma^{2}}\right\}, \tag{9.30}\]</span> where <span class="math inline">\([x]_{+}=x\)</span> for <spanclass="math inline">\(x&gt;0\)</span> and zero otherwise. We call (9.30)he Arrhenius&amp;Current model.</p><p>(9.30) depends only on the dimensionless variable <spanclass="math display">\[    x(t)=\frac{u_0(t)-\theta}{\sigma}, \tag{9.31}\]</span></p><p>and its derivative <span class="math inline">\(\dot{x}\)</span>. Adistance of <span class="math inline">\(u-\theta=-10 mV\)</span> at highnoise (e.g., <span class="math inline">\(\sigma=10 mV\)</span>) is aseffective in firing a cell as a distance of <spanclass="math inline">\(1mV\)</span> at low noise (<spanclass="math inline">\(\sigma=1mV\)</span>).</p><h4id="example-comparison-of-diffusion-model-and-arrheniuscurrent-escape-rate">Example:Comparison of diffusion model and Arrhenius&amp;Current escape rate</h4><p>The interval distribution for the diffusive white noise model(derived from stochastic spike arrival) and that for theArrhenius&amp;Current escape model yields an excellent approximation tothe diffusive noise model.</p><p>An obvious shortcoming of (9.30) is that the instantaneous ratedecreases with <span class="math inline">\(u\)</span> for <spanclass="math inline">\(u&gt;\theta\)</span>. The superthreshold behaviorcan be corrected if we replace the Gaussian <spanclass="math inline">\(\exp (-x^{2})\)</span> by <spanclass="math inline">\(2\exp(-x^{2})/[1+\text{erf}(-x)]\)</span>. Thesubthreshold behavior remains unchanged compared to (9.30) but thesuperthreshold behavior of the escape rate <spanclass="math inline">\(f\)</span> becomes linear.</p><h3 id="stochastic-resonance">Stochastic resonance</h3><p>Existence of an optimum for the noise amplitude which has motivatedthe name <strong>stochastic resonance</strong> is a counterintuitivephenomenon.</p><p>In the absence of noise, a subthreshold stimulus <spanclass="math inline">\(I(t)\)</span> does not generate action potentialsso that no information on the temporal structure of the stimulus can betransmitted. On the other hand, for very large noise <spanclass="math inline">\(\sigma \to \infty\)</span>, spike firing occurs ata constant rate, irrespective of the temporal structure of theinput.</p><p>Even though stochastic resonance does not require periodicity, it istypically studied with a periodic input signal <spanclass="math display">\[    I^{det}(t)=I_0+I_1\cos (\Omega t). \tag{9.32}\]</span> For <span class="math inline">\(t-\hat{t}\gg\tau_m\)</span>,the membrane potential of the noise-free reference trajectory has theform <span class="math display">\[    u_0(t)=u_{\infty}+u_1 \cos (\Omega t+\varphi_1), \tag{9.33}\]</span> where <span class="math inline">\(u_1\)</span> and <spanclass="math inline">\(\varphi_1\)</span> are amplitude and phase of itsperiodic component. We compute the signal-to-noise ratio (SNR).</p><p>The signal <span class="math inline">\(\mathscr{S}\)</span> ismeasured as the amplitude of the power spectral density of the spiketrain evaluated at frequency <spanclass="math inline">\(\Omega\)</span>, i.e., <spanclass="math inline">\(\mathscr{S}=\mathscr{P}(\Omega)\)</span>. Thenoise level <span class="math inline">\(\mathscr{N}\)</span> is usuallyestimated from the noise power <spanclass="math inline">\(\mathscr{P}_{Poisson}\)</span> with the samenumber of spikes as the measured spike train.</p><p><img src="/img/neu_dyn/x653.png" /></p><p>The above figure shows the SNR <spanclass="math inline">\(\mathscr{S}/\mathscr{N}\)</span> of a periodicallystimulated integrate-and-fire neuron as a function of the noise level<span class="math inline">\(\sigma\)</span>. They exhibit a peak at<span class="math display">\[    \sigma^{opt}\thickapprox \frac{2}{3}(\theta-u_{\infty}) \tag{9.34}\]</span></p><p>accordingly, <span class="math display">\[    2\sqrt{\langle \Delta u^{2}\rangle}\thickapprox \theta-u_{\infty}.\tag{9.35}\]</span></p><h4 id="example-extracting-oscillations">Example: Extractingoscillations</h4><p>We study the question whether an integrate-and-fire neuron or a SRMneuron is only sensitive to the total number of spikes that arrive insome time window <span class="math inline">\(T\)</span>, or also to therelative timing of the input spikes.</p><p>We compare two different scenarios of stimulation. In the firstscenario input spikes arrive with a periodically modulated rate, <spanclass="math display">\[    \nu^{in}(t)=\nu_0+\nu_1 \cos (\Omega t) \tag{9.36}      \]</span> with <span class="math inline">\(0&lt;\nu_1&lt;\nu_0\)</span>.Spikes are generated by this inhomogeneous Poisson process.</p><p>In the second scenario input spikes are generated by a homogeneousPoisson process with constant rate <spanclass="math inline">\(\nu_0\)</span>.</p><p>In a large interval <span class="math inline">\(T\gg\Omega^{-1}\)</span>, we expect in both cases a total number of <spanclass="math inline">\(\nu_0T\)</span> input spikes.</p><p>It is found that the spike count numbers <spanclass="math inline">\(n^{(1)}\)</span> and <spanclass="math inline">\(n^{(2)}\)</span> are significantly different ifthe threshold is in the range <span class="math display">\[    u_{\infty}+\sqrt{\langle \Deltau^{2}\rangle}&lt;\theta&lt;u_{\infty}+3\sqrt{\langle \Deltau^{2}\rangle} \tag{9.37}\]</span></p><p>This means that a neuron in the subthreshold regime is capable oftransforming a temporal code (amplitude <spanclass="math inline">\(\nu_1\)</span> of the variations in the input)into a spike count code.</p><h2 id="exercises">Exercises</h2>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (8)</title>
    <link href="/2022/08/26/Neuronal-Dynamics-8/"/>
    <url>/2022/08/26/Neuronal-Dynamics-8/</url>
    
    <content type="html"><![CDATA[<h1 id="noisy-input-models-barrage-of-spike-arrivals">Noisy InputModels: Barrage of Spike Arrivals</h1><p>The online version of this chapter:</p><hr /><p>Chapter 8 Noisy Input Models: Barrage of Spike Arrivalshttps://neuronaldynamics.epfl.ch/online/Ch8.html</p><hr /><h2 id="noise-input">Noise input</h2><p>Modeling the noisiness of the input amounts to splitting the inputcurrent into two components, a deterministic and a stochastic one <spanclass="math display">\[    I(t)=I^{det}(t)+I^{noise}(t), \tag{8.1}\]</span></p><p><span class="math inline">\(I^{det}\)</span>: known or at leastpredictable. <span class="math inline">\(I^{noise}\)</span>:unpredictable or noisy.</p><p>For example, during an in vitro study with intracellular currentinjection, <span class="math inline">\(I^{det}\)</span> would be thecurrent that is set on the switchboard of the current generator, but theactual current fluctuates around the preset value because of finitetemperature.</p><p>A nonlinear integrate-and-fire model with noisy input has the voltageequation <span class="math display">\[    \tau_m\frac{\mathrm{d}}{\mathrm{d}t}u=f(u)+RI^{det}(t)+RI^{noise}(t).\tag{8.2}\]</span> If <span class="math inline">\(u\)</span> reaches thethreshold <span class="math inline">\(\theta_{reset}\)</span>, theintegration is stopped and the membrane potential reset to <spanclass="math inline">\(u_r\)</span>.</p><h3 id="white-noise">White noise</h3><p>Formulate the noise term <spanclass="math inline">\(RI^{noise}\)</span> in the differential equationof the membrane voltage as a 'white noise', <spanclass="math inline">\(RI^{noise}(t)=\xi(t)\)</span>. White noise <spanclass="math inline">\(\xi\)</span> is a stochastic process characterizedby its expectation value, <span class="math display">\[    \langle \xi(t) \rangle =0, \tag{8.3}\]</span> and the autocorrelation <span class="math display">\[    \langle \xi(t)\xi(t&#39;)\rangle=\sigma^{2} \tau_m \delta(t-t&#39;),\tag{8.4}\]</span> where <span class="math inline">\(\sigma\)</span> is theamplitude of the noise (in units of voltage) and <spanclass="math inline">\(\tau_m\)</span> the time constant of (8.2). (8.4)indicates that the process <span class="math inline">\(\xi\)</span> isuncorrelated in time: knowledge of the value <spanclass="math inline">\(\xi\)</span> at time <spanclass="math inline">\(t\)</span> does not enable us to predict its valueat any other time <span class="math inline">\(t&#39;\neq t\)</span>.</p><p>The Fourier transform of the autocorrelation function (8.4) yieldsthe power spectrum. The power spectrum of white noise is flat, i.e., thenoise is equally strong at all frequencies.</p><p>Stochastic differential equation, i.e., an equation for a stochasticprocess, <span class="math display">\[    \tau_m\frac{\mathrm{d}}{\mathrm{d}t}u(t)=f(u(t))+RI^{det}(t)+\xi(t), \tag{8.5}\]</span> also called <strong>Langevin equation</strong>.</p><p>In the mathematical literature, a different style of writing theLangevin equation dominates. <span class="math display">\[    \mathrm{d}u=f(u)\frac{\mathrm{d}t}{\tau_m}+RI^{det}(t)\frac{\mathrm{d}t}{\tau_m}+\sigma\mathrm{d}W_{t}, \tag{8.6}\]</span> where <span class="math inline">\(\mathrm{d}W_{t}\)</span> arethe increments of the <strong>Wiener process</strong> in a short time<span class="math inline">\(\mathrm{d}t\)</span>, that is, <spanclass="math inline">\(\mathrm{d}W_{t}\)</span> are random variablesdrawn from a Gaussian distribution with zero mean and varianceproportional to the step size <spanclass="math inline">\(\mathrm{d}t\)</span> (therefore its standarddeviation is proportional to <spanclass="math inline">\(\sqrt{\mathrm{d}t}\)</span>). White noise which isGaussian distributed is called Gaussian white noise.</p><h4id="example-leaky-integrate-and-fire-model-with-white-noise-input">Example:Leaky integrate-and-fire model with white noise input</h4><p>In the case of the leaky integrate-and-fire model (with voltage scalechosen such that the resting potential is at zero), the stochasticdifferential equation is <span class="math display">\[    \tau_m \frac{\mathrm{d}}{\mathrm{d}t}u(t)=-u(t)+RI^{det}(t)+\xi(t),\tag{8.7}\]</span> which is called the <strong>Ornstein-Uhlenbeckprocess</strong>.</p><p>The fluctuates of the membrane potential have an autocorrelation withcharacteristic time <span class="math inline">\(\tau_m\)</span>.</p><h3 id="noisy-versus-noiseless-membrane-potential">Noisy versusnoiseless membrane potential</h3><p>Consider (8.7) for constant <spanclass="math inline">\(\sigma\)</span>. At <spanclass="math inline">\(t=\hat{t}\)</span> the membrane potential startsat a value <span class="math inline">\(u=u_r=0\)</span>. The solutionfor <span class="math inline">\(t&gt;\hat{t}\)</span> is <spanclass="math display">\[    u(t)=\frac{R}{\tau_m}\int_{0}^{t-\hat{t}} \mathrm{e}^{-s/\tau_m}I^{det}(t-s) \mathrm{d}s+\frac{1}{\tau_m}\int_{0}^{t-\hat{t}}\mathrm{e}^{-s/\tau_m} \xi(t-s) \mathrm{d}s.     \]</span> (8.9)</p><p>Since <span class="math inline">\(\langle \xi(t)\rangle=0\)</span>,the expected trajectory of the membrane potential is <spanclass="math display">\[    u_0(t)=\langleu(t|\hat{t})\rangle=\frac{R}{\tau_m}\int_{0}^{t-\hat{t}}\mathrm{e}^{-s/\tau_m} I^{det}(t-s) \mathrm{d}s.\]</span> (8.10)</p><p>In particular, for constant input current <spanclass="math inline">\(I^{det}(t)\equiv I_0\)</span> we have <spanclass="math display">\[    u_0(t)=u_{\infty}[1-\mathrm{e}^{-(t-\hat{t})/\tau_m}] \tag{8.11}\]</span> with <span class="math inline">\(u_{\infty}=RI_0\)</span>.</p><p>The variance <span class="math inline">\(\langle \Deltau^{2}\rangle\)</span> can be evaluated as <span class="math display">\[    \langle \Delta u^{2}(t)\rangle=\frac{1}{\tau_m^{2}}\int_{0}^{t-\hat{t}} \int_{0}^{t-\hat{t}}\mathrm{e}^{-s/\tau_m} \mathrm{e}^{-s&#39;/\tau_m} \langle\xi(t-s)\xi(t-s&#39;)\rangle \mathrm{d}s&#39; \mathrm{d}s\]</span> (8.12)</p><p>We use <span class="math inline">\(\langle\xi(t-s)\xi(t-s&#39;)\rangle=\sigma^{2}\tau_m\delta(s-s&#39;)\)</span>and perform the integration. The result is <span class="math display">\[    \langle \Deltau^{2}(t)\rangle=\frac{1}{2}\sigma^{2}\left[1-\mathrm{e}^{-2(t-\hat{t})/\tau_m}\right] \tag{8.13}\]</span></p><p>If the threshold is high enough so that firing is a rare event, thetypical distance between the actual trajectory and the mean trajectoryapproaches with time constant <spanclass="math inline">\(\tau_m/2\)</span> a limiting value <spanclass="math display">\[    \sqrt{\langle \Deltau_{\infty}^{2}\rangle}=\frac{1}{\sqrt{2}}\sigma. \tag{8.14}\]</span></p><h3 id="colored-noise">Colored Noise</h3><p>A noise term with a power spectrum which is not flat is calledcolored noise. Colored noise <spanclass="math inline">\(I^{noise}(t)\)</span> can be generated from whitenoise by suitable filtering. For example, low-pass filtering <spanclass="math display">\[    \tau_s\frac{\mathrm{d}I^{noise}(t)}{\mathrm{d}t}=-I^{noise}(t)+\xi(t),\tag{8.15}\]</span> where <span class="math inline">\(\xi(t)\)</span> is the whitenoise process.</p><p>Integrate (8.15) so as to arrive at <span class="math display">\[    I^{noise}(t)=\int_{0}^{\infty} \kappa(s)\xi(t-s) \mathrm{d}s,\tag{8.16}\]</span></p><p>where <span class="math inline">\(\kappa(s)\)</span> is anexponential low-pass filter with time constant <spanclass="math inline">\(\tau_s\)</span> (Actually <spanclass="math inline">\(\kappa(s)=\exp (-s/\tau_s)/\tau_s\)</span>). Theautocorrelation function is therefore <span class="math display">\[    \langle I^{noise}(t)I^{noise}(t&#39;)\rangle =\int_{0}^{\infty}\int_{0}^{\infty} \kappa(s)\kappa(s&#39;)\langle\xi(t-s)\xi(t&#39;-s&#39;)\rangle \mathrm{d}s&#39; \mathrm{d}s.\]</span> (8.17)</p><p>We exploit the definition of the white noise correlation function in(8.4) and find <span class="math display">\[    \langle I^{noise}(t)I^{noist}(t&#39;)\rangle=a \exp\left(-\frac{\lvert t-t&#39; \rvert }{\tau_s}\right) \tag{8.18}\]</span> with a amplitude factor <spanclass="math inline">\(a\)</span>. That means knowledge of the inputcurrent at time <span class="math inline">\(t\)</span> gives us a hintabout the input current shortly afterward, as long as <spanclass="math inline">\(\lvert t&#39;-t \rvert \ll \tau_s\)</span>.</p><p>The noise spectrum is flat for frequencies <spanclass="math inline">\(\omega\ll 1/\tau_s\)</span> and falls off for<span class="math inline">\(\omega&gt;1/\tau_s\)</span>. Sometimes <spanclass="math inline">\(1/\tau_s\)</span> is called the <strong>cut-offfrequency</strong>.</p><h2 id="stochastic-spike-arrival">Stochastic spike arrival</h2><p>If we focus on a leaky integrate-and-fire neuron, shift the voltageso that the resting potential is at zero and concentrate on a singleneuron receiving stochastic input from background neurons hence drop theinput from the network, we arrive at <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}u=-\frac{u}{\tau_m}+\frac{1}{C}I^{ext}(t)+\sum_{k}^{}\sum_{t_k^{(f)}}^{} w_k \delta(t-t_k^{(f)})\]</span> (8.20)</p><p>The membrane potential is reset to <spanclass="math inline">\(u_r\)</span> whenever is reaches the threshold<span class="math inline">\(\theta\)</span>. (8.20) is called<strong>Stein's model</strong>.</p><p>In Stein's model, each input spike generates a postsynaptic potential<span class="math inline">\(\Delta u(t)=w_k\epsilon(t-t^{(f)}_k)\)</span> with <spanclass="math inline">\(\epsilon(s)=\mathrm{e}^{-s/\tau_m}\Theta(s)\)</span>. Integration of (8.20) yields <spanclass="math display">\[    u(t|\hat{t})=u_r \exp(-\frac{t-\hat{t}}{\tau_m})+\frac{1}{C}\int_{0}^{t-\hat{t}} \exp(-\frac{s}{\tau_m})I(t-s) \mathrm{d}s+\sum_{k=1}^{N} \sum_{t_k^{(f)}}^{}w_k \epsilon(t-t_k^{(f)})\]</span> (8.21)</p><p>for <span class="math inline">\(t&gt;\hat{t}\)</span> where <spanclass="math inline">\(\hat{t}\)</span> is the last firing time of theneuron.</p><h3id="membrane-potential-fluctuations-caused-by-spike-arrivals">Membranepotential fluctuations caused by spike arrivals</h3><p>We assume that each input spike evokes a postsynaptic potential <spanclass="math inline">\(w_0 \epsilon(s)\)</span> of the same amplitude andshape, independent of <span class="math inline">\(k\)</span>. The inputstatistics is assumed to be Poisson. Thus, the total input spike train<span class="math display">\[    S(t)=\sum_{k=1}^{N} \sum_{t_k^{(f)}}^{} \delta(t-t_k^{f}),\tag{8.22}\]</span> that arrives at neuron <span class="math inline">\(i\)</span>is a random process with expectation <span class="math display">\[    \langle S(t)\rangle =\nu_0 \tag{8.23}\]</span> and autocorrelation <span class="math display">\[    \langle S(t)S(t&#39;)\rangle=\nu_0^{2}+\nu_0 \delta(t-t&#39;);\tag{8.24}\]</span></p><p>Suppose that we start theh integration of the passive membraneequation at <span class="math inline">\(t=-\infty\)</span> with initialcondition <span class="math inline">\(u_r=0\)</span>. We rewrite (8.21)using the definition of the spike trian in (8.22) <spanclass="math display">\[    u(t)=\frac{1}{C}\int_{0}^{\infty} \exp\left(-\frac{s}{\tau_m}\right)I(t-s) \mathrm{d}s+w_0 \int_{0}^{\infty}\epsilon(s)S(t-s) \mathrm{d}s. \tag{8.25}\]</span></p><p>Using (8.23) and (8.24) we find <span class="math display">\[    \langle u(t)\rangle=u_0(t)=\frac{1}{C}\int_{0}^{\infty} \exp\left(-\frac{s}{\tau_m}\right)I(t-s) \mathrm{d}s+w_0\nu_0\int_{0}^{\infty} \epsilon(s) \mathrm{d}s \tag{8.26}\]</span> and <span class="math display">\[    \langle \Delta u^{2}\rangle=\langle [u(t)-u_0(t)]^{2}\rangle =w_0^{2}\nu_0 \int_{0}^{\infty} \epsilon^{2}(s) \mathrm{d}s.\]</span></p><p>With stochastic spike arrival at excitatory synapses, mean andvariance cannot be changed independently. As we will see next, acombination of excitation and inhibition allows us to increase thevariance while keeping the mean of the potential fixed.</p><h3 id="balanced-excitation-and-inhibition">Balanced excitation andinhibition</h3><p>Suppose we have excitatory input and inhibitory input of the samefrequency. The mean of the stochastic background input vanishes since<span class="math inline">\(\sum_{k}^{} w_k v_k=0\)</span>. Thestochastic arrival of background spikes generates fluctuations of thevoltage with variance <span class="math display">\[    \langle \Delta u^{2}\rangle=\frac{1}{2}\tau_m \sum_{k}^{}w_k^{2}\nu_k\]</span></p><p>Let us now increase all rates by a factor of <spanclass="math inline">\(a&gt;1\)</span> and multiply at the same time thesynaptic efficacies by a factor <spanclass="math inline">\(1/\sqrt{a}\)</span>. Then both mean and varianceof the stochastic background input are the same as before, but the size<span class="math inline">\(w_k\)</span> of the jumps is decreased.</p><p>In the limite of <span class="math inline">\(a\to \infty\)</span> thejump process turns into a diffusion process. The diffusion limit: <spanclass="math display">\[    \frac{w}{\sqrt{a}}S^{exc}-\frac{w}{\sqrt{a}}S^{inh} \to \xi(t)\tag{8.31}\]</span></p><h4 id="example-synaptic-time-constants-and-colored-noise">Example:Synaptic time constants and colored noise</h4><p>In contrast to the previous discussion of balanced input, we nowassume that each spike arrival generated a current pulse <spanclass="math inline">\(\alpha(s)\)</span> of finite duration so that thetotal synaptic input current is <span class="math display">\[    RI(t)=w^{exc}\int_{0}^{\infty} \alpha(s)S^{exc}(t-s) \mathrm{d}s -w^{inh} \int_{0}^{\infty} \alpha(s)S^{inh}(t-s) \mathrm{d}s\]</span> (8.32)</p><p>If the spike arrivel is Poisson with rates <spanclass="math inline">\(a\nu\)</span> and the synaptic weights are <spanclass="math inline">\(w^{exc}=w^{inh}=w/\sqrt{a}\)</span>, then we cantake the limit <span class="math inline">\(a\to \infty\)</span> with nochange of mean or variance. The result is colored noised.</p><p>An instructive case is <spanclass="math inline">\(\alpha(s)=(1/\tau_s)\exp(-s/\tau_s)\Theta(s)\)</span> with synaptic time constant <spanclass="math inline">\(\tau_s\)</span>. In the limit <spanclass="math inline">\(\tau_s\to 0\)</span> we are back to whitenoise.</p><h2 id="subthreshold-vs.-superthreshold-regime">Subthreshold vs.Superthreshold regime</h2><p>An arbitrary time-dependent stimulus <spanclass="math inline">\(I(t)\)</span> is called subthreshold if isgenerates a membrane potential that stays - in the absence of noise -below the firing threshold. Stimuli that induce spikes even in anoise-free neuron are called superthreshold.</p><p>The distinction between sub- and superthreshold stimuli has importantconsequences for the firing behavior of neurons in the presence ofnoise. Consider a leaky integrate-and-fire neuron with constant input<span class="math inline">\(I_0\)</span> for <spanclass="math inline">\(t&gt;0\)</span>. <span class="math display">\[    u_0(t)=u_{\infty}[1-\mathrm{e}^{-t/\tau_m}]+u_r\mathrm{e}^{-t/\tau_m} . \tag{8.33}\]</span> where <span class="math inline">\(u_{\infty}=RI_0\)</span>. If<span class="math inline">\(u_{\infty}&gt;\theta\)</span>, the neuronfires regularly. The interspike interval is <spanclass="math inline">\(s_0\)</span> derived from <spanclass="math inline">\(u_0(s_0)=\theta\)</span>. Thus <spanclass="math display">\[    s_0=\tau \ln \frac{u_{\infty}-u_r}{u_{\infty}-\theta}. \tag{8.34}\]</span></p><p>In the superthreshold regime, the spike train in the presence ofdiffusive noise is simply a noisy version of the regular spike train ofthe noise-free neuron.</p><h4id="example-interval-distribution-in-the-superthreshold-regime">Example:Interval distribution in the superthreshold regime</h4><p>For small noise amplitude <span class="math inline">\(0&lt;\sigma\llu_{\infty}-\theta\)</span>, and superthreshold stimulation, the intervaldistribution is centered at the deterministic interspike interval <spanclass="math inline">\(s_0\)</span>. As long as the mean trajectory isfar away from the threshold, the distribution of membrane potentials hasa Gaussian shape. As time goes on, the distribution of membranepotentials is pushed across the threshold.</p><p>Suppose the membrane potential crosses the threshold with slope <spanclass="math inline">\(u_0&#39;\)</span>, then the interval distributionis approximately given by a Gaussian with mean <spanclass="math inline">\(s_0\)</span> and width <spanclass="math inline">\(\sigma/\sqrt{2}u_0&#39;\)</span>, i.e., <spanclass="math display">\[    P_0(t|0)=\frac{1}{\sqrt{\pi}}\frac{u_0&#39;}{\sigma}\exp\left[-\frac{(u_0&#39;)^{2}(t-s_0)^{2}}{\sigma^{2}}\right] \tag{8.35}\]</span></p><h2 id="diffusion-limit-and-fokker-planck-equation">Diffusion limit andFokker-Planck equation</h2><p>In this section we analyze (8.20) and show how to map it to thediffusion model. The evolution of the probability density as a functionof time is described, in the diffusion limit, by the Fokker-Planckequation. We derive here in the context of a single neuron subject tonoisy input.</p><p>For simplicity we set for the time being <spanclass="math inline">\(I^{ext}=0\)</span> in (8.20). The input spikes atsynapse <span class="math inline">\(k\)</span> are generated by aPoisson process and arrive stochastically with rate <spanclass="math inline">\(\nu_k(t)\)</span>. The probability that no spikearrives in a short time interval <span class="math inline">\(\Deltat\)</span> is therefore <span class="math display">\[    \operatorname{Pr} \{\text{no spike in}\ [t,t+\Deltat]\}=1-\sum_{k}^{} \nu_k(t)\Delta t. \tag{8.36}     \]</span></p><p>Given a value of <span class="math inline">\(u&#39;\)</span> at time<span class="math inline">\(t\)</span>, the probability density offinding a membrane potential <span class="math inline">\(u\)</span> attime <span class="math inline">\(t+\Delta t\)</span> is given by <spanclass="math display">\[    \begin{aligned}        P^{trans}(u,t+\Delta t|u&#39;,t)=\left[1-\Delta t \sum_{k}^{}\nu_k(t)\right]\delta(u-u&#39; \mathrm{e}^{-\Delta t/\tau_m})\\+\Delta t\sum_{k}^{} \nu_k(t)\delta(u-u&#39;\mathrm{e}^{-\Delta t/\tau_m} -w_k).\\    \end{aligned}\]</span> (8.37)</p><p>We will refer to <span class="math inline">\(P^{trans}\)</span> asthe transition law. The evolution of the membrane potential is a MarkovProcess and can be described by <span class="math display">\[    p(u,t+\Delta)=\int_{}^{} P^{trans}(u,t+\Delta t|u&#39;,t)p(u&#39;,t)\mathrm{d}u&#39;. \tag{8.38}\]</span></p><p>Recall that <spanclass="math inline">\(\delta(au)=a^{-1}\delta(u)\)</span>. The result ofthe integration is <span class="math display">\[    \begin{aligned}        p(u,t+\Delta t)=\left[ 1-\Delta t \sum_{k}^{}\nu_k(t)\right]\mathrm{e}^{\Delta t/\tau_m} p(\mathrm{e}^{\Deltat/\tau_m} u,t) \\        +\Delta t\sum_{k}^{} \nu_k(t)\mathrm{e}^{\Delta t/\tau_m}p(\mathrm{e}^{\Delta t/\tau_m}( u-w_k),t).    \end{aligned}\]</span> (8.39)</p><p>So we have <span class="math display">\[    \frac{\partial p(u,t)}{\partialt}==\frac{1}{\tau_m}p(u,t)+\frac{1}{\tau_m}u\frac{\partial }{\partialu}p(u,t)+\sum_{k}^{} \nu_k(t)[p(u-w_k,t)-p(u,t)].\]</span> (8.40)</p><p>Furthermore, if the jump amplitudes <spanclass="math inline">\(w_k\)</span> are small, we can expand the RHS of(8.40) with respect to <span class="math inline">\(u\)</span> about<span class="math inline">\(p(u,t)\)</span>: <spanclass="math display">\[    \tau_m \frac{\partial }{\partial t}p(u,t)=-\frac{\partial }{\partialu}\left[-u+\tau_m \sum_{k}^{}\nu_k(t)w_k\right]p(u,t)+\frac{1}{2}\left[\tau_m \sum_{k}^{}\nu_k(t)w_k^{2}\right]\frac{\partial ^{2}}{\partial u^{2}}p(u,t),\]</span> (8.41)</p><p>where we have neglected terms of order <spanclass="math inline">\(w_k^{3}\)</span> and higher. The expansion in<span class="math inline">\(w_k\)</span> is called the Kramers-Moyalexpansion.</p><p>In (8.41), the first term in rectangular brackets describes thesystematic drift of the membrane potential due to leakage (<spanclass="math inline">\(\propto -u\)</span>) and mean background input(<span class="math inline">\(\propto \sum_{k}^{} \nu_k(t)w_k\)</span>).The second term in rectangular brackets corresponds to a 'diffusionconstant' and accounts for the fluctuations of the membranepotential.</p><p>(8.41) is equivalent to the Langevin equation (8.7) with <spanclass="math inline">\(RI(t)=\tau_m\sum_{k}^{} \nu_k(t)w_k\)</span> andtime-dependent noise amplitude <span class="math display">\[    \sigma^{2}(t)=\tau_m\sum_{k}^{} \nu_k(t)w_k^{2}. \tag{8.42}\]</span></p><p>For the transition from (8.40) to (8.41) we have suppressed <spanclass="math display">\[    \sum_{n=3}^{\infty} \frac{(-1)^{n}}{n!}A_n(t)\frac{\partial^{n}}{\partial u^{n}}p(u,t) \tag{8.43}      \]</span> with <span class="math inline">\(A_n=\tau_m \sum_{k}^{}\nu_k(t)w_k^{n}\)</span>.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x221.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x222.png" /></div></div></div><p>Figures above shows a sequence of models where the size of theweights <span class="math inline">\(w_k\)</span> decreases so that <spanclass="math inline">\(A_n \to 0\)</span> for <spanclass="math inline">\(n\geqslant 3\)</span> while the mean <spanclass="math inline">\(\sum_{k}^{} \nu_k(t)w_k\)</span> and the secondmoment <span class="math inline">\(\sum_{k}^{} \nu_k(t)w_k^{2}\)</span>remain constant. Such a sequence of models does not exist for excitatoryinput alone.</p><h3 id="threshold-and-firing">Threshold and firing</h3><p>We now incorporate a threshold condition into (8.41) and (8.7). Inthe Fokker-Planck equation (8.41), the firing threshold is incorporatedas a boundary condition <span class="math display">\[    p(\theta,t)\equiv 0 \ \text{for all} \ t\]</span></p><div class="note note-warning">            <p>This boundary condition is only for white noise model. For colorednoise the density at threshold is finite, because the effectivefrequency of 'attempts' to push a neuron which has membrane potential<span class="math inline">\(\theta-\delta&lt;u\leqslant \theta\)</span>above threshold is limited by the <strong>cut-off frequency</strong> ofthe noise.</p>          </div><h4 id="example-free-distribution-without-threshold">Example: Freedistribution without threshold</h4><p>The solution of (8.41) with initial condition <spanclass="math inline">\(p(u,\hat{t})=\delta(u-u_r)\)</span> is a Gaussianwith mean <span class="math inline">\(u_0(t)\)</span> and variance <spanclass="math inline">\(\langle \Delta u^{2}(t)\rangle\)</span>, i.e.,<span class="math display">\[    p(u,t)=\frac{1}{\sqrt{2\pi \langle \Delta u^{2}(t)\rangle}}\exp\left\{-\frac{[u(t|\hat{t})-u_0(t)]^{2}}{2\langle \Deltau^{2}(t)\rangle}\right\}\]</span> (8.45)</p><p>In particular, the stationary distribution that is approached in thelimit of <span class="math inline">\(t\to \infty\)</span> for constantinput <span class="math inline">\(I_0\)</span> is <spanclass="math display">\[    p(u,\infty)=\frac{1}{\sqrt{\pi}}\frac{1}{\sigma} \exp\left\{\frac{[u-RI_0]^{2}}{\sigma^{2}}\right\}, \tag{8.46}\]</span> which describes a Gaussian distribution with mean <spanclass="math inline">\(u_{\infty}=RI_0\)</span> and variance <spanclass="math inline">\(\sigma/\sqrt{2}\)</span>.</p><h3 id="interval-distribution-for-the-diffusive-noise-model">Intervaldistribution for the diffusive noise model</h3><p>Let us consider a leaky integrate-and-fire neuron that starts at time<span class="math inline">\(\hat{t}\)</span> with a membrane potential<span class="math inline">\(u_r\)</span> and is driven for <spanclass="math inline">\(t&gt;\hat{t}\)</span> by a known input <spanclass="math inline">\(I(t)\)</span>. We have <spanclass="math display">\[    \operatorname{Pr}\{u_0&lt;u(t)&lt;u_1|u(\hat{t})=u_r\}=\int_{u_0}^{u_1} p(u,t)\mathrm{d}u \tag{8.47}\]</span> where <span class="math inline">\(p(u,t)\)</span> is theprobability density of the membrane potential at time <spanclass="math inline">\(t\)</span>. In the diffusion limit, <spanclass="math inline">\(p(u,t)\)</span> can be found by solution of theFokker-Planck equation (8.41) with initial condition <spanclass="math inline">\(p(u,\hat{t})=\delta(u-u_r)\)</span> and boundarycondition <span class="math inline">\(p(\theta,t)=0\)</span>.</p><p>Imagine that we run 100 simulation trials. The expected fraction ofsimulations that have not yet reached the threshold and therefore still'survive' up to time <span class="math inline">\(t\)</span> is given bythe survivior function, <span class="math display">\[    S_{I}(t|\hat{t})=\int_{-\infty}^{\theta} p(u,t) \mathrm{d}u.\tag{8.48}\]</span></p><p>In view of (7.24), the input-dependent interval distribution istherefore <span class="math display">\[    P_{I}(t|\hat{t})=-\frac{\mathrm{d}}{\mathrm{d}t}\int_{-\infty}^{\theta}p(u,t) \mathrm{d}u. \tag{8.49}\]</span></p><p>In the context of noisy integrate-and-fire neurons <spanclass="math inline">\(P_{I}(t|\hat{t})\)</span> is called thedistribution of 'first passage times'. no general solution is known forthe first passage time problem of the Ornstein-Uhlenbeck process.</p><h4 id="example-numerical-evaluation-of-p_ithatt">Example: Numericalevaluation of <span class="math inline">\(P_{I}(t|\hat{t})\)</span></h4><p>(8.41) has been solved in the absence of a threshold. The transitionprobability from an arbitrary starting value <spanclass="math inline">\(u&#39;\)</span> at time <spanclass="math inline">\(t&#39;\)</span> to a new value <spanclass="math inline">\(u\)</span> at time <spanclass="math inline">\(t\)</span> is <span class="math display">\[    P^{trans}(u,t|u&#39;,t&#39;)=\frac{1}{\sqrt{2\pi\langle \Deltau^{2}(t)\rangle}}\exp \left\{-\frac{[u-u_0(t)]^{2}}{2\langle \Deltau^{2}(t)\rangle}\right\}\]</span> (8.50)</p><p>with <span class="math display">\[    u_0(t)=u&#39;\mathrm{e}^{-(t-t&#39;)/\tau_m} +\int_{0}^{t-t&#39;}\mathrm{e}^{-s&#39;/\tau_m} I(t-s&#39;) \mathrm{d}s \tag{8.51}\]</span></p><p><span class="math display">\[    \langle \Deltau^{2}(t)\rangle=\frac{\sigma^{2}}{2}[1-\mathrm{e}^{-2(t-s)/\tau_m}].\tag{8.52}\]</span></p><p>Because of the Markov property, the probability density to cross thethreshold (not necessarily for the first time) at a time <spanclass="math inline">\(t\)</span>, is equal to the probability to crossit for the first time at <spanclass="math inline">\(t&#39;&lt;t\)</span> and to return back to <spanclass="math inline">\(\theta\)</span> at time <spanclass="math inline">\(t\)</span>, that is, <span class="math display">\[    P^{trans}(\theta,t|u_r,\hat{t})=\int_{\hat{t}}^{t}P_{I}(t&#39;|\hat{t})P^{trans}(\theta,t|\theta,t&#39;) \mathrm{d}t&#39;.\tag{8.53}\]</span></p><h3 id="mean-interval-and-mean-firing-rate-diffusive-noise">Meaninterval and mean firing rate (diffusive noise)</h3><p>Here we express the mean firing rate of a leaky integrate-and-firemodel with diffusive noise as a function of a (constant) input <spanclass="math inline">\(I_0\)</span>.</p><p>For constant input <span class="math inline">\(I_0\)</span> the meaninterspike interval is <span class="math inline">\(\langles\rangle=\int_{0}^{\infty} sP_{I_0}(s|0) \mathrm{d}s=\int_{0}^{\infty}sP_0(s) \mathrm{d}s\)</span>. For the diffusion model (8.7) withthreshold <span class="math inline">\(\theta\)</span>, reset potential<span class="math inline">\(u_r\)</span>, and membrane time constant<span class="math inline">\(\tau_m\)</span>, the mean interval is <spanclass="math display">\[    \langle s\rangle=\tau_m\sqrt{\pi}\int_{\frac{u_r-h_0}{\sigma}}^{\frac{\theta-h_0}{\sigma}} \exp(u^{2})[1+\text{erf}(u)] \mathrm{d}u, \tag{8.54}\]</span> where <span class="math inline">\(h_0=RI_0\)</span> is theinput potential caused by the constant current <spanclass="math inline">\(I_0\)</span>. This expression is sometimes calledthe <strong>Siegert-formula</strong>. The inverse of the mean intervalis the mean firing rate.</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (7)</title>
    <link href="/2022/07/24/Neuronal-Dynamics-7/"/>
    <url>/2022/07/24/Neuronal-Dynamics-7/</url>
    
    <content type="html"><![CDATA[<h1 id="variability-of-spike-trains-and-neural-codes">Variability ofSpike Trains and Neural Codes</h1><p>The online version of this chapter:</p><hr /><p>Chapter 7 Variability of Spike Trains and Neural Codeshttps://neuronaldynamics.epfl.ch/online/Ch7.html</p><hr /><h2 id="spiking-train-variability">Spiking train variability</h2><h3 id="are-neurons-noisy">Are neurons noisy?</h3><p>Termal noise: literally omnipresent. Due to the discrete nature ofelectric charge carriers, the voltage <spanclass="math inline">\(u\)</span> across any electrical resistor <spanclass="math inline">\(R\)</span> fluctuates at finite temperature(Johnson noise). The variance of the fluctuations at rest is <spanclass="math inline">\(\langle \Delta u^{2} \rangle \propto RkTB\)</span>where <span class="math inline">\(k\)</span> is the Boltzmann constant,<span class="math inline">\(T\)</span> the temperature and <spanclass="math inline">\(B\)</span> the bandwidth of the system.Fluctuations due to Johnson noise are of minor importance compared toother noise sources in neurons.</p><p>Another source of noise arises from the finite number of ion channelsin a patch of neuronal membrane. For a given constant membrane potential<span class="math inline">\(u\)</span>, a fraction <spanclass="math inline">\(P_i(u)\)</span> of ion channel of type <spanclass="math inline">\(i\)</span> is open on average. The actual numberof open channels fluctuates around <spanclass="math inline">\(N_iP_i(u)\)</span> where <spanclass="math inline">\(N_i\)</span> is the total number of ion channelsof type <span class="math inline">\(i\)</span> in that patch ofmembrane.</p><h3 id="noise-from-the-network">Noise from the Network</h3><p>Extrinsic noise: noise that are due to signal transmission andnetwork effects (extrinsic noise).</p><p>Synaptic transmission failure imposes a substantial limitation tosignal transmisson with a neuronal network.</p><p>Networks of excitatory and inhibitory neurons with fixed randomconnectivity can produce highly irregular spike trains - even in theabsence of any source of noise.</p><h2 id="mean-firing-rate">Mean Firing Rate</h2><h3 id="rate-as-a-spike-count-and-fano-factor">Rate as a Spike Count andFano Factor</h3><p>An experimentalist observes in trial <spanclass="math inline">\(k\)</span> the spikes of a given neuron. Thefiring rate in trial <span class="math inline">\(k\)</span> is the spikecount <span class="math inline">\(n_k^{sp}\)</span> in an interval ofduration <span class="math inline">\(T\)</span> divided by <spanclass="math inline">\(T\)</span>. <span class="math display">\[    \nu_k=\frac{n_k^{sp}}{T}. \tag{7.1}\]</span></p><p><span class="math inline">\(n_k^{sp}\)</span> mean by <spanclass="math inline">\(\langle n^{sp}\rangle\)</span> and deviations fromthe mean as <span class="math inline">\(\Deltan_{k}^{sp}=n_k^{sp}-\langle n^{sp}\rangle\)</span>. Variability of thespike count measure is characterized by the <strong>FanoFactor</strong>, defined as the variance of the spike count <spanclass="math inline">\(\langle (\Delta n^{sp})^{2}\rangle\)</span>divided by its mean <span class="math display">\[    F=\frac{\langle (\Delta n^{sp})^{2}\rangle}{\langle n^{sp}\rangle}\tag{7.2}\]</span></p><p>Fano In experiments, the mean and varianceare estimated by averaging over <span class="math inline">\(K\)</span>trials <span class="math inline">\(\langle n^{sp}\rangle=(1/K)\sum_{k=1}^{K} n_k^{sp}\)</span> and <span class="math inline">\(\langle(\Delta n^{sp})^{2}\rangle=(1/K) \sum_{k=1}^{K} (\Deltan_k^{sp})^{2}\)</span>.</p><p>The firing rate defined here as spike count divided by themeasurement time <span class="math inline">\(T\)</span> is identical tothe inverse of the mean interspike interval. Since <spanclass="math inline">\(1/\langle x\rangle \neq \langle(1/x)\rangle\)</span>, if we assign a variable <spanclass="math inline">\(\tilde{\nu}(t)=1/(t^{(k+1)}-t^{(k)})\)</span> forall times <span class="math inline">\(t^{(k)}&lt;t\leqslantt^{(k+1)}\)</span>, the temporal average of <spanclass="math inline">\(\tilde{\nu}(t)\)</span> over a much longer time<span class="math inline">\(T\)</span> is not the same as the mean rate<span class="math inline">\(\nu\)</span> defined here as spike countdivided by <span class="math inline">\(T\)</span>.</p><p>Shortback: Too slow for animal!!!</p><h4 id="example-homogeneous-poisson-process">Example: HomogeneousPoisson Process</h4><p>Since the exact firing time of a spike does not matter (as we onlyfocus on <span class="math inline">\(\nu\)</span>), it is tempting todescribe spiking as a Poisson process where spikes occur independentlyand stochastically with a constant rate <spanclass="math inline">\(\nu\)</span>.</p><p>In a homogeneous Poisson process, the probability to find a spike ina short segment of duration <span class="math inline">\(\Deltat\)</span> is <span class="math display">\[    P_{F}(t;t+\Delta t)=\nu \Delta t. \tag{7.3}\]</span></p><p>In other words, spike events are independent of each other and occurwith a constant rate (also called stochastic intensity) defined as <spanclass="math display">\[    \nu= \lim_{\Delta t \to 0} \frac{P_{F}(t;t+\Delta t)}{\Delta t}.\tag{7.4}\]</span></p><p>The expected number of spikes to occur in the measuremet interval<span class="math inline">\(T\)</span> is therefore <spanclass="math display">\[    \langle n^{sp}\rangle=\nu T, \tag{7.5}\]</span></p><p>For a Poisson process, the Fano factor is exactly one.</p><h3id="rate-as-a-spike-density-and-the-peri-stimulus-time-histogram">Rateas a Spike Density and the Peri-Stimulus-Time Histogram</h3><p>Stimulate the neuron with some input sequence, repeated the samesequence several times and the neuronal response is reported in a<strong>Peri-Stimulus-Time Histogram (PSTH)</strong> with bin width<span class="math inline">\(\Delta t\)</span>. <spanclass="math inline">\(t\)</span> is the start of the stimulation and<span class="math inline">\(\Delta t\)</span> defines the time bin forgenerating the histogram.</p><p>The number of occurrences of spikes <spanclass="math inline">\(n_{K}(t;t+\Delta t)\)</span> summed over allrepetitions. The spike density <span class="math display">\[    \rho(t)=\frac{1}{\Delta t}\frac{n_{K}(t;t+\Delta t)}{K}. \tag{7.6}\]</span></p><p>Sometimes the result is smoothed to get a continuous rate variable,usually reported in units of Hz. We call the PSTH the time-dependentfiring rate.</p><p>We have defined the spike train <span class="math display">\[    S(t)=\sum_{f}^{} \delta(t-t^{(f)}) \tag{7.7}\]</span></p><p>If each stimulation can be considered as an independent sample fromthe identical stochastic process, we can define an <strong>instantaneousfiring rate</strong> as an expectation over trials <spanclass="math display">\[    \nu(t)=\langle S(t)\rangle = \frac{1}{K \Delta t}\sum_{k=1}^{K}\int_{t}^{t+\Delta t} S_k(t&#39;) \mathrm{d}t&#39; \tag{7.8-7.9}\]</span></p><p>The PSTH (the right-hand side of (7.9)) provides therefore anempirical estimate of the instantaneous firing rate (the left-handside).</p><p>Shortback: Not possible for animal!!!</p><h4 id="example-inhomogeneous-poisson-process">Example: InhomogeneousPoisson process</h4><p>An inhomogeneous Poisson process can be used to describe the spikedensity measured in a PSTH. In an inhomogeneous Poisson process, spikeevents are independent of each other and occur with an instantaneousfiring rate <span class="math display">\[    \nu(t)=\lim_{\Delta t \to 0} \frac{P_{F}(t;t+\Delta t)}{\Delta t}.\tag{7.10}\]</span></p><p>Therefore, the probability to find a spike in a short segment ofduration <span class="math inline">\(\Delta t\)</span> is <spanclass="math inline">\(P_{F}(t;t+\Delta t)=\nu(t)\Delta(t)\)</span>. Moregenerally, the expected number of spikes in an interval of finiteduration <span class="math inline">\(T\)</span> is <spanclass="math inline">\(\langle n^{sp}\rangle=\int_{0}^{T} \nu(t)\mathrm{d}t\)</span> and the Fano factor is one, as was the case for thehomogeneous Poisson process.</p><p>If a bunch of neurons fire at <spanclass="math inline">\(\hat{t}\)</span>, then the probability to'survive' without firing for <span class="math inline">\(t\)</span>(denoted as <span class="math inline">\(S\)</span>) satisfies <spanclass="math display">\[    \frac{\mathrm{d}S}{\mathrm{d}t}=-\nu (t)\]</span></p><p>so <span class="math display">\[    S(t|\hat{t})=\exp \biggl(-\int_{\hat{t}}^{t} \nu(t&#39;)\mathrm{d}t&#39;\biggr)\]</span></p><p>The probability for a neuron that fires at <spanclass="math inline">\(t\)</span> (the first spike after the spike at<span class="math inline">\(\hat{t}\)</span>) is <spanclass="math display">\[    P(t|\hat{t})=\nu(t)S(t|\hat{t})\]</span></p><h3 id="rate-as-a-population-activity-average-over-several-neurons">Rateas a Population Activity (Average over Several Neurons)</h3><p>Suppose we have a population of neurons with identical properties.The spikes of the neurons in a population <spanclass="math inline">\(m\)</span> are sent off to another population<span class="math inline">\(n\)</span>. The relevant quantity, from thepoint of view of the receiving neuron, is the proportion of activeneurons in the presynaptic population <spanclass="math inline">\(m\)</span>. Formally, we define the<strong>population activity</strong> <span class="math display">\[    A(t)=\frac{1}{\Delta t}\frac{n_{act}(t;t+\Deltat)}{N}=\frac{1}{\Delta t}\frac{\int_{t}^{t+\delta t} \sum_{j}^{}\sum_{f}^{} \delta(t-t_j^{(f)}) \mathrm{d}t}{N}\]</span> (7.11)</p><p>where <span class="math inline">\(N\)</span> is the size of thepopulation, <span class="math inline">\(n_{act}(t;t+\Delta t)\)</span>is the number of spikes (summed over all neurons in the population) thatoccur between <span class="math inline">\(t\)</span> and <spanclass="math inline">\(t+\Delta t\)</span> where <spanclass="math inline">\(\Delta t\)</span> is a small time interval.</p><h2 id="interval-distribution-and-coefficient-of-variation">Intervaldistribution and coefficient of variation</h2><p>Define the estimation of interspike interval (ISI) distributions andinterpreted it as a conditional probability density: <spanclass="math display">\[    P_0(s)=P(t^{(f)}+s|t^{(f)}) \tag{7.12}\]</span> where <span class="math inline">\(\int_{t}^{t+\Delta t}P(t&#39;|t^{(f)}) \mathrm{d}t&#39;\)</span> is the probability that thenext spike occurs in the interval <spanclass="math inline">\([t,t+\Delta t]\)</span> given that the last spikeoccured at time <span class="math inline">\(t^{(f)}\)</span>.</p><p>In ordet to extract the mean firing rate from a stationary intervaldistribution <span class="math inline">\(P_0(s)\)</span>, we start withthe definition of the mean interval, <span class="math display">\[    \langle s \rangle=\int_{0}^{s} sP_0(s) \mathrm{d}s. \tag{7.13}\]</span></p><p>The mean firing rate is the inverse of the mean interval <spanclass="math display">\[    \nu=\frac{1}{\langle s\rangle}=\left[ \int_{0}^{\infty} sP_0(s)\mathrm{d}s \right] ^{-1}\]</span></p><h3 id="coefficient-of-variation-c_v">Coefficient of variation <spanclass="math inline">\(C_{V}\)</span></h3><p>Interspike interval distributions <spanclass="math inline">\(P_0(s)\)</span> derived from a spike train understationary conditions can be broad or sharply peaked. To quantify thewidth of the interval distribution, neuroscientists often evaluate the<strong>coefficient of variation</strong>, short <spanclass="math inline">\(C_{V}\)</span>, defined as the ratio of thestandard deviation and the mean. Therefore the square of the <spanclass="math inline">\(C_{V}\)</span> is <span class="math display">\[    C_{V}^{2}=\frac{\langle \Delta s ^{2}\rangle}{\langle s \rangle^{2}}\]</span> where <span class="math inline">\(\langles\rangle=\int_{0}^{\infty} sP_0(s) \mathrm{d}s\)</span> and <spanclass="math inline">\(\langle \Delta s^{2}\rangle=\int_{0}^{\infty}s^{2}P_0(s) \mathrm{d}s-\langle s \rangle ^{2}\)</span>.</p><p>A Poisson process produces distributions with <spanclass="math inline">\(C_{V}=1\)</span>. A value of <spanclass="math inline">\(C_{V}&gt;1\)</span>, implies that a given spiketrain is less regular that a Poisson process with the same firing rate.If <span class="math inline">\(C_{V}&lt;1\)</span>, then the spike ismore regular.</p><p>Most deterministic integrate-and-fire neurons fire periodically whendriven by a constant stimulus and therefore have <spanclass="math inline">\(C_{V}=0\)</span>. Intrinsically bursting neuronscan have <span class="math inline">\(C_{V}&gt;1\)</span>.</p><h4 id="example-poisson-process-with-absolute-refractoriness">Example:Poisson process with absolute refractoriness</h4><p>We study a Poisson neuron with absolute refractory period <spanclass="math inline">\(\Delta^{abs}\)</span>. For times since last spikelarger than $ ^{abs}$, the neuron is supposed to fire stochasticallywith rate <span class="math inline">\(r\)</span>. The intervaldistribution of a Poisson process with absolute refractoriness is givenby <span class="math display">\[    P_0(s)=    \begin{cases}        0, \quad s&lt;\Delta^{abs} \\        r\exp [-r(s-\Delta_{abs})], \quad s&gt;\Delta^{abs}        \end{cases}\]</span> (7.16)</p><p>(Notice that <span class="math inline">\(\int_{0}^{\infty} P_0(s)\mathrm{d}s=1\)</span>)</p><p>and has a mean <span class="math inline">\(\langle s \rangle=\Delta^{abs}+1/r\)</span> and variance <spanclass="math inline">\(\langle \Delta s^{2}\rangle=1/r^{2}\)</span>. Thecoefficient of variation is therefore <span class="math display">\[    C_{V}=1-\frac{\Delta^{abs}}{\langle s \rangle}\]</span> (7.17)</p><h2 id="autocorrelation-function-and-noise-spectrum">Autocorrelationfunction and noise spectrum</h2><p>Consider a spike train <span class="math inline">\(S_i(t)=\sum_{f}^{}\delta(t-t_i^{(f)})\)</span> of length <spanclass="math inline">\(T\)</span>. We suppose that <spanclass="math inline">\(T\)</span> is sufficiently long so that we canformally consider the limit <span class="math inline">\(T \to\infty\)</span>. The autocorrelation function <spanclass="math inline">\(C_{ii}(s)\)</span> of the spike train is a measurefor the probability to find two spikes at a time interval <spanclass="math inline">\(s\)</span>, i.e. <span class="math display">\[    C_{ii}(s)=\langle S_i(t)S_i(t+s)\rangle _{t}, \tag{7.18}\]</span> where <span class="math display">\[    \langle f(t)\rangle _{t}=\lim_{T \to \infty} \frac{1}{T}\int_{-T/2}^{T/2} f(t) \mathrm{d}t. \tag{7.19}\]</span></p><p>By symmetry, <spanclass="math inline">\(C_{ii}(-s)=C_{ii}(s)\)</span>.</p><p>Define the <strong>power spectrum</strong> (or <strong>power spectraldensity</strong>) of a spike train <span class="math display">\[    \mathscr{P}(\omega)=\lim_{T \to \infty}\mathscr{P}_{T}(\omega),\]</span> where <span class="math inline">\(\mathscr{P}_{T}\)</span> isthe power of a segment of length <span class="math inline">\(T\)</span>of the spike train, <span class="math display">\[    \mathscr{P}_{T}(\omega)=\frac{1}{T}\biggl\lvert \int_{-T/2}^{T/2}S_i(t)\mathrm{e}^{-i\omega t} \mathrm{d}t \biggr\rvert^{2} \tag{7.20}\]</span> The power spectrum <spanclass="math inline">\(\mathscr{P}(\omega)\)</span> of a spike train isequal to the Fourier transform <spanclass="math inline">\(\hat{C}_{ii}(\omega)\)</span> of itsautocorrelation function (Wiener-Khinchin Theorem): <spanclass="math display">\[    \begin{aligned}        \hat{C}_{ii}(\omega) &amp;= \int_{-\infty}^{\infty} \langleS_i(t)S_i(t+s)\rangle \mathrm{e}^{-i\omega s}  \mathrm{d}s \\        &amp;= \lim_{T \to \infty} \frac{1}{T} \int_{-T/2}^{T/2}S_i(t)\int_{-\infty}^{\infty} S_i(t+s)\mathrm{e}^{-i\omegas}  \mathrm{d}s \mathrm{d}t \\        &amp;=\lim_{T \to \infty} \frac{1}{T} \int_{-T/2}^{T/2}  S_i(t)\mathrm{e}^{i\omega t}\mathrm{d}t \int_{-\infty}^{\infty} S_i(s&#39;)\mathrm{e}^{-i\omega s&#39;} \mathrm{d}s&#39; \\        &amp;= \lim_{T \to \infty}\frac{1}{T}\biggl\lvert\int_{-T/2}^{T/2} S_i(t)\mathrm{e}^{-i\omega t} \mathrm{d}t\biggr\rvert^{2}.    \end{aligned}\]</span> (7.21)</p><p>The power spectral density of a spike train during spontaneousactivity is called the noise spectrum of the neuron.</p><hr /><p>Spectral density - Wikipediahttps://en.wikipedia.org/wiki/Spectral_density</p><hr /><h2 id="renewal-statistics">Renewal statistics</h2><p>Ross1.6</p><p>Poisson processes cannot be used to describe realistic interspikeinterval distributions. Spikes are generated in a renewal process, witha stochastic intensity. <span class="math display">\[    \rho(t|\hat{t})=\rho_0(t-\hat{t}) \tag{7.22}\]</span> where <span class="math inline">\(\hat{t}\)</span> is the timesince the last spike. The central assumption of renewal theory is thatthe state does not depend on earlier events. Renewal theory allows tocalculate the interval distribution <span class="math display">\[    P_0(s)=P(t^{(f)}+s|t^{(f)}) \tag{7.23}  \]</span></p><h3 id="survivor-function-and-hazard">Survivor function and hazard</h3><p>Since <span class="math inline">\(\int_{\hat{t}}^{t}P(t&#39;|\hat{t}) \mathrm{d}t&#39;\)</span> is the probability for asecond action potential between <spanclass="math inline">\(\hat{t}\)</span> and <spanclass="math inline">\(t\)</span>. Thus <span class="math display">\[    S(t|\hat{t})=1-\int_{\hat{t}}^{t} P(t&#39;|\hat{t}) \mathrm{d}t&#39;\]</span> is the probability that the neuron stays quiescent between<span class="math inline">\(\hat{t}\)</span> and <spanclass="math inline">\(t\)</span>. <spanclass="math inline">\(S(t|\hat{t})\)</span> is called the<strong>survivor function</strong>. It has an initial value <spanclass="math inline">\(S(\hat{t}|\hat{t})=1\)</span>. The rate of decayof <span class="math inline">\(S(t|\hat{t})\)</span> is defined by <spanclass="math display">\[    \rho(t|\hat{t})=-\frac{\frac{\mathrm{d}}{\mathrm{d}t}S(t|\hat{t})}{S(t|\hat{t})}=\frac{P(t|\hat{t})}{1-\int_{\hat{t}}^{t}P(t&#39;|\hat{t}) \mathrm{d}t&#39;}.\]</span> (7.25)</p><p><span class="math inline">\(\rho(t|\hat{t})\)</span> is called the'age-dependent death rate' or 'hazard' in renewal theory.</p><p>Integrating <span class="math inline">\(\mathrm{d}S/\mathrm{d}t=-\rhoS\)</span> yields the survivor function <span class="math display">\[    S(t|\hat{t})=\exp \left[ -\int_{\hat{t}}^{t} \rho(t&#39;|\hat{t})\mathrm{d}t&#39; \right] \tag{7.26}\]</span></p><p>The interval distribution is given by <span class="math display">\[    P(t|\hat{t})=-\frac{\mathrm{d}}{\mathrm{d}t}S(t|\hat{t})=\rho(t|\hat{t})S(t|\hat{t}),\tag{7.27}\]</span></p><p>(In order to emit its next spike at <spanclass="math inline">\(t\)</span>, the neuron has to survive the interval<span class="math inline">\((\hat{t},t)\)</span> without firing and thenfire at <span class="math inline">\(t\)</span>.)</p><p><span class="math display">\[    P(t|\hat{t})=\rho(t|\hat{t}) \exp \left[-\int_{\hat{t}}^{t}\rho(t&#39;|\hat{t}) \mathrm{d}t&#39; \right]. \tag{7.28}\]</span></p><p>We focus on stationary renewal systems: <span class="math display">\[    P(t|\hat{t})=P_0(t-\hat{t}) \tag{7.29}\]</span> <span class="math display">\[    S(t|\hat{t})=S_0(t-\hat{t}) \tag{7.30}\]</span> <span class="math display">\[    \rho(t|\hat{t})=\rho_0(t-\hat{t}) \tag{7.31}\]</span></p><h3 id="renewal-theory-and-experiments">Renewal theory andexperiments</h3><p>Under experimental conditions where neuronal adaptation is strong,intervals are not independent. A common measure of memory effects in atime series of events with variable intervals <spanclass="math inline">\(s_j\)</span> is the <strong>serial correlationcoefficients</strong> <span class="math display">\[    c_k=\frac{\langle s_{j+k}s_j\rangle _j-\langle s_j\rangle_j^{2}}{\langle s_j^{2}\rangle- \langle s_j\rangle^{2}}\]</span></p><p>Spike-frequency adaptation causes a negative correlation betweensubsequent intervals <spanclass="math inline">\((c_1&lt;0)\)</span>.</p><h3id="autocorrelation-and-noise-spectrum-of-a-renewal-process">Autocorrelationand noise spectrum of a renewal process</h3><p>Let <span class="math inline">\(\nu_i=\langle S_i \rangle\)</span>denote the mean firing rate (expected number of spikes per unit time) ofthe spike train. For large intervals <spanclass="math inline">\(s\)</span>, firing at time <spanclass="math inline">\(t+s\)</span> is independent from whether or notthere was a spike at time <span class="math inline">\(t\)</span>.Therefore, the expectation to find a spike at <spanclass="math inline">\(t\)</span> and another spike at <spanclass="math inline">\(t+s\)</span> approaches for <spanclass="math inline">\(s \to \infty\)</span> a limiting value <spanclass="math inline">\(\lim_{s \to \infty}\langleS_i(t)S_i(t+s)\rangle=\lim_{s \to \infty}C_{ii}(s)=\nu_i^{2}\)</span>.Substract this baseline value and we get a 'normalized' autocorrelation,<span class="math display">\[    C_{ii}^{0}(s)=C_{ii}(s)-\nu_i^{2}, \tag{7.37}\]</span> with <span class="math inline">\(\lim_{s \to\infty}C_{ii}^{0}(s)=0\)</span>. The Fourier transform of (7.37) yields<span class="math display">\[    \hat{C}_{ii}(\omega)=\hat{C}_{ii}^{0}(\omega)+2\pi \nu_i^{2}\delta(\omega). \tag{7.38}\]</span> Thus <span class="math inline">\(\hat{C}_{ii}(\omega)\)</span>diverges at <span class="math inline">\(\omega=0\)</span>. thedivergence is removed by switching to the normalizedautocorrelation.</p><p>Let us suppose that we have found a first spike at <spanclass="math inline">\(t\)</span>. The correlation function for positive<span class="math inline">\(s\)</span> will be denoted by <spanclass="math inline">\(\nu_i C_{+}(s)\)</span> or <spanclass="math display">\[    C_{+}(s)=\frac{1}{\nu_i} C_{ii}(s) \Theta(s) \tag{7.39}\]</span></p><p>The factor <span class="math inline">\(\nu_i\)</span> in (7.39) takescare of the fact that we expect a first spike at <spanclass="math inline">\(t\)</span> with rate <spanclass="math inline">\(\nu_i\)</span>. <spanclass="math inline">\(C_{+}(s)\)</span> gives the conditionalprobability density that, given a spike at <spanclass="math inline">\(t\)</span>, we will find another spike at <spanclass="math inline">\(t+s&gt;t\)</span>. The spike at <spanclass="math inline">\(t+s\)</span> can be the first spike after <spanclass="math inline">\(t\)</span>, or the second one, or the <spanclass="math inline">\(n\)</span>th one, thus for <spanclass="math inline">\(s&gt;0\)</span> <span class="math display">\[    C_{+}(s)=P_0(s)+\int_{0}^{\infty} P_0(s&#39;)P_0(s-s&#39;)\mathrm{d}s&#39;+\int_{0}^{\infty} \int_{0}^{\infty}P_0(s&#39;)P_0(s&#39;&#39;)P_0(s-s&#39;-s&#39;&#39;) \mathrm{d}s&#39;\mathrm{d}s&#39;&#39;+\cdots\]</span> (7.40) or <span class="math display">\[    C_{+}(s)=P_0(s)+\int_{0}^{\infty} P_0(s&#39;)C_{+}(s-s&#39;)\mathrm{d}s&#39; \tag{7.41}\]</span></p><p>Due to the symmetry of <span class="math inline">\(C_{ii}\)</span>,we have <span class="math inline">\(C_{ii}(s)=\nu C_{+}(-s)\)</span> for<span class="math inline">\(s&lt;0\)</span>. So</p><p><span class="math display">\[    C_{ii}(s)=\nu_i[\delta(s)+C_{+}(s)+C_{+}(-s)]. \tag{7.42}\]</span> (the autocorrelation has a <spanclass="math inline">\(\delta\)</span> peak reflecting the trivialautocorrelation of each spike with itself.</p><p>Take the Fourier transform of (7.41) and find <spanclass="math display">\[    \hat{C}_{+}(\omega)=\frac{\hat{P}_0(\omega)}{1-\hat{P}_0(\omega)}.\tag{7.43}\]</span> Together with the Fourier transform of (7.42), we obtain <spanclass="math display">\[    \hat{C}_{ii}(\omega)=\nu_i \Re\left\{\frac{1+\hat{P}_0(\omega)}{1-\hat{P}_0(\omega)}\right\} +2\pi\nu_i^{2}\delta(\omega) \tag{7.45}\]</span></p><h4 id="example-stationary-poisson-process">Example: Stationary Poissonprocess</h4><p>For a Poisson process, <span class="math display">\[    C_{+}(s)=\nu \mathrm{e}^{-\nu s} [1+\nu s+\frac{1}{2}(\nus)^{2}+\cdots ]=\nu \tag{7.46}\]</span></p><p>So the autocorrelation of a Poisson process is <spanclass="math display">\[    C_{ii}(s)=\nu \delta(s)+\nu^{2} \tag{7.47}\]</span></p><p>The Fourier transform of (7.47) yields a flat spectrum with a <spanclass="math inline">\(\delta\)</span> peak at zero: <spanclass="math display">\[    \hat{C}_{ii}(\omega)=\nu+2\pi\nu^{2}\delta(\omega). \tag{7.48}\]</span></p><h4 id="example-poisson-process-with-absolute-refractoriness-1">Example:Poisson process with absolute refractoriness</h4><p>For a Poisson proccess with absolute refractoriness defined in(7.16). The neuron fires with rate <spanclass="math inline">\(r\)</span>. For <span class="math inline">\(\omega\neq 0\)</span>, (7.45) yields the noise spectrum <spanclass="math display">\[    \hat{C}_{ii}(\omega)=\nu\left\{ 1+2\frac{\nu^{2}}{\omega^{2}}[1-\cos (\omega \Delta^{abs})]+2\frac{\nu}{\omega}\sin (\omega \Delta^{abs})\right\}^{-1},\]</span> (7.49)</p><p>For <span class="math inline">\(\omega \to 0\)</span>, the noisespectrum is decreased by a factor <span class="math inline">\([1+2(\nu\Delta^{abs})+(\nu \Delta^{abs})^{2}]^{-1}\)</span>. Explanation: themean interval of a Poisson neuron with absolute refractoriness is <spanclass="math inline">\(\langle s\rangle=\Delta^{abs}+r^{-1}\)</span>.Hence the mean firing rate is <span class="math display">\[    \nu=\frac{r}{1+\Delta^{abs}r}. \tag{7.50}\]</span></p><p>For finite <span class="math inline">\(\Delta^{abs}\)</span> thefiring is more regular than that of a Poisson process with the same meanrate <span class="math inline">\(\nu\)</span>, and hence the spectrumfor <span class="math inline">\(\omega \to 0\)</span> is less noisy.</p><p>This means that Poisson neurons with absolute refractoriness cantransmit slow signals more reliably than a simple Poisson process.</p><h3 id="input-dependent-renewal-theory">Input dependent renewaltheory</h3><h2 id="the-problem-of-neural-coding">The Problem of Neural Coding</h2><h3 id="limits-of-rate-codes">Limits of rate codes</h3><p><strong>Limitations of the spike count code</strong>. Averaging overa large number of spikes takes a long time. In a changing environment, apostsynaptic neuron does not have the time to perform a temporal averageover many (noisy) spikes.</p><p><strong>Limitations of the PSTH</strong>. It needs several trials tobuild up. Nevertheless, the PSTH measure of the instantaneous firingrate can make sense if there are large populations of similar neuronsthat receive the same stimulus.</p><p><strong>Limitations of rate as a population average</strong>. (7.11)requires a homogeneous population of neurons with identical connectionswhich is hardly realistic. Real populations will always have a certaindegree of heterogeneity both in their internal parameters and in theirconnectivity pattern.</p><p>For inhomogeneous populations, the definition (7.11) may be replacedby a weighted average over the population. Suppose that we are studyinga population of neurons which respond to a stimulus <spanclass="math inline">\(\mathbf{x}\)</span>. We may think of <spanclass="math inline">\(\mathbf{x}\)</span> as the location of thestimulus in input space. Neuron <span class="math inline">\(i\)</span>respond best to stimulus <spanclass="math inline">\(\mathbf{x}_i\)</span>. We may say that the spikesof a neuron <span class="math inline">\(i\)</span> 'represent' an inputvector <span class="math inline">\(\mathbf{x}_i\)</span>.</p><p>In a large population, many neurons will be active simultaneouslywhen a new stimulus <span class="math inline">\(\mathbf{x}\)</span> isrepresented. The location of this stimulus can then be estimated fromthe weighted population average <span class="math display">\[    \mathbf{x}^{est}(t)=\frac{\int_{t}^{t+\Delta t} \sum_{j}^{}\sum_{f}^{} \mathbf{x}_j\delta(t-t_j^{(f)})\mathrm{d}t}{\int_{t}^{t+\Delta t} \sum_{j} \sum_{f}^{}\delta(t-t_j^{(f)}) \mathrm{d}t}\]</span> (7.52)</p><h3 id="candidate-temporal-codes">Candidate temporal codes</h3><h4 id="time-to-first-spike-latency-code">Time-to-first-spike: Latencycode</h4><p>Take saccading as an example. After each saccade, the photo receptorsin the retina receive a new visual input. Information about the onset ofa saccades should easily be available in the brain and could serve as aninternal reference signal.</p><p>Experimental evidences indicate that a coding scheme based on thelatency of the first spike transmit a large amount of information.</p><h4 id="phase">Phase</h4><p>We can apply a code by 'time ot first spike' also in the situationwhere the reference signal is not a single event, but a periodic signal.Oscillations of some global variable (for example the populationactivity) are common in the hippocampus, the olfactory system, and otherareas of the brain. These oscillations could serve as an internalreference signal.</p><h4 id="correlations-and-synchrony">Correlations and Synchrony</h4><p>We can also use spikes from other neurons as the reference signal fora spike code. Neurons which represent the same object in a complex sceneconsisting of several objects could be 'labeled' by the fact that theyfire synchronously.</p><p>Not only synchrony but any precise spatio-temporal pulse patterncould be a meaningful event.</p><h4 id="stimulus-reconstruction-and-reverse-correlation">StimulusReconstruction and Reverse Correlation</h4><p><strong>Reverse correlation</strong>: Every time a spike occurs, wenote the time course of the stimulus in a time window of about 100milliseconds immediately before the spike. Averaging the results overseveral spikes yields the typical time course of the stimulus justbefore a spike.</p><h4 id="rate-versus-temporal-codes">Rate versus temporal codes</h4><p>The stimulus reconstruction with kernels can also be considered as arate code based on spike counts. To see this, consider a spike countmeasure with a running time window <spanclass="math inline">\(K(.)\)</span>. We can estimate the rate <spanclass="math inline">\(\nu\)</span> at time <spanclass="math inline">\(t\)</span> by <span class="math display">\[    \nu(t)=\frac{\int_{-\infty}^{\infty} K(\tau)S(t-\tau)\mathrm{d}\tau}{\int_{-\infty}^{\infty} K(\tau) \mathrm{d}\tau}\tag{7.54}\]</span> where <span class="math inline">\(S(t)=\sum_{f=1}^{n}\delta(t-t^{(f)})\)</span> is the spike train. For a rectangular timewindow <span class="math inline">\(K(\tau)=1\)</span> for <spanclass="math inline">\(-T/2&lt;\tau&lt;T/2\)</span> and zero otherwise,reduces exactly to (7.1). Perform the integration over the <spanclass="math inline">\(\delta\)</span> function and we yield <spanclass="math display">\[    \nu(t)=c \sum_{f=1}^{n} K(t-t^{(f)}) \tag{7.55}\]</span> where <span class="math inline">\(c=[\int_{}^{} K(s)\mathrm{d}s]^{-1}\)</span> is a constant.</p><h4 id="example-towards-a-definition-of-rate-codes">Example: Towards adefinition of rate codes</h4><p>We have seen in (7.55) that stimulus reconstruction with a linearkernel can be seen as a special instance of a rate code. This suggests aformal definition of a rate code via the reconstruction procedure: ifall information contained in a spike train can be recovered by thelinear reconstruction procedure of (7.53), the the neuron is using arate code.</p><h2 id="exercises">Exercises</h2><ol type="1"><li><strong>Poisson process in continuous time</strong>. We consider aPoisson neuron model. In every small time interval <spanclass="math inline">\(\Delta t\)</span>, the probability that the neuronfires is given by <span class="math inline">\(\nu \Deltat\)</span>.</li></ol><ol type="a"><li>The probability that the neuron does not fire during a time ofarbitrarily large length <span class="math inline">\(t\)</span>(survivor function <span class="math inline">\(S_0(t)\)</span>) is <spanclass="math display">\[S_0(t)=\mathrm{e}^{-\nu t}.\]</span></li><li>Suppose that the neuron has fired at time <spanclass="math inline">\(t=0\)</span>, then the distribution of intervals<span class="math inline">\(P(t)\)</span>, i.e., the probability densitythat the neuron fires its next spike at a time <spanclass="math inline">\(t\)</span>, is <span class="math display">\[P_0(s)=\nu \mathrm{e}^{-\nu t}.\]</span></li></ol><ol start="2" type="1"><li><strong>Autocorrelation of a Poisson process</strong>. Find theautocorrelation function <span class="math inline">\(C_0(s)=\langleS_i(t)S_i(t+s)\rangle _{t}\)</span> of the homogeneous Poisson processin continuous time.</li><li><strong>Repeatability and random coincidences</strong>. Whatpercentage of spikes coincide between two trials of a Poisson neuronwith arbitrary rate <span class="math inline">\(\nu_0\)</span> under theassumption that trials are sufficiently long?</li><li><strong>Spike count and Fano Factor</strong>. A homogeneous Poissonprocess has a probability to fire in a very small interval <spanclass="math inline">\(\Delta t\)</span> equal to <spanclass="math inline">\(\nu \Delta t\)</span>.</li></ol><ol type="a"><li>The probability to observe exactly <spanclass="math inline">\(k\)</span> spikes in the time interval <spanclass="math inline">\(T\)</span> is <spanclass="math inline">\(P_{k}(T)=[1/k!](\nu T)^{k}\exp (-\nuT)\)</span>.</li><li>For the inhomogeneous Poisson process the mean spike count in aninterval of duration <span class="math inline">\(T\)</span> is <spanclass="math inline">\(\langle k \rangle=\int_{0}^{T} \nu(t)\mathrm{d}t\)</span>.</li><li>Calculate the variance of the spike count and the Fano factor forthe inhomogeneous Poisson process.</li></ol><ol start="5" type="1"><li><strong>From interval distribution to hazard</strong>. Duringstimulation with a stationary stimulus, interspike intervals in a longspike train are found to be independent and given by the distribution<span class="math display">\[P(t|t&#39;)=\frac{(t-t&#39;)}{\tau^{2}}\exp\left(-\frac{t-t&#39;}{\tau}\right)\]</span> for <span class="math inline">\(t&gt;t&#39;\)</span>.</li></ol><ol type="a"><li>Calculate the survivor function <spanclass="math inline">\(S(t|t&#39;)\)</span>, the hazard function <spanclass="math inline">\(\rho(t|t&#39;)\)</span>. <spanclass="math display">\[S(t|t&#39;)=\left(\frac{t-t&#39;}{\tau}+1\right)\exp\left(-\frac{t-t&#39;}{\tau}\right)\]</span> <span class="math display">\[\rho(t|t&#39;)=\frac{t-t&#39;}{\tau(t-t&#39;)+\tau^{2}}\]</span></li><li>A spike train starts at time <span class="math inline">\(0\)</span>and we observed a first spike at time <spanclass="math inline">\(t_1\)</span>. Calculate the probability density<span class="math inline">\(P(t_{n}|t_1)\)</span> that the <spanclass="math inline">\(n\)</span>th spike occurs around <spanclass="math inline">\(t_n\)</span>. (self-convolution for <spanclass="math inline">\(n-2\)</span> times)</li></ol><ol start="6" type="1"><li><strong>Gamma-distribution</strong> Stationary intervaldistributions can often be fitted by a Gamma distrubution (for <spanclass="math inline">\(s&gt;0\)</span>) <span class="math display">\[P(s)=\frac{1}{\Gamma(k)}\frac{s^{k-1}}{\tau^{k}}\mathrm{e}^{-s/\tau}\]</span></li></ol><ol type="a"><li>Assume that intervals are independent and calculate the powerspectrum.</li><li>Calculate the coefficient of variation <spanclass="math inline">\(C_{V}\)</span> <span class="math display">\[C_{V}=\frac{1}{k}\]</span></li></ol><ol start="7" type="1"><li><strong>Poisson with dead time as a renewal process</strong>.Consider a process where spikes are generated with rate <spanclass="math inline">\(\rho_0\)</span>, but after each spike there is adead time of duration <span class="math inline">\(\Delta^{abs}\)</span>.More precisely, we have a renewal process <span class="math display">\[\rho(t|\hat{t})=\rho_0 \quad \text{for}\ t&gt;\hat{t}+\Delta^{abs}\]</span> and zero otherwise. Calculate the interval distribution andthe Fano factor. <span class="math display">\[P(t|\hat{t})=\begin{cases}     0, \hat{t}&lt;t&lt;\hat{t}+\Delta^{abs} \\     \exp (-t-\hat{t}-\Delta^{abs}), t&gt;\hat{t}+\Delta^{abs}\end{cases}\]</span></li></ol>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Information and Entropy (3)</title>
    <link href="/2022/07/24/Information-and-Entropy-3/"/>
    <url>/2022/07/24/Information-and-Entropy-3/</url>
    
    <content type="html"><![CDATA[<h1 id="principle-of-maximum-entropy">Principle of Maximum Entropy</h1><h2 id="problem-setup">Problem Setup</h2><p>The probelm domain needs to be setup. We'll apply the general case totwo examples, one a business model, and the other a model of physicalsystem.</p><h3 id="bergers-burgers">Berger's Burgers</h3><p>The menu has been extended to include a gourmet low-fat tofumeal.</p><h3 id="magnetic-dipole-model">Magnetic Dipole Model</h3><p>Consider a system containing one dipole and a two-sideenvironment</p><h2 id="probabilities">Probabilities</h2><p>We assume that each of the possible states <spanclass="math inline">\(A_i\)</span> has some probability of occupancy<span class="math inline">\(p(A_i)\)</span> where <spanclass="math inline">\(i\)</span> is an index running over the possiblestates.</p><p><span class="math display">\[    1=\sum_{i}^{} p(A_i) \tag{9.1}\]</span></p><p>Probability, and all quantities that are based on probabilities, aresubjective, or observer-dependent.</p><h2 id="entropy">Entropy</h2><p><span class="math display">\[    S=\sum_{i}^{} p(A_i)\log _{2}\left(\frac{1}{p(A_i)}\right)\]</span> (9.2)</p><p>Information is measured in bits.</p><h2 id="constraints">Constraints</h2><p>We consider only one constraint here. If there is a quantity <spanclass="math inline">\(G\)</span> for which each of the states has avalue <span class="math inline">\(g(A_i)\)</span> then we want toconsider only those probability distributions for which the expectedvalue is a known value <spanclass="math inline">\(\widetilde{G}\)</span> <spanclass="math display">\[    \widetilde{G}=\sum_{i}^{} p(A_i)g(A_i) \tag{9.4}\]</span></p><h2 id="maximum-entropy-single-constraint">Maximum Entropy, SingleConstraint</h2><h3 id="probability-formula">Probability Formula</h3><p>The probability distribution <spanclass="math inline">\(p(A_i)\)</span> we want has been derived byothers. It is a function of the dual variable <spanclass="math inline">\(\beta\)</span>: <span class="math display">\[    p(A_i)=2^{-\alpha}2^{-\beta g(A_i)}\]</span> (9.12)</p><p>which implies <span class="math display">\[    \log _{2}\left(\frac{1}{p(A_i)}\right)=\alpha+\beta g(A_i)\]</span> (9.13)</p><p>where <span class="math display">\[    \alpha=\log _{2}\left(\sum_{i}^{} 2^{-\beta g(A_i)}\right)\]</span> (9.14)</p><p><span class="math display">\[    S=\alpha+\beta G \tag{9.15}\]</span> where <span class="math inline">\(S\)</span>,<spanclass="math inline">\(\alpha\)</span>, and <spanclass="math inline">\(G\)</span> are all functions of <spanclass="math inline">\(\beta\)</span>.</p><h3 id="evaluating-the-dual-variable">Evaluating the Dual Variable</h3><p><span class="math display">\[    0=\sum_{i}^{} [g(A_i)-G(\beta)]2^{-\beta g(A_i)}\]</span> (9.21)</p><p>If this equation is multiplied by <spanclass="math inline">\(2^{\beta G(\beta)}\)</span>, the result is <spanclass="math display">\[    0=f(\beta)\]</span> where the function <spanclass="math inline">\(f(\beta)\)</span> is <span class="math display">\[    f(\beta)=\sum_{i}^{} [g(A_i)-G(\beta)]2^{-\beta[g(A_i)-G(\beta)]}\]</span> (9.23)</p><p>It is not difficult to show that <spanclass="math inline">\(f(\beta)\)</span> is a monotonic function of <spanclass="math inline">\(\beta\)</span> since <spanclass="math inline">\(G(\beta)\)</span> is a monotonic function of <spanclass="math inline">\(\beta\)</span>.</p><h3 id="examples">Examples</h3><p>We only focus on the magnetic dipole example. <spanclass="math display">\[    1=p(U)+p(D)\]</span> (9.33)</p><p><span class="math display">\[    \begin{aligned}        \widetilde{E} &amp;= e(U)p(U)+e(D)p(D) \\        &amp;= m_{d}H[p(U)-p(D)] \\    \end{aligned}\]</span> (9.34)</p><p><span class="math display">\[    S=p(U)\log _{2}\left(\frac{1}{p(A)}\right)+p(D)\log_{2}\left(\frac{1}{p(D)}\right)\]</span> (9.35)</p><p>The entropy is the largest, for the energy <spanclass="math inline">\(\widetilde{E}\)</span> and magnetic field <spanclass="math inline">\(H\)</span>, if</p><p><span class="math display">\[    p(U)=2^{-\alpha}2^{-\beta m_{d}H}       \]</span> (9.36)</p><p><span class="math display">\[    p(D)=2^{-\alpha}2^{\beta m_{d}H}\]</span> (9.37)</p><p>where <span class="math display">\[    \alpha=\log _{2}(2^{-\beta m_{d}H}+2^{\beta m_{d}H})\]</span> (9.38)</p><p>and an according <span class="math inline">\(f(\beta)\)</span>.</p><h1 id="energy">Energy</h1><h2 id="principle-of-maximum-entrophy-for-physical-systems">Principle ofMaximum Entrophy for Physical Systems</h2><p>So far we have <span class="math display">\[    1=\sum_{i}^{} p_i\]</span> <span class="math display">\[    E=\sum_{i}^{} p_i E_i\]</span> <span class="math display">\[    S=k_B \sum_{i}^{} p_i \ln \left(\frac{1}{p_i}\right)\]</span> <span class="math display">\[    p_i=\mathrm{e}^{-\alpha} \mathrm{e}^{-\beta E_i}\]</span> <span class="math display">\[    \alpha=\ln \left(\sum_{i}^{} \mathrm{e}^{-\beta E_i}\right)=\frac{S}{k_B}-\beta E\]</span></p><h3 id="differential-forms">Differential Forms</h3><p>Suppose <span class="math inline">\(E_i\)</span> does not depend onan external parameter. Take differentiation. <spanclass="math display">\[    0=\sum_{i}^{} \mathrm{d}p_i\]</span> <span class="math display">\[    \mathrm{d}E=\sum_{i}^{} E_i \mathrm{d}p_i\]</span> <span class="math display">\[    \mathrm{d}S=k_B \beta \mathrm{d}E   \]</span> <span class="math display">\[    \mathrm{d}\alpha=-E \mathrm{d}\beta\]</span> <span class="math display">\[    \mathrm{d}p_i=-p_i(E_i-E)\mathrm{d}\beta\]</span> from which it is not difficult to show <spanclass="math display">\[    \mathrm{d}E=-\left(\sum_{i}^{} p_i(E_i-E)^{2}\right)\mathrm{d}\beta\]</span> <span class="math display">\[    \mathrm{d}S=-k_B \beta\left(\sum_{i}^{} p_i(E_i-E)^{2}\right)\mathrm{d}\beta\]</span> Note that the formula relating <spanclass="math inline">\(\mathrm{d}E\)</span> and <spanclass="math inline">\(\mathrm{d}\beta\)</span>, implies that if <spanclass="math inline">\(E\)</span> goes up then <spanclass="math inline">\(\beta\)</span> goes down, and vice versa.</p><h3 id="differential-forms-with-external-parameters">Differential Formswith External Parameters</h3><p>Each <span class="math inline">\(E_i\)</span> could be written in theform <span class="math inline">\(E_i(H)\)</span>. <spanclass="math display">\[    E=\sum_{i}^{} p_i E_i(H) \tag{11.20}\]</span> So we have <span class="math display">\[    0=\sum_{i}^{} \mathrm{d}p_i\]</span> (11.21) <span class="math display">\[    \mathrm{d}E=\sum_{i}^{} E_i(H)\mathrm{d}p_i+\sum_{i}^{} p_i\mathrm{d}E_i(H)\]</span> (11.22) <span class="math display">\[    \mathrm{d}S=k_B \beta \mathrm{d}E-k_B \beta \sum_{i}^{} p_i\mathrm{d}E_i(H)\]</span> (11.23) <span class="math display">\[    \mathrm{d}\alpha=-E\mathrm{d}\beta-\beta \sum_{i}^{} p_i\mathrm{d}E_i(H)\]</span> (11.24) <span class="math display">\[    \mathrm{d}p_i=-p_i(E(H)-E)\mathrm{d}\beta-p_i \beta\mathrm{d}E_i(H)+p_i \beta \sum_{j}^{} p_j \mathrm{d}E_j(H)\]</span> (11.25) <span class="math display">\[    \mathrm{d}E=-\left[\sum_{i}^{} p_i(E_i(H)-E)^{2}\right]\mathrm{d}\beta+\sum_{i}^{} p_i(1-\beta(E_i(H)-E))\mathrm{d}E_i(H)\]</span> (11.26) <span class="math display">\[    \mathrm{d}S=-k_B \beta\left[\sum_{i}^{} p_i(E_i(H)-E)^{2}\right]\mathrm{d}\beta-k_B \beta^{2}\sum_{i}^{} p_i(E_i(H)-E)\mathrm{d}E_i(H)\]</span> (11.27)</p><h2 id="system-and-environment">System and Environment</h2><h3 id="partition-model">Partition Model</h3><p>Our model for the way the total combination is partitioned into thesystem and the environment. We will use index <spanclass="math inline">\(i\)</span> for the system and the index <spanclass="math inline">\(j\)</span> for the environment.</p><p>Whether system and environment are isolated or interacting does notaffect the states or the physical properties associated with the states,although it may affect the probability of occupancy of the states.</p><h3 id="interaction-model">Interaction Model</h3><p>Consider two adjacent dipoles that exchange their orientations  theone on the left ends up with the orientation that the one on the rightstarted with, and vice versa. There are only a few cases.</p><p>First, if the two dipoles started with the same orientation, nothingwould change. On the other hand,if the two dipoles started withdifferent orientations, the effect would be that the pattern oforientationshas changed. This has happened even though the dipolesthemselves have not moved. Since the energy associated with the twopossible alignments is different, there has been a small change in thelocation of the energy, even though the total energy is unchanged.</p><p>Second, if both dipoles are in the system, or both are in theenvironment, then energy may have shifted position within the system orthe environment, but has not moved between them.</p><p>Third, if the two dipoles started with different alignment, and theyare located one on each side of the boundary between the system and theenvironment, then energy has flowed from the system to the environmentor vice versa. This has happened not because the dipoles have moved, butbecause the orientations have moved.</p><h3 id="extensive-and-intensive-quantities">Extensive and IntensiveQuantities</h3><p>The energies of the system state and of the environment state add upto form the energy of the corresponding total state: <spanclass="math display">\[    E_{t,i,j}=E_{s,i}+E_{e,j} \tag{11.37}\]</span></p><p>The probability of occupancy of total state <spanclass="math inline">\(k\)</span> is the product of the two probabilitiesof the two associated states <span class="math inline">\(i\)</span> and<span class="math inline">\(j\)</span>: <span class="math display">\[    p_{t,i,j}=p_{s,i}p_{e,j}\]</span></p><p>The total energy is <span class="math display">\[    E_t=\sum_{i,j}^{} E_{t,i,j}p_{t,i,j}=\sum_{j}^{}E_{e,j}p_{e,j}+\sum_{i}^{} E_{s,i}p_{s,i}=E_e+E_s\]</span> (11.39)</p><p>This result holds whether the system and environment are isolated orinteracting. Similarly, <span class="math display">\[    S_t=\sum_{i,j}^{} p_{t,i,j}\ln\left(\frac{1}{p_{t,i,j}}\right)=\sum_{j}^{} p_{e,j}\ln\left(\frac{1}{p_{e,j}}\right)+\sum_{i}^{} p_{s,i} \ln\left(\frac{1}{p_{s,i}}\right)=S_{e}+S_{s}    \]</span> (11.40)</p><p>This kind of quantity with the property that its total value is thesum of the value for the two (or more) parts is known as an<strong>extensive quantity</strong>.</p><p>Not all quantities are extensive. In particular, <spanclass="math inline">\(\alpha\)</span> and <spanclass="math inline">\(\beta\)</span> are not. If the system andenvironment are isolated, then there is no reason why <spanclass="math inline">\(\beta_s\)</span> and <spanclass="math inline">\(\beta_e\)</span> would be related. However, if thesystem and environment are interacting, the distribution of energybetween the system and the environment may not be known and thereforethe Principle of Maximum Entropy can be applied only to thecombination.</p><p>Quantities like <span class="math inline">\(\beta\)</span> that arethe same throughout a system when analyzed as a whole are called<strong>intensive</strong>.</p><h3 id="equilibrium">Equilibrium</h3><p>After mixing the system and the environment, the total entrophyincreases. The energies of the system and the environment have changed,and as a result the values of <spanclass="math inline">\(\beta_s\)</span> and <spanclass="math inline">\(\beta_e\)</span> have changed, in oppositedirections. Their new values are the same (each is equal to <spanclass="math inline">\(\beta_t\)</span>), and therefore this new valuelies between the two old values.</p><h3 id="energy-flow-work-and-heat">Energy Flow, Work and Heat</h3><p>Energy can be transferred by heat and work at the same time. Work isrepresented by changes in the energy of the individual states <spanclass="math inline">\(\mathrm{d}E_i\)</span>, and heat by changes in theprobabilities <span class="math inline">\(p_i\)</span>. <spanclass="math display">\[    \mathrm{d}E=\sum_{i}^{} E_i(H)\mathrm{d}p_i+\sum_{i}^{} p_i\mathrm{d}E_i(H)\]</span> (11.52)</p><p>where the first term is heat and the second term is work.</p><h3 id="reversible-energy-flow">Reversible Energy Flow</h3><p><span class="math display">\[    \mathrm{d}S_{s}=k_{B} \beta_{s}\mathrm{d}E_s-k_{B}\beta_s\sum_{i}^{}p_{s,i}\mathrm{d}E_{s,i}(H)=k_{B}\beta_s\left[\mathrm{d}E_{s}-\sum_{i}^{}p_{s,i}\mathrm{d}E_{s,i}(H)\right]=k_{B}\beta_{s}\mathrm{d}q_{s}\]</span> (11.55)</p><p>where <span class="math inline">\(\mathrm{d}q_{s}\)</span> stands forthe heat that comes into the system due to the interactionmechanism.</p><p>This formula applies to the system and a similar formula applies tothe environment: <span class="math display">\[    \mathrm{d}S_{e}=k_{B}\beta_{e}\mathrm{d}q_{e} \tag{11.56}\]</span> The two heats are the same except for sign <spanclass="math display">\[    \mathrm{d}q_{s}=-\mathrm{d}q_{e} \tag{11.57}\]</span></p><p>and it therefore follows that the total entropy <spanclass="math inline">\(S_{s}+S_{e}\)</span> is unchanged (i.e., <spanclass="math inline">\(\mathrm{d}S_{s}=-\mathrm{d}S_{e}\)</span> ) if andonly if the two values of <span class="math inline">\(\beta\)</span> forthe system and environment are the same: <span class="math display">\[    \beta_s=\beta_e \tag{11.58}\]</span></p><p>Reversible changes (with no changes in total entrophy) can involvework and heat and therefore changes in energy and entrophy for thesystem, but the system and the environment must have the same value of<span class="math inline">\(\beta\)</span>. Otherwise, the changes areirreversible. Also, reversible changes result in no change to <spanclass="math inline">\(\beta\)</span>.</p><p>The first-order change formulas given earlier can be written toaccount for reversible interactions with the environment by simplysetting <span class="math inline">\(\mathrm{d}\beta=0\)</span> <spanclass="math display">\[    0=\sum_{i}^{} \mathrm{d}p_i\]</span> (11.59) <span class="math display">\[    \mathrm{d}E=\sum_{i}^{} E_i(H)\mathrm{d}p_i+\sum_{i}^{} p_i\mathrm{d}E_i(H)\]</span> (11.60) <span class="math display">\[    \mathrm{d}S=k_{B}\beta \mathrm{d}E-k_{B}\beta \sum_{i}^{} p_i\mathrm{d}E_i(H)\]</span> (11.61) <span class="math display">\[    \mathrm{d}\alpha=-\beta \sum_{i}^{} p_i \mathrm{d}E_i(H)\]</span> (11.62) <span class="math display">\[    \mathrm{d}p_i=-p_i \beta \mathrm{d}E_i(H)+p_i \beta \sum_{j}^{} p_j\mathrm{d}E_j(H)\]</span> (11.63) <span class="math display">\[    \mathrm{d}E=\sum_{i}^{} p_i(1-\beta(E_i(H)-E))\mathrm{d}E_i(H)\]</span> (11.64) <span class="math display">\[    \mathrm{d}S=-k_{B} \beta^{2}\sum_{i}^{}p_i(E_i(H)-E)\mathrm{d}E_i(H)\]</span> (11.65)</p><h1 id="temperature">Temperature</h1><p>In this chapter we will interpret <spanclass="math inline">\(\beta\)</span> further, and will define itsreciprocal as the temperature of the material.</p><h2 id="temperature-scales">Temperature Scales</h2><p>Recall in Chapter 11 <span class="math display">\[    \frac{\mathrm{d}E}{\mathrm{d}S}=\frac{1}{k_{B}\beta}\]</span></p><p>Define the "absolute temperature" as <span class="math display">\[    T=\frac{1}{k_{B}\beta}\]</span></p><h2 id="heat-engine">Heat Engine</h2><p>The change in energy can be attributed to the effects of work <spanclass="math inline">\(\mathrm{d}w\)</span> and heat <spanclass="math inline">\(\mathrm{d}q\)</span> <span class="math display">\[    \mathrm{d}w=\left(\frac{E}{H}\right) \mathrm{d}H\]</span> <span class="math display">\[    \mathrm{d}q=\sum_{i}^{} E_i(H) \mathrm{d}p_i =T \mathrm{d}S\]</span></p><h1 id="quantum-information">Quantum Information</h1><h2 id="quantum-information-storage">Quantum Information Storage</h2><h3 id="model-1-tiny-classical-bits">Model 1: Tiny Classical Bits</h3><p>Like the magnetic dipole model, this model of the quantum bit behavesessentially like a classical bit except that its size may be very smalland it mamy be able to be measured rapidly.</p><h3 id="model-2-superposition-of-states-the-qubit">Model 2:Superposition of States (the Qubit)</h3><p><span class="math display">\[    \psi=\alpha \psi_0+\beta \psi_1\]</span> When a measurement is made, the values of <spanclass="math inline">\(\alpha\)</span> and <spanclass="math inline">\(\beta\)</span> change so that one of them is <spanclass="math inline">\(1\)</span> and the other is <spanclass="math inline">\(0\)</span>.</p><h3 id="model-3-multiple-qubits-with-entanglement">Model 3: MultipleQubits with Entanglement</h3><p><span class="math display">\[    \psi=\alpha_{00} \psi_{00}+\alpha_{01}\psi_{01}+\alpha_{10}\psi_{10}+\alpha_{11} \psi_{11}\]</span></p><p>This can be thought as two qubits, one corresponding to each of thetwo measurements. These qubits are not independent, but rather are<strong>entangled</strong> in some way. A measurement of, for example,the first qubit will return <span class="math inline">\(0\)</span> withprobability <span class="math inline">\(\lvert \alpha_{00} \rvert^{2}+\lvert \alpha_{01} \rvert ^{2}\)</span> and if it does the wavefuntion collapses to only those stationary states that are consistentwith this measured value <span class="math display">\[    \psi=\frac{\alpha_{00}\psi_{00}+\alpha_{01} \psi_{01}}{\sqrt{\lvert\alpha_{00} \rvert ^{2}+\lvert \alpha_{01} \rvert ^{2}}}\]</span></p><h2 id="bracket-notation-for-qubits">Bracket Notation for Qubits</h2><h3 id="kets-bras-brackets-and-operators">Kets, Bras, Brackets, andOperators</h3><p>A ket is a column vector composed of complex numbers.</p><p><span class="math display">\[    |k\rangle =    \begin{pmatrix}    k_1 \\    k_2 \\    \end{pmatrix}    =    \vec k\]</span></p><p>The two kets <span class="math inline">\(|0\rangle\)</span> and <spanclass="math inline">\(| 1 \rangle\)</span> are used to represent the twological states of qubits, and have a standard vector representation<span class="math display">\[    |0 \rangle =\begin{pmatrix}    1 \\    0 \\    \end{pmatrix}    \quad    |1 \rangle =    \begin{pmatrix}    0 \\    1 \\    \end{pmatrix}\]</span></p><p>The superposition of <span class="math inline">\(|0 \rangle\)</span>and <span class="math inline">\(|1 \rangle\)</span> can be written as<span class="math display">\[    |\psi\rangle =\alpha |0\rangle+\beta|1 \rangle=\begin{pmatrix}    \alpha \\    \beta    \end{pmatrix}\]</span></p><hr /><p>Bras are the Hermitian conjugates of kets. <spanclass="math display">\[    \langle \psi |=(|\psi\rangle)^{\dag}=(\alpha^{*} \ \beta^{*})\]</span> (<span class="math inline">\(^{*}\)</span>) is theconventional notation for the conjugate of a complex number.</p><hr /><p>The dot product is the product of a bra by a ket. It is calledbracket. <span class="math display">\[    \langle q | k \rangle =\sum_{j}^{} q_j^{*}k_j\]</span></p><p>We have the very important property of kets.</p><p><span class="math display">\[    \langle \psi | \psi \rangle =\lvert \alpha \rvert ^{2}+\lvert \beta\rvert ^{2}=1\]</span></p><p>The dot product can be used to compute the probability of a qubit ofbeing in either one of the possible states <spanclass="math inline">\(|0\rangle\)</span> and <spanclass="math inline">\(|1\rangle\)</span>. <span class="math display">\[    \operatorname{Pr}(|0\rangle)=\lvert \langle 0 | \psi\rangle \rvert^{2}=\lvert \alpha \rvert ^{2}   \]</span></p><hr /><p>Operators are objects that transform one ket <spanclass="math inline">\(|k\rangle\)</span> into another ket <spanclass="math inline">\(| q\rangle\)</span>. Operators are representedwith hats: <span class="math inline">\(\widehat{O}\)</span>. <spanclass="math display">\[    \widehat{O} | k\rangle =\begin{pmatrix}    o_{11} &amp; o_{12} \\    o_{21} &amp; o_{22} \\    \end{pmatrix}    \begin{pmatrix}    k_1 \\    k_2 \\    \end{pmatrix}    =    \begin{pmatrix}    q_1 \\    q_2 \\    \end{pmatrix}    =    |q\rangle\]</span></p><p>Operators act on bras in a similar manner <spanclass="math display">\[    \langle k| \hat{O}^{\dag}=\langle q |\]</span></p><p>All quantum operators must be unitary!</p><p>If we know the input and output kets, we have a easy way to constructan operator <span class="math inline">\(\widehat{O}\)</span> using theexterior product of a ket by a bra.</p><p><span class="math display">\[    \widehat{O} | k\rangle=(|q \rangle \langle k|)|k\rangle\]</span></p><h3 id="tensor-product-composite-systems">Tensor Product  CompositeSystems</h3><p>If we have two qubits <spanclass="math inline">\(|\psi\rangle\)</span> and <spanclass="math inline">\(|\phi \rangle\)</span>, the system composed bythese two qubits is represented by <spanclass="math inline">\(|\psi\rangle \otimes |\phi\rangle\)</span>.</p><p>The following four representations of the tensor product are madeequivalent <span class="math display">\[    |q_1 \rangle \otimes |q_2\rangle \equiv |q_1\rangle | q_2 \rangle\equiv |q_1, q_2 \rangle \equiv |q_1q_2\rangle\]</span></p><p>For <span class="math inline">\(n\)</span> qubits, <spanclass="math display">\[    |q_1 \rangle \otimes |q_2 \rangle \otimes \cdots \otimes |q_n\rangle= \bigotimes_{j=1}^{n}|q_j\rangle\]</span></p><p>The dual of a tensor product of kets is the tensor product of thecorresponding bras. <span class="math display">\[    (|q_1q_2\rangle)^{\dag}=(|q_1\rangle \otimes |q_2 \rangle)^{\dag}=\langle q_1 | \otimes \langle q_2|=\langle q_1q_2|\]</span></p><p>The result of the dot product of two composite systems is themultiplication of the individual dot products taken in order. <spanclass="math display">\[    \langle q_1q_2 | w_1w_2 \rangle=(\langle q_1 | \otimes \langleq_2|)(|w_1\rangle \otimes |w_2\rangle)=\langle q_1| w_1\rangle \otimes\langle q_2 | w_2 \rangle\]</span></p><h2 id="no-cloning-theorem">No Cloning Theorem</h2><p>Qubits cannot be cloned.</p><p>To show taht cloning is not possible, let us assume that an operator<span class="math inline">\(\widehat{C}\)</span> takes the informationof one qubit <span class="math inline">\(|\phi_1\rangle\)</span> andcopies it into another "blank" qubit, the result is a qubit <spanclass="math inline">\(|\psi_1\rangle\)</span> identical to <spanclass="math inline">\(|\phi_1\rangle\)</span>, and the original <spanclass="math inline">\(|\phi_1\rangle\)</span> is unmodified. <spanclass="math inline">\(\widehat{C}\)</span> must be unitary. Thus wedefine <span class="math inline">\(\widehat{C}\)</span> <spanclass="math display">\[    | Original \rangle \otimes | Blank \rangle\stackrel{\widehat{C}}{\longrightarrow} |Original \rangle \otimes  |clone \rangle\]</span></p><p>We are now ready to clone two arbitrary qubits <spanclass="math inline">\(|\phi_1\rangle\)</span> and $| _2 $ separately.<span class="math display">\[    \widehat{C} | \phi_1 \rangle | blank \rangle =| \phi_1 \rangle |\psi_1 \rangle\]</span> <span class="math display">\[    \widehat{C} | \phi_2 \rangle | blank \rangle =| \phi_2 \rangle |\psi_2 \rangle\]</span> where it is understood that $| _1 =| _1 $ and <spanclass="math inline">\(| \phi_2 \rangle =| \psi_2 \rangle\)</span>, andwe have given them different names to distinguish original fromcopy.</p><p>Since the cloning machine is unitary, it preserves the dot products,so we can compare the dot product before and after cloning <spanclass="math display">\[    \langle \phi_2 | \langle blank | | \phi_1 \rangle | blank \rangle =\langle \phi_2 | \langle \psi_2 | | \phi_1 \rangle | \psi_1 \rangle\]</span> therefore <span class="math display">\[    \langle \phi_2 | \phi_1  \rangle \langle blank | blank \rangle=\langle \phi_2 | \phi_1 \rangle \langle \psi_2 | \psi_1 \rangle\]</span> The requirements that kets be normalized imposes that <spanclass="math inline">\(\langle blank | blank \rangle =1\)</span>. theabove equation can only be true in two cases: - <spanclass="math inline">\(\langle \phi_2 | \phi_1 \rangle =0\)</span>, whichmeans that $| _1 $ and $| _2 $ are orthogonal. This means that we canclone states chosen at random from a set of orthogonal states. And isequivalent to say that we can clone $| 0 $ and $| 1 $, which we alreadyknew since we do that classically all the time. - <spanclass="math inline">\(\langle \psi_2 | \psi_2 \rangle =1\)</span>, whichmeans that <span class="math inline">\(\psi_2=\psi_1\)</span>, that is,that clones obtained in each operation are identical. If the twooriginals were different, as we had assumed, what this result says isthat the clone is independent from the original, which is quite abizarre property for a clone!</p><h2 id="representation-of-qubits">Representation of Qubits</h2><h3 id="qubits-in-the-bloch-sphere">Qubits in the Bloch sphere</h3><p><span class="math display">\[    | \psi \rangle =\mathrm{e}^{ia} \left( \cos \frac{\theta}{2}| 0\rangle +\sin \frac{\theta}{2} \mathrm{e}^{i\varphi} | 1 \rangle \right)\]</span></p><p>If we ignore the global phase factor $^{ia} $, the two angles <spanclass="math inline">\(\theta\)</span> and <spanclass="math inline">\(\varphi\)</span> define a point in a unit sphere.This sphere is called the Bloch Sphere.</p><h3 id="qubits-and-symmetries">Qubits and symmetries</h3><p>Pauli matrices: <span class="math display">\[    \mathbb{I}=    \begin{pmatrix}    1 &amp; 0 \\    0 &amp; 1 \\    \end{pmatrix}    \quad    \sigma_{x}=    \begin{pmatrix}    0 &amp; 1 \\    1 &amp; 0 \\    \end{pmatrix}    \quad    \sigma_y=    \begin{pmatrix}    0 &amp; -i \\    i &amp; 0 \\    \end{pmatrix}       \quad    \sigma_{z}=    \begin{pmatrix}    1 &amp; 0 \\    0 &amp; -1 \\    \end{pmatrix}\]</span></p><h4 id="action-of-pauli-matrices-on-an-arbitrary-qubit">Action of Paulimatrices on an arbitrary qubit</h4><p>An arbitrary superposition state <span class="math display">\[    | \psi \rangle =\alpha| 0 \rangle +\beta | 1 \rangle =\cos\frac{\theta}{2} | 0 \rangle +\sin \frac{\theta}{2}\mathrm{e}^{i\varphi} | 1 \rangle\]</span></p><p>So <span class="math display">\[    \sigma_x | \psi \rangle =    \begin{pmatrix}    \beta \\    \alpha \\    \end{pmatrix}\]</span> (Rotation of <span class="math inline">\(\pi\)</span> about<span class="math inline">\(x\)</span> axis)</p><p><span class="math display">\[    \sigma_y | \psi \rangle =i    \begin{pmatrix}    -\beta \\    \alpha \\    \end{pmatrix}    \]</span> (Rotation of <span class="math inline">\(\pi\)</span> about<span class="math inline">\(y\)</span> axis)</p><p><span class="math display">\[    \sigma_z | \psi \rangle =    \begin{pmatrix}    \alpha \\    -\beta \\    \end{pmatrix}    \]</span> (Rotation of <span class="math inline">\(\pi\)</span> about<span class="math inline">\(z\)</span> axis)</p><p>Hence Pauli matrices are rotations of <spanclass="math inline">\(\pi\)</span> about each of the axis of the blochsphere.</p><p><span class="math display">\[    \mathrm{e}^{i \sigma_x \theta/2} =\cos \frac{\theta}{2} \mathbb{I}+i\sin \frac{\theta}{2}\sigma_x\]</span></p><p>This result shows us how to do arbitrary rotations of an angle <spanclass="math inline">\(\theta\)</span> about the <spanclass="math inline">\(x\)</span> axis, the resulting operator is oftencalled $R_x()=^{i_x /2} $. The cases of <spanclass="math inline">\(R_y\)</span> and <spanclass="math inline">\(R_z\)</span> are completely analogous.</p><h3 id="quantum-gates">Quantum Gates</h3><h4 id="elementary-quantum-gates">Elementary Quantum Gates</h4><ul><li>Pauli <span class="math inline">\(X\)</span>: <spanclass="math inline">\(X=\begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \\\end{pmatrix}\equiv \sigma_x\)</span>. It is equivalent to doing a <spanclass="math inline">\(NOT\)</span> or bit flip.</li><li>Pauli <span class="math inline">\(Y\)</span>: <spanclass="math inline">\(Y=\begin{pmatrix} 0 &amp; -i \\ i &amp; 0 \\\end{pmatrix}\equiv \sigma_y\)</span>.</li><li>Pauli <span class="math inline">\(Z\)</span>: <spanclass="math inline">\(Z=\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; -1 \\\end{pmatrix}\equiv \sigma_z\)</span>. Changes the internal phase.</li><li>Hadamard: <spanclass="math inline">\(H=\frac{1}{\sqrt{2}}\begin{pmatrix} 1 &amp; 1 \\ 1&amp; 1 \\ \end{pmatrix}\)</span>.</li><li>Phase: <span class="math inline">\(S=\begin{pmatrix} 1 &amp; 0 \\ 0&amp; i \\ \end{pmatrix}\)</span>.</li></ul><p>Below we enumerates some of the properties of the elementary quantumgates.</p><p><span class="math display">\[    H=\frac{1}{\sqrt{2}}(X+Z) \quad HXH=Z\]</span> <span class="math display">\[    XYX=-Y \quad HYH=-Y\]</span> <span class="math display">\[    XZX=-Z \quad HZH=X\]</span> <span class="math display">\[    XR_{y}(\theta) X=R_y(-\theta) \quad XR_z(\theta)X=R_y(-\theta)\]</span></p><h4 id="two-qubit-gates.-controlled-gates">Two-qubit gates. ControlledGates</h4><p>Operators are unitary and square, so quantum gates will always havethe same number of inputs and outputs. Another way to say it is that allquantum gates are naturally reversible.</p><p>The most important two qubit gates are the controlled gates. In acontrolled gate the first input qubit is a control qubit. If is in the$| 1 $ state, it will trigger the gate that acts on the second qubit,otherwise, it will not trigger it and the second qubit will remainunaltered. Say, a control qubit <span class="math inline">\(| \psi\rangle =\begin{pmatrix} \alpha \\ \beta \\ \end{pmatrix}\)</span> withthe second qubit $| $ and the gate <spanclass="math inline">\(U\)</span> results in $| +U | $.</p><p>There are two controlled gates that are very relevant to thealgorithms we will describe later on, the <spanclass="math inline">\(C-X\)</span> also known as <spanclass="math inline">\(C-NOT\)</span> and the <spanclass="math inline">\(C-Z\)</span> also known as <spanclass="math inline">\(C-Phase\)</span>.</p><h2 id="quantum-communication">Quantum Communication</h2><h3 id="teleportation---alice-and-bobs-story">Teleportation - Alice andBob's story</h3><p>Alice and Bob entangled a pair of qubits $| _{AB} $ when they firstmet, <span class="math display">\[    | \phi_{AB} \rangle =\frac{1}{\sqrt{2}} (| 0_{A} \rangle \otimes  |0_{B} \rangle +| 1_{A} \rangle \otimes | 1_{B} \rangle )\]</span></p><p>Life took each of them through separate paths. However, Alice decidesto send Bob a letter in a qubit $| <em>{L} =| 0</em>{L} +| 1_{L} $.</p><p>Alice puts the qubit of the pair she once entangled with Bob in acomposite system with $| _{L} $. The complete three-qubit system can berepresented using tensor products <span class="math display">\[    | \phi_{A}\psi_{L}\phi_{B} \rangle =\frac{1}{\sqrt{2}}\biggl( |0_{A} \rangle  \otimes (\alpha | 0_{L} \rangle +\beta | 1_{L}\rangle )\otimes  | 0_{B} \rangle + | 1_{A} \rangle \otimes (\alpha | 0_{L}\rangle +\beta | 1_{L} \rangle ) \otimes | 1_{B} \rangle \biggr)\]</span></p><p>In practice what the <span class="math inline">\(C-NOT\)</span> doesis transfer the superposition to Alice's Qubit <spanclass="math display">\[    \begin{aligned}        &amp;= \frac{1}{\sqrt{2}}\alpha\biggl( | 0_{A} \rangle \otimes |0_{B} \rangle + | 1_{A} \rangle  \otimes | 1_{B} \rangle \biggr) \otimes| 0_{L} \rangle  \\        &amp;+ \frac{1}{\sqrt{2}}\beta\biggl( | 1_{A} \rangle \otimes |0_{B} \rangle +| 0_{A} \rangle \otimes | 1_{B} \rangle \biggr) \otimes |1_{L} \rangle      \end{aligned}\]</span></p><p>At this point Alice's and Bob's qubit have both the information ofthe superposition that was originally in the letter. The Hadamard gateproduces a new superposition out of the letter as follows <spanclass="math display">\[    \begin{aligned}        &amp;= \frac{1}{\sqrt{2}}\alpha\biggl( | 0_{A} \rangle \otimes |0_{B} \rangle + | 1_{A} \rangle  \otimes | 1_{B} \rangle \biggr) \otimes\frac{1}{\sqrt{2}}(| 0_{L} \rangle+| 1_{L} \rangle )  \\        &amp;+ \frac{1}{\sqrt{2}}\beta\biggl( | 1_{A} \rangle \otimes |0_{B} \rangle +| 0_{A} \rangle \otimes | 1_{B} \rangle \biggr) \otimes\frac{1}{\sqrt{2}}(| 0_{L} \rangle-| 1_{L} \rangle )      \end{aligned}\]</span></p><p>At this point the information about the superposition in the originalletter is no longer in Alice's hands. To appreciate that it is so, weneed to make some manipulations and reordering of the corss products.<span class="math display">\[    \begin{aligned}        &amp;= \frac{1}{2}| 0_{A} \rangle \otimes | 0_{L} \rangle\otimes (\alpha | 0_{B} \rangle +\beta | 1_{B} \rangle ) \\        &amp;+ \frac{1}{2}| 0_{A} \rangle \otimes | 1_{L} \rangle\otimes (\alpha | 0_{B} \rangle -\beta | 1_{B} \rangle ) \\        &amp;+ \frac{1}{2}| 1_{A} \rangle \otimes | 0_{L} \rangle\otimes (\alpha | 1_{B} \rangle +\beta | 0_{B} \rangle ) \\        &amp;+ \frac{1}{2}| 1_{A} \rangle \otimes | 1_{L} \rangle\otimes (\alpha | 1_{B} \rangle -\beta | 0_{B} \rangle ) \\    \end{aligned}\]</span></p><p>The next steps are the key to faultless teleportation - Alicemeasures her two qubits, she will obtain either of $| 0_{A}0_{L} <spanclass="math inline">\(,\)</span>| 0_{A}1_{L} $, $| 1_{A}0_{L} $, or $|1_{A}1_{L} $ with equal probability. - Upon Alice's measurement, Bob'squbit takes the value of one of the four possible superpositions. Andso, the result of her measurement can help Bob unscramble his bit. - Ifshe measured $| 0_{A}0_{L} $, she will tell Bob not to do anything tohis qubit. If she measured $| 0_{A}1_{L} $, Bob will have to correct forthe phase (that can be done with a <spanclass="math inline">\(Z\)</span> gate). If she measured $| 1_{A}0_{L} $,the states in the message have been flipped, and to unflip them Bob willhave to use a bit-flip (a.k.a not, a.k.a <spanclass="math inline">\(X\)</span>) gate. Finally if she measured $|1_{A}1_{L} $, Bob will have to correct both for the phase and the bitflip.</p><p>Alice needs to communicate to bob 2 classical bits. The first bit ofAlice informs about bit-flip errors and the second about phaseerrors.</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>11</title>
    <link href="/2022/07/19/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%8811%EF%BC%89/"/>
    <url>/2022/07/19/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%8811%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><p>~</p><h2 id="green-">Green </h2><h3 id=""></h3><p> <spanclass="math display">\[    \frac{\mathrm{d}}{\mathrm{d}x}\left[p(x)\frac{\mathrm{d}y(x)}{\mathrm{d}x}\right]+q(x)y(x)=0,\quad a&lt;x&lt;b.      \]</span>  <spanclass="math inline">\(p(x)\)</span><spanclass="math inline">\(p&#39;(x)\)</span>  <spanclass="math inline">\(q(x)\)</span>  <spanclass="math inline">\(a&lt;x&lt;b\)</span> <spanclass="math inline">\(p(x)\)</span> . <span class="math inline">\(y_1(x)\)</span>  <spanclass="math inline">\(y_2(x)\)</span>  <spanclass="math inline">\(a&lt;x&lt;b\)</span> .  <spanclass="math inline">\(\delta\)</span>  <spanclass="math display">\[    \frac{\mathrm{d}}{\mathrm{d}x}\left[p(x)\frac{\mathrm{d}g(x;t)}{\mathrm{d}x}\right]+q(x)g(x;t)=\delta(x-t),\quad a&lt;x&lt;b, a&lt;t&lt;b    \]</span>  <span class="math inline">\(x\neq t\)</span> <span class="math inline">\(y_1(x)\)</span>  <spanclass="math inline">\(y_2(x)\)</span>  <spanclass="math display">\[    \begin{aligned}        g(x;t)&amp;=        \begin{cases}        c_1(t)y_1(x)+c_2(t)y_2(x) \equiv g_{&lt;}, \quad x&lt;t \\        d_1(t)y_1(x)+d_2(t)y_2(x) \equiv g_{&gt;}, \quad x&gt;t        \end{cases} \\        &amp;= g_{&lt;}+(g_{&gt;}-g_{&lt;})\eta(x-t), \\    \end{aligned}\]</span></p><p> <spanclass="math inline">\(f(x)\)</span> <span class="math display">\[    \left[p&#39;(t)(g_{&gt;}(t)-g_{&lt;}(t))+2p(t) \left(\frac{\mathrm{d}g_{&gt;}}{\mathrm{d}x}-\frac{\mathrm{d}g_{&lt;}}{\mathrm{d}x}\right)_{x=t}-1\right]f(t)= p(t)(g_{&gt;}(t)-g_{&lt;}(t))f&#39;(t).\]</span>  <span class="math inline">\(f(x)\)</span> <span class="math inline">\(g(x;t)\)</span>  <spanclass="math inline">\(x=t\)</span>  <spanclass="math inline">\(\frac{\mathrm{d}g(x;t)}{\mathrm{d}x}\)</span> <span class="math inline">\(x=t\)</span>  <spanclass="math inline">\(\frac{1}{p(t)}\)</span> <spanclass="math display">\[    \frac{\mathrm{d}g(x;t)}{\mathrm{d}x}\bigg|^{x=t+}_{x=t-}=\frac{1}{p(t)}\]</span></p><h3 id="-green-"> Green</h3><p> <span class="math display">\[    \frac{\mathrm{d}^{2}g}{\mathrm{d}t^{2}}=\delta(t-\tau), \quadt&gt;0, \tau&gt;0,\]</span> <span class="math display">\[    g|_{t=0}=0, \quad \frac{\mathrm{d}g}{\mathrm{d}t}\bigg|_{t=0}=0\]</span>  <span class="math display">\[    g(t;\tau)=(t-\tau)\eta(t-\tau)+\alpha(\tau)t+\beta(\tau).\]</span></p><p> <span class="math display">\[    g(t;\tau)=(t-\tau)\eta(t-\tau).\]</span></p><hr /><p> <span class="math display">\[    \frac{\mathrm{d}^{2}y}{\mathrm{d}t^{2}}=f(t), \quad t&gt;0,\]</span> <span class="math display">\[    y(0)=0, \quad y&#39;(0)=0.\]</span>  <span class="math display">\[    y(t)=\int_{0}^{\infty} g(t;\tau)f(\tau) \mathrm{d}\tau=\int_{0}^{t}(t-\tau)f(\tau) \mathrm{d}\tau.\]</span></p><hr /><p> <span class="math display">\[    \frac{\mathrm{d}^{2}g(t;\tau)}{\mathrm{d}t^{2}}+k^{2}g(t;\tau)=\delta(t-\tau),\quad t&gt;0, \tau &gt;0,\]</span> <span class="math display">\[    g(0;\tau)=0, \quad\frac{\mathrm{d}g(t;\tau)}{\mathrm{d}t}\bigg|_{t=0}=0.\]</span></p><p> <span class="math display">\[    g(t;\tau)=\frac{1}{k} \sin k(t-\tau)\eta(t-r)+C(r)\sin kt+D(r)\coskt.\]</span></p><hr /><p> <span class="math display">\[    \frac{\mathrm{d}^{2}y}{\mathrm{d}t^{2}}+k^{2}y(t)=f(t), \quadt&gt;0,\]</span> <span class="math display">\[    y(0)=0, \quad y&#39;(0)=0\]</span>  <span class="math display">\[    y(t)=\frac{1}{k}\int_{0}^{t} f(\tau)\sin k(t-\tau) \mathrm{d}\tau.\]</span></p><hr /><p> <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}\left[ p(t)\frac{\mathrm{d}g(t;\tau)}{\mathrm{d}t}\right]+q(t)g(t;\tau)=\delta(t-\tau), \quad t&gt;0,\tau&gt;0;\]</span> <span class="math display">\[    g(0;\tau)=0, \quad\frac{\mathrm{d}g(t;\tau)}{\mathrm{d}t}\bigg|_{t=0}=0,\]</span>  <span class="math inline">\(p(t)\)</span>,<spanclass="math inline">\(p&#39;(t)\)</span>  <spanclass="math inline">\(q(t)\)</span>  <spanclass="math inline">\([0,\infty)\)</span>  <spanclass="math inline">\(p(t)\)</span> .</p><p> <span class="math inline">\(y_1(t)\)</span>  <spanclass="math inline">\(y_2(t)\)</span>. Wronsky  <spanclass="math display">\[    W[y_1(t),y_2(t)]\equiv    \begin{vmatrix}    y_1(t) &amp; y_2(t) \\    y_1&#39;(t) &amp; y_2&#39;(t) \\    \end{vmatrix}    \neq 0\]</span></p><p> <span class="math display">\[    g(t;\tau)=\frac{1}{p(\tau)}\frac{y_1(\tau)y_2(t)-y_2(\tau)y_1(t)}{W[y_1(\tau),y_2(\tau)]}\eta(t-\tau).\]</span>  <spanclass="math inline">\(t,\tau&gt;0\)</span>  <spanclass="math display">\[    p(t)=p(-t), \quad q(t)=q(-t),\]</span>  <span class="math display">\[    g(t;\tau)|_{t&lt;\tau}=0, \quad\frac{\mathrm{d}g(t;\tau)}{\mathrm{d}t}\bigg|_{t&lt;\tau}=0.\]</span></p><hr /><p> <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}\left[ p(t)\frac{\mathrm{d}y(t)}{\mathrm{d}t}\right]+q(t) y(t)=f(t), \quad t&gt;0,\]</span> <span class="math display">\[    g(0;\tau)=0, \quad y&#39;(0)=0\]</span>  <span class="math display">\[    y(t)=\int_{0}^{t} g(t;\tau)f(\tau) \mathrm{d}\tau.\]</span></p><hr /><p> <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}\left[ p(t)\frac{\mathrm{d}y(t)}{\mathrm{d}t}\right]+q(t) y(t)=f(t), \quad t&gt;0,\]</span> <span class="math display">\[    g(0;\tau)=A, \quad y&#39;(0)=B\]</span></p><p> <spanclass="math inline">\(g(t;\tau)=g(-\tau;-t)\)</span></p><p><span class="math display">\[    \begin{aligned}    y(t)&amp;=\int_{0}^{\infty} g(t;\tau)f(\tau) \mathrm{d}\tau +\left\{ p(r)\left[ y(\tau)\frac{\mathrm{d}g(t;\tau)}{\mathrm{d}\tau}-g(t;\tau)\frac{\mathrm{d}y(\tau)}{\mathrm{d}\tau}\right]\right\}^{\infty}_{\tau=0}         \\    &amp;=\int_{0}^{t} g(t;\tau) f(\tau) \mathrm{d}\tau-p(0)\left[ A\frac{\mathrm{d}g(t;\tau)}{\mathrm{d}\tau}-Bg(t;\tau)\right]_{\tau=0}    \end{aligned}\]</span></p><h3 id="-green-"> Green</h3><p> <span class="math display">\[    \frac{\mathrm{d}^{2}g(x;\xi)}{\mathrm{d}x^{2}}=\delta(x-\xi), \quada&lt;x,\xi&lt;b,\]</span> <span class="math display">\[    g(a,\xi)=0, \quad g(b;\xi)=0.\]</span></p><p> <span class="math display">\[    g(x;\xi)=(x-\xi)\eta(x-\xi)-\frac{b-\xi}{b-a}(x-a).\]</span></p><hr /><p> <span class="math display">\[    \frac{\mathrm{d}^{2}g(x;\xi)}{\mathrm{d}x^{2}}+k^{2}g(x;\xi)=\delta(x-\xi), \quad a&lt;x,\xi&lt;b,\]</span> <span class="math display">\[    g(a;\xi)=0, \quad g(b;\xi)=0.\]</span></p><p> <span class="math display">\[    g(x;\xi)=    \begin{cases}        \displaystyle -\frac{1}{k}\frac{\sin k(b-\xi)}{\sin k(b-a)}\sink(x-a), \quad a&lt;x&lt;\xi, \\        \displaystyle -\frac{1}{k}\frac{\sin k(\xi-a)}{\sin k(b-a)} \sink(b-x), \quad \xi&lt;x&lt;b.    \end{cases}\]</span></p><hr /><p></p><p><span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}x}\left[ p(x)\frac{\mathrm{d}g(x;\xi)}{\mathrm{d}x}\right]+q(x)g(x;\xi)=\delta(x-\xi), \quad a&lt;x,\xi&lt;b.\]</span> <span class="math display">\[    g(a;\xi)=0, \quad g(b;\xi)=0\]</span></p><p> <span class="math display">\[    \begin{aligned}    g(x;\xi)=&amp;-\frac{1}{p(\xi)}\frac{y_2(b)y_1(\xi)-y_1(b)y_2(\xi)}{y_1(b)y_2(a)-y_1(a)y_2(b)}\frac{y_2(a)y_1(x)-y_1(a)y_2(x)}{W[y_1(\xi),y_2(\xi)]}         \\    &amp;+\frac{1}{p(\xi)}\frac{y_1(\xi)y_2(x)-y_2(\xi)y_1(x)}{W[y_1(\xi),y_2(\xi)]}\eta(x-\xi).    \end{aligned}\]</span></p><p> Green  <spanclass="math display">\[    g(x;\xi)=g(\xi;x).\]</span></p><hr /><p> <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}x}\left[ p(x)\frac{\mathrm{d}y(x)}{\mathrm{d}x}\right]+q(x) y(x)=f(x), \quada&lt;x&lt;b,\]</span> <span class="math display">\[    y(a)=A, \quad y(b)=B,\]</span>  <span class="math display">\[    \begin{aligned}    y(x)&amp;=\int_{a}^{b} g(x;\xi)f(\xi)\mathrm{d}\xi+\left\{p(\xi)\left[y(\xi)\frac{\mathrm{d}g(x;\xi)}{\mathrm{d}\xi}-g(x;\xi)\frac{\mathrm{d}y(\xi)}{\mathrm{d}\xi}\right]\right\}^{\xi=b}_{\xi=a}         \\    &amp;=\int_{a}^{b} g(x;\xi)f(\xi) \mathrm{d}\xi+Bp(b)\frac{\mathrm{d}g(x;\xi)}{\mathrm{d}\xi}\bigg|_{\xi=b}-Ap(a)\frac{\mathrm{d}g(x;\xi)}{\mathrm{d}\xi}\bigg|_{\xi=a}    \end{aligned}\]</span></p><hr /><p> <span class="math display">\[    \frac{\mathrm{d}^{2}g(x;\xi)}{\mathrm{d}x^{2}}-k^{2}g(x;\xi)=\delta(x-\xi),\quad -\infty&lt;x,\xi&lt;\infty,\]</span> <span class="math display">\[    g(x;\xi)\big|_{x\to \pm \infty} \]</span>  <span class="math inline">\(k&gt;0\)</span> <spanclass="math display">\[    g(x;\xi)=-\frac{1}{2k}\mathrm{e}^{-k\lvert x-\xi \rvert }\]</span></p><hr /><p> <span class="math display">\[    \frac{\mathrm{d}^{2}y(x)}{\mathrm{d}x^{2}}-k^{2}y(x)=f(x), \quad-\infty&lt;x&lt;\infty,\]</span> <span class="math display">\[    y(x)\big|_{x\to \pm \infty} \]</span>  <span class="math inline">\(k&gt;0\)</span> <spanclass="math display">\[    y(x)=-\frac{1}{2k}\int_{-\infty}^{\infty} \mathrm{e}^{-k\lvert x-\xi\rvert } f(\xi) \mathrm{d}\xi\]</span></p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (6)</title>
    <link href="/2022/07/11/Neuronal-Dynamics-6/"/>
    <url>/2022/07/11/Neuronal-Dynamics-6/</url>
    
    <content type="html"><![CDATA[<h1 id="adaptation-and-firing-patterns">Adaptation and FiringPatterns</h1><p>The online version of this chapter:</p><hr /><p>Chapter 6 Adaptation and Firing Patternshttps://neuronaldynamics.epfl.ch/online/Ch6.html</p><hr /><h2 id="adaptive-exponential-integrate-and-fire">Adaptive ExponentialIntegrate-and-Fire</h2><p>A single equation is not sufficient to describe the variety of firingpatterns that neurons exhibit in response to a step current. We couplethe voltage equation to abstract current variables <spanclass="math inline">\(w_k\)</span>. The set of equation is <spanclass="math display">\[    \tau_m \frac{\mathrm{d}u}{\mathrm{d}t}=f(u)-R \sum_{k}^{} w_k+RI(t)\]</span> (6.1) <span class="math display">\[    \tau_k\frac{\mathrm{d}w_k}{\mathrm{d}t}=a_k(u-u_{rest})-w_k+b_k\tau_k\sum_{t^{(f)}}^{}\delta(t-t^{(f)}).\]</span> (6.2)</p><p>the adaptation current is fed back to the voltage equation withresistance <span class="math inline">\(R\)</span>. The voltage variable<span class="math inline">\(u\)</span> is reset if the membranepotential reaches the numerical threshold <spanclass="math inline">\(\Theta_{reset}\)</span>. The moment <spanclass="math inline">\(u(t)=\Theta_{reset}\)</span> defines the firingtime <span class="math inline">\(t^{(f)}=t\)</span>. After firing,integration of the voltage restarts at <spanclass="math inline">\(u=u_r\)</span>. The parameters <spanclass="math inline">\(b_k\)</span> are the 'jump' of the spike-triggeredadaptation.</p><div class="note note-info">            <p>One possible biophysical interpretation of the increase is thatduring the action potential calcium enters into the cell so that theamplitude of a calcium-dependent potassium current is increased.</p>          </div><p>The adaptive Exponential Integrate-and-Fire model (AdEx) consists ofan exponential nonlinearity in the voltage equation coupled to a singleadaptation variable <span class="math inline">\(w\)</span> <spanclass="math display">\[    \tau_m \frac{\mathrm{d}u}{\mathrm{d}t}=-(u-u_{rest})+\Delta_{T} \exp\left(\frac{u-\theta_{rh}}{\Delta_{T}}\right)-Rw+RI(t)\]</span> (6.3) <span class="math display">\[    \tau_w \frac{\mathrm{d}w}{\mathrm{d}t}=a(u-u_{rest})-w+b\tau_w\sum_{t^{(f)}}^{} \delta(t-t^{(f)}).\]</span> (6.4)</p><p>At each threshold crossing the voltage is reset to <spanclass="math inline">\(u=u_r\)</span> and the adaptation variable <spanclass="math inline">\(w\)</span> is increased by an amount <spanclass="math inline">\(b\)</span>. Adaptation is characterized by twoparameters: <span class="math inline">\(a\)</span> couples adaptation tothe voltage and is the source of subthreshold adaptation.Spike-triggered adaptation is controlled by a combination of <spanclass="math inline">\(a\)</span> and <spanclass="math inline">\(b\)</span>. The choice of <spanclass="math inline">\(a\)</span> and <spanclass="math inline">\(b\)</span> largely determines the firing patternsof the neuron and can be related to the dynamics of ion channels.</p><h4 id="example-izhikevich-model">Example: Izhikevich model</h4><p>This model uses the quadratic integrate-and-fire model for the firstequation <span class="math display">\[    \tau_m\frac{\mathrm{d}u}{\mathrm{d}t}=-(u-u_{rest})(u-\theta)-Rw+RI(t)\]</span> <span class="math display">\[    \tau_w \frac{\mathrm{d}w}{\mathrm{d}t}=a(u-u_{rest})-w+b\tau_w\sum_{t^{(f)}}^{} \delta(t-t^{(f)}).\]</span></p><p>If <span class="math inline">\(u=\theta_{reset}\)</span>, the voltageis reset to <span class="math inline">\(u=u_r\)</span> and theadaptation variable <span class="math inline">\(w\)</span> is increasedby an amount <span class="math inline">\(b\)</span>. Normally <spanclass="math inline">\(b\)</span> is positive, but <spanclass="math inline">\(b&lt;0\)</span> is also possible.</p><p><img src="/img/neu_dyn/x150.png" /> &gt; Multiple firing patterns incortical neurons. For each type, the neuron is stimulated with a stepcurrent with low or high amplitude.</p><p><img src="/img/neu_dyn/x151.png" /> &gt; Multiple firing patterns inthe AdEx neuron model. The spiking response can be classified by thesteady-state firing behavior (vertical axis: tonic, adapting, bursting)and by its transient initiation pattern as shown along the horizontalaxis: tonic (i.e. no special transient behavior), initial burst, ordelayed spike initiation.</p><h4 id="example-leaky-model-with-adaptation">Example: Leaky model withadaptation</h4><p>Adaptation variables <span class="math inline">\(w_k\)</span> canalso be combined with a standard leaky integrate-and-fire model <spanclass="math display">\[    \tau_m \frac{\mathrm{d}u}{\mathrm{d}t}=-(u-u_{rest})-R\sum_{k}^{}w_k +RI(t)\]</span> (6.7)</p><p><span class="math display">\[    \tau_k \frac{\mathrm{d}w_k}{\mathrm{d}t}=a(u-u_{rest})-w_k+b_k\tau_k\sum_{t^{(f)}}^{} \delta(t-t^{(f)})\]</span> (6.8)</p><p>At the moment of firing, defined by the threshold condition <spanclass="math inline">\(u(t^{(f)})=\theta_{reset}\)</span>, the voltage isreset to <span class="math inline">\(u=u_r\)</span> and the adaptationvariable <span class="math inline">\(w_k\)</span> are increased by anamount <span class="math inline">\(b_k\)</span>.</p><h2 id="firing-patterns">Firing Patterns</h2><h3 id="classification-of-firing-patterns">Classification of FiringPatterns</h3><p>It is advisable to separate the steady-state pattern from the initialtransient phase. The initiation phase refers to the firing pattern rightafter the onset of the current step.</p><p>Three main initiation pattern: the initiation can not bedistinguished from the rest of the spiking response (tonic); the neuronresponds with a significantly greater spike frequency in the transient(initial burst) than in the steady state; the neuronal firing startswith a delay (delay).</p><p>Three main steady state patterns: regularly spaced spikes (tonic);gradually increasing interspike intervals (adapting); or regularalternations between short and long interspike intervals (bursting).</p><p>Irregular firing patterns are also possible in the AdEx model, butthe discussion below is restricted to deterministic models.</p><h4 id="example-tonic-adapting-and-facilitating">Example: Tonic,Adapting and Facilitating</h4><p>When the subthreshold coupling <span class="math inline">\(a\)</span>is small and the voltage reset is low (<span class="math inline">\(u_r\thickapprox u_{rest}\)</span>), the AdEx response is either tonic oradapting. This depends on the jump <spanclass="math inline">\(b\)</span> and the time scale <spanclass="math inline">\(\tau_w\)</span>.</p><p>A large jump with a small time scale creates evenly spaced spikes atlow frequency.On the other hand, a small spike-triggered currentdecaying on a long timescale can accumulate strength over several spikesand therefore successively decreases the net driving current <spanclass="math inline">\(I-w\)</span>. In general, weak but long-lastingspike-triggered currents cause spike-frequency adadptation while shortbut strong currents lead to a prolongation of the refactory period.There is a continuum between purely tonic spiking and stronglyadapting.</p><p>When the spike-triggered curent is depolarizing <spanclass="math inline">\((b&lt;0)\)</span> the interspike interval maygradually decreases, leading to spike-frequency facilitation.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x152.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x153.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x154.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x155.png" /></div></div></div><h3id="phase-plane-analysis-of-non-linear-integrate-and-fire-models-in-two-dimensions">Phaseplane analysis of non-linear integrate-and-fire models in twodimensions</h3><p>In the AdEx model, the <spanclass="math inline">\(u-\)</span>nullcline is again linear in thesubthreshold regime and rises exponentially when <spanclass="math inline">\(u\)</span> is close to <spanclass="math inline">\(\theta\)</span>. Upon current injection, the <spanclass="math inline">\(u-\)</span>nullcline is shifted vertically by anamount proportional to the magnitude of the current <spanclass="math inline">\(I\)</span>.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x156.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x157.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x158.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x159.png" /></div></div></div><p>Each time the trajectory reaches <spanclass="math inline">\(u=\theta_{reset}\)</span>, it will bere-initialized at a reset value <spanclass="math inline">\((u_r,w+b)\)</span>.</p><p>There are three regions of the phase plane with qualitativelydifferent ensuing dynamics. A 'detour reset' corresponds to a downswingof the membrane potential after the end of the action potential. Thedistinction between detour and direct resets is helpful to understandhow different firing patterns arise.</p><h4 id="example-bursting">Example: Bursting</h4><p>Initial burst: a neuron first fires a group of spikes at aconsiderably higher spiking frequency than the steady-statefrequency.</p><p>The shape of the voltage trajectory after the end of the actionpotential (downswing or not) can be used to distinguish between adapting(strictly detour or strictly direct resets) and initial bursting (firstdirect then detour resets).</p><p>By alternation between direct and detour resets, regular bursting canarise.</p><p>AdEx can also produce an irregular alternation. The parameters forirregular firing occupies a small and patchy volume in parameterspace.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x160.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x161.png" /></div></div></div><h3 id="exploring-the-space-of-reset-parameters">Exploring the Space ofReset Parameters</h3><p>Boundaries in the parameter space mark transitions between differenttypes of firing pattern - which are often correlated with types ofcells.</p><p>Apply a step current with an amplitude twice as large as the minimalcurrent necessary to elicit a spike and study the dependence of theobserved firing pattern on the reset parameter <spanclass="math inline">\(u_r\)</span> and <spanclass="math inline">\(b\)</span>. All the other parameters are keptfixed.</p><p><img src="/img/neu_dyn/x162.png" /></p><ul><li>The line separating initial bursting and tonic firing resembles theshape of the <span class="math inline">\(u-\)</span>nullcline. That'sbecause the <span class="math inline">\(u-\)</span>nullcline plays animportant role in determining whether the reset is 'direct' or leads toa 'detour'.</li><li>Regular bursting is possible, if the voltage reset <spanclass="math inline">\(u_r\)</span> is located above the voltagethreshold <span class="math inline">\(\theta\)</span>. Irregular firingpatterns are found within the bursting region of the parameterspace.</li><li>Adapting firing patterns occur only over a restricted range of jumpamplitudes <span class="math inline">\(b\)</span> of the spike-triggeredadaptation current.</li></ul><h4 id="example-piecewise-linear-model">Example: Piecewise-LinearModel</h4><p>Consider a piecewise linear version of the AdEx model <spanclass="math display">\[    f(u)=    \begin{cases}        -(u-u_{rest}) \quad u\leqslant \theta_{rh} \\        \Delta_{T}(u-u_p) \quad \text{otherwise}    \end{cases}\]</span> (6.9)</p><p>with <span class="math display">\[    u_p=\theta_{rh}+\frac{\theta_{rh}-u_{rest}}{\Delta_T},\]</span> (6.10) which we insert into the voltage equation <spanclass="math inline">\(\tau_m\mathrm{d}u/\mathrm{d}t=f(u)+RI-Rw\)</span>. Note that the <spanclass="math inline">\(u-\)</span>nullcline is given by <spanclass="math inline">\(w=f(u)/R+I\)</span> and takes at <spanclass="math inline">\(u=\theta_{rh}\)</span> its minimum value <spanclass="math inline">\(w_{min}=f(\theta_{rh})/R+I\)</span>.</p><p>We assume separation of timescale (<spanclass="math inline">\(\tau_m/\tau_w \ll 1\)</span>) and exploit the factthat the trajectories in the phase plane are nearly horizontal - unlessthey approach the <span class="math inline">\(u-\)</span>nullcline.</p><p>Map the initial condition <spanclass="math inline">\((u_r,w_r)\)</span> after a first reset to thevalue <span class="math inline">\(w_e\)</span> of the adaptationvariable at the end of the trajectory: <spanclass="math inline">\(w_e=M(u_r,w_r)\)</span>. The next reset startsthen from <span class="math inline">\((u_r,w_e+b)\)</span>. Alltrajectories with <span class="math inline">\(w_r&lt;w_{min}\)</span>remain horizontal, so that <spanclass="math inline">\(w_e=w_r\)</span>.</p><p>If <span class="math inline">\(w_r&gt;w_{min}\)</span>, wedistinguish two possible cases. The first one corresponds to a voltagereset below the threshold, <spanclass="math inline">\(u_r&lt;\theta_{rh}\)</span>. A trajectoryinitiated at <span class="math inline">\(u_r&lt;\theta_{rh}\)</span>evolves horizontally until it comes close to the left branch of the<span class="math inline">\(u-\)</span>nullcline. It then follows the<span class="math inline">\(u-\)</span>nullcline at a small distance<span class="math inline">\(x(u)\)</span> below it. The distance can beshown to be <span class="math display">\[    x(u)=\frac{\tau_m}{\tau_w}[I-(a+R^{-1})(u-u_{rest})],       \]</span> (6.11) which vanishes in the limit <spanclass="math inline">\(\tau_m/\tau_w \to 0\)</span>. For <spanclass="math inline">\(u_r&lt;\theta_{rh}\)</span>. <spanclass="math display">\[    M(u_r,w_r)=    \begin{cases}        w_r \quad w_r&lt;f(u_r)/R+I \\        f(\theta_{rh})/R+I \quad \text{otherwise}    \end{cases}\]</span> (6.12) If <spanclass="math inline">\(u_r&gt;\theta_{rh}\)</span> then we have a directreset (i.e.,movement starts to the right) if <spanclass="math inline">\((u_r,w_r)\)</span> lands below the right branch ofthe <span class="math inline">\(u-\)</span>nullcline and a detour resetotherwise <span class="math display">\[    M(u_r,w_r)=    \begin{cases}        w_r \quad w_r&lt;f(u_r)/R+I \\        f(\theta_{rh})/R+I \quad \text{otherwise}    \end{cases}\]</span> (6.13)</p><p><img src="/img/neu_dyn/x163.png" /> &gt; Fig.6.7</p><p>The map <span class="math inline">\(M\)</span> uniquely defines thefiring pattern. Regular bursting is possible only if <spanclass="math inline">\(u_r&gt;\theta_{rh}\)</span> and <spanclass="math inline">\(b&lt;f(u_r)-f(\theta_{rh})\)</span> so that atleast one reset in each burst lands below the <spanclass="math inline">\(u-\)</span>nullcline. For <spanclass="math inline">\(u_r&gt;\theta_{rh}\)</span>, we have tonic spikingwith detour resets when <spanclass="math inline">\(b&gt;f(u_r)+I\)</span> and initial bursting if<spanclass="math inline">\(f(u_r)+I&gt;b&gt;f(u_r)-f(\theta_{rh})+x(\theta_{rh})\)</span>.</p><p>If <span class="math inline">\(u_r\leqslant \theta\)</span> we havetonic spiking with detour resets when <spanclass="math inline">\(b&gt;f(u_r)+I\)</span>, tonic spiking with directreset when <spanclass="math inline">\(b&lt;f(u_r)-f(\theta_{rh})\)</span> and initialbursting if <spanclass="math inline">\(f(u_r)+I&gt;b&gt;f(u_r)-f(\theta_{rh})\)</span>.Note that the rough layout of the parameter regions in Fig.6.7A.</p><h3 id="exploring-the-space-of-subthreshold-parameters">Exploring theSpace of Subthreshold Parameters</h3><p>While the exponential integrate-and-fire model losses stabilityalways via a saddle-node bifurcation, the AdEx can become unstableeither via a Hopf or a saddle-node bifurcation.</p><p>In the AdEx, an eigenvalue analysis shows that the stable fixed pointlooses stability via a Hopf bifurcation if <spanclass="math inline">\(aR&gt;\tau_m/\tau_w\)</span>.</p><div class="note note-info">            <p>Otherwise, when the coupling from voltage to adaptation (parameter<span class="math inline">\(a\)</span>) and back from adaptation tovoltage (parameter <span class="math inline">\(R\)</span>) are bothweak, an increase in the current causes the stable fixed point to mergewith the unstable one, so that both disappear via a saddle-nodebifurcation.</p>          </div><div class="note note-warning">            <p>The type of bifurcation has no influence on the firing pattern(bursting, adapting, tonic) which depends mainly on the choice of resetparameters.</p>          </div><p>However, the subthreshold parameters do control the presence orabsence of oscillations in response to a short current pulse.</p><p><strong>resonator</strong>: a model showing damped oscillations.</p><p><strong>integrator</strong>: a model without damped oscillations.</p><p>The presence of damped oscillations depends non-linearly on <spanclass="math inline">\(a/g_{L}\)</span> and <spanclass="math inline">\(\tau_m/\tau_w\)</span> as summarized in the figurebelow.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x164.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x165.png" /></div></div></div><p>The frequency of the damped oscillation is given by <spanclass="math display">\[    \omega=\frac{4}{\tau_w}\left[aR-\frac{2\tau_w}{\tau_m}\left(1-\frac{\tau_m}{\tau_w}\right)^{2}\right]\]</span> (6.14)</p><h4 id="example-transient-spiking">Example: Transient Spiking</h4><p>Upon the onset of a current step, some neurons may fire a smallnumber of spikes and then remain silent, even if the stimulus ismaintained for a very long time. An AdEx model with subthresholdcoupling <span class="math inline">\(a&gt;0\)</span> can explain thisphenomenon whereas pure spike-triggered adaptation (<spanclass="math inline">\(a=0;b&gt;0\)</span>) cannot account for it,because adaptation would eventually decay back to zero so that theneuron fires another spike.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x166.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x167.png" /></div></div></div><p>Choose parameter <span class="math inline">\(a\)</span> and <spanclass="math inline">\(\tau_w\)</span> such that the neuron is in theresonator regime. The voltage response to a step input then exhibitsdamped oscillations. Phase plane analysis reveals that sometimes severalresets are needed before the trajectory is attracted towards the fixedpoint.</p><h2 id="biophysical-origin-of-adaptation">Biophysical Origin ofAdaptation</h2><p>We now show that the variables <spanclass="math inline">\(w_k\)</span> can be linked to the biophysics ofion channels and dendrites.</p><h3 id="subthreshold-adaptation-by-a-single-slow-channel">Subthresholdadaptation by a single slow channel</h3><p>First our aim is to give a biophysical interpretation of theparameters <span class="math inline">\(a\)</span>, <spanclass="math inline">\(\tau_w\)</span>, and the variable <spanclass="math inline">\(w\)</span> that show up in the adaptation equation<span class="math display">\[    \tau_w \frac{\mathrm{d}w}{\mathrm{d}t}=a(u-E_0)-w.\]</span> (6.15) Here and in the following we write <spanclass="math inline">\(E_0\)</span> instead of <spanclass="math inline">\(u_{rest}\)</span> in order to simplify notationand keep the treatment slightly more general.</p><p>We know rapid activation of the sodium channels, important during theupswing of action potentials, is well approximated by the exponentialnonlinearity in the voltage equation of the AdEx model. We will see nowthat the subthreshold current <span class="math inline">\(w\)</span> islinked to the dynamics of other ion channels with a slower dynamics.</p><p>Let us focus on the model of a membrane with a leak current and asingle, slow, ion channel, say a potassium channel of the Hodgkin-Huxleytype <span class="math display">\[    \tau_m\frac{\mathrm{d}u}{\mathrm{d}t}=-(u-E_{L})-R_{L}g_{K}n^{p}(u-E_k)+R_{L}I_{ext}\]</span> (6.16)</p><p>where <span class="math inline">\(R_{L}\)</span> and <spanclass="math inline">\(E_{L}\)</span> are the resistance and reversalpotential of the leak current, <spanclass="math inline">\(\tau_m=R_{L}C\)</span> the membrane time constant,<span class="math inline">\(g_{K}\)</span> the maximal conductance ofthe open channel and <span class="math inline">\(n\)</span> the gatingvariable (which appears with arbitrary power <spanclass="math inline">\(p\)</span>) with dynamics <spanclass="math display">\[    \frac{\mathrm{d}n}{\mathrm{d}t}=-\frac{n-n_0(u)}{\tau_n(u)}.\]</span> (6.17)</p><p>As long as the membrane potential stays below threshold, we canlinearize the equations (6.16) and (6.17) aroung the resting voltage<span class="math inline">\(E_0\)</span>, given by the fixed pointcondition <span class="math display">\[    E_0=\frac{E_{L}+(R_{L}g_{K})n_0^{p}(E_0)E_{K}}{1+(R_{L}g_{K})n_0^{p}(E_0)}.\]</span> (6.18)</p><p>The resting potential is shifted with respect to the leak reversalpotential if the channel is partially open at rest, <spanclass="math inline">\(n_0(E_0)&gt;0\)</span>.</p><p>We introduce the parameter <span class="math inline">\(\beta=g_{K}pn_0^{p-1}(E_0)(E_0-E_{K})\)</span> and expand <spanclass="math inline">\(n_0(u)=n_0(E_0)+n_0&#39;(u-E_0)\)</span> where<span class="math inline">\(n_0&#39;\)</span> is the derivative <spanclass="math inline">\(\mathrm{d}n_0/\mathrm{d}u\)</span> evaluated at<span class="math inline">\(E_0\)</span>.</p><p>The variable <span class="math inline">\(w=\beta[n-n_0(E_0)]\)</span>then follows the linear equation <span class="math display">\[    \tau_n(E_0) \frac{\mathrm{d}w}{\mathrm{d}t}=a(u-E_0)-w.\]</span> (6.19)</p><p>Note that the time constant of the variable <spanclass="math inline">\(w\)</span> is given by the time constant of thechannel at the resting potential. The parameter <spanclass="math inline">\(a=\beta n_0&#39;(E_0)\)</span> is proportional tothe sensitivity of the channel to a change in the membrane voltage, asmeasured by the slope <spanclass="math inline">\(\mathrm{d}n_0/\mathrm{d}u\)</span> at <spanclass="math inline">\(E_0\)</span>.</p><p>The adaptation variable <span class="math inline">\(w\)</span> iscoupled into the voltage equation in the standart form <spanclass="math display">\[    \tau_m^{eff}=-(u-E_0)-Rw+RI_{ext}.\]</span> (6.20)</p><p>Note that the membrane time constant and the resistance are rescaledby a factor <spanclass="math inline">\(\lambda=1+(R_{L}g_{K})n_0^{p}(E_0)\)</span> withrespect to their values in the passive membrane equation (6.16). Namely,<span class="math inline">\(\tau_m^{eff}=\tau_m/\lambda\)</span>, <spanclass="math inline">\(R=R_{L}/\lambda\)</span>. In fact, both aresmaller because of partial opening of the channel at rest.</p><p>In summary, each channel with nonzero slope <spanclass="math inline">\(\mathrm{d}n_0/\mathrm{d}u\)</span> at <spanclass="math inline">\(E_0\)</span> gives rise to an effective adaptationvariable <span class="math inline">\(w\)</span>. Since there are manychannels, we can expect many variables <spanclass="math inline">\(w_k\)</span>. Those with similar time constantscan be summed and grouped into a single equation. But if time constantsare different by an order of magnitude or more than several adaptationvariables are needed.</p><h3id="spike-triggered-adaptation-arising-from-a-biophysical-ion-channel">Spike-triggeredadaptation arising from a biophysical ion-channel</h3><p>We have seen that some ion channels are partially open at the restingpotential, while others react only when the membrane potential is wellabove the firing threshold. The second group gives a biophysicalinterpretation of the jump amplitude <spanclass="math inline">\(b\)</span> og a spike-triggered adaptationcurrent.</p><p>We now study the change in the state of the ion channel inducedduring the large-amplitude excursion of the voltage trajectory during aspike. During the spike, the target <spanclass="math inline">\(n_0(u)\)</span> of the gating variable is close toone; but since the time constant <spanclass="math inline">\(\tau_n\)</span> is long, the target is not reachedduring the short time that the voltage stays above the activationthreshold.</p><p>Nevertheless, the ion channel is partially activated by the spike.Unless the neuron is firing at a very large firing rate, each additionalspike activate the channel further, always by the same amount <spanclass="math inline">\(\Delta_{n}\)</span>, which depends on the durationof the spike and the activation threshold of the current. Thespike-triggered jump in the adapting current <spanclass="math inline">\(w\)</span> is then <span class="math display">\[    b=\beta \Delta_n. \tag{6.21}\]</span> where <span class="math inline">\(\beta=g_{K}pn_0^{p-1}(E_0)(E_0-E_K)\)</span> has been defined before.</p><h4id="example-calculating-the-jump-b-of-the-spike-triggered-adaptation-current">Example:Calculating the jump <span class="math inline">\(b\)</span> of thespike-triggered adaptation current</h4><p>We consider a gating dynamics <span class="math display">\[    \frac{\mathrm{d}n}{\mathrm{d}t}=-\frac{n-n_0(u)}{\tau_n(u)}.\]</span> (6.22)</p><p>with the steplike activation function <spanclass="math inline">\(n_0(u)=\Theta(u-u_0^{act})\)</span> where <spanclass="math inline">\(u_{0}^{act}=-30\)</span> mV and <spanclass="math inline">\(\tau_n(u)=100\)</span> ms independent of <spanclass="math inline">\(u\)</span>. The activation threshold <spanclass="math inline">\(-30\)</span> mV is above the firing threshold(typically in the range of <span class="math inline">\(-40\)</span> mV).Assuming that, during an action potential the voltage remains for <spanclass="math inline">\(\Delta_{t}=1\)</span> ms above <spanclass="math inline">\(u_0^{act}\)</span>, we found <spanclass="math display">\[    n_{after}-n_{before}\thickapprox \ln(1+\frac{n_{after}-n_{before}}{1-n_{after}})=\frac{\Delta_{t}}{\tau_n}=\Delta_{n}\]</span></p><h3id="subthreshold-adaptation-caused-by-passive-dendrites">Subthresholdadaptation caused by passive dendrites</h3><p>Here we show that a passive dendrite can also give rise to asubthreshold coupling of the form of (6.15).</p><p>We focus on a simple neuron model with two compartments, representingthe soma and the dendrite, superscript <spanclass="math inline">\(s\)</span> and <spanclass="math inline">\(d\)</span> respectively. The two compartments areboth passive with membrane potential <spanclass="math inline">\(V^{s}\)</span>, <spanclass="math inline">\(V^{d}\)</span>, transversal resistance <spanclass="math inline">\(R_{T}^{s}\)</span>, <spanclass="math inline">\(R_{T}^{d}\)</span>, capacity <spanclass="math inline">\(C^{s}\)</span>, <spanclass="math inline">\(C^{d}\)</span> and resting potential <spanclass="math inline">\(u_{rest}\)</span>, <spanclass="math inline">\(E^{d}\)</span>. The two compartments are linked bya longitudinal resistance <span class="math inline">\(R_{L}\)</span>. Ifcurrent is injected only in the soma, then the two-compartment modelwith passive dendrites corresponds to <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}V^{s}=\frac{1}{C^{s}}\left[-\frac{(V^{s}-u_{rest})}{R_{T}^{s}}-\frac{V^{s}-V^{d}}{R_{L}}+I(t)\right],     \]</span> (6.23)</p><p><span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}V^{d}=\frac{1}{C^{d}}\left[-\frac{(V^{d}-E^{d})}{R_{T}^{d}}-\frac{V^{d}-V^{s}}{R_{L}}\right].\]</span> (6.24)</p><p>We assume that <span class="math inline">\(E^{d}=u_{rest}=E\)</span>.In this case the adaptation current is <spanclass="math inline">\(w=-(V^{d}-u_{rest})/R_{L}\)</span> and the twoequations above reduce to <span class="math display">\[    \tau^{eff}\frac{\mathrm{d}V^{s}}{\mathrm{d}t}=-(V^{s}-E)-R^{eff}w\]</span> (6.25) <span class="math display">\[    \tau_w \frac{\mathrm{d}w}{\mathrm{d}t}=a(V^{s}-E)-w\]</span> (6.26) with an effective input resistance <spanclass="math inline">\(R^{eff}=1/[1/R_{T}^{s}+1/R_{L}]\)</span>, aneffective somatic time constant <spanclass="math inline">\(\tau^{eff}=C^{s}R^{eff}\)</span> and effectiveadaptation time constant <spanclass="math inline">\(\tau_w=R_{L}C^{d}/[1+(R_{L}/R_{T}^{d})]\)</span>and a coupling between somatic voltage and adaptation current <spanclass="math inline">\(a=-[R_{L}+(R_{L}^{2}/R_{T}^{d})]^{-1}\)</span>.</p><ul><li><span class="math inline">\(a\)</span> is always negative, whichmeans that passive dendrites introduce a facilitating subthresholdcoupling.</li><li>Facilitation is particularly strong with a small longitudinalresistance.</li><li>The timescale of the facilitation <spanclass="math inline">\(\tau_w\)</span> is smaller than the dendritic timeconstant <span class="math inline">\(R_{T}^{d}C^{d}\)</span> - so that,compared to other 'adaptation' currents, the dendritic current is arelatively fast one.</li></ul><h4 id="example-bursting-with-a-passive-dendrite-and-i_m">Example:Bursting with a Passive Dendrite and <spanclass="math inline">\(I_{M}\)</span></h4><p>Suppose that the action potential can be approximated by aone-millisecond pulse at <span class="math inline">\(0\)</span> mV. Theneach spike brings an increase in the dendritic membrane potential. Interms of the current <span class="math inline">\(w\)</span>, theincrease is <spanclass="math inline">\(b=-aE_0(1-\mathrm{e}^{-1/\tau_w})\)</span>. Again,the spike-triggered jump is always negative, leading to spike-triggeredfacilitation.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x168.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x169.png" /></div></div></div><p>The above figures shows an example where we combined a dendriticcompartment with the linearized effects of the M-current to result inregular bursting.</p><p>The bursting is meditated by the dendritic facilitation which iscounterbalanced by the adapting effects of <spanclass="math inline">\(I_{M}\)</span>.</p><p>This example suggests that the dynamics of spike-triggered currentson multiple timescales can be understood in terms of their stereotypicaleffect on the membrane potential.</p><h2 id="spike-response-model-srm">Spike response model (SRM)</h2><p>We introduced 'filter picture' in section 1.3.5. In this picture, theparameters of the model are replaced by (parametric) functions oftime.</p><p>In contrast to nonlinear integrate-and-fire models, the SRM has no'intrinsic' firing threshold but only the sharp numerical threshold forreset.</p><p>The subthreshold behavior of the SRM is richer than that of theintegrate-and-fire model discussed so far and can account for variousaspects of refractoriness and adaptation.</p><h3 id="definition-of-the-srm">Definition of the SRM</h3><p>In the framework of SRM the state of a neuron is described by asingle variable <span class="math inline">\(u\)</span> which weinterpret as the membrane potential.</p><p>After a short current pulse perturbing <spanclass="math inline">\(u\)</span>, it takes some time before <spanclass="math inline">\(u\)</span> returns to rest. The function <spanclass="math inline">\(\kappa(s)\)</span> describes the time course ofthe voltage response to a short current pulse at time <spanclass="math inline">\(s=0\)</span>. Because the subthreshold behavior ofthe membrane potential is taken as linear, the voltage response <spanclass="math inline">\(h\)</span> to an arbitrary time-dependentstimuating current <span class="math inline">\(I^{ext}(t&#39;)\)</span>is given by the integral <spanclass="math inline">\(h(t)=\int_{0}^{\infty} \kappa(s)I^{ext}(t-s)\mathrm{d}s\)</span>.</p><p>Spike firing is defined by a threshold process. The form of theaction potential and the after-potential is described by a function<span class="math inline">\(\eta\)</span>. The evolution of <spanclass="math inline">\(u\)</span> is given by <spanclass="math display">\[    u(t)=\sum_{f} \eta(t-t^{(f)})+\int_{0}^{\infty}\kappa(s)I^{ext}(t-s) \mathrm{d}s + u_{rest}    \]</span> (6.27)</p><p>Introducing the spike train <spanclass="math inline">\(S(t)=\sum_{f}^{} \delta(t-t^{(f)})\)</span>,(6.27) can be also written as a convolution <spanclass="math display">\[    u(t)=\int_{0}^{\infty} \eta(s)S(t-s) \mathrm{d}s +\int_{0}^{\infty}\kappa(s)I^{ext}(t-s) \mathrm{d}s+u_{rest}\]</span> (6.28)</p><p>The threshold <span class="math inline">\(\theta\)</span> is notfixed, but time-dependent <span class="math display">\[    \theta \to \theta(t)\]</span> (6.29)</p><p>Firing occurs whenever the membrane potential <spanclass="math inline">\(u\)</span> reaches the dynamic threshold <spanclass="math inline">\(\theta(t)\)</span> from below <spanclass="math display">\[    t=t^{(f)} \Leftrightarrow u(t)=\theta(t)\ \text{and}\\frac{\mathrm{d}[u(t)-\theta(t)]}{\mathrm{d}t}&gt;0.\]</span> (6.30)</p><p><img src="/img/neu_dyn/x170.png" /></p><h4 id="example-dynamic-threshold---and-how-to-get-rid-of-it">Example:Dynamic threshold - and how to get rid of it</h4><p>A standard model of the dynamic threshold is <spanclass="math display">\[    \theta(t)=\theta_0+\sum_{f}^{}\theta_1(t-t^{(f)})=\theta_0+\int_{0}^{\infty} \theta_1(s)S(t-s)\mathrm{d}s\]</span> (6.31)</p><p>where <span class="math inline">\(\theta_0\)</span> is the 'normal'threshold of neuron <span class="math inline">\(i\)</span> in theabsence of spiking. After each output spike, the firing threshold of theneuron is increased by an amount <spanclass="math inline">\(\theta_1(t-t^{(f)})\)</span> where <spanclass="math inline">\(t^{(f)}&lt;t\)</span> denote the firing times inthe past.</p><p>For example, during an absolute refractory period <spanclass="math inline">\(\Delta^{abs}\)</span>, we may set <spanclass="math inline">\(\Delta_{\theta}\)</span> for a few milliseconds toa large and positive value so as to avoid any firing and let it relaxback to zero over the next few hundred milliseconds.</p><p>There is no need to interpret the variable <spanclass="math inline">\(u\)</span> as the membrane potential. It is oftenconvenient to transform the variable <spanclass="math inline">\(u\)</span> so as to remove the time-dependence ofthe threshold. <span class="math display">\[    \eta(t-t^{(f)}) \to\eta^{eff}(t-t^{(f)})=\eta(t-t^{(f)})-\theta_1(t-t^{(f)})\]</span> (6.32)</p><p>The argument can also be turned the other way round, so as to removethe spike after-potential and only work with a dynamic threshold.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x171.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x172.png" /></div></div></div><h3 id="interpretation-of-eta-and-kappa">Interpretation of <spanclass="math inline">\(\eta\)</span> and <spanclass="math inline">\(\kappa\)</span></h3><p>(6.27) and (6.30) defines a mathematical model. We'll give abiological interpretation of the terms.</p><p>The kernel <span class="math inline">\(\kappa(s)\)</span> is thelinear response of the membrane potential to an input current. Itdescribes the time course of a deviation of the membrane potential fromits resting value that is caused by a short current pulse ("impulseresponse").</p><p>The kernel <span class="math inline">\(\eta\)</span> describes thestandard form of an action potential of neuron <spanclass="math inline">\(i\)</span> including the negative overshoot whichtypically follows a spike (the spike afterpotential). Graphicallyspeaking, a contribution <span class="math inline">\(\eta\)</span> is'pasted in' each time membrane potential reaches the threshold <spanclass="math inline">\(\theta\)</span>.</p><p>In a simplified model, the form of the action potential may beneglected as long as we keep track of the firing times <spanclass="math inline">\(t^{(f)}\)</span>. The kernel <spanclass="math inline">\(\eta\)</span> describes then simply the 'reset' ofthe membrane potential to a lower value after the spike at <spanclass="math inline">\(t^{(f)}\)</span>. <span class="math display">\[    \eta(t-t^{(f)})=-\eta_0 \exp\left(-\frac{t-t^{(f)}}{\tau_{recov}}\right)\]</span> (6.33)</p><p>with a parameter <span class="math inline">\(\eta_0&gt;0\)</span> anda recovery time constant <spanclass="math inline">\(\tau_{recov}\)</span>. The leakyintegrate-and-fire model is in fact a special case of the SRM, withparameter <span class="math inline">\(\eta_0=(\theta-u_{r})\)</span> and<span class="math inline">\(\tau_{recov}=\tau_m\)</span>.</p><h4 id="example-refractoriness">Example: Refractoriness</h4><p>Absolute refractoriness can be incorporated in the SRM by setting thedynamic threshold during a time <spanclass="math inline">\(\Delta^{abs}\)</span> to an extremely high valuethat cannot be attained.</p><p>Relative refractoriness can be mimicked in various ways. - After aspike the firing threshold returns only slowly back to its normal value(increase in firing threshold). - After the spike the membranepotential, and hence <span class="math inline">\(\eta\)</span>, passesthrough a regime of hyperpolarization (spike after-potential) where thevoltage is below the resting potential. During this phase, morestimulation than usual is needed to drive the membrane potential abovethe threshold. This is equivalent to a transient increase of the firingthreshold. - The responsiveness of the neuron is reduced immediatelyafter a spike. In the SRM we can model the reduced responsiveness bymaking the shape of <span class="math inline">\(\varepsilon\)</span> and<span class="math inline">\(\kappa\)</span> depend on the time since thelast spike timing <span class="math inline">\(\hat{t}\)</span>.</p><p>A slightly more general version of SRM is <spanclass="math display">\[    u(t)=\sum_{f}^{} \eta(t-t^{(f)})+\int_{0}^{\infty}\kappa(t-\hat{t},s)I^{ext}(t-s) \mathrm{d}s+u_{rest}.\]</span> (6.34)</p><h3 id="mapping-the-integrate-and-fire-model-to-the-srm">Mapping theIntegrate-and-Fire Model to the SRM</h3><p>Recall that the leaky integrate-and-fire model follows the equation<span class="math display">\[    \tau_m \frac{\mathrm{d}u_i}{\mathrm{d}t}=-(u_i-E_0)-R\sum_{k}^{} w_k+RI_i(t)\]</span> (6.35)</p><p><span class="math display">\[\{t_i^{(f)}\} \in \{t | u_i(t)=\theta\}.\]</span> (6.36)</p><p><span class="math display">\[\tau_k \frac{\mathrm{d}w_k}{\mathrm{d}t}=a_k(u_i-E_0)-w_k+\tau_k b_k\sum_{t^{(f)}}^{} \delta(t-t^{(f)})\]</span><br />(6.37)</p><p>Let us consider a short current pulse <spanclass="math inline">\(I_i^{out}=-q\delta(t)\)</span> applied to the<span class="math inline">\(RC\)</span> circuit. Let <spanclass="math inline">\(q=C(\theta-u_r)\)</span>, the total reset currentis <span class="math display">\[    I_i^{out}(t)=-C(\theta-u_r)\sum_{f}^{} \delta(t-t_i^{(f)})   \]</span> (6.38)</p><p>We add the output current (6.38) on the right-hand side of (6.35),<span class="math display">\[    \tau_m\frac{\mathrm{d}u_i}{\mathrm{d}t}=-(u_i-E_0)-R\sum_{k}^{} w_k+RI_i(t)-RC(\theta-u_r) \sum_{f}^{} \delta(t-t_i^{(f)}),\]</span> (6.39)</p><p><span class="math display">\[    \tau_k \frac{\mathrm{d}w_k}{\mathrm{d}t}=a_k(u_i-E_0)-w_k+\tau_k b_k\sum_{f}^{} \delta(t-t^{(f)})\]</span> (6.40)</p><p>Now we solve (6.39) and (6.40). - First, we shift the voltage so asto set <span class="math inline">\(E_0\)</span> to zero. - Second, wecalculate the eigenvalues and eigenvectors of the 'free' equations inthe absence of input (and therefore no spikes). If there are <spanclass="math inline">\(K\)</span> adaptation variables, we have a totalof <span class="math inline">\(K+1\)</span> eigenvalues (including <spanclass="math inline">\(\pm \sqrt{R(-a_1-\cdots -a_K)}-1\)</span> and<span class="math inline">\(-1\)</span> with a multiplicity of <spanclass="math inline">\(K-1\)</span>). The associated eigenvectors are<span class="math inline">\(\mathbf{e}_k\)</span> with components <spanclass="math inline">\((e_{k0},\cdots ,e_{kK})^{\mathsf{T}}\)</span>. -Third, we express the response to an impulse <spanclass="math inline">\(\Delta u=1\)</span> in the voltage (noperturbation in the adaptation variables) in terms of the <spanclass="math inline">\(K+1\)</span> eigenvectors: <spanclass="math inline">\((1,0,\cdots ,0)^{\mathsf{T}}=\sum_{k=0}^{K}\beta_k \mathbf{e}_k\)</span>. - Finally, we express the pulse caused bya reset of voltage and adaptation variables in terms of the eigenvectors<span class="math inline">\((-\theta+u_r,b_1,\cdots,b_{K})^{\mathsf{T}}=\sum_{k=0}^{K} \gamma_k \mathbf{e}_k\)</span>.</p><p>The response to the reset pulses yields the kernel <spanclass="math inline">\(\eta\)</span> while the response to voltage pulsesyields the filter <span class="math inline">\(\kappa(s)\)</span> of theSRM <span class="math display">\[    u_i(t)=\sum_{f}^{} \eta(t-t_i^{(f)})+\int_{0}^{\infty}\kappa(s)I_i(t-s) \mathrm{d}s,\]</span> (6.41)</p><p>with kernels <span class="math display">\[    \eta(s)=\sum_{k=0}^{K} \gamma_k e_{k0} \exp (\lambda_k s)\Theta(s),\]</span> (6.42)</p><p><span class="math display">\[    \kappa(s)=\sum_{k=0}^{K} \beta_k e_{k0} \exp (\lambda_k s)\Theta(s).\]</span> (6.43)</p><blockquote><p></p></blockquote><h4 id="example-adaptation-and-bursting">Example: Adaptation andBursting</h4><p>Study a leaky integrate-and-fire model with a single slow adaptationvariable <span class="math inline">\(\tau_w\gg \tau_m\)</span> which iscoupled to the voltage in the threshold regime (<spanclass="math inline">\(a&gt;0\)</span>) and increased during spiking byan amount <span class="math inline">\(b\)</span>.</p><p>With a parameter <span class="math inline">\(\delta=\tau_m/\tau_w \ll1\)</span>, the eigenvalues are <spanclass="math inline">\(\lambda_1=-\tau_w[1-a\delta]\)</span> and <spanclass="math inline">\(\lambda_2=-\tau_w\delta[1+a]\)</span>, associatedto eigenvectors <spanclass="math inline">\(\mathbf{e}_1=(1,a\delta)^{\mathsf{T}}\)</span> and<spanclass="math inline">\(\mathbf{e}_2=(1,-1+\delta+a\delta)^{\mathsf{T}}\)</span>.The resulting spike after-potential kernel <spanclass="math inline">\(\eta(s)\)</span> is shown below. Because of theslow constant <span class="math inline">\(\tau_w\gg \tau_m\)</span>, thekernel <span class="math inline">\(\eta\)</span> has a longhyperpolarizing tail. The neuron model responds to a step current withadaptation, because of accumulation of hyperpolarizing spike-afterpotentials over many spikes.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x173.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x174.png" /></div></div></div><p>As a second example, consider four adaptation currents with differenttime constants <spanclass="math inline">\(\tau_1&lt;\tau_2&lt;\tau_3&lt;\tau_4\)</span>. Weassume pure spike-triggered coupling (<spanclass="math inline">\(a=0\)</span>) so that the integration of thedifferential equations of <span class="math inline">\(w_k\)</span> giveseach an exponential current <span class="math display">\[    w_k(t)=\sum_{f}^{} b_k \exp \left( -\frac{t-t^{(f)}}{\tau_k}\right)\Theta(t-t^{(f)})   \]</span> (6.44)</p><p>Choose as follows: - The time constant of the first current is veryshort and <span class="math inline">\(b_1&lt;0\)</span> (inward current)so as to model the upswing of the action potential (a candidate currentwould be sodium). - A second current (e.g. a fast potassium channel)with a slightly longer time constant is outgoing (<spanclass="math inline">\(b_2&gt;0\)</span>) and leads to the downswing andrapid reset of the membrane potential. - The third current, with a timeconstant of tens of milliseconds is inward (<spanclass="math inline">\(b_3&lt;0\)</span>). - The slowest current is againhyperpolarizing (<span class="math inline">\(b_4&gt;0\)</span>).</p><p>The spike after-potential <span class="math inline">\(\eta\)</span>is shown below.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x639.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x640.png" /></div></div></div><p>Because of the depolarizing spike after-potential induced by theinward current <span class="math inline">\(w_3\)</span>, the neuronmodel responds to a step current of appropriate amplitude with bursts.The bursts end because of the accumulation of the hyperpolarizing effectof the slowest current.</p><h3id="multi-compartment-integrate-and-fire-model-as-a-srm">Multi-compartmentintegrate-and-fire model as a SRM</h3><p>In this section, we want to show that neurons with a linear dendritictree and a voltage threshold for spike firing at the soma can be mappedto the SRM.</p><p>We study an integrate-and-fire model with a passive dendritic treedescribed by <span class="math inline">\(n\)</span> compartments.Membrane resistance, core resistance, and capacity of compartment <spanclass="math inline">\(\mu\)</span> are denoted by <spanclass="math inline">\(R_{T}^{\mu}\)</span>, <spanclass="math inline">\(R_{L}^{\mu}\)</span>, and <spanclass="math inline">\(C^{\mu}\)</span>, respectively. The longitudinalcore resistance between compartment <spanclass="math inline">\(\mu\)</span> and a neighboring compartment <spanclass="math inline">\(\nu\)</span> is <span class="math inline">\(r^{\mu\nu}=(R_{L}^{\mu}+R_{L}^{\nu})/2\)</span>. Compartment <spanclass="math inline">\(\mu=1\)</span> represents the soma and is equippedwith a simple mechanism for spike generation, i.e., with a thresholdcriterion as in the standard integrate-and-fire model. The remainingdendritic compartments (<span class="math inline">\(2\leqslant\mu\leqslant n\)</span>) are passive.</p><p>Each compartment <span class="math inline">\(1\leqslant \mu\leqslantn\)</span> of neuron <span class="math inline">\(i\)</span> may receiveinput <span class="math inline">\(I_i^{\mu}(t)\)</span> from presynapticneurons. As a result of spike generation, there is an additional resetcurrent <span class="math inline">\(\Omega_i(t)\)</span> at the soma.The membrane potential <span class="math inline">\(V_i^{\mu}\)</span> ofcompartment <span class="math inline">\(\mu\)</span> is given by <spanclass="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}V_i^{\mu}=\frac{1}{C_i^{\mu}}\left[-\frac{V_i^{\mu}}{R_{T,i}^{\mu}}-\sum_{\nu}^{}\frac{V_i^{\mu}-V_i^{\nu}}{r_{i}^{\mu \nu}}+I_i^{\mu}(t)-\delta^{\mu1}\Omega_i(t)\right]  \]</span> (6.45)</p><p>where the sum runs over all neighbors of compartment <spanclass="math inline">\(\mu\)</span>. <spanclass="math inline">\(\delta^{\mu \nu}\)</span> is the Kronecker symbol.Below we will identify the somatic voltage <spanclass="math inline">\(V_i^{1}\)</span> with the potential <spanclass="math inline">\(u_i\)</span> of the SRM.</p><p>The solution of (6.45) can be formulated by means of Green'sfunctions <span class="math inline">\(G_{i}^{\mu \nu}(s)\)</span> thatdescribe the impact of an current pulse injected in compartment <spanclass="math inline">\(\nu\)</span> on the membrane potential ofcompartment <span class="math inline">\(\mu\)</span>. The solution is ofthe form <span class="math display">\[    V_{i}^{\mu}(t)=\sum_{\nu}^{} \frac{1}{C_i^{\nu}}\int_{0}^{\infty}G_i^{\mu \nu}(s)[I_i^{\nu}(t-s)-\delta^{\nu 1}\Omega_i(t-s)]\mathrm{d}s.\]</span> (6.46)</p><blockquote><p> compartment <spanclass="math inline">\(\nu\)</span>  compartment <spanclass="math inline">\(\mu\)</span>  Green <spanclass="math inline">\(f(t)\)</span>  <spanclass="math inline">\(\int_{0}^{\infty} \delta(s)f(t-s)\mathrm{d}s\)</span><span class="math inline">\(\delta(t-s)\)</span>  <spanclass="math inline">\(t,s\)</span>  Green  <spanclass="math inline">\(G(t,s)\)</span></p></blockquote><hr /><p>This part is based on Bressloff and Taylor'spaper<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="P. C. Bressloff and J. G. Taylor (1994) Dynamics of compartmental model neurons. Neural Networks 7, pp. 11531165.">[1]</span></a></sup>.</p><p>(6.45) may be written as a linear matrix equation of the form <spanclass="math display">\[    \frac{\mathrm{d}\mathbf{V}}{\mathrm{d}t}=\mathbf{Q}\mathbf{V}(t)+\mathbf{I}(t)\]</span> <span class="math display">\[    \mathbf{Q}_{\mu \nu}=-\frac{\delta^{\mu\nu}}{\tau_{\mu}}+\sum_{\nu&#39;}^{}\frac{\delta^{\nu\nu&#39;}}{\tau_{\mu\nu&#39;}}\]</span> <span class="math display">\[    \mathbf{I}_{\mu}(t)=\frac{1}{C_i^{\mu}}[I_{i}^{\mu}-\delta^{\mu1}\Omega_i(t)]\]</span> where <span class="math display">\[    \frac{1}{\tau_{\mu}}=\frac{1}{C_i^{\mu}}\left[ \sum_{\nu&#39;}^{}\frac{1}{r_i^{\mu\nu&#39;}}+\frac{1}{R^{\mu}_{T,i}}\right], \quad\frac{1}{\tau_{\mu\nu}}=\frac{1}{C_i^{\mu}r_i^{\mu\nu}}\]</span> (the sum runs over all neighbors of compartment <spanclass="math inline">\(\mu\)</span>)</p><p>The above equation may be solved as (a bit different from (6.46))</p><p><span class="math display">\[    V_i^{\mu}(t)=\sum_{\nu}^{} \int_{t_0}^{t}G_i^{\mu\nu}(t-s)I_{\nu}(s) \mathrm{d}s +\sum_{\nu}^{}G_i^{\mu\nu}(t-t_0)V_i^{\nu}(t_0) \quad t\geqslant t_0\]</span> (maybe the second sum stands for initial condition?) with<span class="math display">\[    G_i^{\mu\nu}(t)= [\mathrm{e}^{t \mathbf{Q}}] _{\mu\nu}\]</span></p><p>This coincides with the <spanclass="math inline">\(G_i^{\mu\nu}\)</span> in the textbook except for aconstant (maybe <span class="math inline">\(C_i^{\mu}\)</span> orsomething)</p><hr /><p>We consider a network made up of a set of neurons described by (6.45)and a simple threshold criterion for generating spikes. We assume thateach spike <span class="math inline">\(t_j^{(f)}\)</span> of apresynaptic neuron <span class="math inline">\(j\)</span> evokes, for<span class="math inline">\(t&gt;t_j^{(f)}\)</span>, a synaptic currentpulse <span class="math inline">\(\alpha(t-t_j^{(f)})\)</span> into thepostsynaptic neuron <span class="math inline">\(i\)</span>. The actualamplitude of the current of the current pulse depends on the strength<span class="math inline">\(W_{ij}\)</span> of the synapse that connectsneuron <span class="math inline">\(j\)</span> to neuron <spanclass="math inline">\(i\)</span>. The total input to compartment <spanclass="math inline">\(\mu\)</span> of neuron <spanclass="math inline">\(i\)</span> is thus <span class="math display">\[    I_i^{\mu}(t)=\sum_{j \in \Gamma_i^{\mu}}^{} W_{ij} \sum_{f}^{}\alpha(t-t_j^{(f)}).\]</span> (6.47)</p><p>Here, <span class="math inline">\(\Gamma_i^{\mu}\)</span> denotes theset of all neurons that have a synapse with compartment <spanclass="math inline">\(\mu\)</span> of neuron <spanclass="math inline">\(i\)</span>. The firing times of neuron <spanclass="math inline">\(j\)</span> are denoted by <spanclass="math inline">\(t_j^{(f)}\)</span>.</p><p>We assume that spikes are generated at the soma in the manner of theintegrate-and-fire model. The reset voltage is <spanclass="math inline">\(V_i^{1}=u_r&lt;\theta\)</span>. This is equivalentto a current pulse <span class="math display">\[    \gamma_i(s)=C_i^{1}(\theta-u_r)\delta(s),\]</span> (6.48)</p><p>so that the overall current due to the firing of action potentials atthe soma of neuron <span class="math inline">\(i\)</span> amounts to<span class="math display">\[    \Omega_i(t)=\sum_{f}^{} \gamma_i(t-t_i^{(f)}).\]</span> (6.49)</p><p>Using the above specializations for the synaptic input current andthe somatic reset current the membrane potential (6.46) of compartment<span class="math inline">\(\mu\)</span> in neuron <spanclass="math inline">\(i\)</span> can be written as <spanclass="math display">\[    V_i^{\mu}(t)=\sum_{f}^{} \eta_i^{\mu}(t-t_i^{(f)})+\sum_{\nu}^{}\sum_{j \in \Gamma_i^{\nu}}^{} W_{ij} \sum_{f}^{} \varepsilon_i^{\mu\nu}(t-t_j^{(f)}).\]</span> (6.50)</p><p>with <span class="math display">\[    \varepsilon_i^{\mu \nu}(s)=\frac{1}{C_i^{\nu}}\int_{0}^{\infty}G_i^{\mu \nu}(s&#39;)\alpha(s-s&#39;) \mathrm{d}s&#39;,\]</span> (6.51) <span class="math display">\[    \eta_i^{\mu}(s)=\frac{1}{C_i^{1}}\int_{0}^{\infty} G_i^{\mu1}(s&#39;)\gamma_i(s-s&#39;) \mathrm{d}s&#39;.\]</span> (6.52)</p><p>The kernel <span class="math inline">\(\varepsilon_i^{\mu\nu}(s)\)</span> describes the effect of a presynaptic action potentialarriving at compartment <span class="math inline">\(\nu\)</span> on themembrane potential of compartment <spanclass="math inline">\(\mu\)</span>. Similarly, <spanclass="math inline">\(\eta_i^{\mu}(s)\)</span> describes the response ofcompartment <span class="math inline">\(\mu\)</span> to an actionpotential generated at the soma.</p><p>The triggering of action potentials depends on the somatic membranepotential only. We define <spanclass="math inline">\(u_i=V_i^{1}\)</span>, <spanclass="math inline">\(\eta_i(s)=\eta_i^{1}(s)\)</span> and, for <spanclass="math inline">\(j \in \Gamma_i^{\nu}\)</span>, we set <spanclass="math inline">\(\varepsilon_{ij}=\varepsilon_i^{1\nu}\)</span>.This yields the equation of the SRM <span class="math display">\[    u_i(t)=\sum_{f}^{} \eta_i(t-t_i^{(f)})+\sum_{j}^{} W_{ij}\sum_{f}^{}\varepsilon_{ij}(t-t_j^{(f)}).\]</span> (6.53)</p><h4 id="example-two-compartment-integrate-and-fire-model">Example:Two-compartment integrate-and-fire model</h4><p>Two compartments are characterized by a somatic capacitance <spanclass="math inline">\(C^{1}\)</span> and a dendritic capacitance <spanclass="math inline">\(C^{2}=aC^{1}\)</span>. The membrane time constantis <span class="math inline">\(\tau_0=R^{1}C^{1}=R^{2}C^{2}\)</span> andthe longitudinal time constant <spanclass="math inline">\(\tau_{12}=r^{12}C^{1}C^{2}/(C^{1}+C^{2})\)</span>.The neuron fires, if <spanclass="math inline">\(V^{1}(t)=\theta\)</span>. After each firing thesomatic potential is reset to <span class="math inline">\(u_r\)</span>.This is equivalent to a current pulse <span class="math display">\[    \gamma(s)=q\delta(s),\tag{6.54}\]</span> where <spanclass="math inline">\(q=C^{1}[\theta-u_r]\)</span>. The dendritereceives spike trains from other neurons <spanclass="math inline">\(j\)</span> and we assume that each spike evokes acurrent pulse with time course <span class="math display">\[    \alpha(s)=\frac{1}{\tau_s}\exp \left(-\frac{s}{\tau_s}\right)\Theta(s).\]</span> (6.55)</p><p>With the Green's function we can calculate the response kernels <spanclass="math inline">\(\eta_0(s)=\eta_i^{(1)}\)</span> and <spanclass="math inline">\(\varepsilon_0(s)=\varepsilon_i^{12}\)</span> asdefined in (6.51) and (6.52).</p><hr /><p>Calculate <span class="math inline">\(\eta_0(s)\)</span> (omit thesubscript <span class="math inline">\(i\)</span>): <spanclass="math display">\[    \begin{bmatrix}    \frac{\mathrm{d}V^{1}}{\mathrm{d}t} \\    \frac{\mathrm{d}V^{2}}{\mathrm{d}t}    \end{bmatrix}    =    \begin{bmatrix}    -\frac{1}{C^{1}}\left(\frac{1}{\tau^{12}}+\frac{1}{R^{1}}\right)&amp; \frac{1}{C^{1}\tau^{12}} \\    \frac{1}{aC^{1}\tau^{12}} &amp;-\frac{1}{aC^{1}}\left(\frac{1}{\tau^{12}}+\frac{1}{R^{2}}\right)    \end{bmatrix}    \begin{bmatrix}    V^{1} \\    V^{2} \\    \end{bmatrix}    +    \begin{bmatrix}    -q[\theta-u_r]\delta(t) \\    \frac{1}{\tau_s}\exp \left(-\frac{s}{\tau_s}\right) \Theta(s) \\    \end{bmatrix}\]</span></p><p>which is the form of <span class="math inline">\(\displaystyle\frac{\mathrm{d}\mathbf{V}}{\mathrm{d}t}=\mathbf{Q}\mathbf{V}(t)+\mathbf{I}(t)\)</span>. <span class="math display">\[    \mathbf{Q}=    \begin{bmatrix}    -\frac{a}{(1+a)\tau^{12}}-\frac{1}{\tau_0} &amp;\frac{a}{(1+a)\tau^{12}} \\    \frac{1}{(1+a)\tau^{12}} &amp;-\frac{1}{(1+a)\tau^{12}}-\frac{1}{\tau_0} \\    \end{bmatrix}\]</span></p><p>The eigenvalues of <span class="math inline">\(\mathbf{Q}\)</span>are <span class="math inline">\(\displaystyle -\frac{1}{\tau_0}\)</span>and <span class="math inline">\(\displaystyle-\frac{1}{\tau_0}-\frac{1}{\tau^{12}}\)</span>. The correspondingeigenvectors are <span class="math inline">\((1,1)^{\mathsf{T}}\)</span>and <span class="math inline">\(\displaystyle (1,-\frac{1}{a})^{\mathsf{T}}\)</span>. <span class="math display">\[    G^{11}(t)=\left[\int_{0}^{t} \mathrm{e}^{(t-s)\mathbf{Q}} \delta(s)\mathrm{d}s\right]_{11}=[\mathrm{e}^{t\mathbf{Q}}]_{11}=\frac{1}{1+a}\exp\left(-\frac{s}{\tau_0}\right)\left[1+a\exp\left(-\frac{s}{\tau_{12}}\right)\right]\]</span></p><p>So <span class="math display">\[    \eta_0(s)=-\frac{1}{C^{1}}\int_{0}^{\infty}G^{11}(s&#39;)q\delta(s-s&#39;) \mathrm{d}s&#39;\]</span></p><p>Similarly, <span class="math display">\[    G^{12}(t)=\frac{a}{1+a}\exp\left(-\frac{t}{\tau_0}\right)\left[1-\exp\left(-\frac{t}{\tau^{12}}\right)\right]\]</span> So <span class="math display">\[    \varepsilon_0(s)=\varepsilon^{12}(s)=\frac{1}{C^{2}}\int_{0}^{\infty}G^{12}(s&#39;)\frac{1}{\tau_s}\exp\left(-\frac{s-s&#39;}{\tau_s}\right)\Theta(s-s&#39;) \mathrm{d}s&#39;\]</span></p><hr /><p>We find <span class="math display">\[    \eta_0(s)=-\frac{\theta-u_r}{(1+a)}\exp\left(-\frac{s}{\tau_0}\right)\left[1+a\exp\left(-\frac{s}{\tau_{12}}\right)\right],\]</span></p><p><span class="math display">\[    \varepsilon_0(s)=\frac{1}{(1+a)C^{1}}\exp\left(-\frac{s}{\tau_0}\right)\left[\frac{1-\mathrm{e}^{-\delta_1s}}{\tau_s\delta_1}-\exp\left(-\frac{s}{\tau_{12}}\right)\frac{1-\mathrm{e}^{-\delta_2s}}{\tau_s\delta_2}\right],\]</span> (6.56)</p><p>with <spanclass="math inline">\(\delta_1=\tau_s^{-1}-\tau_0^{-1}\)</span> and<spanclass="math inline">\(\delta_2=\tau_s^{-1}-\tau_0^{-1}-\tau_{12}^{-1}\)</span>.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x176.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x177.png" /></div></div></div><p>The kernel <span class="math inline">\(\varepsilon_0(s)\)</span>describes the voltage response of the soma to an input at the dendrite.It shows the typical time course of an excitatory or inhibitorypostsynaptic potential. The time course of the kernel <spanclass="math inline">\(\eta_0(s)\)</span> is a double exponential andreflects the dynamics of the reset in a two-compartment model.</p><h2 id="summary">Summary</h2><p>For the understanding of the Spike Response Model, we introduce</p><hr /><p>Spike-response model -Scholarpediahttp://www.scholarpedia.org/article/Spike-response_model#Leaky_Integrate-and-fire_model</p><hr /><hr /><p>Adaptive exponential integrate-and-fire model -Scholarpediahttp://www.scholarpedia.org/article/Adaptive_exponential_integrate-and-fire_model</p><hr /><h3 id="exercises">Exercises</h3><ol type="1"><li><strong>Timescale of firing rate decay</strong>. The characteristicfeature of adaptation is that, after the onset of a superthreshold stepcurrent, interspike-intervals become successively longer, or,equivalently, that the momentary firing rate drops. The aim is to make aquantitative prediction of the decay of the firing rate of a leakintegrate-and-fire model with a single adaptation current.</li></ol><ol type="a"><li>Show that the firing rate of (6.7) and (6.8) with constant <spanclass="math inline">\(I\)</span>, constant <spanclass="math inline">\(w\)</span> and <spanclass="math inline">\(a=0\)</span> is <span class="math display">\[f(I,w)=-\left[\tau_m \log\left(1-\frac{\theta_{rh}-u_{reset}}{R(I-w)}\right)\right]^{-1}.      \]</span> (6.57)</li><li>For each spike (i.e., once per interspike interval), <spanclass="math inline">\(w\)</span> jumps by an amount <spanclass="math inline">\(b\)</span>. Show that for <spanclass="math inline">\(I\)</span> constant and <spanclass="math inline">\(w\)</span> averaged over one interspike, (6.8)becomes <span class="math display">\[\tau_w \frac{\mathrm{d}w}{\mathrm{d}t}=-w+b\tau_w f(I,w).\]</span> (6.58)</li><li>At time <span class="math inline">\(t_0\)</span>, a strong currentof amplitude <span class="math inline">\(I_0\)</span> is switched onthat causes transiently a firing rate <span class="math inline">\(f\gg\tau_w\)</span>. Afterward the firing rate decays. Find the effectivetime constant of the firing rate for the case of strong input current.<strong>Solution</strong>: For a) I find that <spanclass="math display">\[f(I,w)=\left[ \tau \log \left(1+\frac{\tau_m(\theta_{rh}-u_{reset})}{R(I-w)-\tau_m(\theta_{rh}-u_{rest})}\right) \right]^{-1}\]</span> which must satisfy <span class="math display">\[\theta_{rh}-u_{rest}&lt;\frac{R(I-w)}{\tau_m}\]</span></li></ol><ol start="2" type="1"><li><strong>Subthreshold resonance</strong>. We study a leakyintegrate-and-fire model with a single adaptation variable <spanclass="math inline">\(w\)</span>.</li></ol><ol type="a"><li>Assume <span class="math inline">\(E_0=u_{rest}\)</span> and castequation (6.7) and (6.8) in the form of (6.27). Set <spanclass="math inline">\(\varepsilon=0\)</span> and calculate <spanclass="math inline">\(\eta\)</span> andn <spanclass="math inline">\(\kappa\)</span>. Show that <spanclass="math inline">\(\kappa(t)\)</span> can be written as a linearcombination <spanclass="math inline">\(\kappa(t)=k_{+}\mathrm{e}^{\lambda_{+}t}+k_{-}\mathrm{e}^{\lambda_{-}t}\)</span> with <spanclass="math display">\[\lambda_{\pm}=\frac{1}{2\tau_m \tau_w}(-(\tau_m+\tau_w)\pm\sqrt{\tau_m^{2}+\tau_w^{2}-2\tau_m\tau_w(1+2aR)})   \]</span> (6.59) and <span class="math display">\[k_{\pm}=\pm \frac{R(\lambda_{\pm}\tau_w+1)}{\tau_m\tau_w(\lambda_{+}-\lambda_{-})}\]</span> (6.60)</li><li>What are the parameters of (6.7),(6.8) that lead to oscillations in<span class="math inline">\(\kappa(t)\)</span>?</li><li>What is the frequency of the oscillation?</li><li>Take the Fourier transform of (6.7),(6.8) and find the function<span class="math inline">\(\hat{R}(w)\)</span> that relates the current<span class="math inline">\(\hat{I}(w)\)</span> at frequency <spanclass="math inline">\(\omega\)</span> to the voltage <spanclass="math inline">\(\hat{u}(w)\)</span> at the same frequency, i.e.,<span class="math inline">\(\hat{u}(w)=\hat{R}(w)\hat{I}(w)\)</span>.Show that, in the case where <span class="math inline">\(\kappa\)</span>has oscillations, the function <spanclass="math inline">\(\hat{R}(w)\)</span> has a global maximum. What isthe frequency where this happens? <strong>Solution</strong>: (6.7),(6.8)can be expressed as <span class="math display">\[\begin{bmatrix}\frac{\mathrm{d}u}{\mathrm{d}t} \\\frac{\mathrm{d}w}{\mathrm{d}t} \\\end{bmatrix}=\begin{bmatrix}-\frac{1}{\tau_m} &amp; -\frac{R}{\tau_m} \\\frac{a}{\tau_w} &amp; -\frac{1}{\tau_w} \\\end{bmatrix}\begin{bmatrix}u \\w \\\end{bmatrix}+\begin{bmatrix}\frac{E_0+RI(t)}{\tau_m} \\-\frac{aE_0}{\tau_w} \\\end{bmatrix}\]</span> Under the assumption that <spanclass="math inline">\(I(t)=\delta(t)\)</span>, <spanclass="math inline">\(E_0=0\)</span>, <spanclass="math inline">\(w(0)=0\)</span>, we get what we want. <spanclass="math inline">\(\tau_m^{2}+\tau_w^{2}-2\tau_m\tau_w(1+2aR)&lt;0\)</span>leads to oscillation of <span class="math inline">\(\kappa(t)\)</span>.The frequency is <span class="math display">\[\sqrt{-\tau_m^{2}-\tau_w^{2}+2\tau_m\tau_w(1+2aR)}/2\pi\]</span> For d), <span class="math display">\[\hat{R}(\omega)=\frac{(2\pi i\tau_w \omega+1)R}{-8\pi^{3}\tau_m\tau_w\omega^{2}+1+aR+2\pi i\omega(2\pi \tau_m+\tau_w)}\]</span> &gt; I give up! Ask 1 and 2.</li></ol><ol start="3" type="1"><li><strong>Integrate-and-fire model with slow adaptation</strong>. Theaim is to relate the leaky integrate-and-fire model with a singeladaptation variable, defined in (6.7) and (6.8) to the Spike ResponseModel in the form of (6.27). Adaptation is slow so that <spanclass="math inline">\(\tau_m/\tau_w=\delta\ll 1\)</span> and allcalculations can be done to first order in <spanclass="math inline">\(\delta\)</span>.</li></ol><ol type="a"><li>Show that the spike after-potential is given by <spanclass="math display">\[\eta(t)=\gamma_1\mathrm{e}^{\lambda_1t} +\gamma_2\mathrm{e}^{\lambda_2t}\]</span> (6.61) <span class="math display">\[\gamma_1=\Delta u (1-\delta-\delta a)-b(1+\delta)\]</span> (6.62) <span class="math display">\[\gamma_2=\Delta u -\gamma_1\]</span> (6.63)</li><li>Derive the input response kernel <spanclass="math inline">\(\kappa(s)\)</span>. &gt; The initial conditionsare <span class="math inline">\(\eta(0)=\Delta(u)\)</span> and <spanclass="math inline">\(w(0)=b\)</span> ? I cannot get the desiredresult.</li></ol><ol type="1"><li><strong>Integrate-and-fire model with time-dependent timeconstant</strong>. Since many channels are open immediately after aspike, the effective membrane time constant after a spike is smallerthan the time constant at rest. Consider an integrate-and-fire modelwith spike-time dependent time constant, i.e., with a membrane timeconstant <span class="math inline">\(\tau\)</span> that is a function ofthe time since the last postsynaptic spike, <spanclass="math display">\[\frac{\mathrm{d}u}{\mathrm{d}t}=-\frac{u}{\tau(t-\hat{t})}+\frac{1}{C}I^{ext}(t);\]</span> (6.64) As usual, <span class="math inline">\(\hat{t}\)</span>denotes the last firing time of the neuron. The neuron fires if <spanclass="math inline">\(u(t)\)</span> hits a fixed threshold <spanclass="math inline">\(\theta\)</span> and integration restarts with areset value <span class="math inline">\(u_r\)</span>.</li></ol><ol type="a"><li>Suppose that the time constant is <spanclass="math inline">\(\tau(t-\hat{t})=2\)</span> ms for <spanclass="math inline">\(t-\hat{t}&lt;10\)</span> ms and <spanclass="math inline">\(\tau(t-\hat{t})=20\)</span> ms for <spanclass="math inline">\(t-\hat{t}\geqslant 10\)</span> ms. Set <spanclass="math inline">\(u_r=-10\)</span> mV. Sketch the time course of themembrane potential for an input current <spanclass="math inline">\(I(t)=q\delta(t-t&#39;)\)</span> arriving at <spanclass="math inline">\(t&#39;=5\)</span> ms or <spanclass="math inline">\(t&#39;=15\)</span> ms. What are the differencesbetween the two cases?</li><li>Integrate (6.64) for arbitrary input with <spanclass="math inline">\(u(\hat{t})=u_r\)</span> as initial condition andinterpret the result.</li></ol><ol start="5" type="1"><li><strong>Spike-triggered adaptation currents</strong>. Consider aleaky integrate-and-fire model. A spike at time <spanclass="math inline">\(t^{(f)}\)</span> generates several adaptationcurrents <spanclass="math inline">\(\mathrm{d}w_k/\mathrm{d}t=-\frac{w_k}{\tau_k}+b_k\delta(t-t^{(f)})\)</span>with <span class="math inline">\(k=1,\cdots ,K\)</span>.</li></ol><ol type="a"><li>Calculate the effect of the adaptation current on the voltage.</li><li>Construct a combination of spike-triggered currents that couldgenerates slow adaptation.</li><li>Construct a combination of spike-triggered currents that couldgenerates bursts.</li></ol><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>P. C. Bressloff and J. G.Taylor (1994) Dynamics of compartmental model neurons. Neural Networks7, pp. 11531165.<a href="#fnref:1" rev="footnote" class="footnote-backref"></a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (5)</title>
    <link href="/2022/07/09/Neuronal-Dynamics-5/"/>
    <url>/2022/07/09/Neuronal-Dynamics-5/</url>
    
    <content type="html"><![CDATA[<h1 id="nonlinear-integrate-and-fire-models">NonlinearIntegrate-and-Fire Models</h1><p>The online version of this chapter:</p><hr /><p>Chapter 5 Nonlinear Integrate-and-Fire Modelshttps://neuronaldynamics.epfl.ch/online/Ch5.html</p><hr /><h2 id="thresholds-in-a-nonlinear-integrate-and-fire-model">Thresholdsin a nonlinear integrate-and-fire model</h2><p>In a general nonlinear integrate-and-fire model with a singlevariable <span class="math inline">\(u\)</span>, the membrane potentialevolves according to <span class="math display">\[    \tau \frac{\mathrm{d}}{\mathrm{d}t}u=f(u)+R(u)I.\]</span> (5.2)</p><p>The dynamics is stopped if <span class="math inline">\(u\)</span>reaches the threshold <spanclass="math inline">\(\theta_{reset}\)</span>. In this case the firingtime <span class="math inline">\(t^{(f)}\)</span> is noted andintegration of the membrane potential equation restarts at time <spanclass="math inline">\(t^{(f)}+\Delta^{abs}\)</span> with initialcondition <span class="math inline">\(u_{r}\)</span>. If not specifiedotherwise, we always assume a constant input resistance <spanclass="math inline">\(R(u)=R\)</span> independent of voltage.</p><h4 id="example-rescaling-and-standard-forms">Example: Rescaling andstandard forms</h4><p>Introduce a new variable <spanclass="math inline">\(\tilde{u}\)</span> by the transformation <spanclass="math display">\[    u(t) \to \tilde{u}(t)=\tau \int_{0}^{u(t)}\frac{\mathrm{d}x}{R(x)}         \]</span> which is possible if <span class="math inline">\(R(x)\neq0\)</span> for all <span class="math inline">\(x\)</span> in theintegration range. In terms of <spanclass="math inline">\(\tilde{u}\)</span> we have <spanclass="math display">\[    \frac{\mathrm{d}\tilde{u}}{\mathrm{d}t}=d(\tilde{u})+I(t)\]</span> with <spanclass="math inline">\(d(\tilde{u})=f(u)/R(u)\)</span>.</p><h3 id="where-is-the-firing-threshold">Where is the firingthreshold?</h3><p>The voltage threshold <span class="math inline">\(\theta\)</span>determined with pulse-like input currents is different from the voltagethreshold determined with prolonged step currents.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x136.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x137.png" /></div></div></div><p>The critical current for initiation of repetitive firing correspondsto the voltage where the stable fixed point disappears, or <spanclass="math inline">\(\theta_{rh}=I_{c}R\)</span>. In the experimentalliterature, the critical current <spanclass="math inline">\(I_c=\theta_{rh}/R\)</span> is called the'rheobase' current. In the mathematical literature, it is called thebifurcation point.</p><h2 id="exponential-integrate-and-fire-model">ExponentialIntegrate-and-Fire Model</h2><p>In the experimental integrate-and-fire model, the differentialequation for the membrane potential is given by <spanclass="math display">\[    \tau \frac{\mathrm{d}}{\mathrm{d}t}u=-(u-u_{rest})+\Delta_{T}\exp\left(\frac{u-\theta_{rh}}{\Delta_{T}}\right)+RI;\]</span> (5.6)</p><p>The first term describe the leak of a passive membrane. The secondterm is an exponential nonlinearity with 'sharpness' parameter <spanclass="math inline">\(\Delta_{T}\)</span> and 'threshold' <spanclass="math inline">\(\theta_{rh}\)</span>.</p><p>If the numerical threshold is chosen sufficiently high, <spanclass="math inline">\(\theta_{reset}\gg \theta+\Delta_{T}\)</span>, theupswing of the action potential for <span class="math inline">\(u\gg\theta+\Delta_{T}\)</span> is so rapid, that it goes to infinity in anincredibly short time, so the exact value of <spanclass="math inline">\(\theta_{reset}\)</span> does not play any role.<span class="math inline">\(\theta_{rh}\)</span> is the threshold foundwith constant (rheobase) current.</p><h3 id="extracting-the-nonlinearity-from-data">Extracting theNonlinearity from Data</h3><p>Why we choose an exponential nolinearity rather than any othernonlinear dependence? After rescaling with the time constant <spanclass="math inline">\(\tau\)</span>, the nonlinearity <spanclass="math inline">\(\tilde{f}(u)=-f(u)/\tau\)</span> is <spanclass="math display">\[    \tilde{f}(u(t))=\frac{1}{C}I(t)-\frac{\mathrm{d}}{\mathrm{d}t}u(t);\]</span></p><p>where <span class="math inline">\(C=\tau/R\)</span> can be intepretedas the capacity of the membrane.</p><p>The slope of the curve at the resting potential is related to themembrane time constant <span class="math inline">\(\tau\)</span> whilethe threshold parameter <span class="math inline">\(\theta_{rh}\)</span>is the voltage at which the function <spanclass="math inline">\(\tilde{f}\)</span> goes through its minimum.</p><h3 id="from-hodgkin-huxley-to-exponential-integrate-and-fire">FromHodgkin-Huxley to Exponential Integrate-and-Fire</h3><p>As long as we are only interested in the initiation phase of theaction potential we can assume a fixed value <spanclass="math inline">\(w=w_{rest}\)</span>.</p><p>For constant <span class="math inline">\(w\)</span>, <spanclass="math display">\[    \frac{\mathrm{d}u}{\mathrm{d}t}=F(u,w_{rest})+I=f(u)+I\]</span> It has three zero-crossings: the first one (left) at <spanclass="math inline">\(u_{rest}\)</span>, corresponding to a stable fixedpoint; a second one (middle) which acts as a threshold <spanclass="math inline">\(\theta\)</span>; and a third one to the right,which is again a stable fixed point and limits the upswing of the actionpotential. The value of the reset threshold <spanclass="math inline">\(\theta_{reset}&gt;\theta\)</span> must be chosenbetween the second and third fixed point.</p><p>Replace the downswing of the action potential in the nonlinearintegrate-and-fire model by an artificial reset of the voltage variableto a value <span class="math inline">\(u_{r}\)</span> whenever <spanclass="math inline">\(u\)</span> hits <spanclass="math inline">\(\theta_{reset}\)</span>.</p><h4 id="example-exponential-activation-of-sodium-channels">Example:Exponential Activation of Sodium Channels</h4><p><span class="math display">\[    C\frac{\mathrm{d}u}{\mathrm{d}t}=-g_{Na}[m_0(u)]^{3}h_{rest}(u-E_{Na})-g_{K}(n_{rest})^{4}(u-E_{K})-g_{L}(u-E_{L})+I,\]</span> (5.15) Potassium and leak currents can now be summed up to anew effective leak term <spanclass="math inline">\(g^{eff}(u-E^{eff})\)</span>. In the voltage rangeclose to the resting potential the driving force <spanclass="math inline">\((u-E_{Na})\)</span> of the sodium current can bewell approximated by <spanclass="math inline">\((u_{rest}-E_{Na})\)</span>. Then the onlyremaining nonlinearity on the right-hand-side of (5.15) arises from the<span class="math inline">\(m_0(u)\)</span>. For voltages around rest,<span class="math inline">\(m_0(u)\)</span> has an exponentialshape.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x144.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x145.png" /></div></div></div><h2 id="quadratic-integrate-and-fire">Quadratic Integrate and Fire</h2><p>A specific instance of a nonlinear integrate-and-fire model is thequadratic model, <span class="math display">\[    \tau\frac{\mathrm{d}}{\mathrm{d}t}u=a_0(u-u_{rest})(u-u_c)+RI,\]</span> (5.16)</p><p>with <span class="math inline">\(a_0&gt;0\)</span> and <spanclass="math inline">\(u_c&gt;u_{rest}\)</span>. The quadraticintegrate-and-fire model is closely related to the so-called <spanclass="math inline">\(\Theta-\)</span>neuron, a canonical type-I neuronmodel.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x146.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x147.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x148.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x149.png" /></div></div></div><h3 id="canonical-type-i-model">Canonical Type I model</h3><p>We'll show a one-to-one relation between the quadraticintegrate-and-fire model and the canonical type I phase model, <spanclass="math display">\[    \frac{\mathrm{d}\phi}{\mathrm{d}t}=[1-\cos \phi]+\Delta I[1+\cos\phi];\]</span> (5.17) Denote by <spanclass="math inline">\(I_{\theta}\)</span> the minimal current necessaryfor repetitive firing of the quadratic integrate-and-fire neuron. With asuitable shift of the voltage scale and constant current <spanclass="math inline">\(I=I_{\theta}+\Delta I\)</span> the equation of thequadratic neuron model can then by cast into the form <spanclass="math display">\[    \frac{\mathrm{d}u}{\mathrm{d}t}=u^{2}+\Delta I.     \tag{5.18}\]</span></p><p>For <span class="math inline">\(\Delta I&gt;0\)</span> the voltageincreases until it reaches the firing threshold <spanclass="math inline">\(\theta \gg 1\)</span> where it is reset to a value<span class="math inline">\(u_r\ll -1\)</span>.</p><p>By the transformation <span class="math display">\[    u(t)=\tan \left( \frac{\phi(t)}{2}\right). \tag{5.19}\]</span> The differential equation (5.18) can be transformed into(5.17). Thus (5.19) with <span class="math inline">\(\phi(t)\)</span>given by (5.17) is a solution to the differential equation of thequadratic integrate-and-fire neuron. The quadratic integrate-and-fireneuron is therefore (in the limit <span class="math inline">\(\theta \to\infty\)</span> and <span class="math inline">\(u_r \to-\infty\)</span>) equivalent to the genetic type I neuron (5.17).</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Information and Entropy (2)</title>
    <link href="/2022/07/08/Information-and-Entropy-2/"/>
    <url>/2022/07/08/Information-and-Entropy-2/</url>
    
    <content type="html"><![CDATA[<h1 id="communications">Communications</h1><h2 id="source-model">Source Model</h2><p>The source is assumed to produce symbols at a rate of <spanclass="math inline">\(R\)</span> symbols per second. The event of theselection of symbol <span class="math inline">\(i\)</span> will bedenoted <span class="math inline">\(A_i\)</span>.</p><p>Suppose that each event <span class="math inline">\(A_i\)</span> isrepresented by a different codeword <spanclass="math inline">\(C_i\)</span> with a length <spanclass="math inline">\(L_i\)</span>.</p><p>An important property of such codewords is that none can be the sameas the first portion of another, longer, codeword. A code that obeysthis property is called a <strong>prefix-condition code</strong>, orsometimes an <strong>instantaneous code</strong>.</p><h3 id="kraft-inequality">Kraft Inequality</h3><p>An important limitation on the distribution of code lengths <spanclass="math inline">\(L_i\)</span> was given by L.G.Kraft, which isknown as the Kraft inequality: <span class="math display">\[    \sum_{i}^{} \frac{1}{2^{L_i}}\leqslant 1 \tag{6.1}\]</span></p><p>Any valid set of distinct codewords obeys this inequality, andconversely for any proposed <span class="math inline">\(L_i\)</span>that obey it, a code can be found.</p><p>Proof: Let <span class="math inline">\(L_{max}\)</span> be the lengthof the longest codeword of a prefix-condition code. There are exactly<span class="math inline">\(2^{L_{max}}\)</span> different patterns of<span class="math inline">\(0\)</span> and <spanclass="math inline">\(1\)</span> of this length. Thus <spanclass="math display">\[    \sum_{i}^{} \frac{1}{2^{L_{max}}}=1 \tag{6.2}\]</span> where this sum is over these patterns. For each shortercodeword of length <span class="math inline">\(k(k&lt;L_{max})\)</span>there are exactly <span class="math inline">\(2^{L_{max}-k}\)</span>patterns that begin with this codeword, and none of those is a validcodeword. In the sum of (6.2) replace the terms corresponding to thosepatterns by a single term equal to <spanclass="math inline">\(1/2^{k}\)</span>. The sum is unchanged. Continuethis process with other short codewords. Some terms that are notcodewords are eliminated. Q.E.D.</p><h3 id="source-entropy">Source Entropy</h3><p>The uncertainty of the identity of the next symbol chosen <spanclass="math inline">\(H\)</span> is the average information gained whenthe next symbol is made known: <span class="math display">\[    H=\sum_{i} p(A_i)\log _{2}\left(\frac{1}{p(A_i)}\right) \tag{6.3}\]</span></p><p>This quantity is also known as the entropy of the source. Theinformation rate, in bits per second, is <spanclass="math inline">\(H\cdot R\)</span> where <spanclass="math inline">\(R\)</span> is the rate at which the source selectsthe symbols, measured in symbols per second.</p><h3 id="gibbs-inequality">Gibbs Inequality</h3><p>This inequality states that the entropy is smaller than or equal toany other average formed using the same probabilities but a differentbut a different function in the logarithm. Specifically, <spanclass="math display">\[    \sum_{i}^{} p(A_i)\log _{2}\left(\frac{1}{p(A_i)}\right)\leqslant\sum_{i}^{} p(A_i)\log _{2}\left(\frac{1}{p&#39;(A_i)}\right) \tag{6.4}\]</span> where <span class="math inline">\(p(A_i)\)</span> is anyprobability distribution and <spanclass="math inline">\(p&#39;(A_i)\)</span> is any other probabilitydistribution, namely, <span class="math display">\[    0\leqslant p&#39;(A_i)\leqslant 1 \tag{6.5}\]</span> and <span class="math display">\[    \sum_{i}^{} p&#39;(A_i)\leqslant 1. \tag{6.6}\]</span> As is true for all probability distributions, <spanclass="math display">\[    \sum_{i}^{} p(A_i)=1. \tag{6.7}\]</span> Proof: <span class="math display">\[    \begin{aligned}        \sum_{i}^{} p(A_i)\log_{2}\left(\frac{1}{p(A_i)}\right)-\sum_{i}^{} p(A_i)\log_{2}\left(\frac{1}{p&#39;(A_i)}\right)&amp;= \sum_{i}^{} p(A_i)\log_{2}\left(\frac{p&#39;(A_i)}{p(A_i)}\right)  \\        &amp;\leqslant \log _{2}\mathrm{e} \sum_{i}^{}p(A_i)\left[\frac{p&#39;(A_i)}{p(A_i)}-1\right] \\        &amp;= \log _{2} \mathrm{e} \left(\sum_{i}^{} p&#39;(A_i)-1\right) \\        &amp;\leqslant 0    \end{aligned}\]</span></p><h2 id="source-coding-theorem">Source Coding Theorem</h2><p>The codewords have an average length, in bits per symbol, <spanclass="math display">\[    L=\sum_{i}^{} p(A_i)L_i \tag{6.11}\]</span></p><p>The Source Coding Theorem states that the average information persymbol is always less than or equal to the average length of a codeword:<span class="math display">\[    H\leqslant L \tag{6.12}\]</span></p><p>This inequality is easy to prove using the Gibbs and Kraftinequalities. Use the Gibbs inequality with <spanclass="math inline">\(p&#39;(A_i)=1/2^{L_i}\)</span>. Thus <spanclass="math display">\[    \begin{aligned}        H&amp;=\sum_{i}^{} p(A_i)\log _{2}\left( \frac{1}{p(A_i)}\right)\\        &amp;\leqslant  \sum_{i}^{} p(A_i)\log_{2}\left(\frac{1}{p&#39;(A_i)}\right) \\        &amp;= \sum_{i}^{} p(A_i)\log _{2}2^{L_i}\\        &amp;= \sum_{i}^{} p(A_i)L_i \\        &amp;= L    \end{aligned}\]</span></p><p>The Source Coding Theorem can also be expressed in terms of rates oftransmission in bits per second by multiplying (6.12) by the symbols persecond <span class="math inline">\(R\)</span>: <spanclass="math display">\[    HR\leqslant LR \tag{6.14}\]</span></p><h2 id="channel-model">Channel Model</h2><p>If the channel perfectly changes its output state in conformance withits input state, it is said to be <strong>noiseless</strong> and in thatcase nothing affects the output except the input.</p><p>Suppose that the channel has a certain maximum rate <spanclass="math inline">\(W\)</span> at which its output can follow changesat the input.</p><p>The <strong>binary</strong> channel has two mutually exclusive inputstates.</p><p>The maximum rate at which information supplied to the input canaffect the output is called the <strong>channel capacity</strong> <spanclass="math inline">\(C=W \log _{2}n\)</span> bits per second. For thebinary channel, <span class="math inline">\(C=W\)</span>.</p><h2 id="noiseless-channel-theorem">Noiseless Channel Theorem</h2><p>It may be necessary to provide temporary storage buffers toaccommodate bursts of adjacent infrequently occurring symbols with longcodewords, and the symbols may not materialize at the output of thesystem at a uniform rate.</p><p>Also, to encode the symbols efficiently it may be necessary toconsider several of them together, in which case the first symbol wouldnot be available at the output until several symbols had been presentedat the input. Therefore high speed operation may lead to highlatency.</p><h2 id="noisy-channel">Noisy Channel</h2><p>For every possible input there may be more than one possible outputoutcome. Denote <strong>transition probabilities</strong> <spanclass="math inline">\(c_{ji}\)</span> the probability of the outputevent <span class="math inline">\(B_j\)</span> occuring when event <spanclass="math inline">\(A_i\)</span> happens. So <spanclass="math display">\[    0\leqslant c_{ji}\leqslant 1 \tag{6.15}\]</span> and <span class="math display">\[    1=\sum_{j}^{} c_{ji} \tag{6.16}\]</span> If the channel is noiseless, for each value of <spanclass="math inline">\(i\)</span> exactly one of the various <spanclass="math inline">\(c_{ji}\)</span> is equal to <spanclass="math inline">\(1\)</span> and all others are <spanclass="math inline">\(0\)</span>. <span class="math display">\[    p(B_j|A_i)=c_{ji} \tag{6.17}\]</span> The unconnditional probability of each output <spanclass="math inline">\(p(B_j)\)</span> is <span class="math display">\[    p(B_j)=\sum_{i}^{} c_{ji}p(A_i) \tag{6.18}\]</span> So by Bayes' Theorem: <span class="math display">\[    p(A_i|B_j)=\frac{p(A_i)c_{ji}}{p(B_j)} \tag{6.19}\]</span> The simplest noisy channel is the symmetric binary channel,for which there is a probability <spanclass="math inline">\(\varepsilon\)</span> of an error, so <spanclass="math display">\[    \begin{bmatrix}    c_{00} &amp; c_{01} \\    c_{10} &amp; c_{11} \\    \end{bmatrix}    =    \begin{bmatrix}    1-\varepsilon &amp; \varepsilon \\    \varepsilon &amp; 1-\varepsilon \\    \end{bmatrix}\]</span></p><p>Define the information that we have learned about the input as aresult of knowing the output as the <strong>mutualinformation</strong>.</p><p>Before we know the output, our uncertainty <spanclass="math inline">\(U_{before}\)</span> about the identity of theinput event is he entropy of the input: <span class="math display">\[    U_{before}=\sum_{i}^{} p(A_i)\log _{2}\left(\frac{1}{p(A_i)}\right)\tag{6.21}\]</span> After some particular output event <spanclass="math inline">\(B_j\)</span> has been observed, the residualuncertainty <span class="math inline">\(U_{after}(B_j)\)</span> aboutthe input event is: <span class="math display">\[    U_{after}(B_j)=\sum_{i}^{} p(A_i|B_j)\log_{2}\left(\frac{1}{p(A_i|B_j)}\right) \tag{6.22}\]</span></p><p>The mutual information <span class="math inline">\(M\)</span> isdefined as the average, over all outputs, of the amount so learned,<span class="math display">\[    M=U_{before}-\sum_{j}^{} p(B_j)U_{after}(B_j) \tag{6.23}\]</span> It is not difficult to prove that <spanclass="math inline">\(M\geqslant 0\)</span>. To prove this, the Gibbsinequality is used, for each <span class="math inline">\(j\)</span>:<span class="math display">\[    \begin{aligned}        U_{after}(B_j)&amp;=\sum_{i}^{} p(A_i|B_j)\log_{2}\left(\frac{1}{p(A_i|B_j)}\right) \\        &amp;\leqslant \sum_{i}^{} p(A_i|B_j)\log_{2}\left(\frac{1}{p(A_i)}\right)         \end{aligned}\]</span> (6.24) So <span class="math display">\[    \begin{aligned}        \sum_{j}^{} p(B_j)U_{after}(B_j) &amp;\leqslant \sum_{j}^{}p(B_j)\sum_{i}^{} p(A_i|B_j)\log _{2}\left(\frac{1}{p(A_i)}\right)  \\        &amp;= \sum_{ji}^{} p(B_j)p(A_i|B_j)\log_{2}\left(\frac{1}{p(A_i)}\right) \\        &amp;= \sum_{ij}^{} p(B_j|A_i)p(A_i)\log_{2}\left(\frac{1}{p(A_i)}\right) \\        &amp;=\sum_{i}^{} p(A_i)\log _{2}\left(\frac{1}{p(A_i)}\right)\\        &amp;=U_{before}              \end{aligned}\]</span> (6.25)</p><p>Substitution in (6.23) and simplification leads to <spanclass="math display">\[    M=\sum_{j}^{} \left(\sum_{i}^{} p(A_i)c_{ji}\right)\log_{2}\left(\frac{1}{\sum_{i}^{} p(A_i)c_{ji}}\right)-\sum_{ij}^{}p(A_i)c_{ji}\log _{2}\left(\frac{1}{c_{ji}}\right)\]</span> (6.26)</p><p>(6.26) was derived for the case where the input "causes" the output.However, such a cause-and-effect relationship is not necessary. The term<strong>mutual information</strong> suggests that it is just as valid toview the output as causing the input, or to ignore completely thequestion of what causes what. Two alternate formulas for <spanclass="math inline">\(M\)</span> shows that <spanclass="math inline">\(M\)</span> can be interpreted in either direction:<span class="math display">\[    \begin{aligned}        M &amp;= \sum_{i}^{} p(A_i)\log_{2}\left(\frac{1}{p(A_i)}\right)-\sum_{j}^{} p(B_j)\sum_{i}^{}p(A_i|B_j)\log _{2}\left(\frac{1}{p(A_i|B_j)}\right) \\        &amp;= \sum_{j}^{} p(B_j)\log_{2}\left(\frac{1}{p(B_j)}\right)-\sum_{i}^{} p(A_i)\sum_{j}^{}p(B_j|A_i)\log _{2}\left(\frac{1}{p(B_j|A_i)}\right)    \end{aligned}\]</span> (6.27)</p><p>So (6.26) is easily checked.</p><h2 id="noisy-channel-capacity-theorem">Noisy Channel CapacityTheorem</h2><p>It is more useful to define the channel capacity so that it dependsonly on the channel, so <span class="math inline">\(M_{max}\)</span>,the maximum mutual information that results from any possible inputprobability distribution, is used.</p><p>Generally speaking, going away from the symmetric case offers few ifany advantages in engineered systems, and in particular the fundamentallimits given by the theorems in this chapter cannot be evaded throughsuch techniques. Therefore the symmetric case gives the right intuitiveunderstanding.</p><p>The channel capacity is defined as <span class="math display">\[    C=M_{max}W \tag{6.29}\]</span> where <span class="math inline">\(W\)</span> is the maximumrate at which the output state can follow changes at the input. Thus<span class="math inline">\(C\)</span> is expressed in bits persecond.</p><p>The channel capacity theorem states that: if the input informationrate in bits per decond <span class="math inline">\(D\)</span> is lessthan <span class="math inline">\(C\)</span> then it is possible (perhapsby dealing with long sequences of inputs together) to code the data insuch a way that the error rate is as low as desired.</p><p>The proof is not a constructive proof. However, there is not yet anygeneral theory of how to design codes from scratch.</p><h2 id="reversibility">Reversibility</h2><p>Some Boolean operations had the property that the input could not bededuced from the output. The <span class="math inline">\(AND\)</span>and <span class="math inline">\(OR\)</span> gates are examples. Otheroperations were reversiblethe <spanclass="math inline">\(EXOR\)</span> gate, when the output is augmentedby one of the two inputs, is an example.</p><h2 id="detail-communication-system-requirements">Detail: CommunicationSystem Requirements</h2><p>The systems are characterized by four measures: throughput, latency,tolerance of errors, and tolerance to nonuniform rate (bursts).Throghput is simply the number of bits per second that such a systemshould, to be successful, accommodate. Latency is the time delay of themessage; it could be defined either as the delay of the start of theoutput after the source begins, or a similar quantity about the end ofthe message (or, for that matter, about any particular features in themessage). The numbers for throughput, in MB (megabytes) or kb (kilobits)are approximate.</p><hr /><p>Channel capacityhttp://web.archive.org/web/20080126223204/http://www.cs.ucl.ac.uk/staff/S.Bhatti/D51-notes/node31.html</p><hr /><h1 id="processes">Processes</h1><p>We know the model of a communication system: - Input (Symbols) -Source Encoder - Compressor - Channel Encoder - Channel - ChannelDecoder - Expander - Source Decoder - Output(Symbols)</p><p>Because each of these steps processes information in some way, it iscalled a <strong>processor</strong> and what it does is called a<strong>process</strong>. The processes we consider here are -<strong>Discrete:</strong> The inputs are members of a set of mutuallyexclusive possibilities, only one of which occurs at a time, and theoutput is one of another discrete set of mutually exclusive events. -<strong>Finite:</strong> The set of possible inputs is finite in number,as is the set of possible outputs. - <strong>Memoryless:</strong> Theprocess acts on the input at some time and produces an output based onthat input, ignoring any prior inputs. -<strong>Nondeterministic:</strong> The process may produce a differentoutput when presented with the same input a second time (the model isalso valid for deterministic processes). Because the process isnondeterministic the output may contain random <strong>noise</strong>. -<strong>Lossy:</strong> It may not be possible to "see" the input fromthe output, i.e., determine the input by observing the output. Suchprocesses are called <strong>lossy</strong> because knowledge about theinput is lost when the output is created (the model is also valid forlossless processes).</p><h2 id="types-of-process-diagrams">Types of Process Diagrams</h2><p>We may use <strong>block diagram</strong>, <strong>circuitdiagram</strong>, <strong>probability diagram</strong> and<strong>information diagram</strong>.</p><h2 id="probability-diagrams">Probability Diagrams</h2><p>We suppose the probability model of a process with <spanclass="math inline">\(n\)</span> inputs and <spanclass="math inline">\(m\)</span> outputs. The <spanclass="math inline">\(n\)</span> inputs are mutually exclusive, as arethe <span class="math inline">\(m\)</span> output states.</p><p>For each <span class="math inline">\(i\)</span> denote theprobability that this input leads to the output <spanclass="math inline">\(j\)</span> as <spanclass="math inline">\(c_{ji}\)</span>. Denote the event associated withthe selection of input <span class="math inline">\(i\)</span> as <spanclass="math inline">\(A_i\)</span> and the event associated with output<span class="math inline">\(j\)</span> as <spanclass="math inline">\(B_j\)</span>.</p><h3 id="example-and-gates">Example: AND Gates</h3><p>The <span class="math inline">\(AND\)</span> gate is deterministicbut is lossy.</p><h3 id="example-binary-channel">Example: Binary Channel</h3><p>Symmetric Binary Channel (SBC), symmetric in the sense that theerrors in the two directions are equally likely.</p><p>It is possible for processes to introduce noise but have no loss, orvice versa.</p><p>Loss of information happens because it is no longer possible to tellwith certainty what the input signal is, when the output is observed.Loss shows up in probability diagram where two or more paths converge onthe same output.</p><p>Noise happens because the output is not determined precisely by theinput. Noise shows up in probability diagram where two or more pathsdiverge from the same input.</p><h2 id="information-loss-and-noise">Information, Loss, and Noise</h2><p>We now return to our model of a general discrete memorylessnondeterministic lossy process, and derive formulas for noise, loss, andinformation transfer.</p><p>The information at the input <span class="math inline">\(I\)</span>is the same as the entropy of this source. <span class="math display">\[    I=\sum_{i}^{} p(A_i)\log _{2}\left(\frac{1}{p(A_i)}\right)\]</span> The output information <span class="math inline">\(J\)</span>can also be expressed in terms of the input probability distribution andthe channel transition matrix: <span class="math display">\[    \begin{aligned}        J &amp;= \sum_{j}^{} p(B_j)\log_{2}\left(\frac{1}{p(B_j)}\right) \\        &amp;= \sum_{j}^{} \left( \sum_{i}^{} c_{ji}p(A_i)\right)\log_{2}\left(\frac{1}{\sum_{i}^{} c_{ji}p(A_i)}\right)    \end{aligned}\]</span></p><p>Note that this measure of information at the output <spanclass="math inline">\(J\)</span> refers to the identity of the outputstate, not the input state. If we've got an output state <spanclass="math inline">\(B_j\)</span>, then the uncertainty of ourknowledge of the input state is <span class="math display">\[    \sum_{i}^{} p(A_i|B_j)\log _{2}\left(\frac{1}{p(A_i|B_j)}\right)\]</span> So the average uncertainty about the input after learning theoutput is <span class="math display">\[    \begin{aligned}        L&amp;=\sum_{j}^{} p(B_j)\sum_{i}^{} p(A_i|B_j)\log_{2}\left(\frac{1}{p(A_i|B_j)}\right) \\        &amp;=\sum_{ij}^{} p(A_i,B_j)\log_{2}\left(\frac{1}{p(A_i|B_j)}\right)    \end{aligned}\]</span></p><p>We have denoted this average uncertainty by <spanclass="math inline">\(L\)</span> and will call it "loss." In the specialcase that the process allows the input state to be identified uniquelyfor each possible output state, the process is "lossless" and <spanclass="math inline">\(L=0\)</span>.</p><p>Denote <span class="math inline">\(M=I-L\)</span> the "mutualinformation". This is an important quantity because it is the amount ofinformation tha gets through the process.</p><p>Some processes have loss but are deterministic. An example is the<span class="math inline">\(AND\)</span> logic gate.</p><p>There is a quantity similar to <span class="math inline">\(L\)</span>that characterizes a nondeterministic process, whether or not it hasloss. Define the noise <span class="math inline">\(N\)</span> of aprocess as the uncertainty in the output, given the input state,averaged over all input states. <span class="math display">\[    \begin{aligned}        N &amp;= \sum_{i}^{} p(A_i)\sum_{j}^{} p(B_j|A_i)\log_{2}\left(\frac{1}{p(B_j|A_i)}\right) \\        &amp;= \sum_{i}^{} p(A_i)\sum_{j}^{} c_{ji}\log_{2}\left(\frac{1}{c_{ji}}\right)    \end{aligned}\]</span></p><p>What may not be obvious, but can be proven easily, is that the mutualinformation <span class="math inline">\(M\)</span> plays exactly thesame sort of role for noise as it does for loss. Since <spanclass="math display">\[    J=\sum_{i}^{} p(B_j)\log _{2}\left(\frac{1}{p(B_j)}\right)\]</span> we have <span class="math display">\[    M=J-N \tag{7.24}\]</span> It follows from these results that <spanclass="math display">\[    J-I=N-L \tag{7.27}\]</span></p><h3 id="example-symmetric-binary-channel">Example: Symmetric BinaryChannel</h3><p>For the SBC with bit error probability <spanclass="math inline">\(\varepsilon\)</span>, these formulas can beevaluated, even if the two input probabilities <spanclass="math inline">\(p(A_0)\)</span> and <spanclass="math inline">\(p(A_1)\)</span> are not equal. If they happen tobe equal (each 0.5), then <span class="math display">\[    I=1\ \text{bit}\]</span> <span class="math display">\[    J=1\ \text{bit}\]</span> <span class="math display">\[    L=N=\varepsilon \log_{2}\left(\frac{1}{\varepsilon}\right)+(1-\varepsilon) \log_{2}\left(\frac{1}{1-\varepsilon}\right)\]</span> <span class="math display">\[    M=1-\varepsilon \log_{2}\left(\frac{1}{\varepsilon}\right)-(1-\varepsilon)\log_{2}\left(\frac{1}{1-\varepsilon}\right)\]</span></p><h2 id="deterministic-examples">Deterministic Examples</h2><p>p88-89</p><h3 id="error-correcting-example">Error Correcting Example</h3><p>p89-90</p><h2 id="capacity">Capacity</h2><p>Call <span class="math inline">\(W\)</span> the maximum rate at whichthe input state of the process can be detected at the output. Then therate at which information flows through the process can be as large as<span class="math inline">\(WM\)</span>. However, this product is not aproperty of the process itself, but on how it is used. The<strong>process capacity</strong> <span class="math inline">\(C\)</span>is defined as <span class="math display">\[    C=WM_{max} \tag{7.32}\]</span></p><h2 id="information-diagrams">Information Diagrams</h2><p>p91</p><h2 id="cascaded-processes">Cascaded Processes</h2><p><img src="/img/inf_and_ent/2022-07-11-17-42-53.png" /></p><p>Consider two processes in <strong>cascade</strong>. This term refersto having the output from one process serve as the input to anotherprocess.</p><p>The matrix of transition probabilities is merely the matrix productof the two transition probability matrices for process 1 and process2.</p><p>Now we seek formulas for <span class="math inline">\(I,J,L,N\)</span>and <span class="math inline">\(M\)</span> of the overall process interms of the corresponding quantities for the component processes.</p><p><span class="math inline">\(I=I_1\)</span> and <spanclass="math inline">\(J=J_2\)</span>. <spanclass="math inline">\(L\)</span> and <spanclass="math inline">\(N\)</span> cannot generally be found exactly from<span class="math inline">\(L_1,L_2,N_1\)</span> and <spanclass="math inline">\(N_2\)</span>, it is possible to find upper andlower bounds for them. <span class="math display">\[    L-N=(L_1+L_2)-(N_1+N_2)\tag{7.33}\]</span> The loss <span class="math inline">\(L\)</span> for theoverall process is not always equal to the sum of the losses for the twocomponents <span class="math inline">\(L_1+L_2\)</span>, but instead<span class="math display">\[    0\leqslant L_1\leqslant L\leqslant L_1+L_2 \tag{7.34}\]</span> so that the loss is bound from above and below. Also, <spanclass="math display">\[    L_1+L_2-N_1\leqslant L\leqslant L_1+L_2 \tag{7.35}\]</span> so that if the first process is noise-free then <spanclass="math inline">\(L\)</span> is exactly <spanclass="math inline">\(L_1+L_2\)</span>.</p><p>There are similar formulas for <span class="math inline">\(N\)</span>in terms of <span class="math inline">\(N_1+N_2\)</span>: <spanclass="math display">\[    0\leqslant N_2\leqslant N\leqslant N_1+N_2 \tag{7.36}\]</span> <span class="math display">\[    N_1+N_2-L_2\leqslant N\leqslant N_1+N_2 \tag{7.37}\]</span> Similar formulas for the mutual information of the cascade<span class="math inline">\(M\)</span> follow from these results: <spanclass="math display">\[    M_1-L_2\leqslant M\leqslant M_1\leqslant I \tag{7.38}\]</span> <span class="math display">\[    M_1-L_2\leqslant M\leqslant M_1+N_1-L_2 \tag{7.39}\]</span> <span class="math display">\[    M_2-N_1\leqslant M\leqslant M_2\leqslant J \tag{7.40}\]</span> <span class="math display">\[    M_2-N_1\leqslant M\leqslant M_2+L_2-N_1 \tag{7.41}\]</span> Other formulas for <span class="math inline">\(M\)</span> areeasily derived from <span class="math inline">\(0\leqslant M\leqslantI\)</span> applied to the first process and the cascade, and <spanclass="math inline">\(M=J-N\)</span> applied to the second process andthe cascade: <span class="math display">\[    \begin{aligned}        M &amp;= M_1+L_1-L \\        &amp;=M_1+N_1+N_2-N-L_2 \\        &amp;=M_2+N_2-N \\        &amp;=M_2+L_2+L_1-L-N_1         \end{aligned}\]</span> where the second formula in each case comes from the use of(7.33).</p><p><span class="math inline">\(M\)</span> cannont exceed either <spanclass="math inline">\(M_1\)</span> or <spanclass="math inline">\(M_2\)</span>. If the second process is lossless,<span class="math inline">\(L_2=0\)</span> and then <spanclass="math inline">\(M=M_1\)</span>. Similarly if the first process isnoiseless, then <span class="math inline">\(N_1=0\)</span> and <spanclass="math inline">\(M=M_2\)</span>.</p><p>The channel capacity <span class="math inline">\(C\)</span> of thecascade satisfies <span class="math inline">\(C\leqslant C_1\)</span>and <span class="math inline">\(C\leqslant C_2\)</span>. However, otherresults relating the channel capacities are not a trivial consequence ofthe formulas above.</p><h1 id="inference">Inference</h1><h2 id="estimation">Estimation</h2><p>We now try to determine the input event when the output has beenobserved. This is the case for communication systems and memorysystems.</p><p>The conditional output probabilities <spanclass="math inline">\(c_{ji}\)</span> are a property of the process, anddo not depend on the input probabilities <spanclass="math inline">\(p(A_i)\)</span>.</p><p>The unconditional probability <spanclass="math inline">\(p(B_j)\)</span> of each output event <spanclass="math inline">\(B_j\)</span> is <span class="math display">\[    p(B_j)=\sum_{i}^{} c_{ji}p(A_i)  \tag{8.1}\]</span> and <span class="math display">\[    p(A_i,B_j)=p(A_i)c_{ji} \tag{8.2}       \]</span> so <span class="math display">\[    p(A_i|B_j)=\frac{p(A_i)c_{ji}}{p(B_j)} \tag{8.3}\]</span> If the process has no loss (<spanclass="math inline">\(L=0\)</span>) then for each <spanclass="math inline">\(j\)</span> exactly one of the input events <spanclass="math inline">\(A_i\)</span> has nonzero probability, andtherefore its probability <spanclass="math inline">\(p(A_i|B_j)\)</span> is <spanclass="math inline">\(1\)</span>.</p><p>We know <span class="math display">\[    U_{before}=\sum_{i}^{} p(A_i)\log _{2}\left(\frac{1}{p(A_i)}\right)\]</span> and the residual uncertainty after some particular outputevent is <span class="math display">\[    U_{after}(B_j)=\sum_{i}^{} p(A_i|B_j)\log_{2}\left(\frac{1}{p(A_i|B_j)}\right)\]</span></p><p>The question is whether <spanclass="math inline">\(U_{after}(B_j)\leqslant U_{before}\)</span>. Theanswer is often, but not always, yes.</p><p><strong>On average</strong>, out uncertainty about the input state isnever increased by learning something about the output state.</p><h3 id="non-symmetric-binary-channel">Non-symmetric Binary Channel</h3><p>For a rare family genetic disease,</p><table><thead><tr class="header"><th></th><th><span class="math inline">\(p(A)\)</span></th><th><span class="math inline">\(p(B)\)</span></th><th><span class="math inline">\(I\)</span></th><th><span class="math inline">\(L\)</span></th><th><span class="math inline">\(M\)</span></th><th><span class="math inline">\(N\)</span></th><th><span class="math inline">\(J\)</span></th></tr></thead><tbody><tr class="odd"><td>Family history</td><td>0.5</td><td>0.5</td><td>1.00000</td><td>0.11119</td><td>0.88881</td><td>0.11112</td><td>0.99993</td></tr><tr class="even"><td>Unknown Family history</td><td>0.9995</td><td>0.0005</td><td>0.00620</td><td>0.00346</td><td>0.00274</td><td>0.14141</td><td>0.14416</td></tr></tbody></table><h3 id="inference-strategy">Inference Strategy</h3><p>One simple strategy for inference is "maximum likelihood". However,sometimes it does not work at all (rare family genetic diseasetest).</p><h2 id="principle-of-maximum-entropy-simple-form">Principle of MaximumEntropy: Simple Form</h2><p>Before the Principle of Maximum Entropy can be used the problemdomain needs to be set up. It is not assumed in this step whichparticular state the system is in (or which state is actually"occupied"); indeed it is assumed that we do not know and cannot knowthis with certainty, and so we deal instead with the probability of eachof the states being occupied.</p><h3 id="bergers-burgers">Berger's Burgers</h3><p>The Principle of Maximum Entropy will be introduced by means of anexample. A fast-food restaurant, Berger's Burgers, offers three meals:burger, chicken, and fish. The price, Calorie count, and probability ofeach meal being delivered cold are as listed below.</p><table><thead><tr class="header"><th>Item</th><th>Entree</th><th>Cost</th><th>Calories</th><th>Probability of arriving hot</th><th>probability of arriving cold</th></tr></thead><tbody><tr class="odd"><td>Value Meal 1</td><td>Burger</td><td>1.00</td><td>1000</td><td>0.5</td><td>0.5</td></tr><tr class="even"><td>Value Meal 2</td><td>Chicken</td><td>2.00</td><td>600</td><td>0.8</td><td>0.2</td></tr><tr class="odd"><td>Value Meal 3</td><td>Fish</td><td>3.00</td><td>400</td><td>0.9</td><td>0.1</td></tr></tbody></table><h3 id="probabilities">Probabilities</h3><p>If we do not know the outcome we may still have some knowledge, andwe use probabilities to express this knowledge.</p><h3 id="entropy">Entropy</h3><p>Our uncertainty is expressed as <span class="math display">\[    S=p(B)\log _{2}\left(\frac{1}{p(B)}\right)+p(C)\log_{2}\left(\frac{1}{p(C)}\right)+p(F)\log _{2}\left(\frac{1}{p(F)}\right)\]</span> In the context of physical systems this uncertainty is knownas the entropy. In communication systems the uncertainty regarding whichactual message is to be transmitted is also known as the entropy of thesource.</p><p>In general the entropy, because it is expressed in terms ofprobabilities, depends on the observer. One person may have differentknowledge of the system from another, and therefore would calculate adifferent numerical value for entropy.</p><p>The Principle of Maximum Entropy is used to discover the probabilitydistribution which leads to the highest value for this uncertainty,thereby assuring that no information is inadvertently assumed. Theresulting probability distribution is not observer-dependent.</p><h3 id="constraints">Constraints</h3><p>If we have additional information then we ought to be able to find aprobability distribution that is better in the sense that it has lessuncertainty.</p><p>We consider we know the expected value of some quantity (thePrinciple of Maximum Entropy can handle multiple constraints but themathematical procedures and formulas become more complicated) If thereis an attribute for which each of the states has a value <spanclass="math inline">\(g(A_i)\)</span> and for which we know the actualvalue <span class="math inline">\(G\)</span>, then we should consideronly those probability distributions for which the expected value isequal to <span class="math inline">\(G\)</span>.</p><p><span class="math display">\[    G=\sum_{i}^{} p(A_i)g(A_i) \tag{8.15}\]</span></p><p>In the case that the average cost is 1.75, <spanclass="math inline">\(S\)</span> attains its maximum when <spanclass="math inline">\(p(B)=0.466\)</span>, <spanclass="math inline">\(p(C)=0.318\)</span>, <spanclass="math inline">\(p(C)=0.318\)</span>, and <spanclass="math inline">\(S=1.517\)</span> bits.</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (4)</title>
    <link href="/2022/07/04/Neuronal-Dynamics-4/"/>
    <url>/2022/07/04/Neuronal-Dynamics-4/</url>
    
    <content type="html"><![CDATA[<h1id="dimensionality-reduction-and-phase-plane-analysis">DimensionalityReduction and Phase Plane Analysis</h1><p>The online version of this chapter:</p><hr /><p>Chapter 4 Dimensionality Reduction and Phase Plane Analysishttps://neuronaldynamics.epfl.ch/online/Ch4.html</p><hr /><h2 id="threshold-effects">Threshold effects</h2><p>In this section we use current pulses ans steps in order to explorethe threshold behavior of the Hodgkin-Huxley model.</p><h3 id="pulse-input">Pulse Input</h3><p>The threshold depends on the stimulation protocol.</p><h3 id="step-current-input">Step Current Input</h3><p>We study the response of the Hodgkin-Huxley model to a step currentof the form <span class="math display">\[    I(t)=I_1+\Delta I \mathscr{H}(t)\]</span> where <span class="math inline">\(\mathscr{H}\)</span> denotesthe Heaviside step function.</p><p><img src="/img/neu_dyn/x87.png" /></p><p>When probing with step currents, there is neither a unique currentthreshold for spike initiation nor for repetitive firing. The triggermechanism for action potentials depends not only on <spanclass="math inline">\(I_2\)</span> but also on the size of the currentstep <span class="math inline">\(\Delta I\)</span>.</p><p>Biologically, the dependence upon the step size arises from thedifferent time constants of activation and inactivation of the ionchannels. We'll see it below.</p><h2 id="reduction-to-two-dimensions">Reduction to two dimensions</h2><h3 id="general-approach">General approach</h3><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x26.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x27.png" /></div></div></div><p>Note that the time scale of <span class="math inline">\(m\)</span> ismuch faster than <span class="math inline">\(n\)</span> and <spanclass="math inline">\(h\)</span>. Moreover, <spanclass="math inline">\(m\)</span> is fast compared to the membrane timeconstant <span class="math inline">\(\tau=C/g_{L}\)</span> of a passivemembrane, which characterizes the evolution of the voltage <spanclass="math inline">\(u\)</span> when all channels are closed. So we maytreat <span class="math inline">\(m\)</span> as an instantaneousvariable, therefore it can be replaced by its steady-state value, <spanclass="math inline">\(m(t) \to m_0[u(t)]\)</span>. (<strong>quasi steadystate approximation</strong>)</p><p>Note that the time constants <spanclass="math inline">\(\tau_n(u)\)</span> and <spanclass="math inline">\(\tau_{h}(u)\)</span> have similar dynamics overthe voltage <span class="math inline">\(u\)</span>. Moreover, the graphsof <span class="math inline">\(n_0(u)\)</span> and <spanclass="math inline">\(1-h_0(u)\)</span> are also similar.</p><p>We use a linear approximation <spanclass="math inline">\((b-h)\thickapprox an\)</span> with some constants<span class="math inline">\(a,b\)</span> and set <spanclass="math inline">\(w=b-h=an\)</span>. With <spanclass="math inline">\(h=b-w\)</span>, <spanclass="math inline">\(n=w/a\)</span>, and <spanclass="math inline">\(m=m_0(u)\)</span>, equations (2.4)-(2.5) become<span class="math display">\[    C\frac{\mathrm{d}u}{\mathrm{d}t}=-g_{Na}[m_0(u)]^{3}(b-w)(u-E_{Na})-g_{K}(\frac{w}{a})^{4}(u-E_{K})-g_{L}(u-E_{L})+I,\tag{4.3}\]</span> or <span class="math display">\[    \frac{\mathrm{d}u}{\mathrm{d}t}=\frac{1}{\tau}[F(u,w)+RI], \tag{4.4}\]</span> with <span class="math inline">\(R=g_{L}^{-1}\)</span>, <spanclass="math inline">\(\tau=RC\)</span> and some function <spanclass="math inline">\(F\)</span>. We also have <spanclass="math display">\[    \frac{\mathrm{d}w}{\mathrm{d}t}=\frac{1}{\tau_{w}}G(u,w), \tag{4.5}\]</span></p><h4 id="example-morris-lecar-model">Example: Morris-Lecar model</h4><p>The Morris-Lecar equations read <span class="math display">\[    C\frac{\mathrm{d}u}{\mathrm{d}t}=-g_1 \hat{m}_0(u)(u-V_1)-g_2\hat{w}(u-V_2)-g_{L}(u-V_{L})+I, \tag{4.6}\]</span> <span class="math display">\[    \frac{\mathrm{d}\hat{w}}{\mathrm{d}t}=-\frac{1}{\tau(u)}[\hat{w}-w_0(u)].\tag{4.7}\]</span></p><p>(4.6) doesn't have the factor <spanclass="math inline">\((b-w)\)</span> which closes the channel for highvoltage. Another difference is that neither <spanclass="math inline">\(\hat{m}_0\)</span> nor <spanclass="math inline">\(\hat{w}\)</span> have exponents. In the followingwe consider (4.6) and (4.7) as a model in its own right and drop thehats over <span class="math inline">\(m_0\)</span> and <spanclass="math inline">\(w\)</span>.</p><p>It's reasonable to approximate the voltage dependence by <spanclass="math display">\[    m_0(u)=\frac{1}{2}\left[ 1+ \tanh \left(\frac{u-u_1}{u_2}\right)\right] \tag{4.8}\]</span> <span class="math display">\[    w_0(u)=\frac{1}{2}\left[ 1+ \tanh \left(\frac{u-u_3}{u_4}\right)\right] \tag{4.9}\]</span> with parameters <span class="math inline">\(u_1,\cdots,u_4\)</span>, and to approximate the time constant by <spanclass="math display">\[    \tau(u)=\frac{\tau_{w}}{\cosh(\frac{u-u_3}{2u_4})} \tag{4.10}\]</span> with a further parameter <spanclass="math inline">\(\tau_{w}\)</span>.</p><p><strong>Example: FitzHugh-Nagumo model</strong></p><p>FitzHugh and Nagumo obtained sharp pulse-like oscillationsreminiscent of trains of action potentials by defining the function<span class="math inline">\(F(u,w)\)</span> and <spanclass="math inline">\(G(u,w)\)</span> as<br /><span class="math display">\[    \begin{cases}        F(u,w)=u-\frac{1}{3}u^{3}-w, \\        G(u,w)=b_0+b_1u-w,    \end{cases}    \tag{4.11}\]</span></p><p>where <span class="math inline">\(u\)</span> is the membrane voltageand <span class="math inline">\(w\)</span> is a recovery variable.</p><h3 id="mathematical-steps">Mathematical steps</h3><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x92.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x93.png" /></div></div></div><p>The overall aim of the approach is to replace the variables <spanclass="math inline">\(n\)</span> and <spanclass="math inline">\(h\)</span> in the Hodgkin-Huxley model by a singleeffective variable <span class="math inline">\(w\)</span>. During anaction potential, the variables <spanclass="math inline">\(n(t)\)</span> and <spanclass="math inline">\(h(t)\)</span> stay close to a straight line.</p><p>A minimal condition for the projection is that the approximation isperfect while the neuron is at rest. First, we introduces new variables<span class="math display">\[    x=n-n_0(u_{rest}) \tag{4.12}\]</span> <span class="math display">\[    y=h-h_0(u_{rest}). \tag{4.13}\]</span> At rest, we have <spanclass="math inline">\(x=y=0\)</span>.</p><p>Second, the points <spanclass="math inline">\((n_0(u),h_0(u))\)</span> as a function of <spanclass="math inline">\(u\)</span> define a curve. The slope of the curveat <span class="math inline">\(u=u_{rest}\)</span> yields the rotationangle <span class="math inline">\(\alpha\)</span> via <spanclass="math display">\[    \tan \alpha=\frac{ \frac{\mathrm{d}h_0}{\mathrm{d}u}\big|_{u_{rest}}}{ \frac{\mathrm{d}n_0}{\mathrm{d}u}\big |_{u_{rest}}}\]</span> Rotate the coordinates <span class="math display">\[    \begin{pmatrix}    z_1 \\    z_2 \\    \end{pmatrix}    =    \begin{pmatrix}    \cos \alpha &amp; \sin \alpha \\    -\sin \alpha &amp; \cos \alpha \\    \end{pmatrix}    \begin{pmatrix}    x \\    y \\    \end{pmatrix}.\]</span></p><p><img src="/img/neu_dyn/x94.png" /></p><p>The inverse transform <span class="math display">\[    \begin{pmatrix}    x \\    y \\    \end{pmatrix}    =    \begin{pmatrix}    \cos \alpha &amp; -\sin \alpha \\    \sin \alpha &amp; \cos \alpha \\    \end{pmatrix}    \begin{pmatrix}    z_1 \\    z_2 \\    \end{pmatrix}.\]</span></p><p>Project <span class="math inline">\((n,h)\)</span> to the lineexpanded by <span class="math inline">\(z_1\)</span>, namely, <spanclass="math inline">\(z_2=0\)</span>. <span class="math display">\[    n&#39;=n_0(u_{rest})+z_1\cos \alpha, \tag{4.17}\]</span> <span class="math display">\[    h&#39;=h_0(u_{rest})+z_1\sin \alpha. \tag{4.18}\]</span></p><p>From (4.15) we find <span class="math display">\[    \frac{\mathrm{d}z_1}{\mathrm{d}t}=\cos \alpha\frac{\mathrm{d}n}{\mathrm{d}t}+\sin \alpha\frac{\mathrm{d}h}{\mathrm{d}t}. \tag{4.19}\]</span> Since <span class="math display">\[    \frac{\mathrm{d}n}{\mathrm{d}t}=-\frac{1}{\tau_n(u)}[n-n_0(u)]\]</span> and an similar equation for <spanclass="math inline">\(h\)</span>, we have <span class="math display">\[    \frac{\mathrm{d}z_1}{\mathrm{d}t}=-\cos \alpha \frac{z_1 \cos\alpha+n_0(u_{rest})-n_0(u)}{\tau_{n}(u)}- \sin \alpha \frac{z_1 \sin\alpha+h_0(u_{rest})-h_0(u)}{\tau_{h}(u)}. \tag{4.20}\]</span> which is of the form <spanclass="math inline">\(\mathrm{d}z_1/\mathrm{d}t=G(u,z_1)\)</span>, asdesired.</p><p>It is convenient to rescale <span class="math inline">\(z_1\)</span>and define <span class="math display">\[    w=- n_0(u_{rest})\tan \alpha-z_1 \sin \alpha. \tag{4.21}\]</span> If we introduce <span class="math inline">\(a=-\tan\alpha\)</span> and <span class="math inline">\(b=an_0(u_{rest})+h_0(u_{rest})\)</span>, we find from (4.17) <spanclass="math inline">\(n&#39;=w/a\)</span> and from (4.18) <spanclass="math inline">\(h&#39;=b-w\)</span>. We also have <spanclass="math inline">\(\mathrm{d}w/\mathrm{d}t=G(u,w)\)</span>. Actually,<span class="math display">\[    \frac{\mathrm{d}w}{\mathrm{d}t}=-\frac{1}{\tau(u)}[w-w_0(u)].\tag{4.22}    \]</span> with <span class="math display">\[    w_0(u)=-\sin \alpha[n_0(u)\cos \alpha+h_0(u)\sin \alpha-b\sin\alpha] \tag{4.23}\]</span> In practice, <span class="math inline">\(w_0(u)\)</span> and<span class="math inline">\(\tau(u)\)</span> can be fitted by (4.9) and(4.10), respectively.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x95.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x96.png" /></div></div></div><h2 id="phase-plane-analysis">Phase plane analysis</h2><p>We know <span class="math display">\[    \begin{pmatrix}    \Delta u \\    \Delta w \\    \end{pmatrix}    =    \begin{pmatrix}    \dot{u} \\    \dot{v} \\    \end{pmatrix}    \Delta t. \tag{4.24}        \]</span></p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x97.png" /></div><div class="group-image-wrap"><img src="/img/neu_dyn/x98.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x99.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x100.png" /></div></div></div><p>Case A is stable, Case C and D are unstable. Stability in case Bcannot be decided with the information available from the picture alone.C and D are saddle points.</p><h3 id="nullclines">Nullclines</h3><p>The set of points with <span class="math inline">\(\dot{u}=0\)</span>is called the <span class="math inline">\(u-\)</span>nullcline.</p><h3 id="stability-of-fixed-points">Stability of Fixed Points</h3><p>The local stability of a fixed point <spanclass="math inline">\((u_{FP},w_{FP})\)</span> is determined bylinearization of the dynamics at the intersection. With <spanclass="math inline">\(\mathbf{x}=(u-u_{FP},w-w_{FP})^{\mathsf{T}}\)</span>,we have after the linearization <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}\mathbf{x}=    \begin{pmatrix}    F_u &amp; F_w \\    G_u &amp; G_w \\    \end{pmatrix}    \mathbf{x}\tag{4.25}\]</span></p><p>Stability of the fixed point <spanclass="math inline">\(\mathbf{x}=0\)</span> in (4.25) requires that thereal part of both eigenvalues be negative. The necessary and sufficientcondition for stability is therefore <span class="math display">\[    F_u+G_w&lt;0 \quad \text{and} \quad F_u G_w-F_w G_u&gt;0.\]</span> If <span class="math inline">\(F_u G_w-F_w G_u&lt;0\)</span>,the fixed point is then called a saddle point.</p><h4 id="example-linear-model">Example: Linear model</h4><p><span class="math display">\[    \dot{u}=au-w \\    \dot{w}=\varepsilon(bu-w), \tag{4.27}\]</span> with positive constants <spanclass="math inline">\(b,\varepsilon&gt;0\)</span>. The <spanclass="math inline">\(u-\)</span>nullcline is <spanclass="math inline">\(w=au\)</span>, the <spanclass="math inline">\(w-\)</span>nullcline is <spanclass="math inline">\(w=bu\)</span>. For the moment we assume <spanclass="math inline">\(a&lt;0\)</span>.</p><p>For the sake of completeness we also study the linear system <spanclass="math display">\[    \dot{u}=-au+w \\    \dot{w}=\varepsilon(bu-w), 0&lt;a&lt;b, \tag{4.28}\]</span> with positive constants <spanclass="math inline">\(a,b\)</span>, and <spanclass="math inline">\(\varepsilon\)</span>.</p><p>Recall the theorem of Poincare-Bendixson.</p><p>In dimensionless variables the FitzHugh-Nagumo model is <spanclass="math display">\[    \frac{\mathrm{d}u}{\mathrm{d}t}=u-\frac{1}{3}u^{3}-w+I \tag{4.29}\]</span> <span class="math display">\[    \frac{\mathrm{d}w}{\mathrm{d}t}=\varepsilon(b_0+b_1u-w). \tag{4.30}\]</span> Time is measured in units of <spanclass="math inline">\(\tau\)</span> and <spanclass="math inline">\(\varepsilon=\tau/\tau_w\)</span> is the ratio ofthe two time scales.</p><h2 id="type-i-and-type-ii-neuron-models">Type I and Type II neuronmodels</h2><p>The onset of repetitive firing under constant current injection ischaracterized by a minimal current <spanclass="math inline">\(I_{\theta}\)</span>, also called the rheobasecurrent. Mathematically speaking, the point <spanclass="math inline">\(I_{\theta}\)</span> where the transition in thenumber or stability of fixed points occurs is called a bifurcation pointand <span class="math inline">\(I\)</span> is the bifurcationparameter.</p><h3 id="type-i-models-and-saddle-node-onto-limit-cycle-bifurcation">TypeI Models and Saddle-Node-onto-Limit-Cycle Bifurcation</h3><p>Neuron models with a continuous gain function. Mathematically, asaddle-node-onto-limit-cycle bifurcation generically gives rise to atype I behavior.</p><p><img src="/img/neu_dyn/x108.png" /></p><h4 id="example-morris-lecar-model-1">Example: Morris-Lecar model</h4><p>Depending on the choice of parameters, the Morris-Lecar model is ofeither type I or type II.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x109.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x110.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x111.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x112.png" /></div></div></div><h4 id="example-hodgkin-huxley-model-reduced-to-two-dimensions">Example:Hodgkin-Huxley Model Reduced to Two Dimensions</h4><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x113.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x114.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x115.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x116.png" /></div></div></div><p>(4.14)</p><h3 id="type-ii-models-and-saddle-node-off-limit-cycle-bifurcation">TypeII Models and Saddle-Node-off-Limit-Cycle Bifurcation</h3><p>There is no reason why a limit cycle should appear directly at thebifurcation point - it can also exist before the bifurcation point isreached. In this case, the limit cycle does not pass through the ruinsof the fixed point and therefore has finite frequency. This gives riseto a type II neuron model. (Saddle-Node-off-Limit-Cycle)</p><h4id="example-hodgkin-huxley-model-reduced-to-two-dimensions-1">Example:Hodgkin-Huxley Model Reduced to Two Dimensions</h4><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x117.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x118.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x119.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x120.png" /></div></div></div><p>(4.15)</p><p>(4.15) shows the same neuron model as (4.14) except for one singlechange in parameter: the time scale <spanclass="math inline">\(\tau_{w}\)</span> in (4.5) for the <spanclass="math inline">\(w-\)</span>dynamics is slightly faster.</p><h4 id="example-saddle-node-without-limit-cycle">Example: Saddle-nodewithout limit cycle</h4><p>Not all saddle-node bifurcations lead to a limit cycle. If the slopeof the <span class="math inline">\(w-\)</span>nullcline of theFitzHugh-Nagumo model defined in (4.29) and (4.30) is smaller than one,it is possible to have three fixed points, one of them unstable and theother two stable. The system is therefore bistable. If <spanclass="math inline">\(I&gt;0\)</span> is big enough so that the leftstable fixed point and the saddle merge and disappear. Since the rightfixed point remains stable, no oscillation occurs.</p><h3 id="type-ii-models-and-hopf-bifurcation">Type II Models and HopfBifurcation</h3><p>From the solution of the stability problem in (4.25) we know that theeigenvalues <span class="math inline">\(\lambda_{+/-}\)</span> form acomplax conjugate pair with a real part <spanclass="math inline">\(\gamma\)</span> and a imaginary part <spanclass="math inline">\(+/-\omega\)</span> (Fig. 4.16). The fixed point isstable if <span class="math inline">\(\gamma&lt;0\)</span>. At thetransition point, the real part vanishes and the eigenvalues are <spanclass="math display">\[    \lambda_{\pm}=\pm i \sqrt{F_u G_w-G_u F_w}. \tag{4.31}\]</span> These eigenvalues correspond to an oscillatory solution with afrequency given by <span class="math inline">\(\omega=\sqrt{F_u G_w-G_uF_w}\)</span>.</p><p><img src="/img/neu_dyn/x121.png" /> (4.16)</p><p>Whenever we have a Hopf bifurcation, be it subcritical orsupercritical, the limit cycle starts with finite frequency.</p><p>In a supercritical Hopf bifurcation, the amplitude of the oscillationgrows with the stimulation <span class="math inline">\(I\)</span>. Suchperiodic oscillations of small amplitude should be intepreted asspontaneous subthreshold oscillations.</p><p>Only models with a subcritical Hopf-bifurcation give rise tolarge-amplitude oscillations close to the bifurcation point.</p><h4 id="example-fitzhugh-nagumo-model">Example: FitzHugh-Nagumomodel</h4><p>In Fig. 4.10, if the slope of the <spanclass="math inline">\(w-\)</span>nullcline is larger than one, there isonly one fixed point, whatever <span class="math inline">\(I\)</span>.With increasing current <span class="math inline">\(I\)</span>, thefixed point moves to the right. Eventually it loses stability via a Hopfbifurcation.</p><h2 id="threshold-and-excitability">Threshold and excitability</h2><p>The Hodgkin-Huxley model does not have a clear-cut firingthreshold.</p><p>For stimulation with a short current pulse of variable amplitude,models with saddle-node bifurcation (on or off a limit cycle) indeedhave a threshold whereas models where firing arises via a Hopfbifurcation have not. However, even models with Hopf bifurcation canshow threshold-like behavior for current pulse if the dynamics of <spanclass="math inline">\(w\)</span> are consideraby slower than that of<span class="math inline">\(u\)</span>.</p><h3 id="type-i-models">Type I models</h3><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x124.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x125.png" /></div></div></div><p>The stable manifold of an saddle point. All trajectories with initialcondition to the right of the stable manifold must make a detour aroundthe unstable fixed point before they can reach the stable fixed point.Trajectories with initial conditions to the left of the stable manifoldreturn immediately toward the stable fixed point.</p><p>The stable manifold acts as a threshold for spike initiation, if theneuron model is probed with a isolated current pulse.</p><h4 id="example-canonical-type-i-model">Example: Canonical type Imodel</h4><p>Consider the one-dimensional model <span class="math display">\[    \frac{\mathrm{d}\phi}{\mathrm{d}t}=q(1-\cos \phi)+I(1+\cos \phi)\tag{4.32}\]</span> where <span class="math inline">\(q&gt;0\)</span> is aparameter and <span class="math inline">\(I\)</span> with <spanclass="math inline">\(0&lt;\lvert I \rvert &lt;q\)</span> the appliedcurrent. The variable <span class="math inline">\(\phi\)</span> is thephase along the limit cycle trajectory. Formally, a spike is said tooccur whenever <span class="math inline">\(\phi=\pi\)</span>.</p><p>For <span class="math inline">\(I&lt;0\)</span>, the phase equation<span class="math inline">\(\mathrm{d}\phi/\mathrm{d}t\)</span> has twofixed points. The resting state is at the stable fixed point <spanclass="math inline">\(\phi=\phi_r\)</span>. The unstable fixed point at<span class="math inline">\(\phi=\theta\)</span> acts as athreshold.</p><p>For all currents <span class="math inline">\(I&gt;0\)</span>, we have<span class="math inline">\(\mathrm{d}\phi/\mathrm{d}t&gt;0\)</span>, sothat the system is circling along the limit cycle. For <spanclass="math inline">\(I \to 0\)</span>, the velocity along thetrajectory around <span class="math inline">\(\phi=0\)</span> tends tozero. Therefore the frequency of the oscillation <spanclass="math inline">\(\nu=1/T(I)\)</span> decreases to zero.</p><h3 id="hopf-bifurcations">Hopf Bifurcations</h3><p>There is a continuum of trajectories.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x127.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x128.png" /></div></div></div><p>Nevertheless, if the time scale of the <spanclass="math inline">\(u\)</span> dynamics is much faster than that ofthe <span class="math inline">\(w-\)</span>dynamics, then there is acritical regime where the sensitivity to the amplitude of the inputcurrent pulse can be extremely high.</p><p>In models with Hopf bifurcation, the peak of the response is alwaysreached with roughly the same delay, independently of the size of theinput pulse. It is the amplitude of the response that increases rapidlybut continuously.</p><h2id="separation-of-time-scales-and-reduction-to-one-dimension">Separationof time scales and reduction to one dimension</h2><p>We measure time in units of <span class="math inline">\(\tau\)</span>and take <span class="math inline">\(R=1\)</span> in (4.4) and (4.5).Then <span class="math display">\[    \frac{\mathrm{d}u}{\mathrm{d}t}=F(u,w)+I \tag{4.33}\]</span> <span class="math display">\[    \frac{\mathrm{d}w}{\mathrm{d}t}=\varepsilon G(u,w) \tag{4.34}\]</span> where <spanclass="math inline">\(\varepsilon=\tau/\tau_w\)</span>. If <spanclass="math inline">\(\tau_w\gg \tau\)</span>, then <spanclass="math inline">\(\varepsilon\ll 1\)</span>. In this situation thetime scale the governs the evolution of <spanclass="math inline">\(u\)</span> is much faster than that of <spanclass="math inline">\(w\)</span>. In the mathematical literature thelimit of <span class="math inline">\(\varepsilon \to 0\)</span> iscalled 'singular perturbation'. Oscillatory behavior for small <spanclass="math inline">\(\varepsilon\)</span> is called a 'relaxationoscillation'. Trajectories slowly follow the <spanclass="math inline">\(u-\)</span>nullcline, except at the knees of thenullcline where they jump to a different branch.</p><p><img src="/img/neu_dyn/x129.png" /></p><p>In the above figure the middle branch of the <spanclass="math inline">\(u-\)</span>nullcline (where <spanclass="math inline">\(\dot{u}&gt;0\)</span>) acts as a threshold forspike initiation.</p><p>We can exploit the separation of times scales for a further reductionof the two-dimensional system of equations to a single variable. Aninput current <span class="math inline">\(I(t)\)</span> acts on thevoltage dynamics, but has no direct influence on the variable <spanclass="math inline">\(w\)</span>. Moreover, in the limit of <spanclass="math inline">\(\varepsilon\ll 1\)</span>, the influence of thevoltage <span class="math inline">\(u\)</span> on the <spanclass="math inline">\(w-\)</span>variable via (4.34) is negligible.Hence, we can set <span class="math inline">\(w=w_{rest}\)</span> andsummarize the voltage dynamics of spike initiation by a single equation<span class="math display">\[    \frac{\mathrm{d}u}{\mathrm{d}t}=F(u,w_{rest})+I. \tag{4.35}\]</span></p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x130.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x131.png" /></div></div></div><h4 id="example-piecewise-linear-nullclines">Example: Piecewise linearnullclines</h4><p><img src="/img/neu_dyn/x132.png" /></p><p><span class="math display">\[    \frac{\mathrm{d}u}{\mathrm{d}t}=f(u)-w+I \tag{4.36}\]</span> <span class="math display">\[    \frac{\mathrm{d}w}{\mathrm{d}t}=\varepsilon(bu-w) \tag{4.37}\]</span></p><p>with <span class="math inline">\(f(u)=au\)</span> for <spanclass="math inline">\(u&lt;0.5\)</span>, <spanclass="math inline">\(f(u)=a(1-u)\)</span> for <spanclass="math inline">\(0.5&lt; u&lt; 1.5\)</span> and <spanclass="math inline">\(f(u)=c_0+c_1u\)</span> for <spanclass="math inline">\(u&gt;1.5\)</span> where <spanclass="math inline">\(a\)</span>,<spanclass="math inline">\(c_1&lt;0\)</span> are parameters and <spanclass="math inline">\(c_0=-0.5a-1.5c_1\)</span>. Furthermore, <spanclass="math inline">\(b&gt;0\)</span> and <spanclass="math inline">\(0&lt;\varepsilon\ll 1\)</span>.</p><p>The rest state is at <span class="math inline">\(u=w=0\)</span>.<span class="math inline">\(u=1\)</span> acts as a threshold. Let us nowsuppose that neuron receives a weak and constant background currentduring our threshold-search experiments. A constant current shifts the<span class="math inline">\(u-\)</span>nullcline vertically upward.Hence the point where <span class="math inline">\(\dot{u}=0\)</span>shifts leftward and therefore the voltage threshold for pulsestimulation sits now at a lower value.</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Information and Entropy (1)</title>
    <link href="/2022/07/01/Information-and-Entropy-1/"/>
    <url>/2022/07/01/Information-and-Entropy-1/</url>
    
    <content type="html"><![CDATA[<h1 id="bits">Bits</h1><h2 id="the-boolean-bit">The Boolean Bit</h2><h2 id="the-circuit-bit">The Circuit Bit</h2><h2 id="the-control-bit">The Control Bit</h2><h2 id="the-physical-bit">The Physical Bit</h2><h2 id="the-quantum-bit">The Quantum Bit</h2><p>There are three features of quantum mechanics, reversibility,superposition, and entanglement, that make qubits or collections ofqubits different from Boolean bits.</p><h2 id="the-classical-bit">The Classical Bit</h2><h1 id="codes">Codes</h1><h2 id="symbol-space-size">Symbol Space Size</h2><h2 id="use-of-spare-capacity">Use of Spare Capacity</h2><p>There are many strategies to deal with unused code patterns. Here aresome: - Ignore - Map to other values - Reserve for future expansion -Use for control codes - Use for common abbreviations</p><h3 id="binary-coded-decimal-bcd">Binary Coded Decimal (BCD)</h3><h3 id="genetic-code">Genetic Code</h3><h3 id="telephone-area-codes">Telephone Area Codes</h3><h3 id="ip-addresses">IP Addresses</h3><h3 id="ascii">ASCII</h3><h2 id="extension-of-codes">Extension of Codes</h2><h2 id="fixed-length-and-variable-length-codes">Fixed-Length andVariable-Length Codes</h2><h3 id="morse-code">Morse Code</h3><h2 id="detail-ascii">Detail: ASCII</h2><h2 id="detail-integer-codes">Detail: Integer Codes</h2><h3 id="binary-code">Binary Code</h3><h3 id="binary-gray-code">Binary Gray Code</h3><h3 id="s-complement">2's Complement</h3><h3 id="signmagnitude">Sign/Magnitude</h3><h3 id="s-complement-1">1's Complement</h3><h2 id="detail-the-genetic-code">Detail: The Genetic Code</h2><h2 id="detail-ip-addresses">Detail: IP Addresses</h2><h2 id="detail-morse-code">Detail: Morse Code</h2><h3 id="problem-2-universality">Problem 2: Universality</h3><p>A Boolean function <span class="math inline">\(F(A,B)\)</span> issaid to be universal if any arbitrary boolean if any arbitrary booleanfunction can be constructed by using nested <spanclass="math inline">\(F(A,B)\)</span> functions.</p><ul><li><p><span class="math inline">\(OR\)</span> and <spanclass="math inline">\(AND\)</span> are not universal functions since<span class="math inline">\(OR\)</span> (<spanclass="math inline">\(AND\)</span>) is monotonicincreasing(decreasing)</p></li><li><p><span class="math inline">\(XOR\)</span> is not universal. Wedefine a nested function <span class="math inline">\(XOR\)</span>s to bean expression <span class="math inline">\(f\)</span> drawn from the set<spanclass="math inline">\(EXPR=\{0,1,A,B,XOR(f_{\alpha},f_{\beta})\}\)</span>,where <span class="math inline">\(f_{\alpha}\)</span> and <spanclass="math inline">\(f_{\beta}\)</span> are also drawn from the set<span class="math inline">\(EXPR\)</span>, but only a finite number oftimes. Consider the property <span class="math inline">\(E\)</span>: afunction <span class="math inline">\(f_{\gamma}\)</span> has property<span class="math inline">\(E\)</span> if and only if <spanclass="math inline">\(0,2\)</span>, or <spanclass="math inline">\(4\)</span> of these values is <spanclass="math inline">\(1\)</span>. If function <spanclass="math inline">\(f\)</span> is equal to <spanclass="math inline">\(AND(A,B)\)</span>, then it does not have property<span class="math inline">\(E\)</span>. <spanclass="math inline">\(f\)</span> cannot be equal to <spanclass="math inline">\(A,B,0\)</span> or <spanclass="math inline">\(1\)</span>, and so it must be of the form <spanclass="math inline">\(XOR(f_{\alpha},f_{\beta})\)</span>. Noting that<span class="math inline">\(XOR\)</span> produces a <spanclass="math inline">\(1\)</span> only upon input of a <spanclass="math inline">\(1\)</span> and a <spanclass="math inline">\(0\)</span>, i.e., an 'unpaired' <spanclass="math inline">\(1\)</span>. We obtain that exactly one of <spanclass="math inline">\(f_{\alpha}\)</span> and <spanclass="math inline">\(f_{\beta}\)</span> has odd output, namely notsatisfying property <span class="math inline">\(E\)</span>. (assume<span class="math inline">\(f_{\alpha}\)</span> has odd output) So <spanclass="math inline">\(f_{\alpha}\)</span> is another <spanclass="math inline">\(XOR\)</span> function. Eventually you run out offunctions and thus <span class="math inline">\(f\)</span> cannot beequal to <span class="math inline">\(AND(A,B)\)</span>.</p></li><li><p><span class="math inline">\(NAND\)</span> is a universalfunction. Actually: <figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">b_15</span>=N <br><span class="hljs-attr">b_14</span>=NAND(A,B)<br><span class="hljs-attr">b_13</span>=NAND(b_14,A)<br><span class="hljs-attr">b_12</span>=NAND(A,<span class="hljs-number">1</span>)<br><span class="hljs-attr">b_11</span>=NAND(b_12,B)<br><span class="hljs-attr">b_10</span>=NAND(B,<span class="hljs-number">1</span>)<br><span class="hljs-attr">b_9</span>=NAND(NAND(A,B),NAND(b_12,b_10))<br><span class="hljs-attr">b_8</span>=NAND(NAND(b_10,b_12),<span class="hljs-number">1</span>)<br><span class="hljs-attr">b_7</span>=NAND(b_8,<span class="hljs-number">1</span>)<br><span class="hljs-attr">b_6</span>=NAND(b_9,<span class="hljs-number">1</span>)<br><span class="hljs-attr">b_5</span>=NAND(b_10,<span class="hljs-number">1</span>)<br><span class="hljs-attr">b_4</span>=NAND(b_11,<span class="hljs-number">1</span>) <br><span class="hljs-attr">b_3</span>=NAND(b_12,<span class="hljs-number">1</span>) <br><span class="hljs-attr">b_2</span>=NAND(b_14,<span class="hljs-number">1</span>) <br><span class="hljs-attr">b_1</span>=NAND(b_13,<span class="hljs-number">1</span>) <br><span class="hljs-attr">b_0</span>=Z <br></code></pre></td></tr></table></figure></p></li></ul><h1 id="compression">Compression</h1><p>two types of compression: - Lossless or reversible compression -Lossy or irreversible compression</p><h2 id="variable-length-encoding">Variable-Length Encoding</h2><p>We put off the discussion of this technique until Chapter 5.</p><h2 id="run-length-encoding">Run Length Encoding</h2><p>Ex: "a B B B B B a a a B B a a a a" could be encoded as "a 1 B 5 a 3B 2 a 4".</p><h2 id="static-dictionary">Static Dictionary</h2><h2 id="semi-adaptive-dictionary">Semi-adaptive Dictionary</h2><h2 id="dynamic-dictionary">Dynamic Dictionary</h2><p><strong>LZW compression technique</strong></p><h3 id="the-lzw-patent">The LZW Patent</h3><h2 id="irreversible-techniques">Irreversible Techniques</h2><h2 id="detail-lzw-compression">Detail: LZW Compression</h2><hr /><p>LZWhttps://blog.csdn.net/krossford/article/details/49157531?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3&amp;utm_source=qq&amp;utm_medium=social&amp;utm_oi=808074114588893184</p><hr /><p>LZW<span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(\Rightarrow\)</span>  <spanclass="math inline">\(\Rightarrow\)</span>  <spanclass="math inline">\(\Rightarrow\)</span>ASCII80-25590-511</p><p>LZWrrrrrrrrrirr&gt; Normally, on receipt of a transmitted codeword, the decoder canlook up its string in the dictionary and then output it and use itsfirst character to complete the partially formed last dictionary entry,and then start the next dictionary entry. Thus only one dictionarylookup is needed. However, the algorithm presented above uses twolookups, one for the first character, and a later one for the entirestring. Why not use just one lookup for greater efficiency? &gt;&gt;There is a special case illustrated by the transmission of code 271in this example, where the string corresponding to the received code isnot complete. The first character can be found but then before theentire string is retrieved, the entry must be completed. This happenswhen a character or a string appears for the first time three times in arow, and is therefore rare. The algorithm above works correctly, at acost of an extra lookup that is seldom needed and may slow the algorithmdown.</p><h2 id="detail-2-d-discrete-cosine-transformation">Detail: 2-D DiscreteCosine Transformation</h2><h3 id="discrete-linear-transformation">Discrete LinearTransformation</h3><h3 id="discrete-cosine-transformation">Discrete CosineTransformation</h3><p>TimothySauerp409-p449DFT.</p><h1 id="errors">Errors</h1><h2 id="extension-of-system-model">Extension of System Model</h2><p>Extend the model for information handling to include "channelcoding." The channel encoder adds bits to the message so that in case itgets corrupted in some way, the channel encoder will know that andpossibly even be able to repair the damage.</p><h2 id="how-do-errors-happen">How do Errors Happen?</h2><p>In the usual case, we will usually assume that different bits getcorrupted independently, but in some cases errors in adjacent bits arenot independent of each other, but instead have a common underlyingcause (i.e., the errors may happen in bursts).</p><h2 id="detection-vs.-correction">Detection vs. Correction</h2><p>detect the error and then let the person or system that uses theoutput know that an error has occurred.</p><p>Have the channel decoder attempt to repair the message by correctingthe error.</p><p>In both cases, extra bits are added to the messages to make themlonger, so the message contains redundancy. The channel, by allowingerrors to occur, actually introduces information. The decoder isirreversible in that it discards some information but keep the originalinformation if well designed.</p><h2 id="hamming-distance">Hamming Distance</h2><p>The number of bits that are different between the two.</p><h2 id="single-bits">Single Bits</h2><h2 id="multiple-bits">Multiple Bits</h2><h3 id="parity">Parity</h3><p>Detect: Chang the 8-bit string into 9 bits. The added bit would be 1if the number of bits equal to 1 is odd, and 0 otherwise. It is mostoften used when the likelihood of an error is very small, and there isno reason to suppose that errors of adjacent bits occur together, andthe receiver is able to request a retransmission of the data.</p><h3 id="rectangular-codes">Rectangular Codes</h3><p>Rectangular codes can provide single error correction and doubleerror detection simultaneously. Refer to p48.</p><h3 id="hamming-code">Hamming Code</h3><hr /><p>Hamming Codehttps://www.cnblogs.com/Philip-Tell-Truth/p/6669854.html</p><hr /><h2 id="block-codes">Block Codes</h2><p>If the number of data bits in the block is <spanclass="math inline">\(k\)</span>, the the number of parity bits is <spanclass="math inline">\(n-k\)</span>, and it is customary to call such acode an <span class="math inline">\((n,k)\)</span> block code. Thus theHamming Code just described is <spanclass="math inline">\((7,4)\)</span>.</p><p>It is also customary to include in the parentheses the minimumHamming distance <span class="math inline">\(d\)</span> between any twovalid codewords, or original data items, in the form <spanclass="math inline">\((n,k,d)\)</span>. We have following blocktypes</p><table><thead><tr class="header"><th>Parity bits</th><th>Block size</th><th>Payload</th><th>Code rate</th><th>Block code type</th></tr></thead><tbody><tr class="odd"><td>2</td><td>3</td><td>1</td><td>0.33</td><td>(3,1,3)</td></tr><tr class="even"><td>3</td><td>7</td><td>4</td><td>0.57</td><td>(7,4,3)</td></tr><tr class="odd"><td>4</td><td>15</td><td>11</td><td>0.73</td><td>(15,11,3)</td></tr><tr class="even"><td>5</td><td>31</td><td>26</td><td>0.84</td><td>(31,26,3)</td></tr><tr class="odd"><td>6</td><td>63</td><td>57</td><td>0.90</td><td>(63,57,3)</td></tr><tr class="even"><td>7</td><td>127</td><td>120</td><td>0.94</td><td>(127,120,3)</td></tr><tr class="odd"><td>8</td><td>255</td><td>247</td><td>0.97</td><td>(255,247,3)</td></tr></tbody></table><h2 id="advanced-codes">Advanced Codes</h2><p>Bose-Chaudhuri-Hocquenhem (BCH) codes.</p><p>Irving S. Reed and Gustave Solomon of MIT Lincoln Laboratory. The<span class="math inline">\((256,224,5)\)</span> and <spanclass="math inline">\((224,192,5)\)</span> Reed-Solomon codes are usedin CD players.</p><h2 id="detail-check-digits">Detail: Check Digits</h2><h3 id="credit-cards">Credit Cards</h3><h3 id="isbn">ISBN</h3><h3 id="issn">ISSN</h3><h1 id="probability">Probability</h1><h2 id="events">Events</h2><p><strong>Outcome</strong> is the symbol selected, whether or not it isknown to us. <strong>Event</strong> is a subset of the possible outcomesof an experiment.</p><p>When a selection is made, then, there are several events. One is theoutcome itself. This is called a <strong>fundamental event</strong>.</p><p>The special event in which any symbol at all is selected is called<strong>universal event</strong>. The special "event" in which no symbolis seleted is called the <strong>null event</strong>. The null eventcannot happen because an outcome is only defined after a selection ismade.</p><p>A set of events which do not overlap is said to be <strong>mutuallyexclusive</strong>. A set of events, one of which is sure to happen, isknown as <strong>exhaustive</strong>. A set of events that are bothmutually exclusive and exhaustive is known as a<strong>partition</strong>. The partition that consists of all thefundamental events will be called the <strong>fundamentalpartition</strong>.</p><p>A partition consisting of a small number of events, some of which maycorrespond to many symbols, is known as a <strong>coarse-grainedpartition</strong> whereas a partition with many events is a<strong>fine-grained partition</strong>.</p><h2 id="known-outcomes">Known Outcomes</h2><h2 id="unknown-outcomes">Unknown Outcomes</h2><h2 id="joint-events-and-conditional-probabilities">Joint Events andConditional Probabilities</h2><p><span class="math display">\[    p(A,B)=p(B)p(A|B)=p(A)p(B|A) \tag{5.5}\]</span></p><p>This formula is known as Bayes' Theorem.</p><h2 id="averages">Averages</h2><h2 id="information">Information</h2><p><span class="math display">\[    I=\sum_{i}^{} p(A_i)\log _{2}\left( \frac{1}{p(A_i)}\right)\tag{5.14}\]</span></p><p>This quantity is called the <strong>entropy of a source</strong>.</p><h2 id="properties-of-information">Properties of Information</h2><p>If there are two events in the partition with probabilities <spanclass="math inline">\(p\)</span> and <spanclass="math inline">\((1-p)\)</span>, the information per symbol is<span class="math display">\[    I=p\log _{2}(\frac{1}{p})+(1-p)\log _{2}(\frac{1}{1-p}) \tag{5.16}\]</span> which attains its largest (1 bit) for <spanclass="math inline">\(p=0.5\)</span>.</p><p>For partitions with more than two possible events the information persymbol can be higher. If there are <spanclass="math inline">\(n\)</span> possible events the information persymbol lies between <span class="math inline">\(0\)</span> and <spanclass="math inline">\(\log _{2}(n)\)</span> bits, the maximum valuebeing achieved when all probabilities are equal.</p><h2 id="efficient-source-coding">Efficient Source Coding</h2><p>If a source has <span class="math inline">\(n\)</span> possiblesymbols then a fixed-length code for it would require <spanclass="math inline">\(\log _{2}(n)\)</span> bits per symbol.</p><p>There is a general procedure for constructing codes of this sortwhich are very efficient (in fact, they require an average of less tha<span class="math inline">\(I+1\)</span> bits per symbol, even if <spanclass="math inline">\(I\)</span> is considerably below <spanclass="math inline">\(\log _{2}(n)\)</span>). We'll introduce Huffmancodes below.</p><h2 id="detail-efficient-source-code">Detail: Efficient Source Code</h2><p>Huffman code: p66-p68</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>10</title>
    <link href="/2022/06/28/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%8810%EF%BC%89/"/>
    <url>/2022/06/28/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%8810%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><h2 id=""></h2><p> <span class="math inline">\(n\)</span>  <spanclass="math display">\[    \frac{\mathrm{d}y_i}{\mathrm{d}x}=f_i(x,y_1,\cdots,y_n)\quad(i=1,\cdots,n),\tag{10.14}\]</span>  <span class="math inline">\(f_1,\cdots,f_n\)</span>  <span class="math inline">\(D \subset\mathbb{R}^{n+1}\)</span>  <span class="math inline">\((x,y_1,\cdots,y_n)\)</span>  <span class="math inline">\(y_1,\cdots,y_n\)</span> .</p><p> <span class="math inline">\(V=V(x,y_1,\cdots ,y_n)\)</span> <span class="math inline">\(D\)</span>  <spanclass="math inline">\(G\)</span>  <spanclass="math inline">\(x,y_1,\cdots ,y_n\)</span> . <span class="math inline">\(V(x,y_1,\cdots ,y_n)\)</span>10.14 <spanclass="math inline">\(G\)</span>  <spanclass="math display">\[    \Gamma\colon \quad y_1=y_1(x),\cdots ,y_n=y_n(x) \quad (x \in J)\]</span>  <span class="math inline">\(V\)</span>  <spanclass="math display">\[    V(x,y_1(x),\cdots ,y_n(x))= \quad (x \in J)\]</span>  <span class="math inline">\((x,y_1,\cdots ,y_n) \in\Gamma\)</span>  <span class="math display">\[    V(x,y_1,\cdots ,y_n)=\]</span>  <spanclass="math inline">\(\Gamma\)</span> .  <spanclass="math display">\[    V(x,y_1,\cdots ,y_n)=C \tag{10.15}\]</span> 10.14 <span class="math inline">\(G\)</span><strong></strong> <span class="math inline">\(C\)</span>.</p><p>.</p><h2 id=""></h2><p><strong> 10.1</strong>  <spanclass="math inline">\(\Phi(x,y_1,\cdots ,y_n)\)</span>  <spanclass="math inline">\(G_1\)</span> . <span class="math display">\[    \Phi(x,y_1,\cdots ,y_n)=C \tag{10.18}\]</span> 10.14 <spanclass="math inline">\(G_1\)</span>  <spanclass="math display">\[    \frac{\partial \Phi}{\partial x}+\frac{\partial \Phi}{\partialy_1}f_1+\cdots +\frac{\partial \Phi}{\partial y_n}f_n=0\tag{10.19}      \]</span>  <span class="math inline">\((x,y_1,\cdots ,y_n)\inG_1\)</span> .</p><p><strong> 10.2</strong>10.1410.1810.17.</p><p><strong></strong> <spanclass="math inline">\(\Phi\)</span> 0 <spanclass="math inline">\(\displaystyle \frac{\partial \Phi}{\partialy_n}\neq 0\)</span>. 10.18 <spanclass="math display">\[    y_n=g(x,y_1,\cdots ,y_{n-1},C), \tag{10.23}\]</span> 10.14 <span class="math inline">\(n-1\)</span> <span class="math inline">\(y_n\)</span>.  <spanclass="math inline">\(n-1\)</span>  <spanclass="math inline">\(y_n=g(x,u_1(x),\cdots ,u_{n-1}(x),C)\)</span>10.1410.14.</p><p>10.14 <span class="math inline">\(n\)</span>  <spanclass="math display">\[    \Phi_i(x,y_1,\cdots ,y_n)=C_i \tag{10.28}\]</span> <span class="math inline">\((i=1,\cdots ,n)\)</span>. <span class="math inline">\(G_1\)</span>  Jacobi <span class="math display">\[    \frac{D(\Phi_1,\cdots ,\Phi_n)}{D(y_1,\cdots ,y_n)}\neq 0,\tag{10.29}\]</span>  <span class="math inline">\(G_1\)</span><strong></strong>.</p><p><strong> 10.3</strong> 10.14 <spanclass="math inline">\(G_1\)</span>  <spanclass="math inline">\(n\)</span> 10.28.10.14 <span class="math inline">\(G_1\)</span> <span class="math display">\[    y_1=\varphi_1(x,C_1,\cdots ,C_n),\cdots ,y_n=\varphi_n(x,C_1,\cdots,C_n), \tag{10.30}\]</span>  <span class="math inline">\(C_1,\cdots ,C_n\)</span> <span class="math inline">\(n\)</span>10.14 <spanclass="math inline">\(G_1\)</span> .</p><p><strong></strong>10.2910.28<span class="math inline">\(y_1,\cdots ,y_n\)</span>.10.30. 10.28 <spanclass="math inline">\(x\)</span> <span class="math display">\[    \varphi_1&#39;=f_1,\cdots ,\varphi_n&#39;=f_n,\]</span> 10.3010.14.</p><p>10.28 <span class="math inline">\(C_j\)</span> <span class="math display">\[    \frac{D(\varphi_1,\cdots ,\varphi_n)}{D(C_1,\cdots ,C_n)}=\left[\frac{D(\Phi_1,\cdots ,\Phi_n)}{D(y_1,\cdots ,y_n)}\right]^{-1} \neq0,        \]</span>  <span class="math inline">\(C_1,\cdots ,C_n\)</span>.</p><p>10.14 <span class="math inline">\(G_1\)</span>10.30..</p><p>10.14 <spanclass="math inline">\(n\)</span> .</p><p>10.14 <spanclass="math inline">\(k(1\leqslant k\leqslant n)\)</span> <span class="math display">\[    V_i(x,y_1,\cdots ,y_n)=C_i \quad(i=1,\cdots ,k). \tag{10.35}\]</span>  <span class="math inline">\(\displaystyle \left(\frac{\partial V_i}{\partial y_j}\right)_{k \times n}\)</span> <span class="math inline">\(k\)</span>.  <spanclass="math inline">\(k\)</span>10.3510.14 <spanclass="math inline">\(k\)</span> .</p><p>10.35 <spanclass="math inline">\(H(z_1,\cdots ,z_k)\)</span> <span class="math display">\[    H(V_1(x,y_1,\cdots ,y_n),\cdots ,V_k(x,y_1,\cdots ,y_n))=C\]</span> 10.14.  <spanclass="math inline">\(V_i\)</span> </p><h2 id=""></h2><p><strong> 10.4</strong>  <spanclass="math inline">\(P_0=(x_0,y_1^{0},\cdots ,y_n^{0})\in G\)</span>. <span class="math inline">\(P_0\)</span>  <spanclass="math inline">\(G_0 \subsetG\)</span>10.14 <spanclass="math inline">\(G_0\)</span>  <spanclass="math inline">\(n\)</span> .</p><p><strong></strong> <span class="math inline">\(P_0\)</span>.  <spanclass="math inline">\(n\)</span> .</p><p><strong> 10.5</strong> 10.14 <spanclass="math inline">\(n\)</span> .</p><p><strong> 10.6</strong> 10.2810.14 <spanclass="math inline">\(G_0\)</span>  <spanclass="math inline">\(n\)</span> .  <spanclass="math inline">\(G_0\)</span> 10.14<span class="math display">\[    V(x,y_1,\cdots ,y_n)=C\]</span> 10.28 <span class="math display">\[    V(x,y_1,\cdots ,y_n)=h[\Phi_1(x,y_1,\cdots ,y_n),\cdots,\Phi_n(x,y_1,\cdots ,y_n)], \tag{10.41}\]</span>  <span class="math inline">\(h\)</span>.</p><p><strong></strong> <span class="math display">\[    J:=\frac{\partial (\Phi_1,\cdots ,\Phi_n)}{\partial (y_1,\cdots,y_n)}\neq 0,    \]</span>  <span class="math display">\[    y_i=y_i(x,\Phi_1,\cdots ,\Phi_n),i=1,\cdots ,n,\]</span>  <span class="math inline">\(V\)</span> <span class="math inline">\(H\)</span><span class="math display">\[    H(x,\Phi_1,\cdots ,\Phi_n) :=V(x,y_1,\cdots ,y_n).\]</span>  <span class="math inline">\(H\)</span>  <spanclass="math inline">\(x\)</span> .  <spanclass="math display">\[    \frac{\partial H}{\partial x}=\frac{\partial V}{\partialx}+\sum_{i=1}^{n} \frac{\partial V}{\partial y_i}\frac{\partialy_i}{\partial x}.\]</span>  <span class="math display">\[    \frac{\partial y_i}{\partial x}=-\frac{1}{J} \frac{\partial(\Phi_1,\cdots ,\Phi_i,\cdots ,\Phi_n)}{\partial (y_1,\cdots ,x,\cdots,y_n)}.\]</span></p><p> <span class="math display">\[    \frac{\partial H}{\partial x}=\frac{\partial V}{\partialx}-\frac{1}{J}\sum_{i=1}^{n} \frac{\partial V}{\partial y_i}\frac{\partial (\Phi_1,\cdots ,\Phi_i,\cdots ,\Phi_n)}{\partial(y_1,\cdots ,x,\cdots ,y_n)}=\frac{1}{J}\frac{\partial (V,\Phi_1,\cdots,\Phi_n)}{\partial (x,y_1,\cdots ,y_n)}\]</span>  <span class="math display">\[    \frac{\partial V}{\partial x}+\sum_{i=1}^{n} \frac{\partialV}{\partial y_i}f_i=0,\]</span>  <spanclass="math inline">\(V\)</span>  <spanclass="math inline">\(\Phi\)</span> <span class="math inline">\((1,f_1,\cdots ,f_n)\)</span> <span class="math inline">\(\displaystyle\frac{\partial H}{\partial x}=0\)</span>.</p><p> <span class="math inline">\(V=H(\Phi_1,\cdots,\Phi_n)\)</span>.</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Through the Heat Equation</title>
    <link href="/2022/06/24/Through-the-Heat-Equation/"/>
    <url>/2022/06/24/Through-the-Heat-Equation/</url>
    
    <content type="html"><![CDATA[<h1 id="the-heat-equation">The Heat Equation</h1><h2 id="derivation-of-the-heat-equation">Derivation of the heatequation</h2><p>Consider an infinite metal plate which we model as the plane <spanclass="math inline">\(\mathbb{R}^{2}\)</span>, and suppose we are givenan initial heat distribution at time <spanclass="math inline">\(t=0\)</span>. Let the temperature at the point<span class="math inline">\((x, y)\)</span> at time <spanclass="math inline">\(t\)</span> be denoted by <spanclass="math inline">\(u(x,y,t)\)</span>.</p><p>Consider a small square centered at <spanclass="math inline">\((x_0,y_0)\)</span> with sides parallel to the axisand of side length <span class="math inline">\(h\)</span>. The amount ofheat energy in <span class="math inline">\(S\)</span> at time <spanclass="math inline">\(t\)</span> is given by <spanclass="math display">\[    H(t)= \sigma \iint_{S} u(x,y,t) \mathrm{d}x \mathrm{d}y,\]</span> where <span class="math inline">\(\sigma&gt;0\)</span> is aconstant called the specific heat of the material. Therefore, the heatflow into <span class="math inline">\(S\)</span> is <spanclass="math display">\[    \frac{\partial H}{\partial t}= \sigma \iint_{S} \frac{\partialu}{\partial t} \mathrm{d}x\mathrm{d}y,\]</span> which is approximately equal to <span class="math display">\[    \sigma h^{2} \frac{\partial u}{\partial t}(x_0,y_0,t),\]</span> since the area of <span class="math inline">\(S\)</span> is<span class="math inline">\(h^{2}\)</span>. Now we apply Newton's law ofcooling, which states that heat flows from the higher to lowertemperature at a rate proportional to the difference, that is, thegradient.</p><p>The heat flow through the vertical side on the right is therefore<span class="math display">\[    - \kappa h \frac{\partial u}{\partial x} (x_0+\frac{h}{2},y_0,t),\]</span> where <span class="math inline">\(\kappa&gt;0\)</span> is theconductivity of the material. A similar argument for the other sidesshows that the total heat flow through the square <spanclass="math inline">\(S\)</span> is given by <spanclass="math display">\[    \kappa h \left[ \frac{\partial u}{\partialx}(x_0+\frac{h}{2},y_0,t)- \frac{\partial u}{\partialx}(x_0-\frac{h}{2},y_0,t)+\frac{\partial u}{\partialy}(x_0,y_0+\frac{h}{2},t)-\frac{\partial u}{\partialy}(x_0,y_0-\frac{h}{2},t)\right].\]</span> Applying the mean value theorem and letting <spanclass="math inline">\(h\)</span> tend to zero, we find that <spanclass="math display">\[    \frac{\sigma}{\kappa} \frac{\partial u}{\partial t}= \frac{\partial^{2}u}{\partial x^{2}}+ \frac{\partial ^{2}u}{\partial y^{2}};\]</span> this is called the <strong>time-dependent heatequation</strong>, often abbreviated to the heat equation.</p><h2 id="steady-state-heat-equation-in-the-disc">Steady-state heatequation in the disc</h2><p>After a long period of time, there is no more heat exchange, so thatthe system reaches thermal equilibrium and <spanclass="math inline">\(\displaystyle \frac{\partial u}{\partialt}=0\)</span>. In this case, the time-dependent heat equation reduces tothe <strong>steady-state heat equation</strong> <spanclass="math display">\[    \frac{\partial ^{2}u}{\partial x^{2}}+\frac{\partial ^{2}u}{\partialy^{2}}=0. \tag{1}\]</span> The operator <span class="math inline">\(\displaystyle\frac{\partial ^{2}}{\partial x^{2}}+\frac{\partial ^{2}}{\partialy^{2}}\)</span> is of such importance in mathematics and physics that itis often abbreviated as <span class="math inline">\(\Delta\)</span> andgiven a name: the Laplace operator or <strong>Laplacian</strong>. So thesteady-state heat equation is written as <span class="math display">\[    \Delta u=0,\]</span> and solutions to this equation are called <strong>harmonicfunctions</strong>.</p><p>Consider the unit disc in the plane <span class="math display">\[    D=\{(x, y) \in \mathbb{R}^{2}\colon x^{2}+y^{2}&lt;1\},\]</span> whose boundary is the unit circle <spanclass="math inline">\(C\)</span>. In polar coordinates <spanclass="math inline">\((r,\theta)\)</span>, with <spanclass="math inline">\(0\leqslant r\)</span> and <spanclass="math inline">\(0\leqslant \theta&lt;2\pi\)</span>, we have <spanclass="math display">\[    D=\{(r,\theta) \colon 0\leqslant r&lt;1\} \quad\text{and} \quadC=\{(r,\theta)\colon r=1\}.\]</span> The problem. often called the <strong>Dirichletproblem</strong> (for the Laplacian on the unit disc), is to solve thesteady-state heat equation in the unit disc subject to the boundarycondition <span class="math inline">\(u=f\)</span> on <spanclass="math inline">\(C\)</span>. This corresponds to fixing apredetermined temperature distribution on the circle, waiting a longtime, and then looking at the temperature distribution inside thedisc.</p><p>While the method of separation of variables will turn out to beuseful for equation (1), a difficulty comes from the fact that theboundary condition is not easily expressed in terms of rectangularcoordinates. Since this boundary condition is best described by thecoordinates <span class="math inline">\((r,\theta)\)</span>, namely<span class="math inline">\(u(1,\theta)=f(\theta)\)</span>, we rewritethe Laplacian in polar coordinates. An application of the chain rulegives: <span class="math display">\[    \Delta u= \frac{\partial ^{2}u}{\partial r^{2}}+\frac{1}{r}\frac{\partial u}{\partial r}+\frac{1}{r^{2}} \frac{\partial^{2}u}{\partial \theta^{2}}.\]</span> We now multiply both sides by <spanclass="math inline">\(r^{2}\)</span>, and since <spanclass="math inline">\(\Delta u=0\)</span>, we get <spanclass="math display">\[    r^{2}\frac{\partial ^{2}u}{\partial r^{2}}+r\frac{\partialu}{\partial r}=-\frac{\partial ^{2}u}{\partial \theta^{2}}.\]</span> Separating these variables, and looking for a solution of theform <span class="math inline">\(u(r,\theta)=F(r)G(\theta)\)</span>, wefind <span class="math display">\[    \frac{r^{2}F&#39;&#39;(r)+rF&#39;(r)}{F(r)}=-\frac{G&#39;&#39;(\theta)}{G(\theta)}.\]</span> Since the two sides depend on different variables, they mustboth be constant, say equal to <spanclass="math inline">\(\lambda\)</span>. We therefore get the followingequations: <span class="math display">\[    \begin{cases}        G&#39;&#39;(\theta)+\lambda G(\theta)=0, \\        r^{2} F&#39;&#39;(r)+rF&#39;(r)-\lambda F(r)=0.    \end{cases}\]</span> Since <span class="math inline">\(G\)</span> must be periodicof period <span class="math inline">\(2\pi\)</span>, this implies that<span class="math inline">\(\lambda\geqslant 0\)</span> and (as we haveseen before) that <span class="math inline">\(\lambda=m^{2}\)</span>where <span class="math inline">\(m\)</span> is an integer; hence <spanclass="math display">\[    G(\theta)= \tilde{A} \mathrm{e}^{im\theta} +\tilde{B}\mathrm{e}^{-im\theta} .\]</span> An application of Euler's identity, <spanclass="math inline">\(\mathrm{e}^{ix} =\cos x+ i \sin x\)</span>, allowsone to rewrite <span class="math inline">\(G\)</span> in terms ofcomplex exponentials, <span class="math display">\[    G(\theta)= A \mathrm{e}^{im\theta} +B \mathrm{e}^{-im \theta} .\]</span></p><p>With <span class="math inline">\(\lambda=m^{2}\)</span> and <spanclass="math inline">\(m\neq 0\)</span>, two simple solution of theequation in <span class="math inline">\(F\)</span> are <spanclass="math inline">\(F(r)=r^{m}\)</span> and <spanclass="math inline">\(F(r)=r^{-m}\)</span>. If <spanclass="math inline">\(m=0\)</span>, then <spanclass="math inline">\(F(r)=1\)</span> and <spanclass="math inline">\(F(r)=\log r\)</span> are two solutions. If <spanclass="math inline">\(m&gt;0\)</span>, we note that <spanclass="math inline">\(r^{-m}\)</span> grows unboundedly large as <spanclass="math inline">\(r\)</span> tends to zero, so <spanclass="math inline">\(F(r)G(\theta)\)</span> is unbounded at theoriginl; the same occurs when <span class="math inline">\(m=0\)</span>and <span class="math inline">\(F(r)= \log r\)</span>. We reject thesesolutions as countrary to our intuition. Therefore, we are left with thefollowing special functions: <span class="math display">\[    u_{m}(r,\theta)= r^{\lvert m \rvert }\mathrm{e}^{im\theta} , \quad m\in \mathbb{Z}.\]</span> We now make the important observation that (1) is linear, andso as in the case of the vibrating string, we may superpose the abovespecial solutions to obtain the presumed general solution: <spanclass="math display">\[    u(r,\theta)= \sum_{m=-\infty}^{\infty} a_m r^{\lvert m \rvert}\mathrm{e}^{im \theta} .\]</span> If this expression gave all the solutions to the steady-stateheat equation, then for a reasonable <spanclass="math inline">\(f\)</span> we should have <spanclass="math display">\[    u(1,\theta)= \sum_{m=-\infty}^{\infty} a_m \mathrm{e}^{im \theta} =f(\theta).\]</span> We therefore ask again in this context: given any reasonablefunction <span class="math inline">\(f\)</span> on <spanclass="math inline">\([0,2\pi]\)</span> with <spanclass="math inline">\(f(0)= f(2\pi)\)</span>, can we find coefficients<span class="math inline">\(a_m\)</span> so that <spanclass="math display">\[    f(\theta)= \sum_{m=-\infty}^{\infty} a_m \mathrm{e}^{im \theta}\]</span></p><blockquote><p>solution to Euler equation: <span class="math display">\[r^{2}F&#39;&#39;(r)+rF&#39;(r)-n^{2}F(r)=0,\]</span> which are twice differentiable when <spanclass="math inline">\(r&gt;0\)</span>, are given by linear combinationsof <span class="math inline">\(r^{n}\)</span> ad <spanclass="math inline">\(r^{-n}\)</span> when <spanclass="math inline">\(n\neq 0\)</span>, and <spanclass="math inline">\(1\)</span> and <span class="math inline">\(\logr\)</span> when <span class="math inline">\(n=0\)</span>. Write <spanclass="math inline">\(F(r)=g(r)r^{n}\)</span>, then <spanclass="math inline">\(rg&#39;(r)+2ng(r)=c\)</span> where <spanclass="math inline">\(c\)</span> is a constant.</p></blockquote><p><strong>Dirichlet problem in the rectangle</strong> <spanclass="math inline">\(R=\{(x, y)\colon 0\leqslant x\leqslant\pi,0\leqslant y\leqslant 1\}\)</span> and <span class="math display">\[    u(x,0)=f_0(x) \quad u(x,1)=f_1(x) \quad u(0,y)=u(\pi,y)=0,\]</span> If <span class="math inline">\(f_0\)</span> and <spanclass="math inline">\(f_1\)</span> have Fourier expansions <spanclass="math display">\[    f_0(x)=\sum_{k=1}^{\infty} A_k \sin kx \quad f_1(x)=\sum_{k=1}^{\infty} B_k \sin kx,\]</span> then <span class="math display">\[    u(x, y)= \sum_{k=1}^{\infty} \left( \frac{\sinh k(1-y)}{\sinh k}A_k+\frac{\sinh ky}{\sinh k}B_k\right) \sin kx\]</span></p><h2 id="the-poisson-kernel-and-dirichlets-problem-in-the-unit-disc">ThePoisson kernel and Dirichlet's problem in the unit disc</h2><p>To adapt Abel summability to the context of Fourier series, we definethe Abel means of the function $f() _{n=-}^{} a_n ^{in } $ by <spanclass="math display">\[    A_r(f)(\theta)= \sum_{n=-\infty}^{\infty} r^{\lvert n \rvert }a_n\mathrm{e}^{in \theta} .\]</span> It is natural to write <spanclass="math inline">\(c_0=a_0\)</span>, and <spanclass="math inline">\(c_n= a_n \mathrm{e}^{in \theta}+a_{-n}\mathrm{e}^{-in \theta}\)</span> for <spanclass="math inline">\(n&gt;0\)</span>.</p><p>Since <span class="math inline">\(f\)</span> is integrable, $a_n $ isuniformly bounded in <span class="math inline">\(n\)</span>, so that<span class="math inline">\(A_r(f)\)</span> converges absolutely anduniformly for each <span class="math inline">\(0\leqslantr&lt;1\)</span>. <span class="math display">\[    A_r(f)(\theta)=(f * P_r)(\theta),\]</span> where <span class="math inline">\(P_{r}(\theta)\)</span> isthe <strong>Poisson kernel</strong> given by <spanclass="math display">\[    P_{r}(\theta)= \sum_{n=-\infty}^{\infty} r^{\lvert n \rvert}\mathrm{e}^{in \theta}\]</span> Actually, if <span class="math inline">\(0\leqslantr&lt;1\)</span>, then <span class="math display">\[    P_{r}(\theta)= \frac{1-r^{2}}{1-2r \cos \theta+r^{2}}.\]</span> The Poisson kernel is a good kernel, as <spanclass="math inline">\(r\)</span> tends to <spanclass="math inline">\(1\)</span> from below.</p><p><strong>Theorem</strong> The Fourier series of an integrable functionon the circle is Abel summable to <span class="math inline">\(f\)</span>at every point of continuity. Moreover, if <spanclass="math inline">\(f\)</span> is continuous on the circle, then theFourier series of <span class="math inline">\(f\)</span> is uniformlyAbel summable to <span class="math inline">\(f\)</span>.</p><p>We now return to Dirichlet problem in the unit disc with boundarycondition <span class="math inline">\(u=f\)</span> on the circle. Weexpected that a solution was given by <span class="math display">\[    u(r,\theta)= \sum_{m=-\infty}^{\infty} a_mr^{\lvert m \rvert}\mathrm{e}^{im \theta} ,\]</span> where <span class="math inline">\(a_m\)</span> was the <spanclass="math inline">\(m^{th}\)</span> Fourier coefficient of <spanclass="math inline">\(f\)</span>. In other words, we were led to take<span class="math display">\[    u(r,\theta)= A_{r}(f)(\theta)= \frac{1}{2\pi} \int_{-\pi}^{\pi}f(\varphi)P_{r}(\theta-\varphi) \mathrm{d}\varphi.\]</span></p><p><strong>Theorem</strong> Let <span class="math inline">\(f\)</span>be an integrable function defined on the unit circle. Then the function<span class="math inline">\(u\)</span> defined in the unit disc by thePoisson integral <span class="math display">\[    u(r,\theta)=(f*P_{r})(\theta)\]</span> has the following properties: - u has two continuousderivatives in the unit disc and satisfies <spanclass="math inline">\(\Delta u=0\)</span>. - If <spanclass="math inline">\(\theta\)</span> is any point of continuity of<span class="math inline">\(f\)</span>, then <spanclass="math display">\[    \lim_{r \to 1}u(r,\theta)=f(\theta).\]</span> If <span class="math inline">\(f\)</span> is continuouseverywhere, then this limit is uniform. - If <spanclass="math inline">\(f\)</span> is continuous, then <spanclass="math inline">\(u(r,\theta)\)</span> is the unique solution to thesteady-state heat equation in the disc which satisfies the above twoconditions.</p><p>We only prove the last property. Suppose <spanclass="math inline">\(v\)</span> solves the steady-state heat equationin the disc and converges to <span class="math inline">\(f\)</span>uniformly as <span class="math inline">\(r\)</span> tends to <spanclass="math inline">\(1\)</span> from below. For each fixed <spanclass="math inline">\(r\)</span> with <spanclass="math inline">\(0&lt;r&lt;1\)</span>, the function <spanclass="math inline">\(v(r,\theta)\)</span> has a Fourier series <spanclass="math display">\[    \sum_{n=-\infty}^{\infty} a_n(r)\mathrm{e}^{in \theta}\quad\text{where}\quad a_n(r)=\frac{1}{2\pi} \int_{-\pi}^{\pi}v(r,\theta)\mathrm{e}^{-in \theta}  \mathrm{d}\theta.     \]</span> Taking into account that <spanclass="math inline">\(v(r,\theta)\)</span> solves the equation <spanclass="math display">\[    \frac{\partial ^{2}v}{\partial r^{2}}+\frac{1}{r}\frac{\partialv}{\partial r}+\frac{1}{r^{2}}\frac{\partial ^{2}v}{\partial\theta^{2}}=0, \tag{7}\]</span> we find that <span class="math display">\[    a&#39;&#39;_n(r)+\frac{1}{r}a_n&#39;(r)-\frac{n^{2}}{r^{2}}a_n(r)=0. \tag{8}\]</span> Indeed, we may first multiply (7) by $^{-in } $ and integratein <span class="math inline">\(\theta\)</span>. Then, since <spanclass="math inline">\(v\)</span> is periodic, two integrations by partsgive <span class="math display">\[    \frac{1}{2\pi} \int_{-\pi}^{\pi}  \frac{\partial ^{2}v}{\partial\theta^{2}}(r,\theta)\mathrm{e}^{-in \theta}  \mathrm{d}\theta = -n^{2}a_n(r).\]</span> Finally, we may interchange the order of differentiation andintegration, which is permissible since <spanclass="math inline">\(v\)</span> has two continuous derivatives; thisyields (8).</p><p>Therefore, we must have <span class="math inline">\(a_n(r)= A_nr^{n}+B_n r^{-n}\)</span> for some constants <spanclass="math inline">\(A_n\)</span> and <spanclass="math inline">\(B_n\)</span>, when <span class="math inline">\(n\neq 0\)</span>. To evaluate the constants, we first observe that eachterm <span class="math inline">\(a_n(r)\)</span> is bounded because<span class="math inline">\(v\)</span> is bounded, therefore <spanclass="math inline">\(B_n=0\)</span>. Tho find <spanclass="math inline">\(A_n\)</span> we let <span class="math inline">\(r\to 1\)</span>. Since <span class="math inline">\(v\)</span> convergesuniformly to <span class="math inline">\(f\)</span> we find that <spanclass="math display">\[    A_n= \frac{1}{2\pi} \int_{-\pi}^{\pi} f(\theta)\mathrm{e}^{-in\theta}  \mathrm{d}\theta.\]</span> By a similar argument, this formula also holds when <spanclass="math inline">\(n=0\)</span>. Our conclusion is that for each<span class="math inline">\(0&lt;r&lt;1\)</span>, the Fourier series of<span class="math inline">\(v\)</span> is given by the series of <spanclass="math inline">\(u(r,\theta)\)</span>, so by the uniqueness ofFourier series for continuous functions, we must have <spanclass="math inline">\(u=v\)</span>.</p><p><strong>Remark.</strong> By part (iii) of the theorem, we mayconclude that if <span class="math inline">\(u\)</span> solves <spanclass="math inline">\(\Delta u=0\)</span> in the disc, and converges to<span class="math inline">\(0\)</span> uniformly as <spanclass="math inline">\(r \to 1\)</span>, then <spanclass="math inline">\(u\)</span> must be identically <spanclass="math inline">\(0\)</span>. However, if uniform convergence isreplaced by pointwise convergence, this conclusion may fail.</p><p>Actually, If <span class="math inline">\(P_{r}(\theta)\)</span>denotes the Poisson kernel, then the function <spanclass="math display">\[    u(r,\theta)= \frac{\partial P_r}{\partial \theta}=\frac{2(r^{2}-1)r\sin \theta}{(1-2r\cos \theta+r^{2})^{2}}\]</span> defined for <span class="math inline">\(0\leqslantr&lt;1\)</span> and <span class="math inline">\(\theta \in\mathbb{R}\)</span>, satisfies: - <span class="math inline">\(\Deltau=0\)</span> in the disc. - <span class="math inline">\(\lim_{r \to1}u(r,\theta)=0\)</span> for each <spanclass="math inline">\(\theta\)</span>.</p><p>However, <span class="math inline">\(u\)</span> is not identicallyzero.</p><hr /><p>Solve Laplace's equation <span class="math inline">\(\Deltau=0\)</span> in the semi infinite strip <span class="math display">\[    S=\{(x, y)\colon 0&lt;x&lt;1,0&lt;y\},\]</span> subject to the following boundary conditions <spanclass="math display">\[    \begin{cases}        u(0,y)=0 \quad y\geqslant 0, \\        u(1,y)=0 \quad y\geqslant 0,        u(x,0)=f(x) \quad 0\leqslant x\leqslant 1       \end{cases}\]</span> where <span class="math inline">\(f\)</span> is a givenfunction, with of course <spanclass="math inline">\(f(0)=f(1)=0\)</span>. Write <spanclass="math display">\[    f(x)= \sum_{n=1}^{\infty} a_n \sin (n \pi x)\]</span> and expand the general solution in terms of the specialsoluions given by <span class="math display">\[    u_n(x,y)= \mathrm{e}^{-n\pi y} \sin (n\pi x).\]</span> Express <span class="math inline">\(u\)</span> as an integralinvolving <span class="math inline">\(f\)</span>, analogous to thePoisson integral formula.</p><p>Answer:</p><p>By considering the odd extension of <spanclass="math inline">\(f\)</span> and following the derivation ofPoisson's kernel with $^{-y} $ and $^{i t} $ replacing <spanclass="math inline">\(r\)</span> and $^{it} $, respectively, we obtain<span class="math display">\[    u(x,y)= \frac{1}{2} \int_{-1}^{1} f(t)Q_y(x-t) \mathrm{d}t\]</span> where <span class="math display">\[    Q_y(t)= \frac{1-\mathrm{e}^{-2\pi y} }{1-2\mathrm{e}^{-\pi y} \cos\pi t+ \mathrm{e}^{-2\pi y} }.\]</span> or, using the fact that <span class="math inline">\(f\)</span>is odd, we have the alternate form <span class="math display">\[    u(x,y)=\frac{1}{2} \int_{0}^{1} f(t)Q(x,t) \mathrm{d}t\]</span> where <span class="math display">\[    Q(x,t)=\frac{1-\mathrm{e}^{-2\pi y} }{1-2\mathrm{e}^{-\pi y} \cos\pi (x-t)+ \mathrm{e}^{-2\pi y} }-\frac{1-\mathrm{e}^{-2\pi y}}{1-2\mathrm{e}^{-\pi y} \cos \pi (x+t)+ \mathrm{e}^{-2\pi y} }.\]</span></p><hr /><p>Consider the Dirichlet problem in the annulus defined by <spanclass="math inline">\(\{(r,\theta)\colon \rho&lt;r&lt;1\}\)</span>,where <span class="math inline">\(0&lt;\rho&lt;1\)</span> is the innerradius. The problem is to solve <span class="math display">\[    \frac{\partial ^{2}u}{\partial r^{2}}+\frac{1}{r}\frac{\partialu}{\partial r}+\frac{1}{r^{2}} \frac{\partial ^{2}u}{\partial\theta^{2}}=0\]</span> subject to the boundary conditions <spanclass="math display">\[    \begin{cases}        u(1,\theta)=f(\theta), \\        u(\rho,\theta)=g(\theta),    \end{cases}\]</span> where <span class="math inline">\(f\)</span> and <spanclass="math inline">\(g\)</span> are given continuous functions.</p><p>Arguing as we have previously for the Dirichlet problem in the disc,we can hope to write <span class="math display">\[    u(r,\theta)= \sum_{}^{} c_n(r) \mathrm{e}^{in \theta}\]</span> with <spanclass="math inline">\(c_n(r)=A_nr^{n}+B_nr^{-n},n\neq 0\)</span>. Set<span class="math display">\[    f(\theta) \sim \sum_{}^{} a_n \mathrm{e}^{in \theta}  \quad\text{and}\quad g(\theta) \sim \sum_{}^{} b_n \mathrm{e}^{in \theta}.       \]</span> We want <span class="math inline">\(c_n(1)=a_n\)</span> and<span class="math inline">\(c_n(\rho)=b_n\)</span>. This leads to thesolution <span class="math display">\[    u(r,\theta)= \sum_{n\neq 0}^{} \left(\frac{1}{\rho^{n}-\rho^{-n}}\right)[((\rho/r)^{n}-(r/\rho)^{n})a_n+(r^{n}-r^{-n})b_n]\mathrm{e}^{in \theta}+a_0+(b_0-a_0) \frac{\log r}{\log \rho}.  \]</span></p><p>Show that as a result we have <span class="math display">\[    u(r,\theta)-(P_{r}*f)(\theta)\to 0 \quad \text{as}\ r\to 1\\text{uniformly in}\ \theta\]</span> and <span class="math display">\[    u(r,\theta)-(P_{\rho/r}*g)(\theta)\to 0 \quad \text{as}\ r\to \rho\\text{uniformly in}\ \theta\]</span></p><h2 id="the-heat-equation-on-the-circle">The heat equation on thecircle</h2><p>As a final illustration, we return to the original problem of heatdiffusion considered by Fourier.</p><p>Suppose we are given an initial temperature distribution at <spanclass="math inline">\(t=0\)</span> on a ring and that we are asked todescribe the temperature at points on the ring at times <spanclass="math inline">\(t&gt;0\)</span>.</p><p>The ring is modeled by the unit circle. A point on this circle isdescribed by its angle <span class="math inline">\(\theta=2\pix\)</span>, where the variable <span class="math inline">\(x\)</span>lies between <span class="math inline">\(0\)</span> and <spanclass="math inline">\(1\)</span>. If <spanclass="math inline">\(u(x,t)\)</span> denotes the temperature at time<span class="math inline">\(t\)</span> of a point described by the angle<span class="math inline">\(\theta\)</span>, then consideration similarto the ones given in Chapter 1 show that <spanclass="math inline">\(u\)</span> satisfies the differential equation<span class="math display">\[    \frac{\partial u}{\partial t}= c\frac{\partial ^{2}u}{\partialx^{2}}.  \tag{9}\]</span></p><p>The constant <span class="math inline">\(c\)</span> is a positivephysical constant which depends on the material of which thhe ring ismade. After rescaling the time variable, we may assume that <spanclass="math inline">\(c=1\)</span>. If <spanclass="math inline">\(f\)</span> is our initial data, we impose thecondition <span class="math display">\[    u(x,0)=f(x).\]</span> To solve the problem, we separate variables and look forspecial solutions of the form <span class="math display">\[    u(x,t)=A(x)B(t).\]</span> Then inserting this expression for <spanclass="math inline">\(u\)</span> into the heat equation we get <spanclass="math display">\[    \frac{B&#39;(t)}{B(t)}= \frac{A&#39;&#39;(x)}{A(x)}.\]</span> Both sides are therefore constant, say equal to <spanclass="math inline">\(\lambda\)</span>. Since <spanclass="math inline">\(A\)</span> must be periodic of period <spanclass="math inline">\(1\)</span>, we see that the only possibility is<span class="math inline">\(\lambda=-4\pi^{2}n^{2}\)</span>, where <spanclass="math inline">\(n \in \mathbb{Z}\)</span>. Then <spanclass="math inline">\(A\)</span> is a linear combination of theexponentials $^{2inx} $ and $^{-2inx} $, and <spanclass="math inline">\(B(t)\)</span> is a multiple of$<sup>{-4</sup>{2}n^{2}t} $. By superposing these solutions, we are ledto <span class="math display">\[    u(x,t)=\sum_{n=-\infty}^{\infty} a_n \mathrm{e}^{-4\pi^{2}n^{2}t}\mathrm{e}^{2\pi inx}, \tag{10}\]</span> where, setting <span class="math inline">\(t=0\)</span>, wesee that <span class="math inline">\(\{a_n\}\)</span> are the Fouriercoefficients of <span class="math inline">\(f\)</span>.</p><p>Note that when <span class="math inline">\(f\)</span> is Riemannintegrable, the coefficients <span class="math inline">\(a_n\)</span>are bounded, and since the factor <spanclass="math inline">\(\mathrm{e}^{-4\pi^{2}n^{2}t}\)</span> tends tozero extremely fast, the series defining <spanclass="math inline">\(u\)</span> converges. In fact, in this case, <spanclass="math inline">\(u\)</span> is twice differentiable and solvesequation (9).</p><p>For a better understanding of the properties of our solution (10), wewrite it as <span class="math display">\[    u(x,t)=(f*H_t)(x),\]</span> where <span class="math inline">\(H_t\)</span> is the<strong>heat kernel for the circle</strong>, given by <spanclass="math display">\[    H_{t}(x)=\sum_{n=-\infty}^{\infty} \mathrm{e}^{-4\pi^{2}n^{2}t}\mathrm{e}^{2\pi inx} , \tag{11}\]</span> and where the convolution for functions with period <spanclass="math inline">\(1\)</span> is defined by <spanclass="math display">\[    (f*g)(x)= \int_{0}^{1} f(x-y)g(y) \mathrm{d}y.\]</span></p><p>The natural question with regard to the boundary condition is thefollowing: do we have $u(x,t) f(x) $ as <spanclass="math inline">\(t\)</span> tends to <spanclass="math inline">\(0\)</span>, and in what sense? A simpleapplication of the Parsevel identity shows that this limit holds in themean square sense, namely <span class="math display">\[    \int_{0}^{1} \lvert u(x,t)-f(x) \rvert ^{2} \mathrm{d}x \to 0 \quad\text{as}\ t\to 0.\]</span></p><p>An analogy between the heat kernel and the Poisson kernel:</p><p><span class="math display">\[    u(\theta,\tau)= \sum_{}^{} a_n \mathrm{e}^{-n^{2}\tau}\mathrm{e}^{in \theta} =(f* h_{\tau})(\theta)\]</span> of the equation <span class="math display">\[    \frac{\partial u}{\partial \tau}=\frac{\partial ^{2}u}{\partial\theta^{2}} \quad \text{with}\ 0\leqslant \theta\leqslant 2\pi \\text{and} \ \tau&gt;0,\]</span> with boundary condition $u(,0)= f() <em>{}^{} a_n ^{in } $.Here <span class="math inline">\(h_{\tau}(\theta)=\sum_{n=-\infty}^{\infty} \mathrm{e}^{-n^{2}\tau} \mathrm{e}^{in\theta}\)</span>. This version of the heat kernel on <spanclass="math inline">\([0,2\pi]\)</span> is the analogue of the Poissonkernel, which can be written as $P</em>{r}()=_{n=-}^{} e^{-n }^{in } $with $r= ^{-} $(and so <span class="math inline">\(0&lt;r&lt;1\)</span>corresponds to <span class="math inline">\(\tau&gt;0\)</span>).</p><p>Unlike in the case of the Poisson kernel, there is no elementaryformula for the heat kernel. Nevertheless, it turns out that it is agood kernel. The proof is not obvious and requires the use of thecelebrated Poisson summation formula. As a corollary, we will also findthat <span class="math inline">\(H_t\)</span> is everywhere positive, afact that is also not obvious from its defining expression (11). We can,however, give the following heuristic argument for the positivity of<span class="math inline">\(H_t\)</span>. Suppose that we begin with aninitial temperature distribution <span class="math inline">\(f\)</span>which is everywhere <span class="math inline">\(\leqslant 0\)</span>.Then it is physically reasonable to expect <spanclass="math inline">\(u(x,t)\leqslant 0\)</span> for all <spanclass="math inline">\(t\)</span> since heat travels from hot to cold.Now <span class="math display">\[    u(x,t)= \int_{0}^{1} f(x-y)H_t(y) \mathrm{d}y.\]</span> If <span class="math inline">\(H_{t}\)</span> is negative forsome <span class="math inline">\(x_0\)</span>, then we may choose <spanclass="math inline">\(f\leqslant 0\)</span> supported near <spanclass="math inline">\(0\)</span>, and this would imply <spanclass="math inline">\(u(x_0,t)&gt;0\)</span>, which is acontradiction.</p><h2 id="the-time-dependent-heat-equation-on-the-real-line">Thetime-dependent heat equation on the real line</h2><p>Here we study the analogous problem on the real line.</p><p>Consider an infinite rod, which we model by the real line, andsuppose that we are given an initial temperature distribution <spanclass="math inline">\(f(x)\)</span> on the rod at time <spanclass="math inline">\(t=0\)</span>. We wish now to determine thetemperature <span class="math inline">\(u(x,t)\)</span> at a point <spanclass="math inline">\(x\)</span> at time <spanclass="math inline">\(t&gt;0\)</span>. Considerations similar to theones given in Chapter 1 show that when <spanclass="math inline">\(u\)</span> is appropriately normalized, it solvesthe following PDE: <span class="math display">\[    \frac{\partial u}{\partial t}= \frac{\partial ^{2}u}{\partialx^{2}}, \tag{12}\]</span> called the <strong>heat equation</strong>. The initialcondition we impose is <spanclass="math inline">\(u(x,0)=f(x)\)</span>.</p><p>Just as in the case of the circle, the solution is given in terms ofa convolution. Indeed, define the <strong>heat kernel</strong> of theline by <span class="math display">\[    \mathcal{H}_{t}(x)= K_{\delta}(x), \quad \text{with} \ \delta=4\pit,\]</span> so that <span class="math display">\[    \mathcal{H}_{t}(x)=\frac{1}{(4\pi t)^{1/2}} \mathrm{e}^{-x^{2}/4t}\quad \text{and} \quad \hat{\mathcal{H}}_{t}(\xi)= \mathrm{e}^{-4\pi^{2}t \xi ^{2}} .\]</span></p><p>Taking the Fourier transform of equation (12) in the <spanclass="math inline">\(x\)</span> variable leads to <spanclass="math display">\[    \frac{\partial \hat{u}}{\partial t}(\xi, t)= -4\pi^{2} \xi^{2}\hat{u}(\xi,t).\]</span> Fixing <span class="math inline">\(\xi\)</span>, this is anODE in the variable <span class="math inline">\(t\)</span>, so thereexists a constant <span class="math inline">\(A(\xi)\)</span> so that<span class="math display">\[    \hat{u}(\xi,t)= A(\xi) \mathrm{e}^{-4\pi^{2}\xi^{2}t} .\]</span> We may also take the Fourier transform of the initialcondition and obtain <span class="math inline">\(\hat{u}(\xi,0)=\hat{f}(\xi)\)</span>, hence <spanclass="math inline">\(A(\xi)=\hat{f}(\xi)\)</span>. This leads to thefollowing theorem.</p><p><strong>Theorem 2.1</strong> Given <span class="math inline">\(f\in\mathcal{S}(\mathbb{R})\)</span>, let <span class="math display">\[    u(x,t)=(f*\mathcal{H}_{t})(x) \quad t&gt;0\]</span> where <span class="math inline">\(\mathcal{H}_{t}\)</span> isthe heat kernel. Then: 1. The function <spanclass="math inline">\(u\)</span> is <spanclass="math inline">\(C^{2}\)</span> when <span class="math inline">\(x\in \mathbb{R}\)</span> and <span class="math inline">\(t&gt;0\)</span>,and <span class="math inline">\(u\)</span> solves the heat equation. 2.<span class="math inline">\(u(x,t) \to f(x)\)</span> uniformly in <spanclass="math inline">\(x\)</span> as <span class="math inline">\(t \to0\)</span>. Hence if we set <spanclass="math inline">\(u(x,0)=f(x)\)</span>, then <spanclass="math inline">\(u\)</span> is continuous on the closure of theupper half-plane <spanclass="math inline">\(\overline{\mathbb{R}_{+}^{2}}=\{(x,t)\colon x\in\mathbb{R}, t\geqslant 0\}\)</span>. 3. <spanclass="math inline">\(\int_{-\infty}^{\infty} \lvert u(x,t)-f(x) \rvert^{2} \mathrm{d}x \to 0\)</span> as <span class="math inline">\(t \to0\)</span>.</p><p><strong>proof</strong> Because <span class="math inline">\(u= f*\mathcal{H}_{t}\)</span>, taking the Fourier transform in the <spanclass="math inline">\(x-variable\)</span> gives <spanclass="math inline">\(\hat{u}=\hat{f} \hat{\mathcal{H}}_{t}\)</span>,and so $(,t)=() <sup>{-4</sup>{2}^{2}t} $. The Fourier inversion formulagives <span class="math display">\[    u(x,t)= \int_{-\infty}^{\infty} \hat{f}(\xi)\mathrm{e}^{-4\pi^{2}t\xi^{2}} \mathrm{e}^{2\pi i \xi x}  \mathrm{d}\xi .\]</span> By differentiating under the integral sign, one verifies 1. Infact, one observes that <span class="math inline">\(u\)</span> isindefinitely differentiable. Note that 2 is an immediate consequence ofCorollary 1.7. Finally, by Plancherel's formula, we have <spanclass="math display">\[    \begin{aligned}        \int_{-\infty}^{\infty} \lvert u(x,t)-f(x) \rvert ^{2}\mathrm{d}x &amp;= \int_{-\infty}^{\infty} \lvert\hat{u}(\xi,t)-\hat{f}(\xi) \rvert ^{2} \mathrm{d}\xi \\        &amp;= \int_{-\infty}^{\infty} \lvert \hat{f}(\xi) \rvert ^{2}\lvert \mathrm{e}^{-4\pi^{2}t \xi^{2}} -1 \rvert ^{2} \mathrm{d}\xi.    \end{aligned}\]</span></p><p>To see that this last integral goes to <spanclass="math inline">\(0\)</span> as <span class="math inline">\(t \to0\)</span>, we argue as follows: since <spanclass="math inline">\(\lvert \mathrm{e}^{-4\pi^{2}t \xi ^{2}} -1 \rvert\leqslant 2\)</span> and <span class="math inline">\(f\in\mathcal{S}(\mathbb{R})\)</span>, we can find <spanclass="math inline">\(N\)</span> so that <span class="math display">\[    \int_{\lvert \xi \rvert \geqslant N}^{} \lvert \hat{f}(\xi) \rvert^{2}\lvert \mathrm{e}^{-4\pi^{2}t \xi^{2}} -1 \rvert  \mathrm{d}\xi&lt;\varepsilon,\]</span> and for all small <span class="math inline">\(t\)</span> wehave <span class="math inline">\(\sup_{\lvert \xi \rvert \leqslantN}\lvert \hat{f}(\xi) \rvert ^{2}\lvert \mathrm{e}^{-4\pi^{2}t \xi^{2}}-1 \rvert ^{2}&lt;\varepsilon/2N\)</span> since <spanclass="math inline">\(\hat{f}\)</span> is bounded. Thus <spanclass="math display">\[    \int_{\lvert \xi \rvert \leqslant N}^{} \lvert \hat{f}(\xi) \rvert^{2}\lvert \mathrm{e}^{-4\pi^{2}t \xi^{2}} -1 \rvert ^{2} \mathrm{d}\xi&lt; \varepsilon \quad \text{for all small}\ t.     \]</span> This completes the proof of the theorem.</p><h2 id="the-steady-state-heat-equation-in-the-upper-half-plane">Thesteady-state heat equation in the upper half-plane</h2><p>The equation we are now concerned with is <spanclass="math display">\[    \Delta u= \frac{\partial ^{2}u}{\partial x^{2}}+\frac{\partial^{2}u}{\partial y^{2}}=0  \tag{13}\]</span> in the upper half-plane <spanclass="math inline">\(\mathbb{R}_{+}^{2}=\{(x, y)\colon x\in\mathbb{R},y&gt;0\}\)</span>. The boundary condition we require is <spanclass="math inline">\(u(x,0)=f(x)\)</span>. The kernel that solves thisproblem is called the <strong>Poisson kernel</strong> for the upperhalf-plane, and is given by <span class="math display">\[    \mathcal{P}_{y}(x)=\frac{1}{\pi}\frac{y}{x^{2}+y^{2}}\quad\text{where} \ x\in \mathbb{R}\ \text{and} y&gt;0.\]</span> This is the analogue of the Poisson kernel for the disc.</p><p>Note that for each fixed <span class="math inline">\(y\)</span> thekernel <span class="math inline">\(\mathcal{P}_{y}\)</span> is only ofmoderate decrease as a function of <spanclass="math inline">\(x\)</span>, so we will use the theory of theFourier transform appropriate for these types of functions.</p><p>We proceed as in the case of the time-dependent heat equation, bytaking the Fourier transform of equation (13) in the <spanclass="math inline">\(x\)</span> variable, thereby obtaining <spanclass="math display">\[    -4\pi^{2}\xi^{2}\hat{u}(\xi,y)+\frac{\partial ^{2}\hat{u}}{\partialy^{2}}(\xi,y)=0     \]</span> with the boundary condition <spanclass="math inline">\(\hat{u}(\xi,0)=\hat{f}(\xi)\)</span>. The generalsolution of this ODE in <span class="math inline">\(y\)</span> (with<span class="math inline">\(\xi\)</span> fixed) takes the form <spanclass="math display">\[    \hat{u}(\xi,y)= A(\xi) \mathrm{e}^{-2\pi \lvert \xi \rvert y}+B(\xi) \mathrm{e}^{2\pi\lvert \xi \rvert y} .\]</span> If we disregard the second term because of its rapidexponential increase we find, after setting <spanclass="math inline">\(y=0\)</span>, that <span class="math display">\[    \hat{u}(\xi,y)=\hat{f}(\xi) \mathrm{e}^{-2\pi \lvert \xi \rvert y} .\]</span> Therefore <span class="math inline">\(u\)</span> is given interms of the convolution of <span class="math inline">\(f\)</span> witha kernel whose Fourier transform is $^{-2y} $. This is precisely thePoisson kernel given above, as we prove next.</p><p><strong>Lemma 2.4</strong> The following two identities hold: <spanclass="math display">\[    \int_{-\infty}^{\infty} \mathrm{e}^{-2\pi\lvert \xi \rvert y}\mathrm{e}^{2\pi i \xi x}  \mathrm{d}\xi = \mathcal{P}_y(x),\]</span> <span class="math display">\[    \int_{-\infty}^{\infty} \mathcal{P}_y(x)\mathrm{e}^{-2\pi ix \xi}\mathrm{d}x= \mathrm{e}^{-2\pi\lvert \xi \rvert y} .\]</span></p><p><strong>Lemma 2.5</strong> The Poisson kernel is a good kernel on<span class="math inline">\(\mathbb{R}\)</span> as <spanclass="math inline">\(y \to 0\)</span>.</p><p>The following theorem establishes the existence of a solution to ourproblem.</p><p><strong>Theorem 2.6</strong> Given <span class="math inline">\(f \in\mathcal{S}(\mathbb{R})\)</span>,let <span class="math inline">\(u(x,y)=(f*\mathcal{P}_{y})(x)\)</span>. Then: 1. <spanclass="math inline">\(u (x, y)\)</span> is <spanclass="math inline">\(C^{2}\)</span> in <spanclass="math inline">\(\mathbb{R}^{2}_{+}\)</span> and <spanclass="math inline">\(\Delta u=0\)</span>. 2. <spanclass="math inline">\(u(x, y) \to f(x)\)</span> uniformly as <spanclass="math inline">\(y \to 0\)</span>. 3. <spanclass="math inline">\(\int_{-\infty}^{\infty} \lvert u(x, y)-f(x) \rvert^{2} \mathrm{d}x \to 0\)</span> as <span class="math inline">\(y \to0\)</span>. 4. If <span class="math inline">\(u(x,0)=f(x)\)</span>, then<span class="math inline">\(u\)</span> is continuous on the closure<span class="math inline">\(\overline{\mathbb{R}^{2}_{+}}\)</span> ofthe upper half-plane, and vanishes at infinity in the sense that <spanclass="math display">\[    u(x, y) \to 0 \quad \text{as} \ \lvert x \rvert +y \to \infty.\]</span></p><h2 id="heat-kernel">Heat kernel</h2><p>Another application related to the Poisson summation formula and thetheta function is the time-dependent heat equation on the circle. Asolution to the equation <span class="math display">\[    \frac{\partial u}{\partial t}=\frac{\partial ^{2}u}{\partial x^{2}}\]</span> subject to <span class="math inline">\(u(x,0)=f(x)\)</span>,where <span class="math inline">\(f\)</span> is periodic of period <spanclass="math inline">\(1\)</span>, was given in the previous chapter by<span class="math display">\[    u(x,t)=(f*H_t)(x)\]</span> where <span class="math inline">\(H_t(x)\)</span> is the heatkernel on the circle, that is, <span class="math display">\[    H_t(x)= \sum_{n=-\infty}^{\infty} \mathrm{e}^{-4\pi^{2}n^{2}t}\mathrm{e}^{2\pi inx} .\]</span> Note in particular that with our definition of the generalizedtheta function in the previous section, we have <spanclass="math inline">\(\Theta(x|4\pi it)=H_t(x)\)</span>. Also, recallthat the heat equation on <spanclass="math inline">\(\mathbb{R}\)</span> gave rise to the heat kernel.<span class="math display">\[    \mathcal{H}_{t}(x)=\frac{1}{(4\pi t)^{1/2}}\mathrm{e}^{-x^{2}/4t}\]</span> where <spanclass="math inline">\(\hat{\mathcal{H}}_{t}(\xi)=\mathrm{e}^{-4\pi^{2}\xi^{2}t}\)</span>.The fundamental relation between these two objects is an immediateconsequence of the Poisson summation formula:</p><p><strong>Theorem 3.3</strong> The heat kernel on the circle is theperiodization of the heat kernel on the real line: <spanclass="math display">\[    H_t(x)= \sum_{n=-\infty}^{\infty} \mathcal{H}_{t}(x+n).\]</span> Although the proof that <spanclass="math inline">\(\mathcal{H}_t\)</span> is a good kernel on <spanclass="math inline">\(\mathbb{R}\)</span> was fairly straightforward, weleft open the harder problem that <spanclass="math inline">\(H_t\)</span> is a good kernel on the circle. Theabove results allow us to resolve this matter.</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>9</title>
    <link href="/2022/06/23/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%889%EF%BC%89/"/>
    <url>/2022/06/23/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%889%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><h2 id="sturm-">Sturm </h2><p> <span class="math display">\[    y&#39;&#39;+p(x)y&#39;+q(x)y=0 \tag{9.1}\]</span>  <span class="math inline">\(p(x)\)</span>  <spanclass="math inline">\(q(x)\)</span>  <spanclass="math inline">\(J\)</span> .</p><p><strong>9.1</strong>9.1 <spanclass="math inline">\(J\)</span> .</p><p>.</p><p><strong>9.1</strong> <spanclass="math inline">\(y=\varphi_1(x)\)</span>  <spanclass="math inline">\(y=\varphi_2(x)\)</span> 9.1-  -.</p><p><strong></strong> Wronsky  Liouville.</p><p><strong>9.2Sturm </strong><span class="math display">\[    y&#39;&#39;+P(x)y&#39;+Q(x)y=0 \tag{9.6}\]</span>  <span class="math display">\[    y&#39;&#39;+p(x)y&#39;+R(x)y=0 \tag{9.7}\]</span>  <span class="math inline">\(p(x),Q(x)\)</span> <span class="math inline">\(R(x)\)</span>  <spanclass="math inline">\(J\)</span>  <spanclass="math display">\[    R(x)\geqslant Q(x) \quad (x \in J) \tag{9.8}\]</span> .  <span class="math inline">\(y=\varphi(x)\)</span>9.6 <span class="math inline">\(x_1\)</span> <span class="math inline">\(x_2\)</span> .9.7 <span class="math inline">\(y=\psi(x)\)</span> <span class="math inline">\(x_1\)</span>  <spanclass="math inline">\(x_2\)</span>  <spanclass="math inline">\(x_0\)</span><span class="math inline">\(x_0 \in[x_1,x_2]\)</span>.</p><p>9.8 <span class="math display">\[    R(x)&gt;Q(x) \tag{9.8*}\]</span>  <span class="math inline">\(x_0 \in(x_1,x_2)\)</span></p><p><strong></strong> <spanclass="math inline">\(v(x)=\psi(x)\varphi&#39;(x)-\varphi(x)\psi&#39;(x)\)</span> <span class="math inline">\(v&#39;(x)+p(x)v(x)\)</span>  <spanclass="math inline">\([x_1,x_2]\)</span> .</p><p><strong></strong>9.1. <span class="math inline">\(R(x)=Q(x)\)</span></p><p> <span class="math inline">\(y=\varphi(x)\)</span>9.1.  <spanclass="math inline">\(y=\varphi(x)\)</span>  <spanclass="math inline">\(J\)</span>  <spanclass="math inline">\(J\)</span><strong></strong> <spanclass="math inline">\(J\)</span> <strong></strong>.  <spanclass="math inline">\(y=\varphi(x)\)</span>  <spanclass="math inline">\(J\)</span>  <spanclass="math inline">\(J\)</span> <strong></strong>.</p><p><strong>1</strong>9.1 <spanclass="math display">\[    q(x)\leqslant 0 \quad(x \in J)\]</span> .  <spanclass="math inline">\(y&#39;&#39;+p(x)y&#39;=0\)</span>  <spanclass="math inline">\(y=\psi(x) \equiv 1\)</span> </p><p><strong>2</strong> <span class="math display">\[    y&#39;&#39;+Q(x)y=0 \tag{9.13}\]</span>  <span class="math inline">\(Q(x)\)</span>  <spanclass="math inline">\(a\leqslant x&lt;\infty\)</span> <span class="math display">\[    Q(x)\geqslant m&gt;0 \quad(m )\]</span> 9.13 <spanclass="math inline">\(y=\varphi(x)\)</span>  <spanclass="math inline">\([a,\infty)\)</span> <spanclass="math inline">\(\displaystyle \frac{\pi}{\sqrt{m}}\)</span>.</p><p> <span class="math display">\[    y&#39;&#39;+\frac{1}{4x^{2}}y=0 \quad(1\leqslant x&lt;\infty)\]</span>  <span class="math display">\[    y=\sqrt{x}(C_1+C_2 \ln x)\]</span> 2 <spanclass="math inline">\(Q(x)&gt;0\quad(a\leqslantx&lt;\infty)\)</span></p><p> <span class="math inline">\(Q(x)\)</span> <span class="math display">\[    Q(x)\leqslant M \quad(a\leqslant x&lt;\infty)\]</span>  <span class="math inline">\(M&gt;0\)</span>. <span class="math inline">\(y=\varphi(x)\)</span> <span class="math inline">\(\displaystyle\frac{\pi}{\sqrt{M}}\)</span></p><p> <span class="math inline">\(y&#39;&#39;+My=0\)</span> <span class="math inline">\(y=\sin [\sqrt{M}(x-c)]\)</span> <span class="math inline">\(c\)</span> .</p><h2 id="s-l-">S-L </h2><p> <span class="math display">\[    [p(x)y&#39;]&#39;+[q(x)+\lambda r(x)]y=0 \tag{9.16}\]</span></p><p> <span class="math inline">\(\lambda\)</span> <span class="math inline">\(p(x),q(x)\)</span> <span class="math inline">\(r(x)\)</span>  <spanclass="math inline">\(a\leqslant x\leqslant b\)</span> <spanclass="math inline">\(p(x)\)</span>  <spanclass="math inline">\(p(x)&gt;0\)</span>  <spanclass="math inline">\(r(x)&gt;0\)</span>.  <spanclass="math display">\[    Ky(a)+Ly&#39;(a)=0,\quad My(b)+Ny&#39;(b)=0, \tag{9.17}\]</span>  <span class="math inline">\(K,L,M,N\)</span> <span class="math display">\[    K^{2}+L^{2}&gt;0, \quad M^{2}+N^{2}&gt;0.\]</span></p><p> <strong>Sturm-Liouville </strong>. <span class="math inline">\(\lambda= \lambda_0\)</span> <span class="math inline">\(y=\varphi_0(x)\)</span>. <span class="math inline">\(\lambda_0\)</span><strong></strong><spanclass="math inline">\(y=\varphi_0(x)\)</span><strong></strong> <spanclass="math inline">\(C \neq 0\)</span><span class="math inline">\(y=C\varphi_0(x)\)</span> .</p><p> Prfer<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label=". Sturm. 1987">[1]</span></a></sup><span class="math display">\[    y(x)=\rho(x) \sin \theta(x), \quad p(x)y&#39;(x)=\rho(x) \cos\theta(x)\]</span> 9.16 <span class="math display">\[    \theta&#39;(x)= \frac{\cos ^{2}\theta(x)}{p(x)}+[q(x)+\lambda r(x)]\sin ^{2}\theta(x),\]</span> <span class="math display">\[    \rho&#39;(x)= \rho(x)[\frac{1}{p(x)}-q(x)- \lambda r(x)] \cos\theta(x) \sin \theta(x);\]</span></p><p>9.16 <span class="math display">\[    y&#39;&#39;(x)=-\frac{\rho(x)}{p(x)}[q(x)+\lambda r(x)] \sin\theta(x)- \frac{\rho(x)p&#39;(x)}{p^{2}(x)} \cos \theta(x)\]</span></p><p> <span class="math display">\[    y(a) \cos \theta_a-p(a)y&#39;(a) \sin \theta_a=0, \quad y(b) \cos\theta_b- p(b)y&#39;(b) \sin \theta_b=0.\]</span></p><p>9.16 <span class="math display">\[    y&#39;&#39;+(\lambda+q(x))y=0, \tag{9.20}       \]</span>  <span class="math inline">\(q(x)\)</span>  <spanclass="math inline">\([0,1]\)</span> 9.17<span class="math display">\[    y(0)\cos \alpha - y&#39;(0) \sin \alpha=0, \quad y(1)\cos \beta-y&#39;(1) \sin \beta=0, \tag{9.21}\]</span>  <span class="math inline">\(0\leqslant\alpha&lt;\pi, 0&lt;\beta\leqslant \pi\)</span>.</p><blockquote><p>.</p></blockquote><p>9.20 <spanclass="math inline">\(y=\varphi(x,\lambda)\)</span> <span class="math display">\[    \varphi(0,\lambda)= \sin \alpha, \quad \varphi&#39;(0,\lambda)=\cos\alpha. \tag{9.22}\]</span> 9.21.  <span class="math display">\[    \varphi(x,\lambda)= \rho(x,\lambda)\sin \theta(x,\lambda), \quad\varphi&#39;(x,\lambda)=\rho(x,\lambda) \cos \theta(x,\lambda),\]</span>  <span class="math display">\[    \begin{cases}        \rho(x,\lambda)=\sqrt{[\varphi(x,\lambda)]^{2}+[\varphi&#39;(x,\lambda)]^{2}}\quad(&gt;0), \\        \theta(x,\lambda)= \arctan\frac{\varphi(x,\lambda)}{\varphi&#39;(x,\lambda)} \quad (0\leqslantx\leqslant 1).              \end{cases}\]</span> 9.21 <span class="math display">\[    \theta(0,\lambda)= \arctan \frac{\sin \alpha}{\cos\alpha}=\alpha+j\pi, \tag{9.23}\]</span>  <span class="math inline">\(j\)</span> . <spanclass="math inline">\(\theta=\theta(x,\lambda)\)</span>  <spanclass="math display">\[    \theta(1,\lambda)= \beta+ k\pi, \tag{9.24}\]</span></p><p><span class="math inline">\(\theta(x)\)</span>  <spanclass="math display">\[    \theta&#39;=\cos ^{2}\theta+[\lambda+q(x)]\sin ^{2}\theta \tag{9.25}\]</span></p><p>9.23 <span class="math inline">\(j=0\)</span> <spanclass="math inline">\(\theta(0,\lambda)=\alpha\)</span>.</p><p><strong> 9.2</strong> <spanclass="math inline">\(\omega(\lambda)=\theta(1,\lambda)\)</span>. <span class="math inline">\(\omega(\lambda)\)</span>  <spanclass="math inline">\(-\infty&lt;\lambda&lt;\infty\)</span>.</p><p><strong></strong>9.25 <spanclass="math inline">\(\lambda\)</span>  <spanclass="math display">\[    \frac{\mathrm{d}}{\mathrm{d}x} \frac{\partial \theta}{\partial\lambda}= [\lambda+q(x)-1] \sin 2\theta \frac{\partial \theta}{\partial\lambda} + \sin ^{2} \theta, \tag{9.27}\]</span></p><p> <span class="math inline">\(\theta(0,\lambda)=\alpha\)</span> <span class="math display">\[    \frac{\partial \theta}{\partial \lambda}(0,\lambda)=0. \tag{9.28}\]</span> 9.27 <span class="math inline">\(\displaystyle\frac{\partial \theta}{\partial \lambda}\)</span> .9.28 <span class="math display">\[    \frac{\partial \theta}{\partial \lambda}(x,\lambda)= \int_{0}^{x}\mathrm{e}^{\int_{t}^{x} E(s,\lambda) \mathrm{d}s} \sin^{2}\theta(t,\lambda) \mathrm{d}t\]</span>  <span class="math display">\[    E(s,\lambda)= [\lambda+q(s)-1] \sin 2\theta(s,\lambda).\]</span></p><p> <span class="math inline">\(\sin ^{2} \theta(x,\lambda)\)</span><span class="math inline">\(0\leqslant x\leqslant1\)</span>.  <span class="math display">\[    \omega&#39;(\lambda)=\frac{\partial \theta}{\partial\lambda}(1,\lambda)&gt;0,\]</span> .</p><p><strong> 9.3</strong> <spanclass="math inline">\(-\infty&lt;\lambda&lt;\infty\)</span> <spanclass="math inline">\(\omega(\lambda)&gt;0\)</span> <spanclass="math display">\[    \lim_{\lambda \to -\infty} \omega(\lambda)=0.\]</span> <strong></strong>9.25 <spanclass="math inline">\(\theta(x,\lambda)&gt;0, 0&lt;x\leqslant1\)</span>.  <spanclass="math inline">\(\omega(\lambda)=\theta(1,\lambda)&gt;0,-\infty&lt;\lambda&lt;\infty\)</span>.</p><p> <span class="math inline">\(0\leqslant\alpha&lt;\pi\)</span> <spanclass="math inline">\((x,\theta)\)</span>  <spanclass="math inline">\(A(0,\pi-\varepsilon)\)</span>  <spanclass="math inline">\(B(1,\varepsilon)\)</span>.  <spanclass="math inline">\(\theta=\theta(x,\lambda)\)</span> <span class="math inline">\(x= \bar{x}_1\)</span> <spanclass="math inline">\(\theta&#39;(\bar{x}_1,\lambda)\geqslant  AB K=2 \varepsilon-\pi\)</span>. 9.25 <spanclass="math display">\[    \theta&#39;(\bar{x}_1,\lambda)=\cos ^{2}\theta+[\lambda+q(\bar{x}_1)]\sin ^{2}\theta\]</span>  <span class="math inline">\(\lambda \to-\infty\)</span> <spanclass="math inline">\(\theta&#39;(\bar{x}_1,\lambda)&lt;2\varepsilon-\pi\)</span> .  <spanclass="math inline">\(\sin \theta \geqslant \varepsilon\)</span><span class="math display">\[    1+[\lambda+M]\sin ^{2}\varepsilon&lt;2 \varepsilon-\pi\]</span>  <span class="math inline">\(M= \max\{q(x)\colon0\leqslant x\leqslant 1\}\)</span> <spanclass="math inline">\(AB\)</span>  <spanclass="math inline">\(\lambda\)</span> .  <spanclass="math inline">\(\lambda \to -\infty\)</span> <spanclass="math inline">\(\omega(\lambda)\to 0\)</span>.</p><p></p><hr /><p>()(10)https://zhuanlan.zhihu.com/p/151401565</p><hr /><p><strong> 9.4</strong> <span class="math inline">\(\lambda \to\infty\)</span>  <span class="math inline">\(\omega(\lambda)\to\infty\)</span>.</p><p><strong></strong> <span class="math display">\[    \int_{0}^{1} \frac{\theta&#39;}{\cos ^{2}\theta+[\lambda+q(x)]\sin^{2}\theta}\mathrm{d}x=\int_{\alpha}^{\omega(\lambda)}  \frac{\mathrm{d}\theta}{\cos^{2}\theta+[\lambda+q(x)]\sin ^{2}\theta}=1\]</span>  <span class="math inline">\(\omega(\lambda)\)</span> <span class="math inline">\(\lambda \to\infty\)</span> <spanclass="math inline">\(\sin \theta\)</span>.</p><p><strong> 9.3</strong>S-L <spanclass="math display">\[    \lambda_0&lt;\lambda_1&lt;\cdots &lt;\lambda_k&lt;\cdots\]</span>  <span class="math display">\[    \lim_{k \to \infty} \lambda_{k} = \infty.\]</span></p><blockquote><p> 9-2 </p></blockquote><h2 id=""></h2><p>S-L  <span class="math display">\[    y&#39;&#39;+[\lambda+ q(x)]y=0, \tag{9.33}\]</span>  <span class="math display">\[    \begin{cases}        y(0)\cos \alpha- y&#39;(0) \sin \alpha=0, \\        y(1)\cos \beta- y&#39;(1) \sin \beta=0, \tag{9.34}    \end{cases}\]</span>  <span class="math inline">\(\lambda\)</span> <span class="math inline">\(q(x)\)</span>  <spanclass="math inline">\(0\leqslant x\leqslant 1\)</span> <span class="math inline">\(\alpha\)</span>  <spanclass="math inline">\(\beta\)</span>  <spanclass="math display">\[    0\leqslant \alpha&lt;\pi, \quad 0&lt;\beta\leqslant \pi.\]</span></p><p> <span class="math inline">\(\lambda_n\)</span> <span class="math inline">\(\varphi(x,\lambda_n)\)</span>. <span class="math inline">\(C \neq 0\)</span>  <spanclass="math inline">\(C\varphi(x,\lambda_n)\)</span> .</p><p><strong> 9.4</strong>S-L.</p><p> <span class="math inline">\(x=0\)</span>  Wronsky.</p><p> <span class="math display">\[    \varphi_n(x)= \varphi(x, \lambda_n) \quad (n=0,1,2,\cdots ).\tag{9.35}\]</span></p><p><strong> 9.5</strong> 9.35 <spanclass="math inline">\(0\leqslant x\leqslant 1\)</span> <span class="math display">\[    \int_{0}^{1} \varphi_n(x)\varphi_k(x) \mathrm{d}x=    \begin{cases}        0, \quad n\neq k; \\        \delta_k&gt;0, \quad n\neq k.    \end{cases}\]</span></p><p><strong></strong> <span class="math display">\[    (\lambda_n-\lambda_k) \varphi_n(x) \varphi_k(x)=\frac{\mathrm{d}}{\mathrm{d}x}[\varphi_n(x)\varphi&#39;_k(x)-\varphi&#39;_n(x)\varphi_k(x)],\]</span> .</p><p><strong> 9.6</strong><spanclass="math inline">\(\varphi_n(x)\)</span>  <spanclass="math inline">\([0,1]\)</span>  <spanclass="math inline">\(n\)</span> .</p><p> <span class="math inline">\(\varphi_n(x)\)</span>  <spanclass="math inline">\(\theta(t,\lambda_n)\)</span>  <spanclass="math inline">\(k \pi\)</span> <spanclass="math inline">\(\theta=0\)</span>  <spanclass="math inline">\(1\)</span>.</p><p><strong> 9.7</strong> <spanclass="math inline">\(f(x)\)</span>  <spanclass="math inline">\(0\leqslant x\leqslant 1\)</span>  Riemann <span class="math display">\[    \int_{0}^{1} f(x)\varphi_n(x) \mathrm{d}x=0 \quad(n=0,1,2,\cdots ),\]</span>  <span class="math inline">\(f(x)\)</span>.</p><p> S-L </p><p></p><hr /><p>Sturmhttps://cadal.edu.cn/cardpage/bookCardPage?ssno=06504331&amp;source=card</p><hr /><p>P126-P131.</p><p> 9.7 9.35Riemann <spanclass="math inline">\(\mathcal{R}\{[0,1];\mathbb{R}^{1}\}\)</span>.  <span class="math inline">\([0,1]\)</span> <span class="math inline">\(f(x)\)</span>9.35Fourier <spanclass="math display">\[    f(x) \sim \sum_{n=0}^{\infty} a_n \varphi_n(x), \tag{9.36}\]</span>  <span class="math display">\[    a_n=\frac{1}{\delta_n} \int_{0}^{1} f(x)\varphi_n(x) \mathrm{d}x\quad(n=0,1,2,\cdots ),\]</span>  <span class="math display">\[    \delta_n= \int_{0}^{1} \varphi_n^{2}(x) \mathrm{d}x.\]</span> .</p><p><strong> 9.8</strong> <spanclass="math inline">\(f(x)\)</span>  <spanclass="math inline">\([0,1]\)</span>  DirichletFourier 9.36.</p><p></p><hr /><p>Sturmhttps://cadal.edu.cn/cardpage/bookCardPage?ssno=06504331&amp;source=card</p><hr /><p>P132-P143.</p><h3 id="-s-l-"> S-L </h3><p><span class="math display">\[    \begin{cases}        y&#39;&#39;+[\lambda+q(x)]y= f(x), \\        y(0)\cos \alpha- y&#39;(0) \sin \alpha=0, \quad y(1)\cos\beta-y&#39;(1)\sin \beta=0.    \end{cases}\]</span></p><p> <span class="math inline">\(\lambda\)</span> S-L  <spanclass="math inline">\(\lambda\)</span>  <spanclass="math inline">\(\lambda_m\)</span>   <spanclass="math display">\[    \int_{0}^{1} f(x)\varphi_m(x) \mathrm{d}x=0,\]</span>  <span class="math inline">\(\varphi_m(x)\)</span> <span class="math inline">\(\lambda_m\)</span>.</p><p><strong></strong> <span class="math inline">\(\lambda\)</span></p><p><span class="math display">\[    \int_{0}^{1} f\varphi_m = \int_{0}^{1} y&#39;&#39;\varphi_m+[\lambda_m +q]y\varphi_m= \int_{0}^{1}y&#39;&#39;\varphi_m-\varphi_m&#39;&#39;y= (y&#39;\varphi_m-y\varphi_m&#39;)\bigg |_{0}^1=0\]</span></p><h2 id=""></h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>.Sturm. 1987<a href="#fnref:1" rev="footnote" class="footnote-backref"></a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>About Bezier Curve</title>
    <link href="/2022/06/20/About-Bezier-Curve/"/>
    <url>/2022/06/20/About-Bezier-Curve/</url>
    
    <content type="html"><![CDATA[<p>Bezier .Project2Bezier.</p><hr /><p>Fitting cubic Bzier curves | Raph Leviens bloghttps://raphlinus.github.io/curves/2021/03/11/bezier-fitting.html</p><hr /><p>Project2Bezier.LaTeXMarkdownnaiveProject2</p><p>.png</p><p><spanclass="math inline">\(h\)</span>Curve1Curve2Curve3</p><figure><img src="/img/bezier_curve/fig4.jpg" alt="fig4" /><figcaption aria-hidden="true">fig4</figcaption></figure><p> <spanclass="math inline">\(h_1\)</span><spanclass="math inline">\(h_2\)</span> <spanclass="math inline">\(P_1=(x_i,y_i)\)</span><spanclass="math inline">\(P_2=(x_{i+1},y_{i+1})\)</span>Bezier<spanclass="math inline">\(h_1\)</span> <spanclass="math inline">\(Q_1=(x_3,y_3)\)</span><spanclass="math inline">\(P_1\)</span><spanclass="math inline">\(h_2\)</span> <spanclass="math inline">\(Q_2=(x_4,y_4)\)</span><spanclass="math inline">\(P_2\)</span><spanclass="math inline">\(P_1\)</span> <spanclass="math inline">\(p\)</span> <spanclass="math inline">\(P_2\)</span> <spanclass="math inline">\(q\)</span> <span class="math display">\[    \begin{aligned}        x_3 &amp; =x_1+\frac{h_1}{\sqrt{1+p^{2}}}, &amp; y_3 &amp;=y_1+\frac{ph_1}{\sqrt{1+p^{2}}}, \\        x_4 &amp; =x_2-\frac{h_2}{\sqrt{1+q^{2}}}, &amp; y_4 &amp;=y_2-\frac{qh_2}{\sqrt{1+q^{2}}}  \\    \end{aligned}\]</span> Bezier <span class="math display">\[    \begin{aligned}        b_x &amp;=\frac{3h_1}{\sqrt{1+p^{2}}},                                         &amp;b_y &amp;=\frac{3ph_1}{\sqrt{1+p^{2}}},                                         \\        c_x &amp;=3(x_2-x_1)-\frac{6h_1}{\sqrt{1+p^{2}}}-\frac{3h_2}{\sqrt{1+q^{2}}},  &amp;c_y &amp;=3(y_2-y_1)-\frac{6ph_1}{\sqrt{1+p^{2}}}-\frac{3qh_2}{\sqrt{1+q^{2}}}  \\        d_x &amp;=-2(x_2-x_1)+\frac{3h_1}{\sqrt{1+p^{2}}}+\frac{3h_2}{\sqrt{1+q^{2}}},&amp; d_y &amp;=-2(y_2-y_1)+\frac{3ph_1}{\sqrt{1+p^{2}}}+\frac{3qh_2}{\sqrt{1+q^{2}}}\\    \end{aligned}\]</span> Bezier <span class="math display">\[    \begin{aligned}        x(t) &amp; =x_1+b_xt+c_xt^{2}+d_xt^{3} \\        y(t) &amp; =y_1+b_yt+c_yt^{2}+d_yt^{3} \\    \end{aligned}\]</span>  <span class="math inline">\(p_f\)</span></p><h2 id="h_1h_2bezier"><spanclass="math inline">\(h_1,h_2\)</span>Bezier</h2><p> <span class="math inline">\(h_1=h_2=h\)</span><span class="math inline">\(p_f\)</span> <spanclass="math inline">\(h \to 0\)</span> <span class="math display">\[    \frac{y(t)-y_1}{x(t)-x_1}=\frac{p+[\frac{\sqrt{1+p^{2}}(y_2-y_1)}{h}-2p-\frac{\sqrt{1+p^{2}}q}{\sqrt{1+q^{2}}}]t+[-\frac{2\sqrt{1+p^{2}}(y_2-y_1)}{3h}+p+\frac{\sqrt{1+p^{2}}q}{\sqrt{1+q^{2}}}]t^{2}}{1+[\frac{\sqrt{1+p^{2}}(x_2-x_1)}{h}-2-\frac{\sqrt{1+p^{2}}}{\sqrt{1+q^{2}}}]t+[-\frac{2\sqrt{1+p^{2}}(x_2-x_1)}{3h}+1+\frac{\sqrt{1+p^{2}}}{\sqrt{1+q^{2}}}]t^{2}}\top\]</span>  <span class="math inline">\(h\to 0\)</span> <spanclass="math inline">\(p_f\)</span></p><p> <span class="math inline">\(h\)</span> <spanclass="math inline">\(p_f\)</span> <spanclass="math inline">\(h\)</span> <spanclass="math inline">\(p_f\)</span>1989LasserdeCasteljauBezier<spanclass="math inline">\(\pi\)</span>Bezier<spanclass="math inline">\(P_1Q_1\)</span><spanclass="math inline">\(P_2Q_2\)</span> <spanclass="math inline">\(p_f\)</span></p><p>Curve2h</p><figure><img src="/img/bezier_curve/fig5.jpg" alt="fig5" /><figcaption aria-hidden="true">fig5</figcaption></figure><h2 id="h_1h_2"><spanclass="math inline">\(h_1,h_2\)</span></h2><p> <spanclass="math inline">\(s\)</span> <spanclass="math inline">\(h_1\)</span> <spanclass="math inline">\(h_2\)</span> <spanclass="math inline">\(p_f\)</span> <spanclass="math inline">\(s\)</span></p><h3 id="g2hermitealvin-penner">G2Hermite/AlvinPenner</h3><p> <span class="math inline">\(p_f\)</span> <spanclass="math inline">\(s\)</span><spanclass="math inline">\(p_f\)</span><span class="math display">\[    \frac{\mathrm{d}^{2}y}{\mathrm{d}x^{2}}=\frac{y&#39;&#39;(t)x&#39;(t)-x&#39;&#39;(t)y&#39;(t)}{(x&#39;(t))^{3}}\]</span>  <span class="math display">\[    s(x)=y_1+p(x-x_1)+\biggl[\frac{3(y_2-y_1)}{(x_2-x_1)^{2}}-\frac{2p+q}{x_2-x_1}\biggr](x-x_1)^{2}+\biggl[\frac{p+q}{(x_2-x_1)^{2}}-\frac{2(y_2-y_1)}{(x_2-x_1)^{3}}\biggr](x-x_1)^{3}\]</span>  <spanclass="math display">\[    \begin{aligned}        s&#39;&#39;(x_1) &amp;=\frac{y&#39;&#39;(0)x&#39;(0)-x&#39;&#39;(0)y&#39;(0)}{(x&#39;(0))^{3}}\\        s&#39;&#39;(x_2) &amp;=\frac{y&#39;&#39;(1)x&#39;(1)-x&#39;&#39;(1)y&#39;(1)}{(x&#39;(1))^{3}}\\    \end{aligned}\]</span>  <span class="math display">\[    \begin{aligned}        \frac{(1+p^{2})(h_2(p-q)+\sqrt{1+q^{2}}(p(x_1-x_2)-y_1+y_2))}{3h_1^{2}\sqrt{1+q^{2}}}&amp; = \frac{(2p+q)(x_1-x_2)-3(y_1-y_2)}{(x_1-x_2)^{2}}  \\        \frac{(1+q^{2})(h_1(p-q)-\sqrt{1+p^{2}}(q(x_1-x_2)-y_1+y_2))}{3h_2^{2}\sqrt{1+p^{2}}}&amp; = \frac{-(p+2q)(x_1-x_2)+3(y_1-y_2)}{(x_1-x_2)^{2}} \\    \end{aligned}\]</span>  <spanclass="math inline">\(h_1\)</span> <spanclass="math inline">\(h_2\)</span><spanclass="math inline">\(p=q\)</span><span class="math display">\[    h_1=h_2=\frac{\sqrt{1+p^{2}}}{3}(x_2-x_1)\]</span></p><p> <span class="math inline">\(p \neq q\)</span><spanclass="math inline">\(h_1\)</span> <spanclass="math inline">\(h_2\)</span>Bezier<spanclass="math inline">\((x_1,x_2,y_1,y_2,p,q)=(17,20,4.5,7.0,3,-0.198)\)</span><spanclass="math inline">\(h_1\)</span> <spanclass="math inline">\(3.162\)</span><spanclass="math inline">\(h_2\)</span> <spanclass="math inline">\(1.019\)</span></p><p>Carl de Boor etal1987<spanclass="math inline">\(h_1,h_2\)</span> <spanclass="math inline">\(0,1,2,3\)</span> <spanclass="math inline">\(\text{dist}(s,p_f)=O(\lvert P_1P_2\rvert^{6})\)</span> <spanclass="math inline">\(\text{dist}(s,p_f)\)</span>$P_1P_2 $ <spanclass="math inline">\(1\)</span></p><p>AlvinPenner2019RahpLevien's Blog</p><h3id=""></h3><p>Raph Levien's Blog</p><p><span class="math inline">\((0,0)\)</span> <spanclass="math inline">\((1,0)\)</span> <spanclass="math inline">\(\theta_0\)</span> <spanclass="math inline">\(\theta_1\)</span> <spanclass="math inline">\(\delta_0\)</span> <spanclass="math inline">\(\delta_1\)</span> <spanclass="math inline">\(s\)</span>Bezier <spanclass="math inline">\(p_f\)</span></p><figure><img src="/img/bezier_curve/cubic.png" alt="cubic" /><figcaption aria-hidden="true">cubic</figcaption></figure><p> <span class="math inline">\((\delta_0\cos\theta_0,\delta_0\sin \theta_0)\)</span> <spanclass="math inline">\((1-\delta_1\cos \theta_1,\delta_1\sin\theta_1)\)</span></p><p> <span class="math inline">\(q=-\tan\theta_1\)</span> <span class="math display">\[    s(x)=\tan \theta_0 x-(2\tan \theta_0-\tan \theta_1)x^{2}+(\tan\theta_0-\tan \theta_1)x^{3}\]</span></p><p>Bezier <span class="math display">\[    \begin{aligned}        x(t) &amp; =3\delta_0 \cos \theta_0 t+3(1-2\delta_0 \cos\theta_0+\delta_1\cos \theta_1)t^{2}+(-2+3\delta_0 \cos\theta_0-3\delta_1 \cos \theta_1)t^{3} \\        y(t) &amp; =3\delta_0\sin \theta_0t-3(2\delta_0\sin\theta_0+\delta\sin \theta_1)t^2+3(\delta_0\sin \theta_0+\delta_1\sin\theta_1)t_3                  \\    \end{aligned}\]</span></p><p> <span class="math inline">\(p_f\)</span> <spanclass="math inline">\(x\)</span> <spanclass="math inline">\(s\)</span> <spanclass="math inline">\(x\)</span></p><figure><img src="/img/bezier_curve/area.png" alt="area" /><figcaption aria-hidden="true">area</figcaption></figure><p>Green <span class="math display">\[    \text{area}=\frac{3}{20}(2\delta_0\sin \theta_0+2\delta_1\sin\theta_1-\delta_0\delta_1\sin (\theta_0+\theta_1))\]</span></p><p></p><p><span class="math display">\[    \frac{1}{12}(\tan \theta_0+ \tan\theta_1)=\frac{3}{20}(2\delta_0\sin \theta_0+2\delta_1\sin\theta_1-\delta_0\delta_1\sin (\theta_0+\theta_1))\]</span></p><p> <spanclass="math inline">\(\delta_0\)</span> <spanclass="math inline">\(\delta_1\)</span></p><p><span class="math display">\[    \delta_1=\frac{\frac{5}{9}(\tan \theta_0+\tan \theta_1)-2\delta_0\sin \theta_0}{2\sin \theta_1-\delta_0 \sin (\theta_0+\theta_1)}\]</span></p><p>imagemomentx-</p><figure><img src="/img/bezier_curve/xmoment.png" alt="xmoment" /><figcaption aria-hidden="true">xmoment</figcaption></figure><p>Green <span class="math display">\[    \begin{aligned}        \text{moment}_x &amp; = \int_{0}^{1} x p_f(x)\mathrm{d}=\int_{0}^{1} x(t)y(t)x&#39;(t)\mathrm{d}t                                                                         \\                        &amp; = \frac{1}{280}(34\delta_0\sin\theta_0+50\delta_1\sin \theta_1+15\delta_0^{2}\sin \theta_0\cos\theta_0-15\delta_1^{2}\sin \theta_1\cos \theta_1 \\                        &amp; -\delta_0\delta_1(33\sin \theta_0\cos\theta_1+9\cos \theta_0\sin \theta_1)-9\delta_0^{2}\delta_1\sin(\theta_0+\theta_1)\cos \theta_0            \\                        &amp; +9\delta_0\delta_1^{2}\sin(\theta_0+\theta_1)\cos \theta_1)    \end{aligned}\]</span>  <span class="math display">\[\begin{equation}    \begin{aligned}        \frac{\tan \theta_0}{30}+\frac{\tan \theta_1}{20} &amp; =        \frac{1}{280}(34\delta_0\sin \theta_0+50\delta_1\sin\theta_1+15\delta_0^{2}\sin \theta_0\cos \theta_0-15\delta_1^{2}\sin\theta_1\cos \theta_1                                            \\                                                          &amp;-\delta_0\delta_1(33\sin \theta_0\cos \theta_1+9\cos \theta_0\sin\theta_1)-9\delta_0^{2}\delta_1\sin (\theta_0+\theta_1)\cos \theta_0 \\                                                          &amp;+9\delta_0\delta_1^{2}\sin (\theta_0+\theta_1)\cos \theta_1)    \end{aligned}\end{equation}\]</span> 13 <spanclass="math inline">\(\delta_0\)</span> <spanclass="math inline">\(\delta_1\)</span>RaphLevien's Blog <spanclass="math inline">\(\text{moment}_x\)</span> <spanclass="math inline">\(\delta_1\)</span> <spanclass="math inline">\(\delta_0\)</span></p><h3 id=""></h3><p> <span class="math inline">\(h_1,h_2\)</span><span class="math inline">\(\delta_0,\delta_1\)</span> <spanclass="math inline">\(\displaystylek=\frac{y_2-y_1}{x_2-x_1}\)</span></p><p> <span class="math display">\[    x&#39;=\frac{x-x_1}{x_2-x_1}, \quady&#39;=\frac{y-y_1}{x_2-x_1}-\frac{y_2-y_1}{(x_2-x_1)^{2}}(x-x_1)\]</span>  <span class="math inline">\((x_1,y_1)\)</span><span class="math inline">\((x_2,y_2)\)</span> <spanclass="math inline">\((0,0)\)</span> <spanclass="math inline">\((1,0)\)</span> <spanclass="math display">\[    \tan \theta_0=p-k, \quad \tan \theta_1=-q+k\]</span>  <span class="math display">\[    x=x_1+(x_2-x_1)x&#39;, \quad y=y_1+(y_2-y_1)x&#39;+(x_2-x_1)y&#39;\]</span>  <span class="math display">\[    \begin{aligned}        h_1 &amp; =\delta_0(x_2-x_1) \\        h_2 &amp; =\delta_1(x_2-x_1) \\    \end{aligned}\]</span></p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter19</title>
    <link href="/2022/06/11/Chapter19/"/>
    <url>/2022/06/11/Chapter19/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><h2 id=""></h2><p><span class="math display">\[    E=h \nu \\    p=\frac{h}{\lambda}\]</span></p><p></p><p></p><p>1927-</p><h3 id=""></h3><p> <span class="math display">\[    \hbar=\frac{h}{2\pi}\]</span></p><p> <spanclass="math inline">\(\mathbf{p}\)</span> <spanclass="math inline">\(E\)</span> <span class="math display">\[    \Psi(\mathbf{r},t)=\psi_0\exp\left({-\frac{i}{\hbar}(Et-\mathbf{p}\cdot \mathbf{r})} \right)\]</span>  <span class="math display">\[    \rho(\mathbf{r},t)=\lvert \Psi(\mathbf{r},t) \rvert ^{2}\]</span>  <span class="math inline">\(t\)</span> <spanclass="math inline">\(\mathbf{r}\)</span></p><p><strong></strong> -  <spanclass="math display">\[    \int_{\Omega}^{} \Psi^{*}(\mathbf{r},t) \Psi(\mathbf{r},t)\mathrm{d}V =1       \]</span></p><p><span class="math display">\[    \rho(\mathbf{r},t)=\lvert \Psi(\mathbf{r},t) \rvert ^{2}\]</span> - </p><ul><li><p></p></li><li><p></p></li></ul><p><strong></strong> omitted</p><h2 id=""></h2><p><span class="math display">\[    \Delta x \Delta p \geqslant \hbar /2    \]</span>  <span class="math display">\[    \Delta x=\sqrt{(x-\bar{x})^{2}}\]</span> <span class="math inline">\(\Deltap\)</span> </p><p><strong>-</strong> <span class="math display">\[    \Delta E \Delta t \geqslant  \hbar /2\]</span>  <span class="math inline">\(\Delta E\)</span><span class="math inline">\(\Delta t\)</span></p><h2 id=""></h2><p> <span class="math display">\[    \Psi(x,t)=\Psi_0 \exp\left({\frac{i}{\hbar}(p_x x-Et)} \right)\]</span></p><p> <span class="math display">\[    i \hbar \frac{\partial }{\partial t} \Psi(x,t)=-\frac{\hbar^{2}}{2m}\frac{\partial ^{2}}{\partial x^{2}} \Psi(x,t)\]</span></p><p></p><p><span class="math display">\[    i \hbar \frac{\partial }{\partial t}\Psi(x,t)=\left(-\frac{\hbar^{2}}{2m} \frac{\partial ^{2}}{\partialx^{2}}+U\right) \Psi(x,t)\]</span></p><p><strong></strong> <span class="math display">\[    \hat{H} \Phi (\mathbf{r})=E \Phi(\mathbf{r})        \]</span></p><p> <span class="math display">\[    \hat{H}=-\frac{\hbar ^{2}}{2m} \nabla ^{2}+U(\mathbf{r})\]</span></p><p> <span class="math inline">\(U\)</span> </p><p> <span class="math display">\[    T(t) \propto \mathrm{e}^{- \frac{i}{\hbar} Et}\]</span>  <span class="math display">\[    \Psi(\mathbf{r},t)=\Phi (\mathbf{r}) \mathrm{e}^{- \frac{i}{\hbar}Et}\]</span></p><p>.</p><h2 id=""></h2><ul><li></li><li></li><li></li><li>Schrodinger</li><li></li></ul><h2 id=""></h2><h3 id=""></h3><p><span class="math display">\[    U(x)=    \begin{cases}        \infty, \quad x&lt;0  x&gt;L \\        0, \quad 0\leqslant x\leqslant L    \end{cases}\]</span></p><p><span class="math display">\[    \Phi(x)=C\sin (kx+\delta)\]</span> <span class="math display">\[    k= \frac{n \pi}{L}\]</span>  <span class="math display">\[    \Phi(x)=    \begin{cases}        \sqrt{\frac{2}{L}} \sin \frac{n \pi}{L}x, \quad 0\leqslantx\leqslant L \\        0, \quad else    \end{cases}\]</span></p><p> <span class="math display">\[    E=\frac{k^{2} \hbar^{2}}{2m}=n^{2}E_1\]</span>  <span class="math display">\[    E_1= \frac{\pi^{2} \hbar^{2}}{2m L^{2}}\]</span> .</p><p> <span class="math display">\[    p=\pm n \frac{\pi \hbar}{L}\]</span>  <span class="math display">\[    \lambda_n= \frac{2L}{n}\]</span> </p><p> <span class="math display">\[    \Psi_1(x,t)=\Phi_1(x)\exp(-\frac{i}{\hbar}E_1t)=\frac{1}{2i}\sqrt{\frac{2}{L}}(\mathrm{e}^{ik_1x}-\mathrm{e}^{-ik_1x}) \mathrm{e}^{-\frac{i}{\hbar}E_1t}\]</span></p><h3 id=""></h3><p></p><p> <span class="math inline">\(E\)</span>  <spanclass="math display">\[    E_n=(n+\frac{1}{2})\hbar \omega =(n+\frac{1}{2}) h \nu\]</span>  <span class="math display">\[    E_0=\frac{1}{2} \hbar \omega\]</span></p><h3 id=""></h3><p><strong></strong></p><p><strong></strong></p><p><strong></strong></p><h2 id=""></h2><h3 id=""></h3><p><span class="math display">\[    E_n= -\frac{me^{4}}{2\hbar^{2}(4\pi\varepsilon_0)^{2}}\frac{1}{n^{2}}=-13.6 \frac{1}{n^{2}}    \]</span>  $n=1,2,$<strong></strong></p><h3 id=""></h3><p> <span class="math display">\[    L^{2}=l(l+1) \hbar^{2}\]</span> <span class="math inline">\(l=0,1,2,\cdots ,n-1\)</span><strong></strong></p><p></p><p> <span class="math inline">\(z\)</span>  <spanclass="math display">\[    L_z= m \hbar\]</span> <span class="math inline">\(m=-l,-l+1,\cdots, l-1,l\)</span><strong></strong></p><p> <span class="math inline">\(l\)</span><span class="math inline">\(m\)</span>  <spanclass="math inline">\((2l+1)\)</span> </p><h3 id=""></h3><p></p><p><span class="math display">\[    \Delta E=- \mathbf{\mu}_l \cdot \mathbf{B}=\frac{e}{2m_0} \mathbf{L}\cdot \mathbf{B}= \frac{e}{2m_0} L_z B=m(\frac{e \hbar}{2m_0})B\]</span> <span class="math inline">\(m=-l,-l+1,\cdotsl-1,l\)</span></p><p>.</p><h3 id="-1"></h3><p> <spanclass="math display">\[    \Psi_{nlm}(r,\theta,\phi)=R_{nl}(r) Y_{lm}(\theta,\phi)\]</span>  <span class="math inline">\((r,\theta,\phi)\)</span> <span class="math inline">\(\mathrm{d}V\)</span> <span class="math display">\[    \lvert R_{nl}(r) \rvert ^{2} \lvert Y_{lm}(\theta,\phi) \rvert ^{2}r^{2} \sin \theta \mathrm{d}r \mathrm{d}\theta \mathrm{d} \phi\]</span>  <spanclass="math inline">\(rr+\mathrm{d}r\)</span> <spanclass="math display">\[    W_{nl}(r)\mathrm{d}r= \lvert r R_{nl}(r) \rvert ^{2}= u_{nl}^{2}(r)\mathrm{d}r\]</span>  <span class="math inline">\(Y_{lm}\)</span><spanclass="math inline">\(u_{nl}^{2}(r)=r^{2}R_{nl}^{2}(r)\)</span><strong></strong>..</p><p> <span class="math inline">\((\theta,\phi)\)</span><span class="math inline">\(\mathrm{d}\Omega\)</span> <span class="math display">\[    W_{lm}(\theta,\phi) \mathrm{d}\Omega= \lvert Y_{lm}(\theta,\phi)\rvert ^{2} \mathrm{d}\Omega                       \]</span>  <span class="math inline">\(R_{nl}(r)\)</span><span class="math inline">\(\lvert Y_{lm}(\theta,\phi)\rvert ^{2}\)</span> <strong></strong>..</p><h3 id=""></h3><p>Stern-Gerlach </p><p> <spanclass="math display">\[    S_z= m_s \hbar\]</span> <spanclass="math inline">\(m_s=-\frac{1}{2},\frac{1}{2}\)</span><strong></strong> <spanclass="math inline">\(s=1/2\)</span>.</p><p> <span class="math display">\[    S=\sqrt{s(s+1)}\hbar = \frac{\sqrt{3}}{2} \hbar\]</span></p><p>.</p><p> <spanclass="math inline">\((n,l,m_l,m_s)\)</span> -  <spanclass="math inline">\(n\)</span>  <spanclass="math inline">\(E_n\)</span> -  <spanclass="math inline">\(l\)</span>  <spanclass="math inline">\(L\)</span> <spanclass="math inline">\(E_{nl}\)</span> -  <spanclass="math inline">\(m_l\)</span> - <span class="math inline">\(m_s\)</span></p><p><strong></strong><strong></strong>$s=1/2,3/2,$ <strong></strong>$s=0,1,<span class="math inline">\((\)</span>s=1<spanclass="math inline">\()\)</span>$ <spanclass="math inline">\(K\)</span>(<spanclass="math inline">\(s=0\)</span>)</p><p>..</p><p> <span class="math inline">\(n\)</span><strong></strong> <spanclass="math inline">\(l\)</span><strong></strong>.</p><p> <span class="math inline">\(n\)</span> <span class="math display">\[    N_n= 2n^{2}\]</span></p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter18</title>
    <link href="/2022/06/11/Chapter18/"/>
    <url>/2022/06/11/Chapter18/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><h2 id=""></h2><h3 id=""></h3><p>..</p><p><strong></strong><spanclass="math inline">\(E(T)\)</span>.</p><p><span class="math display">\[    E(T)=\frac{}{}=\frac{\Delta P}{\Delta S}\]</span></p><p><strong></strong><span class="math inline">\(\lambda\)</span><spanclass="math inline">\(E_{\lambda}\)</span> <spanclass="math display">\[    E_{\lambda}(T)= \frac{\mathrm{d}P_{\lambda}}{\mathrm{d}\lambda}\]</span> <span class="math display">\[    M(T)= \int_{0}^{\infty} M_{\lambda}(T) \mathrm{d}\lambda\]</span></p><p><strong></strong> <span class="math display">\[    \alpha = \frac{}{}\]</span></p><p><strong></strong> <span class="math display">\[    \alpha(\lambda,T)=\frac{}{}\]</span></p><p><strong></strong> <span class="math display">\[    \rho(\lambda,T)=\frac{}{}\]</span></p><p><strong></strong> <span class="math display">\[    \tau(\lambda,T)\]</span></p><p><strong></strong><spanclass="math inline">\(\alpha(\lambda,T)=1, \alpha=1\)</span><strong></strong><spanclass="math inline">\(\alpha=0\)</span><spanclass="math inline">\(\rho=1\)</span> <strong></strong><spanclass="math inline">\(0&lt;\alpha&lt;1\)</span></p><h3 id=""></h3><p><strong>Stefan - Boltzmann </strong> <spanclass="math display">\[    M(T)= \sigma T^{4}\]</span> </p><p><strong>Wien </strong> <span class="math display">\[    T \lambda_m= b\]</span>  <spanclass="math inline">\(\lambda_m\)</span>  <spanclass="math inline">\(T\)</span> .</p><p><strong></strong> <span class="math display">\[    \frac{E(\lambda,T)}{\alpha(\lambda,T)}= M_{b}(\lambda,T)\]</span><span class="math inline">\(M_b\)</span> <spanclass="math inline">\(M_b\)</span> .</p><p><strong></strong></p><h3 id="-1"></h3><p><strong></strong>Maxwell <spanclass="math display">\[    M(T,v)=\alpha v^{3} \mathrm{e}^{-\beta v/T}\]</span></p><p><strong>-</strong> <spanclass="math display">\[    M(T,v)=\frac{2\pi v^{3}}{c^{2}} kT\]</span></p><p><strong></strong> <span class="math display">\[    M_b(\nu ,T)=\frac{2\pi \nu^{2}}{c^{2}}\frac{h\nu}{\mathrm{e}^{\frac{h\nu}{kT}} -1}\]</span></p><p>1900 <span class="math display">\[    E_n=nh\nu \quad (n=1,2,\cdots )\]</span></p><p><strong></strong>  <spanclass="math display">\[    \frac{E_{\lambda}(T)}{\alpha_{\lambda}(T)}=M_{\lambda}(T)\]</span> .</p><h2 id=""></h2><p></p><h2 id=""></h2><p> <span class="math inline">\(\Delta \lambda=\lambda-\lambda_0\)</span> <spanclass="math inline">\(\theta\)</span>.</p><p>X<spanclass="math inline">\(\nu_0\)</span><spanclass="math inline">\(m_0\)</span><spanclass="math inline">\(p,E\)</span> <spanclass="math display">\[    \begin{aligned}    h\nu_0+m_0c^{2}&amp;=h \nu+mc^{2} \\    \frac{h}{\lambda_0}\mathbf{n_0}&amp;=\frac{h}{\lambda} \mathbf{n}+m\mathbf{v} \\    m&amp;=\frac{m_0}{\sqrt{1-v^{2}/c^{2}}} \\    \end{aligned}  \]</span></p><p><strong></strong> <span class="math display">\[    \Delta \lambda=\lambda-\lambda_0=\frac{h}{m_0c}(1-\cos\theta)=2\lambda_c \sin ^{2} \frac{\theta}{2}\]</span>  <span class="math inline">\(\displaystyle\lambda_c=\frac{h}{m_0c}=2.43 \times 10^{-12} \text{m}\)</span>. <span class="math inline">\(\theta\)</span></p><p><strong></strong></p><h2 id="bohr">/Bohr</h2><p><strong></strong> <span class="math display">\[    \frac{1}{\lambda}=\frac{4}{B}(\frac{1}{2^{2}}-\frac{1}{n^{2}})\]</span></p><p><strong>Rydberg</strong>  <spanclass="math display">\[    \tilde{\nu}=R(\frac{1}{m^{2}}-\frac{1}{n^{2}})=T(m)-T(n)    \]</span>  <span class="math inline">\(\displaystyle\widetilde{\nu}=\frac{1}{\lambda}\)</span> <spanclass="math inline">\(R\)</span> Rydberg</p><h3 id="bohr">Bohr</h3><p><strong>Bohr</strong> - -  <span class="math display">\[      m v_n r_n=n \frac{h}{2\pi}=n \hbar  \]</span> -  <span class="math display">\[    h\nu=E_m-E_n\]</span></p><p>Bohr  <span class="math display">\[    r_n=n^{2} \frac{\varepsilon_0h^{2}}{\pi m e^{2}} =n^{2} r_1\]</span>  <span class="math inline">\(r_1=a_0=0.053\text{nm}\)</span> Bohr</p><p> <span class="math inline">\(U_n\)</span>  <spanclass="math inline">\(E_{kn}\)</span>  <span class="math display">\[    \begin{aligned}    U_n&amp;=\frac{2E_1}{n^{2}} \\    E_{kn}&amp;=-\frac{E_1}{n^{2}}    \end{aligned}\]</span>  <span class="math inline">\(E_1=-13.6\text{eV}\)</span></p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter17</title>
    <link href="/2022/06/11/Chapter17/"/>
    <url>/2022/06/11/Chapter17/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><h2 id=""></h2><p><strong></strong></p><ul><li>: <spanclass="math inline">\(E\)</span></li><li></li><li>&gt;  &gt; <spanclass="math display">\[  E_y=E_{y0}\cos [\omega t-kx+\varphi_1] \\  E_x=E_{x0} \cos [\omega t-kx+\varphi_2]  \]</span></li><li>.<strong></strong></li><li>.</li></ul><h2 id="malus">Malus</h2><p>-.</p><p><strong></strong> <span class="math display">\[    I_2=I_1\cos ^{2}\alpha\]</span></p><h2 id=""></h2><p><strong></strong> <strong></strong><spanclass="math inline">\(\theta_B\)</span><strong></strong><strong></strong><strong><spanclass="math inline">\(\displaystyle\frac{\pi}{2}\)</span></strong></p><p> <span class="math display">\[    \tan \theta_B=\frac{n_2}{n_1}\]</span>  <span class="math inline">\(n_1\)</span> <spanclass="math inline">\(n_2\)</span>. ## </p><p>.<strong>o</strong><strong>e</strong></p><p><strong></strong>. oe.<strong></strong>.</p><p>ooe.</p><p>oe.</p><p>.  <spanclass="math inline">\(\varepsilon\)</span>.</p><p>.</p><p>.</p><p> <spanclass="math inline">\(d\)</span>  $=n_o-n_ed $  <span class="math display">\[    \Delta \varphi=\frac{2\pi}{\lambda}\delta=\frac{2\pi}{\lambda}\lvertn_o-n_e \rvert d\]</span></p><p> <span class="math display">\[    \lvert n_e-n_o \rvert d= \frac{\lambda}{4}\]</span> .</p><p> <span class="math inline">\(\Delta\varphi\)</span> <span class="math inline">\(I_1,I_2\)</span> <span class="math display">\[    I_{out}=I_1+I_2+2\sqrt{I_1I_2}\cos \Delta \varphi\]</span></p><p>.</p><p> <spanclass="math inline">\(2\alpha\)</span> <spanclass="math inline">\(\alpha\)</span>.</p><p>.</p><p> <spanclass="math inline">\(\frac{\pi}{4}\)</span> .</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter16</title>
    <link href="/2022/06/11/Chapter16/"/>
    <url>/2022/06/11/Chapter16/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><p></p><h2 id=""></h2><p><strong>Q</strong> <spanclass="math inline">\(d\)</span> <spanclass="math inline">\((\lambda,)\)</span></p><ol type="1"><li><span class="math inline">\(\displaystyle E(\theta)=2\cos\frac{\Delta \varphi}{2}E_{p}\)</span> <spanclass="math inline">\(\displaystyle \Delta \varphi=\frac{2\pid}{\lambda}\sin \theta\)</span></li></ol><p><span class="math inline">\(n\)</span><spanclass="math inline">\(\displaystyle E(\theta)=E_p \frac{\sin (\frac{n\Delta \varphi}{2})}{\sin (\frac{\Delta \varphi}{2})}\)</span></p><blockquote><p> <span class="math inline">\(\Delta=d \sin\theta\)</span></p></blockquote><ol start="2" type="1"><li>  <span class="math display">\[I(\theta)=4I_0\cos ^{2}\frac{\Delta \varphi}{2}=4I \cos^{2}(\frac{\pi}{\lambda}d\sin \theta)\]</span> <span class="math inline">\(\displaystyleI_0=\frac{1}{2}\sqrt{\frac{\varepsilon}{\mu}}E_p^{2}\)</span></li></ol><p><span class="math inline">\(\displaystyle d\sin \theta= \pm n\lambda\)</span><span class="math inline">\(\displaystyled\sin \theta=\pm (2n-1)\frac{\lambda}{2}\)</span></p><p> <span class="math display">\[    \Delta x= \frac{L}{d} \lambda\]</span></p><ol start="3" type="1"><li><strong></strong> <span class="math display">\[V=\frac{I_{\max}-I_{\min}}{I_{\max}+I_{\min}}\]</span>  <spanclass="math inline">\(1\)</span> <spanclass="math inline">\(E_1,E_2\)</span> <spanclass="math display">\[V= \frac{2\sqrt{I_1I_2}}{I_1+I_2}\]</span></li></ol><p><strong>Q</strong></p><p><strong>Q</strong>.</p><blockquote><p></p></blockquote><p>. <span class="math inline">\(D\lambda/d\)</span> <spanclass="math inline">\(D\)</span> <spanclass="math inline">\(d\)</span> .</p><h2 id=""></h2><p></p><p> -- </p><h2 id=""></h2><p> <span class="math display">\[    L= \sum_{i}^{} n_i s_i\]</span></p><p></p><h3 id=""></h3><p> <spanclass="math inline">\(e\)</span> ..</p><p><strong></strong></p><p><span class="math display">\[    d= \frac{\lambda}{4n}\]</span></p><p><strong></strong></p><p> <spanclass="math inline">\(n&lt;n_0\)</span> <span class="math display">\[    d= \frac{\lambda}{4n}\]</span> </p><h3 id=""></h3><p><strong></strong></p><p><span class="math display">\[    2ne+ \frac{\lambda}{2} = k \lambda\]</span>  <span class="math display">\[    2ne+ \frac{\lambda}{2} = (2k+1) \frac{\lambda}{2}\]</span>  <span class="math display">\[    \Delta e= \frac{\lambda}{2n}\]</span>  <span class="math display">\[    \Delta x \thickapprox \frac{\lambda}{2n \theta}\]</span></p><p><strong></strong></p><p><span class="math display">\[    r \thickapprox \sqrt{2eR}\]</span></p><p> <span class="math display">\[    r=\sqrt{\frac{(2k-1)R\lambda}{2n}}\]</span>  <span class="math display">\[    r= \sqrt{\frac{kR\lambda}{n}}\]</span>  <span class="math display">\[    r_{k+m}^{2}- r_{k}^{2}= mR\lambda\]</span></p><p> ###</p><p></p><h2 id=""></h2><p><span class="math display">\[    E(\theta)=E_0 \frac{\sin \beta}{\beta},\quad \beta=\frac{\pi a \sin\theta}{\lambda}\]</span>  <spanclass="math inline">\(E_0\)</span></p><h3 id=""></h3><p> <span class="math display">\[    a \sin  \theta = \pm k \lambda, \quad k=1,2,3,\cdots\]</span></p><p> <span class="math display">\[    a\sin \theta= \pm (2m+1) \frac{\lambda}{2}, \quad m=1,2,\cdots\]</span></p><p>0 <span class="math display">\[    \Delta \theta_0 = 2 \theta_1 \thickapprox  2 \frac{\lambda}{a}\]</span></p><p> <span class="math inline">\(k\)</span> <span class="math display">\[    \Delta \theta_k = \frac{\lambda}{a}\]</span></p><h2 id=""></h2><p>.</p><p><strong>Q</strong>  <span class="math inline">\((\lambda,a, d, E_0)\)</span> <span class="math inline">\(E(\theta)=\)</span> <span class="math inline">\(\delta=d \sin\theta\)</span> <spanclass="math inline">\(\displaystyle \varphi= \frac{2\pi d \sin\theta}{\lambda}\)</span>.  <spanclass="math inline">\(\displaystyle E_p=E_0 \frac{\sin\beta}{\beta}\)</span> <span class="math display">\[    E=E_p \frac{\sin \varphi}{\sin (\frac{\varphi}{2})}=E_0 \frac{\sin\beta}{\beta}\frac{\sin (\varphi)}{\sin (\frac{\varphi}{2})}\]</span></p><p> <span class="math inline">\(N\)</span> <spanclass="math display">\[      E=E_0 \frac{\sin \beta}{\beta} \frac{\sin (\frac{N\varphi}{2})}{\sin(\frac{\varphi}{2})}\]</span></p><p> <span class="math inline">\(\displaystyle \beta=\frac{\pi a \sin\theta}{\lambda}, \varphi= \frac{2\pi d \sin\theta}{\lambda}\)</span></p><p><strong></strong>.</p><h2 id=""></h2><p><strong></strong> <span class="math display">\[    d\sin \theta = k\lambda, \quad k=0, \pm 1,\cdots\]</span></p><p> <spanclass="math inline">\(E_0\)</span> <span class="math display">\[    E=NE_0 \\    I=N^{2} I_0\]</span></p><p><strong></strong></p><p><span class="math display">\[    d \sin \theta = \pm k \frac{\lambda}{N},\quad k=1,2,\cdotsN-1,N+1,\cdots\]</span></p><p><strong></strong>  <spanclass="math inline">\(N-1\)</span> <spanclass="math inline">\(N-2\)</span></p><blockquote><p>4%</p></blockquote><p><strong>Q:</strong> <spanclass="math inline">\(k,k+1\)</span> <spanclass="math inline">\((N,d,\lambda)\)</span></p><p><span class="math display">\[    \theta_{k+1}-\theta_k=\frac{\lambda}{d}\]</span></p><p> <span class="math inline">\(k\)</span> <spanclass="math display">\[    \Delta \theta_k=\frac{2\lambda}{N d}\]</span></p><p> <span class="math inline">\(k\)</span> <spanclass="math display">\[    \Delta \theta_k&#39;=\frac{\lambda}{Nd}\]</span></p><ul><li><span class="math inline">\(N\)</span> <spanclass="math inline">\(N\)</span>.</li><li><span class="math inline">\(d\)</span><spanclass="math inline">\(d\)</span>.</li><li><span class="math inline">\(a\)</span><spanclass="math inline">\(a\)</span>.</li></ul><p><strong></strong>  <spanclass="math inline">\(d \sin \theta =k\lambda, (k=0,\pm 1,\pm 2,\cdots)\)</span> <span class="math inline">\(a \sin \theta=m\lambda,(m=\pm 1,\pm 2,\cdots)\)</span><strong></strong>.</p><p> <span class="math display">\[    \begin{cases}        \sin \theta =\frac{k\lambda}{d} \\        \sin \theta=\frac{m\lambda}{a}    \end{cases}\]</span>  <span class="math display">\[    k=m\frac{d}{a} , (m=\pm 1, \pm 2,\cdots )    \]</span>  <span class="math inline">\(k,m\)</span></p><p><strong></strong> <span class="math display">\[    R=\frac{\lambda}{\Delta \lambda}\]</span></p><p><strong>Q:</strong>    <spanclass="math display">\[    R=\frac{\lambda}{\Delta \lambda}=kN\]</span></p><p> <span class="math inline">\(k\)</span> </p><h2 id=""></h2><p><strong>Airy disk</strong> 84%</p><p></p><p><span class="math display">\[    d \sin \theta_1 \thickapprox 1.22 \lambda\]</span></p><p></p><p> <spanclass="math display">\[    \theta_1=  \frac{1.22\lambda}{d}\]</span></p><p>. <span class="math display">\[    R=\frac{d}{1.22\lambda}\]</span> <span class="math inline">\(d\)</span> .</p><h2 id="x">X</h2><p> <span class="math display">\[    2d\sin \theta=k \lambda\]</span></p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter15</title>
    <link href="/2022/06/11/Chapter15/"/>
    <url>/2022/06/11/Chapter15/</url>
    
    <content type="html"><![CDATA[<h1 id="chapter-15-">Chapter 15 </h1><h2 id="maxwell-">Maxwell </h2><p></p><p>Maxwell's idea  <spanclass="math inline">\(\mathbf{D}=\varepsilon_0\mathbf{E}+\mathbf{P}\)</span> <spanclass="math inline">\(\mathbf{P}=n q \mathbf{l}\)</span></p><p> <span class="math display">\[    \overrightarrow{j}_D=\frac{\partial \overrightarrow{D}}{\partial t}\]</span>  <span class="math display">\[    I_D= \iint_S \frac{\partial \overrightarrow{D}}{\partial t} \cdot\mathrm{d} \overrightarrow{S}\]</span></p><p> <span class="math display">\[    I_D=\frac{\mathrm{d}}{\mathrm{d}t} \mathbf{\Phi}_{D}(t)\]</span></p><p><strong>Q</strong>RC <spanclass="math inline">\(I_c\)</span> <spanclass="math inline">\(I_d\)</span>  <spanclass="math display">\[    j_D=\frac{\partial \mathbf{D}}{\partial t}\]</span>  <span class="math display">\[    \displaystyle  I_D= j_D S=\frac{\mathrm{d}D}{\mathrm{d}t}S=\frac{\mathrm{d}(DS)}{\mathrm{d}t}=\frac{\mathrm{d}(\sigma_0(t)S)}{\mathrm{d}t}=\frac{\mathrm{d}q}{\mathrm{d}t}=I_C\]</span></p><p>.</p><p><strong>Q</strong> <span class="math inline">\(I_c,I_d\)</span> <spanclass="math inline">\(H\)</span> <spanclass="math display">\[    \oint_L \mathbf{H} \cdot \mathrm{d}\mathbf{l}=I_c+I_d=\iint_S\frac{\partial \mathbf{D}}{\partial t} \mathrm{d} \mathbf{S}\]</span> .</p><p></p><p><strong>Q</strong> <span class="math inline">\(d \llR\)</span> <spanclass="math inline">\(I_c\)</span>C.</p><p> <span class="math display">\[    \mathbf{H}=\frac{r}{2\pi R^{2}}I_c \quad (r&lt;R)\]</span>  <span class="math inline">\(\displaystyle \mathbf{B}=\mu_0\mathbf{H}\)</span>.</p><p><strong></strong> <spanclass="math inline">\(I_c\)</span> <spanclass="math inline">\(I_d\)</span> <spanclass="math inline">\(I_d\)</span> <spanclass="math inline">\(I_c\)</span>.  <spanclass="math inline">\(I_D=I_c+ I_d\)</span></p><p><strong></strong> <spanclass="math inline">\(S\)</span>.</p><p> <span class="math display">\[    \oint_S \mathbf{D} \cdot \mathrm{d} \mathbf{S} =q \\    \oint_S \mathbf{j}_c \cdot \mathrm{d}\mathbf{S}=-\frac{\mathrm{d}q}{\mathrm{d}t}       \]</span> .</p><p> T96  <spanclass="math inline">\(\displaystyle I_c=-I_d\)</span></p><p><strong></strong>.</p><p><img src="images/2022-04-18-15-15-48.png" /></p><h2 id="maxwell-">Maxwell </h2><p><span class="math display">\[    \begin{cases}        \oiint_S \mathbf{D} \cdot \mathrm{d}\mathbf{S}=q_0 \\        \oiint_S \mathbf{B} \cdot \mathrm{d}\mathbf{S}=0 \\        \oint_l \mathbf{E} \cdot \mathrm{d}\mathbf{l}=-\iint_S\frac{\partial \mathbf{B}}{\partial t} \cdot \mathrm{d}\mathbf{S} \\        \oint_l \mathbf{H} \cdot \mathrm{d}\mathbf{l} =I +\iint_S\frac{\partial D}{\partial t} \cdot \mathrm{d}\mathbf{S}    \end{cases}\]</span></p><p> <span class="math display">\[    \begin{cases}        \nabla \cdot \mathbf{D}= \rho_0 \\        \nabla \cdot \mathbf{B}= 0 \\        \nabla \times \mathbf{E} =-\frac{\partial \mathbf{B}}{\partialt} \\        \nabla \times \mathbf{H}=\mathbf{j}_c+\frac{\partial\mathbf{D}}{\partial t} \\    \end{cases}\]</span></p><p> <span class="math display">\[    \begin{cases}        \nabla \cdot \mathbf{D}= 0 \\        \nabla \cdot \mathbf{B}= 0 \\        \nabla \times \mathbf{E} =-\frac{\partial \mathbf{B}}{\partialt} \\        \nabla \times \mathbf{H}=\frac{\partial \mathbf{D}}{\partial t}\\    \end{cases}\]</span></p><p> <span class="math display">\[    \begin{cases}        \mathbf{D} = \varepsilon \mathbf{E} \\        \mathbf{B} = \mu \mathbf{H} \\        \mathbf{j} = \gamma \mathbf{E}    \end{cases}\]</span></p><h2 id=""></h2><h3 id=""></h3><p>1 <spanclass="math inline">\(E,H,c\)</span>RHR</p><p>2 <span class="math display">\[    \begin{cases}        \frac{\partial^{2} B}{\partial t^{2}}=c^{2} \frac{\partial ^{2}E}{\partial z^{2}} \\        \frac{\partial^{2}E}{\partial t^{2}}=c^{2} \frac{\partial ^{2}B}{\partial z^{2}} \\    \end{cases}\]</span>  <span class="math inline">\(\varepsilon_0\mu_0=\frac{1}{c^{2}}\)</span> 3 <spanclass="math inline">\(\displaystyle c=\frac{1}{ \sqrt{\varepsilon\mu}}\)</span></p><p>4 <span class="math inline">\(E=cB\)</span></p><p><strong></strong>1T99 <spanclass="math inline">\(\displaystyle E_{0x}=0\)</span><spanclass="math inline">\(\displaystyle B_{0x}=0\)</span> <spanclass="math inline">\(x\)</span>.</p><p><strong>Q</strong><spanclass="math inline">\(E,B\)</span>  <spanclass="math inline">\(x\)</span> <spanclass="math inline">\(E\)</span> <spanclass="math inline">\(y\)</span>.</p><p> <spanclass="math inline">\(B\)</span>Maxwell<span class="math inline">\(B_y\)</span> <spanclass="math inline">\(t\)</span>. <spanclass="math inline">\(B_y\)</span>.</p><p>2 <strong>Q</strong><spanclass="math inline">\(\displaystyle E=E(z,t)\mathbf{i}\)</span>, <spanclass="math inline">\(\displaystyle B=B(z,t)\mathbf{j}\)</span></p><p>3  <span class="math display">\[    n=\sqrt{\mu_r \varepsilon_r}\]</span> 4<span class="math inline">\(E,B\)</span>  <spanclass="math display">\[    \frac{1}{2}\varepsilon E^{2}=\frac{1}{2} \mu H^{2}\]</span>  <span class="math inline">\(\displaystyleE=uB\)</span></p><h3 id=""></h3><ul><li><p> <span class="math display">\[  \omega=\varepsilon_0E^{2}\]</span></p></li><li><p> <span class="math display">\[  \bar{\omega}=\frac{1}{2}\varepsilon_0E_0^{2}=\frac{1}{2}\mu_0H_0^{2}        \]</span> </p></li><li><p> <span class="math display">\[  S= \omega u\]</span>  <span class="math inline">\(W/m^{2}\)</span> <spanclass="math inline">\(J/(s\cdot m^{2})\)</span>  <spanclass="math display">\[  \mathbf{S}=\mathbf{E} \times \mathbf{H}\]</span> (Poynting).</p></li><li><p> <span class="math display">\[  I=\bar{S}=\bar{E}\bar{H}=\frac{1}{2}\sqrt{\frac{\varepsilon}{\mu}}E_0^{2}     \]</span> .</p></li></ul><h3 id=""></h3><p> <span class="math display">\[    w=\frac{EH}{c}\]</span>  <span class="math display">\[    g= \frac{EH}{c^{2}}=\frac{S}{c^{2}}\]</span> <span class="math display">\[    \bar{g}=\frac{\bar{S}}{c^{2}}=\frac{I}{c^{2}}\]</span></p><p> <span class="math display">\[    P=c \bar{g}= \frac{I}{c}\]</span></p><h3 id=""></h3><p>LC.</p><h3 id=""></h3>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter14</title>
    <link href="/2022/06/11/Chapter14/"/>
    <url>/2022/06/11/Chapter14/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><h2 id=""></h2><h3 id=""></h3><p><span class="math display">\[    \varepsilon = - \frac{\mathrm{d}\Phi}{\mathrm{d}t}\]</span></p><p></p><h2 id=""></h2><p><span class="math display">\[    \mathbf{E}_k= \mathbf{v} \times \mathbf{B}\]</span></p><p><span class="math display">\[    \varepsilon = \int_{l}^{} (\mathbf{v} \times \mathbf{B}) \cdot\mathrm{d}\mathbf{l}\]</span></p><h2 id="">/</h2><h3 id=""></h3><h3 id=""></h3><p><span class="math display">\[    \varepsilon = -\frac{\mathrm{d}\Phi}{\mathrm{d}t} = - \iint_{S}\frac{\partial \mathbf{B}}{\partial t} \cdot \mathrm{d}\mathbf{S}\]</span>  <span class="math display">\[    \varepsilon= \oint_{l} \mathbf{E}_i \cdot \mathrm{d}\mathbf{l}\]</span></p><p> <span class="math display">\[    \oint_{l} \mathbf{E}_i \cdot \mathrm{d}\mathbf{l}=-\frac{\mathrm{d}\Phi}{\mathrm{d}t} = - \iint_{S} \frac{\partial\mathbf{B}}{\partial t} \cdot \mathrm{d}\mathbf{S}\]</span></p><p><strong>Q:</strong> </p><p><span class="math display">\[    \varepsilon = \int_{a}^{b} (\mathbf{v} \times \mathbf{B}) \cdot\mathrm{d}\mathbf{l} + \int_{a}^{b} \mathbf{E}_i \cdot\mathrm{d}\mathbf{l}\]</span></p><h3 id=""></h3><p><strong></strong></p><p><span class="math display">\[    I \propto E_i \propto \frac{\partial \mathbf{B}}{\partial t} \propto\omega\]</span></p><p><span class="math display">\[    P \propto \omega^{2}\]</span></p><p></p><p><strong></strong></p><p>.</p><p><strong></strong></p><p></p><p> <span class="math display">\[    \mathbf{B} = \frac{1}{2} \bar{\mathbf{B}}\]</span></p><p>.</p><p> <span class="math inline">\(\frac{1}{4}\)</span>.</p><h2 id=""></h2><h3 id=""></h3><p></p><p><span class="math display">\[    \Psi_{21} = M_{21} I_{1}\]</span> <span class="math display">\[    \Psi_{12} = M_{12} I_2\]</span> <span class="math display">\[    M_{12}=M_{21}=M\]</span></p><p><span class="math inline">\(M\)</span><strong></strong></p><p> <span class="math inline">\(M\)</span>  <spanclass="math display">\[    \varepsilon_{21}=-M \frac{\mathrm{d}I_1}{\mathrm{d}t}\]</span> <span class="math display">\[    \varepsilon_{12}=-M \frac{\mathrm{d}I_2}{\mathrm{d}t}\]</span></p><p><span class="math inline">\(H=VA ^{-1} s = \Omegas\)</span></p><p>.<span class="math inline">\(M\)</span>.</p><p><strong>Q:</strong>  <span class="math inline">\(S, l,N_1, N_2\)</span></p><p><span class="math display">\[    M= \mu_0 \frac{N_1N_2}{l}S\]</span></p><h3 id=""></h3><p><span class="math display">\[    \Psi = LI\]</span> <span class="math display">\[    \varepsilon_{L} = -L \frac{\mathrm{d}I}{\mathrm{d}t}\]</span></p><p> <span class="math inline">\(I\)</span> .<spanclass="math inline">\(L\)</span></p><p><strong>Q:</strong>  <spanclass="math inline">\(S,l,N\)</span></p><p><span class="math display">\[    L= \mu_0 \frac{N^{2}}{l}S= \mu_0 n^{2} V\]</span>  <span class="math display">\[    L= \mu n^{2} V\]</span></p><p><strong>Q:</strong>  <spanclass="math inline">\(S,l,N_1,N_2\)</span></p><p><span class="math display">\[    M= \sqrt{L_1L_2}\]</span></p><p><strong></strong> <span class="math display">\[    k= \frac{M}{\sqrt{L_1L_2}}\]</span></p><p> <span class="math inline">\(k=1\)</span> <spanclass="math inline">\(k&lt;1\)</span></p><p></p><p><strong>&amp;</strong></p><p><strong></strong><strong></strong></p><p><strong>Q:</strong> <span class="math inline">\(L_1,L_2,M\)</span></p><p><span class="math display">\[    L=L_1+L_2+2M\]</span></p><p><strong>Q:</strong> <span class="math inline">\(L_1,L_2,M\)</span></p><p><span class="math display">\[    L=L_1+L_2-2M\]</span></p><p> <spanclass="math inline">\(L_1=L_2=L_0\)</span> <spanclass="math inline">\(L=4L_0\)</span> <spanclass="math inline">\(L=0\)</span>.</p><h2 id=""></h2><h3 id=""></h3><p> <span class="math display">\[    W_{m}= \frac{1}{2} LI^{2}\]</span></p><p><strong>Q:</strong>  <spanclass="math inline">\(\mathbf{B},V,\mu\)</span> <spanclass="math inline">\(W_{m}\)</span></p><p><span class="math display">\[    W_{m}= \frac{1}{2} \frac{B^{2}}{\mu}V\]</span></p><h3 id=""></h3><p><span class="math display">\[    w_{m} = \frac{1}{2} \frac{B^{2}}{\mu} =\frac{1}{2} \muH^{2}=\frac{1}{2} BH\]</span></p><p><span class="math display">\[    W_{m}= \iiint w_m \mathrm{d}V\]</span></p><h3 id=""></h3><p> <span class="math display">\[    W_m= \frac{1}{2} L_1I_1^{2}+\frac{1}{2} L_2I_2^{2} \pm MI_1I_2\]</span></p><p> <span class="math inline">\(\pm MI_1I_2\)</span></p><p></p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter13</title>
    <link href="/2022/06/11/Chapter13/"/>
    <url>/2022/06/11/Chapter13/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><h2 id=""></h2><p> <spanclass="math inline">\((r,\mathbf{v})\)</span> <spanclass="math display">\[    \mathbf{\mu}_l=\frac{1}{2} e \mathbf{v} r\]</span></p><p> <span class="math display">\[    \mathbf{\mu}_l=-\frac{e}{2m} \mathbf{L}\]</span></p><p><strong></strong></p><p><span class="math display">\[    \mathbf{\mu}_s=-\frac{e}{m} \mathbf{S}\]</span></p><p> <span class="math inline">\(\mathbf{S}\)</span><span class="math inline">\(\mathbf{\mu}_s\)</span></p><p><strong></strong></p><p></p><p><span class="math display">\[    \mathbf{\mu}_m= \sum_{i}^{} (\mathbf{\mu}_{li}+\mathbf{\mu}_{si})\]</span></p><p><strong></strong> <span class="math inline">\(\mathbf{\mu}_m\neq 0\)</span></p><p><strong></strong> <spanclass="math inline">\(\mathbf{\mu}_m=0\)</span></p><p>.</p><h2 id=""></h2><p> <span class="math display">\[    \mathbf{M}= \lim_{\Delta V \to 0} \frac{\sum_{i}^{}\mathbf{\mu}_{mi}}{\Delta V}\]</span></p><p>A <span class="math inline">\(\cdot\)</span> m <spanclass="math inline">\(^{-1}\)</span></p><p> <span class="math inline">\(\mathbf{M}\)</span> <spanclass="math inline">\(\mathbf{M}\)</span> .</p><p><strong></strong></p><p><span class="math display">\[    \mathbf{\alpha}&#39;= \mathbf{M} \times \mathbf{e}_n\]</span></p><p> <span class="math inline">\(L\)</span> <span class="math display">\[    \oint_{l} \mathbf{M} \cdot \mathrm{d}\mathbf{l} =I&#39;\]</span></p><h2 id=""></h2><h3 id=""></h3><p><span class="math display">\[    \mathbf{H}= \frac{\mathbf{B}}{\mu_0} -\mathbf{M}\]</span></p><p>A/m</p><p><span class="math display">\[    \mathbf{M}= \chi_{m} \mathbf{H}     \]</span></p><h3 id=""></h3><p> <span class="math inline">\(\chi_{m}\)</span> . <spanclass="math inline">\(\mathbf{M}\)</span>  <spanclass="math inline">\(\mathbf{H}\)</span> .</p><p><span class="math display">\[    \mathbf{B}= \mu_0 (1+ \chi_{m})\mathbf{H}=\mu_0 \mu_r \mathbf{H}\]</span></p><p> <span class="math inline">\(\mu_{r}=1+\chi_{m}\)</span>.  <span class="math inline">\(\mu=\mu_0\mu_{r}\)</span>  <span class="math display">\[    \mathbf{B}= \mu \mathbf{H}\]</span></p><h3 id=""></h3><p> <span class="math display">\[    \oint_{l} \mathbf{H} \cdot \mathrm{d}\mathbf{l}= I_0\]</span></p><p><span class="math inline">\(I_0\)</span></p><p><strong></strong></p><p><span class="math display">\[    \oiint_{S} \mathbf{B}\cdot \mathrm{d}\mathbf{S}=0\]</span></p><h3 id=""></h3><p> <spanclass="math display">\[    B_{1n}=B_{2n} \\    H_{1t}=H_{2t}\]</span></p><p><strong>Q:</strong> <span class="math inline">\(\mathbf{B}\)</span> <span class="math display">\[    \frac{\tan \theta_1}{\tan \theta_2}=\frac{\mu_1}{\mu_2}\]</span></p><h2 id=""></h2><p><strong></strong> </p><p><strong></strong> <spanclass="math inline">\(\mathbf{B}\)</span>  <spanclass="math inline">\(\mathbf{H}\)</span> </p><p></p><p><strong></strong> </p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter12</title>
    <link href="/2022/06/11/Chapter12/"/>
    <url>/2022/06/11/Chapter12/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><h2 id=""></h2><h3 id=""></h3><p><span class="math display">\[    \mathbf{j}=\frac{\mathrm{d}I}{\mathrm{d}S_{\perp}}\mathbf{e}_n\]</span> </p><p> <span class="math inline">\(q\)</span> <spanclass="math inline">\(n\)</span> <spanclass="math inline">\(\mathbf{v}_d\)</span> <spanclass="math display">\[    \mathbf{j}=nq \mathbf{v}_d\]</span></p><p><strong></strong></p><p><span class="math display">\[    I= \iint_{S} \mathbf{j} \cdot \mathrm{d}\mathbf{S}\]</span></p><p><strong></strong></p><p><span class="math display">\[    \oiint_{S} \mathbf{j} \cdot \mathrm{d}\mathbf{S}=-\frac{\mathrm{d}q_{}}{\mathrm{d}t}         \]</span></p><p> <span class="math display">\[    \oiint_{S} \mathbf{j} \cdot \mathrm{d}\mathbf{S}=0\]</span></p><p><strong></strong><strong></strong></p><p> <span class="math display">\[    \oiint_{S} \mathbf{E}\cdot \mathrm{d}\mathbf{S}=\frac{1}{\varepsilon_0} \sum_{i}^{} q_i    \]</span> </p><p><span class="math display">\[    \oint_{l} \mathbf{E} \cdot\mathrm{d}  \mathbf{l}=0\]</span> </p><h3 id=""></h3><p></p><p><span class="math display">\[    \mathbf{j} = \frac{1}{\rho} \mathbf{E}= \sigma \mathbf{E}\]</span>  <span class="math inline">\(\sigma\)</span></p><p><strong></strong></p><p><span class="math display">\[    \sigma= \frac{n e^{2} \tau}{m}\]</span>  <span class="math inline">\(n\)</span><span class="math inline">\(\tau\)</span></p><p><strong></strong></p><p><span class="math display">\[    w=\sigma \mathbf{E}^{2}\]</span></p><h3 id=""></h3><p><strong></strong></p><p><span class="math display">\[    \mathbf{E}_k=\frac{\mathbf{F}_k}{q}\]</span></p><p></p><p><strong></strong></p><p><span class="math display">\[    E=\frac{W}{q}=\int_{-}^{+} \mathbf{E}_k \cdot \mathrm{d}\mathbf{l}  \]</span></p><p><strong></strong></p><p><span class="math display">\[    \mathbf{j}=\sigma(\mathbf{E}+\mathbf{E}_k)\]</span></p><h2 id=""></h2><p>1 <span class="math inline">\(\mathbf{F}=q \mathbf{v} \times\mathbf{B}\)</span></p><p>2 <span class="math inline">\(\mathbf{F}= I \mathbf{L} \times\mathbf{B}\)</span></p><p>3 <span class="math inline">\(\mathbf{M}= \mathbf{m} \times\mathbf{B}\)</span></p><h2 id="-">-</h2><p><strong>Biot-Savart-Laplace Law</strong> <spanclass="math display">\[    \mathrm{d}\mathbf{B}= \frac{\mu_0}{4\pi} \frac{I\mathrm{d}\mathbf{l} \times \mathbf{r}}{r^{3}}\]</span></p><p><span class="math inline">\(\mu_0=4\pi \times 10^{-7}\)</span>(T<span class="math inline">\(\cdot\)</span> m/A) </p><p><span class="math display">\[    \mathbf{B}=\int_{}^{}  \mathrm{d}\mathbf{B}=\frac{\mu_0}{4\pi}\int_{L}\frac{\mathbf{I} \mathrm{d}\mathbf{l} \times \mathbf{r}}{r^{3}}\]</span></p><p><strong>Q:</strong>  <spanclass="math inline">\(r\)</span>  <spanclass="math display">\[    \mathbf{B}=\frac{\mu_0 I}{2\pi r}\]</span></p><p><strong>Q:</strong> </p><p><span class="math display">\[    \mathbf{B}=\frac{\mu_0 IR^{2}}{2(R^{2}+z^{2})^{3/2}}\]</span></p><p> <span class="math inline">\(z=0\)</span> <spanclass="math display">\[    B_{O}=\frac{\mu_0 I}{2R}\]</span></p><p> <span class="math inline">\(z\gg R\)</span> <spanclass="math display">\[    B_{P} \thickapprox \frac{\mu_0 IR^{2}}{2z^{3}}= \frac{\mu_0}{2\pi}\frac{IS}{z^{3}}\]</span></p><p> <span class="math inline">\(\mathbf{m}=IS\mathbf{e}_n\)</span> <span class="math display">\[    \mathbf{B}_{P}= \frac{\mu_0}{2\pi} \frac{\mathbf{m}}{z^{3}}\]</span></p><p><spanclass="math inline">\(\mathbf{m}\)</span><strong></strong>.  <spanclass="math inline">\(N\)</span>  <spanclass="math inline">\(m=NIS\)</span></p><p><strong>Q:</strong> </p><p><span class="math display">\[    \mathbf{B}_{O}=\mu_0 nI\]</span></p><p></p><p><strong>Q:</strong>  <spanclass="math inline">\(\mathbf{B}=\frac{1}{2}\mu_0 nI\)</span></p><h2 id="">/</h2><p> <span class="math inline">\(S\)</span>  <spanclass="math display">\[    \Phi_{m}= \iint_{S} \mathbf{B} \cdot \mathrm{d}\mathbf{S}   \]</span></p><p><strong></strong></p><p><span class="math display">\[    \oiint_{S} \mathbf{B}\cdot \mathrm{d}\mathbf{S}=0\]</span></p><p><strong></strong></p><p><span class="math display">\[    \oint_{l} \mathbf{B}\cdot \mathrm{d}\mathbf{l}= \mu_0 \sum_{}^{} I\]</span>  <span class="math inline">\(I\)</span>..</p><p><strong>Q:</strong>  <spanclass="math display">\[    B(z)=\frac{\mu_0 \alpha}{2}\]</span>  <span class="math inline">\(\alpha\)</span></p><p></p><p><strong>Q:</strong> </p><p></p><p><strong>Q:</strong>  <span class="math inline">\(n,\mathrm{d}\mathbf{l}, S, q, \mathbf{v}\)</span> <spanclass="math inline">\(\mathbf{r}\)</span>  <spanclass="math inline">\(\mathrm{d}\mathbf{B}\)</span></p><p><span class="math display">\[    \mathrm{d}\mathbf{B}= \frac{\mu_0}{4\pi}\frac{q \mathrm{d}N\mathbf{v} \times \mathbf{r}}{r^{3}}\]</span>  <span class="math display">\[    \mathbf{B}= \frac{\mu_0}{4\pi} \frac{q \mathbf{v} \times\mathbf{r}}{r^{3}}\]</span></p><p> <span class="math inline">\(v \ll c\)</span></p><p> Kelvin-Stokes Theorem </p><p><span class="math display">\[    \nabla \times \mathbf{B}= \mu_0 \mathbf{j}\]</span></p><p></p><h2 id=""></h2><p><span class="math display">\[    \mathbf{F}= \int_{L}^{} I \mathrm{d}\mathbf{l} \times \mathbf{B}\]</span></p><p></p><p><strong>Q:</strong>  <spanclass="math display">\[    \mathbf{M}= \mathbf{m} \times \mathbf{B}\]</span>  <span class="math inline">\(\mathbf{m} = NI\mathbf{S}\)</span></p><p><strong>Q:</strong>  <spanclass="math inline">\(I\)</span> <spanclass="math inline">\(\mathbf{B}\)</span>  <spanclass="math display">\[    A=I \Delta \Phi\]</span></p><p> <span class="math inline">\(I\)</span>  <spanclass="math inline">\(\mathbf{B}\)</span> <spanclass="math inline">\(\Phi&gt;0\)</span>. .</p><h2 id=""></h2><p><strong>Q:</strong>  <spanclass="math inline">\(\mathbf{B}\)</span> <spanclass="math inline">\(\mathbf{m}\)</span>  <spanclass="math inline">\(- \mathbf{B}\)</span> </p><p><strong></strong></p><p><span class="math display">\[    U_{H}= R_{H} \frac{IB}{d}\]</span></p><p><span class="math inline">\(d\)</span> <spanclass="math inline">\(\mathbf{B}\)</span> <spanclass="math inline">\(R_H=\frac{1}{qn}\)</span> </p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter11</title>
    <link href="/2022/06/11/Chapter11/"/>
    <url>/2022/06/11/Chapter11/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><h2 id=""></h2><h3 id=""></h3><p> <span class="math display">\[    \mathbf{E}= \frac{\sigma}{\varepsilon_0} \mathbf{n}\]</span>  <span class="math inline">\(\sigma\)</span> ..</p><p><span class="math inline">\(U_{}=U_0, E_{}=0,\sigma_{}=0\)</span></p><p></p><h3 id=""></h3><p>..</p><h3 id=""></h3><p></p><h3 id=""></h3><h2 id=""></h2><h2 id=""></h2><h3 id=""></h3><p><strong></strong> -  <spanclass="math inline">\(p\neq 0\)</span>  - <span class="math inline">\(p=0\)</span> </p><p><strong></strong></p><h3 id=""></h3><p> <span class="math display">\[    \mathbf{P}= \lim_{\Delta V \to 0} \frac{\sum_{i}^{}\mathbf{p}_i}{\Delta V}\]</span>  <span class="math inline">\(\Delta V\)</span>. <span class="math inline">\(\mathbf{p}_i\)</span> C/m<spanclass="math inline">\(^{2}\)</span></p><p> <spanclass="math inline">\(n\)</span> <spanclass="math display">\[    \mathbf{P}=n \mathbf{p}\]</span></p><h3 id=""></h3><p>.</p><p> $S $ <span class="math display">\[    \sigma&#39;=\mathbf{P} \cdot \mathbf{e}_n\]</span></p><p> <span class="math display">\[    q&#39;=-\oiint _{S} \mathbf{P} \cdot \mathrm{d}\mathbf{S}\]</span></p><h3 id=""></h3><p><span class="math inline">\(E=E_0+E&#39;\)</span> . <spanclass="math inline">\(E&#39;\)</span> .<span class="math display">\[    \mathbf{P}= \chi_e \varepsilon_0 \mathbf{E}\]</span>  <span class="math inline">\(\chi_e\)</span>..</p><h3 id=""></h3><p><span class="math display">\[    \oint_{l} \mathbf{E}_0 \cdot \mathrm{d}l=0\]</span></p><h3 id=""></h3><p> <span class="math display">\[    \varepsilon_0 \mathbf{E}+\mathbf{P}=\mathbf{D}\]</span>  <span class="math display">\[    \oiint_{S} \mathbf{D} \cdot \mathrm{d} \mathbf{S}= \sum_{S}^{}q_0       \]</span> <span class="math inline">\(\mathbf{D}\)</span> - <spanclass="math inline">\(\mathbf{E}_0\)</span>  -<span class="math inline">\(\mathbf{D}\)</span>  -.  <spanclass="math inline">\(\mathbf{P}\)</span>  <spanclass="math inline">\(\mathbf{E}\)</span>.</p><p> <span class="math display">\[    \mathbf{D}=\varepsilon_0 E+\chi_e \varepsilon_0 E=\varepsilon_0(1+\chi_e)\mathbf{E}\]</span></p><p> <span class="math inline">\(\varepsilon_r=1+ \chi_e\)</span> <span class="math display">\[    \mathbf{D}=\varepsilon_0 \varepsilon_r \mathbf{E}=\varepsilon\mathbf{E}\]</span></p><p></p><p><strong>Q</strong>  <spanclass="math inline">\((\mathbf{P},R)\)</span>  <spanclass="math display">\[    \mathbf{E}&#39;=-\frac{P}{3 \varepsilon_0}\]</span></p><h3 id=""></h3><p><strong></strong>  <spanclass="math inline">\(\mathbf{D}\)</span>  <spanclass="math inline">\(\mathbf{E}\)</span> </p><p><strong></strong>.</p><p> <span class="math display">\[    \frac{\tan \theta_1}{\tan \theta_2} =\frac{\varepsilon_1}{\varepsilon_2}\]</span></p><p></p><p><strong></strong></p><p> <span class="math inline">\(\varepsilon_i\)</span> <span class="math display">\[    \mathbf{E}_{it}=\mathbf{E}_{jt}, \quad\mathbf{D}_{in}=\mathbf{D}_{jn}\]</span></p><p>.</p><h2 id=""></h2><p><span class="math inline">\(n\)</span>  <spanclass="math display">\[    W=\frac{1}{2} \sum_{i}^{} q_i U_i\]</span>  <span class="math inline">\(U_i\)</span>  <spanclass="math inline">\(q_i\)</span>  <spanclass="math inline">\(q_i\)</span> </p><p> <span class="math display">\[    W=\frac{1}{2} \int_{Q}^{} U \mathrm{d}q\]</span></p><p></p><p><strong><spanclass="math inline">\(Q,R\)</span></strong> <spanclass="math display">\[    W=\frac{Q^{2}}{8\pi \varepsilon_0 R}\]</span></p><h3 id=""></h3><p> <span class="math display">\[    w_{e}=\frac{1}{2} \mathbf{D} \cdot \mathbf{E}\]</span></p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter10</title>
    <link href="/2022/06/11/Chapter10/"/>
    <url>/2022/06/11/Chapter10/</url>
    
    <content type="html"><![CDATA[<h1 id="chapter-10-">Chapter 10 </h1><h2 id=""></h2><p><strong>Coulomb's Law</strong> <span class="math display">\[    \mathbf{F}=\frac{q_1q_2}{4\pi \varepsilon_0 r^{2}}\mathbf{e_r}\]</span></p><p> <span class="math display">\[    \varepsilon_0=\frac{1}{4\pi k}=8.854\times 10^{-12}\]</span></p><p><strong>Charge Conservation</strong> The total electric charge in anisolated system never changes</p><h2 id=""></h2><p></p><p> <span class="math display">\[    \mathbf{E}=\frac{\mathbf{F}}{q_0}=\frac{q}{4\pi \varepsilon_0r^{2}}\mathbf{e_r}\]</span> <spanclass="math inline">\(q_0\)</span> &gt;</p><p><strong>Electric Dipole</strong></p><p><strong>Electric Dipole Moment</strong> <spanclass="math display">\[    \mathbf{p}=q \mathbf{l}\]</span></p><h3 id=""></h3><p> <span class="math display">\[    \mathbf{E_B}=-\frac{\mathbf{p}}{4\pi \varepsilon_0 r^{3}}\]</span></p><p> <span class="math inline">\(A\)</span><span class="math display">\[    \mathbf{E_A}=\frac{2\mathbf{p}}{4\pi \varepsilon_0 r^{3}}\]</span>  <span class="math inline">\(r\)</span> <spanclass="math inline">\(A\)</span></p><p><strong>Q</strong>  <spanclass="math inline">\((\mathbf{p},\mathbf{r},\theta)\)</span> <span class="math display">\[    \mathbf{E}=\frac{1}{4\pi \varepsilon_0r^{3}}\left[\frac{3(\mathbf{p}\cdot\mathbf{r})\mathbf{r}}{r^{2}}-\mathbf{p}\right]\]</span></p><p><strong>Q</strong>  <spanclass="math inline">\(d\)</span> <span class="math display">\[    E=\frac{\lambda}{2\pi \varepsilon_0 d}\]</span></p><p><strong>Q</strong>  <span class="math display">\[    E_x=E_y=\frac{\lambda}{4\pi \varepsilon_0d}\]</span></p><p><strong>Q</strong>  <spanclass="math inline">\((q,r,z)\)</span> <span class="math display">\[    E_z=\frac{qz}{4\pi \varepsilon_0 (r^{2}+z^{2})^{\frac{3}{2}}}\]</span></p><p><strong>Q</strong>  <span class="math display">\[    \mathrm{d}E=\frac{\mathrm{d}q}{4\pi \varepsilon_0}\frac{z}{(r^{2}+z^{2})^{\frac{3}{2}}}\]</span>  <span class="math display">\[    E=\frac{\sigma}{2\varepsilon_0}\left(1-\frac{z}{\sqrt{R^{2}+z^{2}}}\right)\]</span>  - <span class="math inline">\(\displaystyle z\to \infty\colon E=0\)</span> - <span class="math inline">\(\displaystyle z \gg R\colon E=\frac{Q}{4\pi \varepsilon_0 z^{2}}\)</span> - <spanclass="math inline">\(\displaystyle z \to 0 \colon E=\frac{\sigma}{2\varepsilon_0}\)</span></p><h3 id=""></h3><p><span class="math display">\[    \mathbf{F}=q_0 \mathbf{E}\]</span></p><p><strong>Q</strong>  <span class="math display">\[    \mathbf{F}=0\]</span></p><p><strong>Q</strong>  <spanclass="math inline">\((p,E,\theta)\)</span> <spanclass="math display">\[    \mathbf{M}=\mathbf{p} \times \mathbf{E}\]</span></p><p><strong>Q</strong>  <spanclass="math inline">\((\mathbf{p}=p \mathbf{k},\mathbf{E}(\mathbf{r}))\)</span> <span class="math display">\[    \mathbf{F}=p \frac{\partial \mathbf{E}}{\partial z}\]</span></p><p><strong>Q</strong> . <spanclass="math inline">\(\mathbf{p}=(p_x,p_y,p_z),\mathbf{E}(\mathbf{r})\)</span> <span class="math display">\[    \mathbf{F}=\mathbf{p} \cdot \nabla \mathbf{E}\]</span></p><blockquote><p>9</p></blockquote><h2 id=""></h2><h3 id=""></h3><p><span class="math display">\[    \mathrm{d}N=E\mathrm{d}S \cos \theta\]</span>  <span class="math inline">\(\theta\)</span> <spanclass="math inline">\(S\)</span> <spanclass="math inline">\(\mathbf{n}\)</span> <spanclass="math inline">\(E\)</span></p><h3 id="flux">flux</h3><p><span class="math display">\[    \mathrm{d}\Phi=\mathbf{E}\cdot\mathrm{d}\mathbf{S}=E\mathrm{d}S_{\perp }\]</span>  <spanclass="math inline">\(\mathrm{d}\mathbf{S}=\mathrm{d}S\mathbf{e_n}\)</span> </p><p> <span class="math display">\[    \Phi=\iint_{S} \mathbf{E}\cdot \mathrm{d}\mathbf{S}\]</span> </p><h3 id="-1"></h3><p><strong></strong></p><p><span class="math display">\[    \mathrm{d}\Omega =\frac{\mathbf{e_r}\cdot\mathrm{d}\mathbf{S}}{r^{2}}=\sin \theta\mathrm{d}\theta\mathrm{d}\varphi\]</span></p><p>&gt; </p><p><strong>Q</strong>  o <spanclass="math inline">\(4\pi\)</span></p><p><strong>Q</strong>  <span class="math inline">\(\Delta\Omega\)</span><span class="math inline">\(q\)</span> <spanclass="math display">\[    \Delta \Phi=\frac{q \Delta \Omega}{4\pi \varepsilon_0}\]</span> <strong>Q</strong>  o <spanclass="math display">\[    \oiint_{S} \frac{\mathbf{e_r}\cdot \mathrm{d}\mathbf{S}}{r^{2}}=0\]</span>  <spanclass="math inline">\(q\)</span>0.<strong></strong></p><p> <span class="math display">\[    N=\frac{q}{\varepsilon_0}\]</span></p><p> <span class="math display">\[    \oiint_{S} \mathbf{E}\cdot\mathrm{d}\mathbf{S}=\frac{1}{\varepsilon_0}\sum_{i}^{} q_i\]</span> <span class="math inline">\(S\)</span>Gauss</p><p> <span class="math display">\[    \oiint_{S}\mathbf{E}\cdot\mathrm{d}\mathbf{S}=\frac{1}{\varepsilon_0}\iiint_{V} \rho \mathrm{d}V\]</span></p><h3 id=""></h3><p> <span class="math display">\[    \begin{cases}        \displaystyle \mathbf{E}=\frac{\rho}{3 \varepsilon_0}\mathbf{r},r &lt; R \\        \displaystyle \mathbf{E}=\frac{q}{4\pi\varepsilon_0r^{3}}\mathbf{r}, r\geqslant R \\    \end{cases}\]</span></p><h3 id=""></h3><p> <span class="math inline">\(\Delta V\)</span> <span class="math display">\[    \frac{\rho \mathrm{d}V}{\varepsilon_0}=\oiint \mathbf{E}\cdot\mathrm{d}\mathbf{S}=(\nabla \cdot \mathbf{E})\mathrm{d}V\]</span>   <spanclass="math display">\[    \nabla \cdot \mathbf{E}=\frac{\rho}{\varepsilon_0}\]</span> <strong>E <spanclass="math inline">\(\varepsilon_0\)</span></strong></p><h2 id=""></h2><h3 id=""></h3><p> <spanclass="math display">\[    \oint_{l}\mathbf{E} \cdot \mathrm{d}\mathbf{l}=0\]</span></p><h3 id="-1"></h3><p><span class="math display">\[    V_1-V_2= \int_{1}^{2} \mathbf{E}\cdot  \mathrm{d}\mathbf{l}\]</span> <span class="math display">\[    -\mathrm{d}V=\mathbf{E}\cdot \mathrm{d}\mathbf{l}\]</span></p><p> <span class="math display">\[    V_{P}=\frac{q}{4\pi \varepsilon_0 r}\]</span></p><p><strong>Q</strong>  omitted</p><p><strong>Q</strong> </p><p><strong>Q</strong>  <spanclass="math inline">\((p,r,\theta)\)</span> <spanclass="math display">\[    V=\frac{q}{4\pi \varepsilon_0 r_{+}} -\frac{q}{4\pi \varepsilon_0r_{-}}=\frac{q(r_{-}-r_{+})}{4\pi \varepsilon_0 r_{+} r_{-}}=\frac{p\cos\theta}{4\pi \varepsilon_0 r^{2}}=\frac{\mathbf{p}\cdot \mathbf{r}}{4\pi\varepsilon_0 r^{3}}        \]</span></p><p><strong>&amp;</strong> <spanclass="math display">\[    \mathbf{\tau}=\mathbf{l}\sigma_{e}=\frac{\Delta p}{\Delta S}\]</span></p><p>A <span class="math display">\[    V_{A}=\frac{l\sigma_{e}}{4\pi \varepsilon_0}\Omega, (z&gt;0) \\    V_{A}=-\frac{l\sigma_{e}}{4\pi \varepsilon_0}\Omega, (z&lt;0) \\\]</span>  <span class="math inline">\(\displaystyle\frac{\tau}{\varepsilon_0}\)</span></p><p><strong>Q</strong>  <spanclass="math inline">\((R,x)\)</span>  <spanclass="math inline">\(\lambda(\theta)=\lambda_0 \sin \theta\)</span> <span class="math display">\[    U(x)=\int_{0}^{\pi} \frac{R\lambda_0 \sin \theta\mathrm{d}\theta}{4\pi \varepsilon_0(R^{2}+x^{2}-2Rx \cos\theta)^{\frac{1}{2}}}=\frac{\lambda_0}{2\pi \varepsilon_0}\]</span>  &gt;</p><p></p><h3 id=""></h3><ul><li></li><li></li></ul><p><strong></strong> <span class="math display">\[    \nabla V=\frac{\mathrm{d}V}{\mathrm{d}n}\mathbf{e_n}\]</span> </p><p> <span class="math display">\[    \mathbf{E}=-\nabla V=-(\frac{\partial V}{\partialx}\mathbf{i}+\frac{\partial V}{\partial y}\mathbf{j}+\frac{\partialV}{\partial z}\mathbf{k})\]</span> </p><h3 id=""></h3><p><span class="math display">\[    W=qV\]</span> <span class="math display">\[    W_1-W_2=\int_{1}^{2} q_0 \mathbf{E} \cdot \mathrm{d}\mathbf{l}\]</span></p><p><strong>Q</strong>  <spanclass="math inline">\(\mathbf{p}\)</span>  <spanclass="math inline">\(\mathbf{E}\)</span>  <spanclass="math display">\[    W_{\text{dipole}}=-\mathbf{p}\cdot \mathbf{E}   \]</span></p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/05/21/%E6%B0%B8%E4%B9%85%E8%AE%B0%E5%BD%95%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <url>/2022/05/21/%E6%B0%B8%E4%B9%85%E8%AE%B0%E5%BD%95%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>2020</p><p>2020</p><p>Russia Beyond1195</p><p>100</p><p>1917</p><p>201721</p><p>100</p><p>215021</p><p>@OedoSoldier</p><p><strong></strong></p>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/05/18/%E8%AE%A1%E7%A7%91%E5%AF%BC%E5%A4%8D%E4%B9%A0/"/>
    <url>/2022/05/18/%E8%AE%A1%E7%A7%91%E5%AF%BC%E5%A4%8D%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><h2 id=""></h2><ul><li></li><li></li><li></li><li>1642BlaisePascal<strong></strong>Pascaline</li><li>1673Gottfried Wilhelm vonLeibniz<strong></strong>SteppedReckoner</li><li>1804JosephJacquard<strong></strong>JacquardLoom</li><li>1822CharlesBabbage<strong></strong>Difference Engine</li><li>1833<strong></strong><strong></strong>AnalyticalEngine<strong>5</strong></li><li>Ada A. LovelaceByron<strong></strong>1979<strong>Ada</strong></li><li>1866HermanHollerith<strong></strong>TabulatingMachine1896<strong></strong>Tabulating MachineCompany,TMC1911CTR<strong>IBM</strong></li><li>1934<sub>1941Z-3<strong></strong>Z-11934</sub>1938Z-2</li><li>1939~1942John V.AtanasoffABCAtanasoff-BerryComputer<strong></strong></li><li>1940~1942ColossusEnigma</li><li>1944Howard H.AikenMark-I(ASCC)1945~1947Mark-IIIBMMark-IMark-II1945bugMark-II</li><li>ITbugBugY2KCOBOL</li><li>1946215ENIAC (Electronic Numerical IntegratorandComputer)<strong></strong><strong></strong><strong></strong><strong></strong><strong></strong></li><li>1951614-UNIVAC</li></ul><h2 id=""></h2><ul><li>1946-1958<strong></strong><strong></strong><strong></strong><strong></strong></li><li>1958-1964FORTRANCOBOLALGOL</li><li>1964-1971<strong></strong><strong></strong>I/O<strong></strong>&gt;196518-241969181995<strong></strong></li><li>1971-</li><li></li></ul><h2 id=""></h2><ul><li><p>1976Cray-12.5Cray-1Cray-3Cray-4</p></li><li><p>200920162018Summit2021Fugaku</p></li><li><p></p></li><li><p>## </p></li><li><p>1936<strong></strong>195010<strong></strong><strong></strong>1944ENIAC1945EDVAC</p></li><li><p>EDVAC5</p></li><li><p></p></li><li><p></p></li><li><p>1956738103</p></li><li><p>195881103</p></li><li><p>(ACM)(IEEE)(USENIX)(INFORMS)(AAAS)(SIAM)</p></li><li><p>(MAA)(ASA)(APS)(AIP)</p></li><li><p>(ITTC)(ISSC)(MRS)</p></li><li><p>(CCF)(CIE)(CIC)(ORSC)(CIPS)</p></li><li><p><strong></strong></p></li><li><p>19621991IEEE-CS/ACMComputingCurricula(CC1991)CC202019561998200120152019</p></li></ul><h1 id=""></h1><h2 id=""></h2><p></p><h2 id=""></h2><p>-<strong></strong>-  -<strong></strong>--C/C++JavaBASICVBPythonJavaScript-  -</p><h2 id=""></h2><ul><li>1946ENIAC</li><li>1951  Grace Hopper,/</li><li>1957 IBM, John Backus, FORTRAN</li><li>1970 Niklaus Wirth, Pascal</li><li>1995 Sun, James Gosling, Java</li><li>2006 </li></ul><h3 id=""></h3><p><img src="img//1652886440718.png" /></p><ul><li>FORTRANALGOLBASICPascalC</li><li>SmalltalkAdaC++JavaC#</li><li>LISPMLHaskell</li><li>Prolog</li></ul><h1 id=""></h1><h2 id="-1"></h2><ul><li></li><li>1968</li><li></li><li></li><li></li><li><strong></strong><strong></strong><strong></strong><strong></strong><strong></strong><strong></strong><strong></strong>/</li><li></li><li></li><li><strong></strong></li><li><strong>n</strong></li><li></li><li><strong></strong><strong></strong><strong></strong><strong></strong><strong></strong><spanclass="math inline">\(&lt;a_{i-1},a_i&gt;\)</span><strong></strong></li><li></li><li>/</li><li><strong></strong></li></ul><h2 id=""></h2><p> -stack<strong></strong>Top<strong></strong>Bottom<strong></strong>- /push/popLIFO -bottomnulltop=bottom-  - -queue<strong></strong>Rear<strong></strong>Front- /FIFO -  -frontrear-  -noderootedge- depth/level0 -height0 -/ -  -0202- // ---enqueqedeqeueuMingetMin</p><h2 id=""></h2><ul><li>al-Khwrizm</li><li></li><li></li><li>O-Notation, <span class="math inline">\(\Omega-\)</span> Notation,<span class="math inline">\(\Theta-\)</span> Notation, o-Notation, <spanclass="math inline">\(\omega-\)</span> Notation</li></ul><h1 id=""></h1><ul><li>1736</li></ul><h2 id=""></h2><ul><li>1</li><li>//</li><li>/</li><li></li><li></li></ul><h2 id=""></h2><ol type="1"><li>0</li><li>1()</li><li>112()</li><li>(3)<img src="img//1652955674112.png" /> </li></ol><h2 id=""></h2><ol type="1"><li></li><li></li><li>(2),(3)(3)</li><li> <imgsrc="img//1652956389908.png" /> </li></ol><h1 id=""></h1><h2 id=""></h2><ul><li></li><li>1/2-</li><li></li></ul><h2 id=""></h2><p>Dijkstra=</p><h2 id=""></h2><p>DijkstraPrimKrustal</p><h1 id=""></h1><h2 id=""></h2><p></p><p> - <strong></strong><spanclass="math inline">\(\{s_1,\cdots ,s_n\}\)</span> - - - <span class="math inline">\(\{q_s,q_1,\cdots,q_m,q_h\}\)</span>  -</p><p></p><h2 id=""></h2><p>  <spanclass="math inline">\((\Gamma, Q, \delta)\)</span> <spanclass="math inline">\(\Gamma\)</span> <spanclass="math inline">\(Q\)</span> <spanclass="math inline">\(\delta\)</span> </p><h2 id=""></h2><ul><li><span class="math inline">\(\{0,1, , \Box \}\)</span> <span class="math inline">\(f\)</span>ASCII</li><li></li><li> <span class="math inline">\(T(n)\)</span> <span class="math inline">\(4T(n)\)</span></li><li><span class="math inline">\(\tilde{M}\)</span></li></ul><p>-Church TuringThesis</p><h1 id=""></h1><p></p><p></p><h2 id=""></h2><p> -  - --</p><h2 id=""></h2><ul><li><p><strong></strong><strong></strong></p></li><li><p></p></li><li><p></p></li><li><p>Complex Instruction Set Computer,CISCx86</p></li><li><p>Reduced Instruction Set Computer,RISCARMMIPSRISC-V</p></li></ul><p><img src="img//1652970748489.png" /></p><h2 id="cpu">CPU</h2><p>MIPS5IFIDEXEMEMWB</p><ul><li></li><li>CPUCPUCPU=CPI=CPI/</li><li></li><li>CPIClock Cycle per Instruction</li><li>54</li><li>CPU</li><li>CPU</li><li></li></ul><h2 id=""></h2><ul><li></li><li></li><li></li><li></li><li></li></ul><p><img src="img//1652972012696.png" /></p><ul><li>RegisterCPU</li><li>Random AccessMemoryRAMRAM</li><li>Static Random AccessMemorySRAM</li><li>Dynamic Random AccessMemoryDRAM</li><li>FlashMemory100~1000</li><li>MagneticDiskFloppyDiskHard disk</li></ul><h2 id=""></h2><h2 id="-2"></h2><p></p><h2 id="-1"></h2><ul><li>1945~1955</li><li>1955~1965</li><li>1965~1980</li><li>1980</li></ul><p><strong></strong>CPU </p><p>Windows</p><p>MacOSUnix</p><p>Linux</p><h2 id=""></h2><p></p><p><img src="img//1653049741603.png" /></p><ul><li>CPU&gt;ProcessOS  &gt;  &gt;</li></ul><p><img src="img//1653049970728.png" /></p><p><img src="img//1653050056563.png" /></p><ul><li></li><li></li><li></li><li><strong></strong>running<strong></strong>ready<strong>/</strong>blocked</li></ul><p><img src="img//1653050401779.png" /></p><ul><li><p>-&gt;/</p></li><li><p>-&gt;</p></li><li><p>-&gt;</p></li><li><p>-&gt;</p></li><li><p>/</p></li><li><p>/</p></li><li><p>FCFS RRSPNSRTHRRN</p></li></ul><p><img src="img//1653050725806.png" /></p><h3 id="round-rubin">Round-Rubin</h3><p></p><h3 id=""></h3><ul><li><strong></strong></li><li><strong></strong></li><li> </li><li><strong></strong><strong></strong></li></ul><h3 id=""></h3><p><strong></strong>Ccharint</p><p></p><p><strong></strong></p><p> -  (Mutualexclusion) -  (Hold andwait) -  (Nopreemption):  - (Circular wait):</p><h3 id=""></h3><p></p><h3 id=""></h3><p></p><p></p><h3 id=""></h3><p> <strong></strong></p><p></p><h3 id=""></h3><p></p><p>DOS, Unix,windowswindowsLinuxX-window</p><h3 id=""></h3><ul><li>TCP/IP</li><li>shell: command line interface, ls,find/search,man:ps,shutdown,mount,mkdir:compilers,debuggers</li><li>OS I/O: data buffering and formattingMath: common utilities,APIs: (cos,sin,abs,sqrt)</li><li></li></ul><h1 id=""></h1><p>Software</p><p> - 1946 - 1957 -1958 - 1966 - 1967 -1992Objectory -1997 - 2001</p><p><strong></strong>121</p><p><strong></strong></p><p><img src="img//1653052971746.png" /></p><p><strong></strong></p><p><strong></strong><strong></strong></p><p><img src="img//1653053062495.png" /></p><h2 id=""></h2><p>        </p><p></p><p></p><p><img src="img//1653053551964.png" /></p><p> <span class="math inline">\(\neq\)</span> </p><p></p><p> <img src="img//1653053978724.png" /></p><p><img src="img//1653053995856.png" /></p><h2 id=""></h2><p> [IEEE610.12 90]   --  -</p><ul><li>D-designDecomposition<strong></strong><strong></strong></li></ul><p><img src="img//1653054460718.png" /></p><h3 id=""></h3><p>-- - -</p><p>--/ -- Safe Home Access </p><p><img src="img//1653054983497.png" /></p><p> -<strong></strong>-<strong></strong>-<strong></strong>- <strong></strong>DomainModel</p><h3 id=""></h3><p> -  - -</p><p><img src="img//1653055592719.png" /></p><p></p><p><img src="img//1653055620712.png" /></p><p><img src="img//1653055689270.png" /></p><h2 id=""></h2><p>  - -  -</p><p>100</p><p></p><p></p><h2 id=""></h2><p></p><p><img src="img//1653055984840.png" /></p><h3 id=""></h3><p> - -</p><p><strong></strong></p><p> --</p><p> -- (1)  (2) </p><p>0null</p><h3 id=""></h3><p>-<strong></strong><strong></strong><strong></strong></p><h1 id=""></h1><h2 id="-3"></h2><p></p><p><strong></strong><strong></strong><strong></strong></p><p> -5 - 1- 9 -1553 -1919</p><p></p><p> - 1975DESDataEncryption Standard - 1976Diffie-Hellman2015 -1977RSA2002RonRivestAdi ShamirLeonardAdleman -19862000- 2005ABEWaters- 20052018Oded Regev  2005 LWELWE  - 2009FHEIBM CraigGentry-2013iO-iO</p><p> <img src="img//1653125604211.png" /></p><h2 id=""></h2><h3 id=""></h3><p></p><h3 id=""></h3><p></p><h3 id=""></h3><p></p><h3 id=""></h3><p></p><p><img src="img//1653126172410.png" /></p><h3 id=""></h3><p></p><p> --- -12-</p><h2 id=""></h2><p>ASCII </p><h3 id=""></h3><p> (DES) - 1977 - 64 56  -  -  16 Feistel </p><p> -  64bit  -16</p><p> - DES56 <span class="math inline">\(2^{56}=7\times 10^{16}\)</span> -AES128128192256  <spanclass="math inline">\(2^{128}=3 \times 10^{38}\)</span></p><h3 id=""></h3><p> -  -</p><p> - - -  -</p><h3 id=""></h3><ul><li> </li><li></li><li>RSARivest,ShamirAdleman19761978</li></ul><p>  )PKEDSKSKPKPKSK</p><h3 id="rsa-">RSA </h3><p>RSA - en - dn - BobAlice -Alice - Alice - Bob</p><p><img src="img//1653132273563.png" /></p><p>PK={e,n}SK={d,n}</p><p>RSA - pqn=pq<spanclass="math inline">\(\varphi(n)=(p-1)(q-1)\)</span> -e <spanclass="math inline">\(\operatorname{gcd}(e,\varphi(n))\)</span>e<span class="math inline">\(\varphi(n)\)</span>  <spanclass="math inline">\(\varphi(n)\)</span> - d <spanclass="math inline">\(de\equiv 1(\operatorname{mod}\varphi(n))\)</span> -  RSA <span class="math inline">\(\log_{2} n\)</span> <spanclass="math inline">\(c=E(m)\equiv m^{e} (\operatorname{mod} n)\)</span><span class="math inline">\(D(c)\equiv c^{d}(\operatorname{mod} n)\)</span></p><p><img src="img//1653133224606.png" /></p><p>Eve</p><h3 id=""></h3><p> - MD5SHA-15MD5SHA-1</p><p></p><h1 id=""></h1><h2 id="-4"></h2><p><strong></strong></p><p><strong></strong>ISO</p><p></p><h3 id=""></h3><p></p><h3 id=""></h3><p></p><p></p><h3 id=""></h3><p>ConfidentialityIntegrityAvailability</p><p></p><h3id=""></h3><p>COMSECCOMPUSECINFOSECIACS/IA/</p><h2 id=""></h2><p></p><h3 id=""></h3><p><strong></strong> -Masquerade -Replay -Modification -Denial ofService</p><p><strong></strong> - -</p><p><strong></strong></p><p><img src="img//1653180176516.png" /></p><h3 id="isoosi">ISO/OSI</h3><p><strong></strong>5</p><p><strong></strong> --- OSI </p><h3 id=""></h3><p></p><p></p><p><strong></strong></p><h3 id=""></h3><p><strong></strong> - - -</p><p><img src="img//1653180478854.png" /></p><h2 id=""></h2><ul><li>TCSEC-<strong></strong></li><li>TNI, Trusted NetworkInterpretation<strong></strong></li><li>CC</li><li>-<strong></strong></li><li> BS 7799</li><li></li><li></li></ul><h3 id="tcsec">TCSEC</h3><p>TCSEC1970198512TCSEC</p><p>TCSECABCD27</p><p></p><p><img src="img//1653180665988.png" /></p><h3 id=""></h3><ul><li>1986</li><li>1987</li><li>1994</li><li>1999</li><li>2001</li><li>20152014 2020201576</li></ul><p>https://www.cert.org.cn/</p><h2 id=""></h2><h3 id=""></h3><p><strong></strong><strong></strong></p><p>RootKitRootKitActiveX</p><h3 id=""></h3><p><strong></strong>  /</p><p><strong></strong>FTPWEB</p><h3 id=""></h3><ul><li></li><li></li><li></li><li></li><li></li></ul><p><img src="img//1653181249874.png" /></p><h3 id="">/</h3><p><strong></strong><strong></strong><strong></strong><strong></strong></p><p>6</p><p><strong></strong></p><h3 id="hackercracker">HackerCracker</h3><ul><li> (Intrusion)</li><li> Free</li><li></li></ul><h3 id="--ctf">--CTF</h3><p>CTFCapture The FlagCTFflag flag{xxxxxxxx}96DEFCON</p><ul><li>JeopardyWeb </li><li>Attack-Defense</li><li>Mix CTFiCTFCTF</li></ul><h3 id=""></h3><p></p><p></p><h2 id=""></h2><p></p><p> - 2013  11  12 - 2014  02  27  - 2017  6</p><p> -  - -  - -</p><p>20182272018421</p><p><img src="img//1653183467276.png" /></p><h3 id=""></h3><p></p><p> --URLo0--</p><h3 id=""></h3><ul><li></li><li></li><li></li><li></li></ul><h1 id=""></h1><h2 id=""></h2><table><thead><tr class="header"><th>Internet</th><th>internet</th></tr></thead><tbody><tr class="odd"><td></td><td></td></tr><tr class="even"><td>TCP/IP</td><td></td></tr><tr class="odd"><td>TCP/IP</td><td>TCP/IP</td></tr><tr class="even"><td></td><td></td></tr></tbody></table><h3 id=""></h3><p> -  -host</p><p> -</p><p><img src="img//1653195788435.png" /></p><h3 id=""></h3><p>IPAR</p><h3 id=""></h3><p> -  -<strong></strong>Host</p><p> -FTTHDSL- WiFi4G/5G -WiFi4G/5G<strong></strong></p><p><img src="img//1653196531024.png" /></p><h3 id="-ftth">-FTTH</h3><p>FTTH - FTTH: Fiber To The Home - -AONPON - </p><p>PON - PON: Passive Optical Network - OLT - ONU ONT -  ONT OLT</p><p><img src="img//1653196544036.png" /></p><h3 id="-dsl">-DSL</h3><p>DSLDigital Subscriber Line(DSLAM) -DSL -</p><p> - 24-52 Mbps3.5-16 Mbps</p><p>FTTHDSL</p><p><img src="img//1653196771247.png" /></p><h3 id="-">-</h3><p>Cable -<strong></strong> - -  40 Mbps 1.2 Gbps30 100 Mbps </p><p>HFC -</p><p>FTTH80% DSL</p><p><img src="img//1653196989323.png" /></p><h3 id="-">-</h3><p> </p><p>WLAN - 10 -802.11b/g/nWiFi1154450 Mbps</p><p> - 10 -2G/3G/4G/5G - 0.1-1000Mbps</p><h3 id="-">-</h3><p> -  -  WiFi  4G</p><p> - 100Mbps1Gbps10Gbps</p><p>WiFi - 1154450Mbps</p><p>WiFi1GbpsNATDSL</p><h3 id="-">-</h3><p>bit -  - bit</p><p> -  -<strong></strong> -<strong></strong></p><p>ByteK/M/G <spanclass="math inline">\(2^{10}\)</span>BitK/M/G<span class="math inline">\(10^{3}\)</span> </p><h3 id="-">-</h3><p>10-100 Gbps</p><h3 id="-">-</h3><p>Twisted Pair -  -   1 -   4  -5 100 Mbps-1 Gbps - 6 10Gbps</p><p> -  -  - 100Mbps</p><h3 id="-">-</h3><p> -  -  - - </p><p> -  -  - /</p><p> - WiFi10-100 Mbps10 -3/4/5G-10 -  -45Mbps -36000km280ms</p><h3 id=""></h3><p> -  -</p><p> -  -<strong></strong></p><h3 id=""></h3><p> 1  -   -</p><p> 2  - - <strong></strong></p><h3 id=""></h3><p><img src="img//1653198823526.png" /></p><h2 id=""></h2><h3 id=""></h3><p> -networkprotocol - </p><p> -  - -</p><p></p><h3 id=""></h3><p> -</p><p> - </p><p> -</p><p> -</p><p><img src="img//1653199101892.png" /></p><h3 id="osi">OSI</h3><p>OSI 7 ISODay,Zimmermann1983</p><p> -  01Bits on the wire - Mechanical  -  E lectronic -  Timing  - M edium </p><p> Data Link Layer - <strong> Neighboring</strong> -<strong></strong>Framing - -  MAC address 48 - overwhelming -MAC</p><p>Network Layer - <strong>host to host</strong> -<strong></strong>Routing-- QoS -</p><p>MACIPIPIPARPMACIPIP</p><p>Transport Layer - -  host -   - --</p><p>Session -</p><p>Presentation Layer -</p><p>Application Layer -</p><p><img src="img//1653199198313.png" /></p><h3 id="tcpip">TCP/IP</h3><p>TCP/IP ARPANET  -  TCP/IP - Vint Cerf  Bob Kahn  1974 </p><p>Link Layer -</p><p>Internet Layer -IPv4  IPv6 </p><p>Transport Layer -TCP UDP</p><p>Application Layer -DNSHTTPFTPSMTP...</p><p><strong> TCP/IP  TCP/IP </strong><strong></strong></p><p><img src="img//1653232383653.png" /></p><p>TCP/IP  -   -  TCP -</p><p>IP  -  IP over everything - Everything over IP) -  IP</p><h3 id="ositcpip">OSITCP/IP</h3><p>7  4  - TCP/IP- TCP/IP  OSI    -TCP/IP  OSI </p><p> - OSI<strong></strong>- TCP/IP </p><p> - OSI  -TCP/IP IP</p><p><img src="img//1653232852570.png" /></p><h3 id=""></h3><p><img src="img//1653232887840.png" /></p><h2 id=""></h2><ul><li>1957101</li><li>19582ARPA</li><li>19698ARPANETUCLA/SRI/UCSB/Utah</li><li>197224DARPANSFUCLAUCSB</li><li>1983ARPANETTCP/IP</li><li>80Internet</li><li>1990APRANETNSF</li></ul><p> -   MCI  IBM  Qwest CISCO --&gt;-&gt; - -      - </p><p><img src="img//1653233308149.png" /></p><h3 id=""></h3><p> - 1987 - 1994  <strong>64K</strong></p><p> -  CHINANET  UNINET  CMNETCERNET  CSTNET  - 2019 8.8T - 4G  551   4G  900  - 2020 5G  70  7  - IPv4  3.8  IPv6 50903  /32  - .CN  2304  -  APP 359 </p><h3 id=""></h3><p> - 1987  20</p><p> - 2000 2000 - - -</p><p> -  Sigcomm -  -  3GPP ITU T  IETF </p><h2 id=""></h2><p></p><h1 id=""></h1><h2 id="-1"></h2><h3 id=""></h3><p> The Internet Of Things (IOT)IOT)4A </p><p><img src="img//1653263435582.png" /></p><p>Internet  Internet of People for P2P</p><p> Internet of Things for P2T orT2T</p><p></p><h2 id=""></h2><ul><li>1999MITMIT Kevin Ashton RFID</li><li>2005ITU  RFID</li><li>2008IBM</li><li>2009CERP IoT    /RFID  20</li><li>2015</li></ul><h3 id=""></h3><ul><li>1999  MIT  Auto ID CenterEPC(Electronic Product Code    (Internet of Things) </li><li>2005  11  17   ITU ITU Internet Reports2005 The Internet of Things   </li><li>2008  11  IBM CEO  </li><li>2009  1  28   </li><li>2009  2  24  IBM  CEO   </li><li>2009  8  7    </li><li>2012  2       </li><li>2013  2  17    </li></ul><h3 id=""></h3><p>   </p><p> Intelligent Interconnection Of Things (     </p><p><strong></strong></p><h3 id="vs">VS</h3><p> - -   </p><p> -  -    </p><p> -  RFID  -  </p><p> -  - </p><h2 id=""></h2><h3 id=""></h3><ul><li></li><li>         </li><li>  </li><li> </li></ul><h3 id="wsn">WSN</h3><ul><li><strong></strong>Sensor</li><li><strong></strong> WSN Wireless Sensor Networks</li><li><strong></strong></li><li><strong></strong></li></ul><h3 id=""></h3><p><strong></strong></p><p>WSN <imgsrc="img//1653264727418.png" /></p><p>WSN <imgsrc="img//1653264754966.png" /></p><h3 id=""></h3><p><strong></strong>SensingProcessingCommunicationPower</p><p>Mica Mote</p><h3 id=""></h3><p><strong></strong></p><p></p><p></p><h3 id=""></h3><p> -MAC </p><p> -</p><p> -</p><p> -</p><h3 id="rfid">RFID</h3><ul><li> RFID (Radio Frequency Identification)      </li><li>     </li><li> RFID   RFID  RFID</li><li>   </li></ul><h3 id=""></h3><ul><li></li><li></li><li></li><li> E2PROM</li></ul><h3 id="wsnrfid">WSNRFID</h3><p>RFID  WSN  <imgsrc="img//1653274751691.png" /></p><p>RFID  WSN  <imgsrc="img//1653274771263.png" /></p><h3 id="5g">5G</h3><p>5 G(The 5 th Generation Mobile Networks)  2 G4 G      </p><h3id=""></h3><ul><li>1G  FM 300Hz3400Hz </li><li>2G 2G </li><li>3G  4  CDMA</li><li>4G  TD LTE  FDD LTE</li><li>5G</li></ul><p><img src="img//1653274879035.png" /></p><h3 id="5g4g">5G4G</h3><p> 5 G  4 G  -  5 G 4 G  1000  -  10  100  10 Gbps 4 G  100Mbps  5  -   10 100  -   MMC   10 </p><p> 5 G  4 G </p><h2 id=""></h2><p>M2M</p><h3 id=""></h3><p> Vehicle Ad Hoc Networks(VANET)</p><p>V2V Vehicle to Vehicle V2I Vehicle to Infrastructure </p><p> <img src="img//1653275175434.png" /></p><h3 id=""></h3><p> 3</p><p> <img src="img//1653275221375.png" /></p><p> <img src="img//1653275229396.png" /></p><h3 id=""></h3><ul><li></li><li> 21.2 </li><li> 2030</li></ul><p> <img src="img//1653275278798.png" /></p><h3 id=""></h3><p>NASA  JPL  Sensor Webs</p><p></p><h2 id=""></h2><p> -  -  - -  - </p><p> -  - -    - -  - </p><h1 id=""></h1><p></p><p></p><p></p><p></p><p>2D3DIIDOODOut ofDistribution</p><p></p><p></p><p>2030</p><h2 id="-1"></h2><p></p><p></p><p></p><p> -  - - - -</p><h3 id=""></h3><p></p><h3 id=""></h3><p></p><p><img src="img//1653276210906.png" /></p><h2 id=""></h2><h3 id=""></h3><ul><li>1950Computing Machinery andIntelligence</li><li>I propose to consider the question, Can machinesthink?</li><li></li><li>30%</li></ul><h3 id=""></h3><ul><li>1956</li><li></li><li></li></ul><h3 id="-1"></h3><ul><li>1957</li><li>1958</li><li>1965</li><li>1973</li><li>1982</li><li>1986</li><li>1989CNN</li><li>1991DART</li><li>1995SVM</li><li>1997LSTM</li><li>2006</li><li>2009GNN</li><li>2012AlexNet</li><li>2014GAN</li><li>2017:Tensorflow LiteCaffe2</li></ul><h3 id="1956-1970">1956-1970</h3><p>Bellman-Ford</p><p>1957RichardBellman</p><p><img src="img//1653277469884.png" /></p><table><thead><tr class="header"><th></th><th></th></tr></thead><tbody><tr class="odd"><td>Agent</td><td></td></tr><tr class="even"><td>Environment</td><td></td></tr><tr class="odd"><td>Action</td><td></td></tr><tr class="even"><td>State</td><td></td></tr><tr class="odd"><td>Reward</td><td></td></tr></tbody></table><p>Frank Rosenblatt 1928-19711958Rosenblatt</p><p>-1965DENRAL- , MACSYMA,600</p><p> <img src="img//1653278352084.png" /></p><ul><li>ShakeySRI19661972</li><li>ShakeyAI</li></ul><h3 id="1966-1974">1966-1974</h3><p>196520</p><p>XOR</p><p> The spirit is willing but the flesh is weak. </p><p></p><p>AIAI</p><p>19736BBCBBC</p><p>LighthillDebate</p><p>AI</p><p>AIAIAI</p><h3 id="1970-1988">1970-1988</h3><p></p><p>DECXCON-R1DEC</p><p><strong></strong><strong></strong><strong></strong></p><h3 id="1986-">1986-</h3><p>-  - -</p><p>supportvectorMachineCortesVapnik19952012SVM</p><p><img src="img//1653278878360.png" /></p><p>SVM(b)(c)ABSVMABABSVMSVM</p><ul><li>1986HintonMLPBPSigmoid</li><li>1989LeCunCNN-LeNet</li><li>1997LSTMRNN</li><li>2006Hinton+</li><li>2011ReLU</li><li>2012HintonImageNetCNNAlexNetSVM</li></ul><h2 id=""></h2><p><img src="img//1653278927103.png" /></p><p><img src="img//1653278944233.png" /></p><p><img src="img//1653278955230.png" /></p><ul><li><p></p></li><li><p></p></li><li><p></p></li><li><p></p></li><li><p></p></li><li><p></p></li></ul><h2 id="-1"></h2><p> ----</p><p></p><h1 id=""></h1><h2 id=""></h2><p>Machine Learning</p><h3 id=""></h3><p> -  - -  -</p><h3 id=""></h3><ul><li></li><li></li><li></li><li></li><li></li></ul><h3 id="supervised-learningslide167">SupervisedLearningSlide167</h3><ul><li></li><li>ClassificationRegression</li><li></li><li></li></ul><p><img src="img//1653358669242.png" /></p><table><thead><tr class="header"><th></th><th></th><th></th></tr></thead><tbody><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td></td><td></td><td></td></tr></tbody></table><p>- 1 -2Data augmentation</p><h3id=""></h3><p> - 1 -2TrainingsetValidationsetTestset- 3 - 4</p><h3 id=""></h3><p><strong></strong> -PCAPrincipal components analysis -AutoEncoder K-means</p><h3 id=""></h3><p>Semi-supervised Learning -</p><p>Transfer Learning - A B</p><p>Reinforcement Learning - - -</p><p><img src="img//1653359132215.png" /></p><p></p><h3id=""></h3><p> <img src="img//1653359170901.png" /></p><p> <img src="img//1653359213291.png" /></p><h3 id=""></h3><ul><li></li><li></li><li></li><li></li></ul><h3 id=""></h3><p>1975</p><p>5238</p><h3 id=""></h3><p>Expert System - -1994 </p><p> - DENDRAL -4</p><p> -  -</p><h3 id=""></h3><p></p><p>SymbolismDecisionTree - - -ID3CARTC4.5GBDT/XGBoost</p><p>Connectionism - - -1969 -(BP)</p><h3id="statistical-learning">StatisticalLearning</h3><p> -- (Support Vector MachineSVM)(Kernel Methods) -SVM</p><h3 id=""></h3><p> - -2006 -2012ImageNetAlexNet</p><p> - BPBackPropagation -CNNConvolutional NeuralNetwork -RNNRecurrent NeuralNetwork -GANGenerative AdversarialNetwork - GCNGraphConvolutional Network -QDQNDeep QNetwork</p><h2 id=""></h2><h3 id=""></h3><p><img src="img//1653360068577.png" /></p><h3 id=""></h3><p><img src="img//1653360083405.png" /></p><p> <imgsrc="img//1653360118733.png" /></p><p>Rosenblatt1958 -  -T1</p><h3 id=""></h3><p>MLP1969 <imgsrc="img//1653360221337.png" /></p><p>MLP <imgsrc="img//1653360247274.png" /></p><p>MLP <imgsrc="img//1653360371173.png" /></p><p>MLP -  -</p><p>MLP -  -AND</p><p> <img src="img//1653360617236.png" /></p><p><img src="img//1653360621352.png" /></p><p> <img src="img//1653360676188.png" /></p><p> <img src="img//1653360693193.png" /> <imgsrc="img//1653360727578.png" /> <imgsrc="img//1653360709860.png" />thresholdfunctions</p><p>MLP -  -MLP</p><h3 id=""></h3><ul><li></li><li></li><li></li><li>MLP</li></ul><p></p><h3 id=""></h3><p>1.2. 3.4.</p><h3id="mlp-">MLP-</h3><h2 id=""></h2><h3 id=""></h3><p></p><p>MLP</p><p><img src="img//1653361369080.png" /></p><h3 id="1-">1-</h3><p></p><h3 id="2-">2-</h3><p><img src="img//1653361431161.png" /></p><h3 id="lenet">LeNet</h3><p><img src="img//1653361465518.png" /></p><h3 id=""></h3><p><img src="img//1653361482409.png" /></p><p>5<em>5</em>3 (5<em>5</em>3 = 75 + )</p><h3 id="stride">Stride</h3><p><img src="img//1653361529779.png" /></p><h3 id=""></h3><p><img src="img//1653361862893.png" /></p><h3 id="-1"></h3><p><img src="img//1653361907372.png" /></p><p>d28*28d</p><p><img src="img//1653361924377.png" /></p><p>28<em>28</em>6</p><h3 id="pooling">Pooling</h3><p><img src="img//1653361948002.png" /></p><p><img src="img//1653361951065.png" /></p><p>28*28 - - </p><h3id=""></h3><p><img src="img//1653361983899.png" /></p><p><img src="img//1653361992883.png" /></p><p>CNN</p><h3 id=""></h3><p> -</p><h3id="recurrentneuralnetwork">RecurrentNeuralNetwork</h3><p><img src="img//1653362215075.png" /></p><h3 id="rnn">RNN</h3><p><img src="img//1653362247106.png" /></p><p>WU ht</p><h3id="rnn">RNN</h3><p><img src="img//1653362274806.png" /></p><ol start="2" type="1"><li>-&gt; </li><li>-&gt; </li><li>-&gt;</li><li></li></ol><h3 id="rnn">RNN</h3><p><img src="img//1653362372812.png" /></p><h3id="seq2seq">Seq2Seq+</h3><p><img src="img//1653362418434.png" /></p><p></p><h3 id="rnn">RNN</h3><p><img src="img//1653362669984.png" /></p><h1 id=""></h1><h2 id=""></h2><p>*</p><ul><li>1965Roberts</li><li>1973</li><li>1997Shi &amp; Malik</li><li>2004SIFT</li></ul><h2 id="-2"></h2><h3 id=""></h3><p>RGBXYZ</p><h3 id=""></h3><p> ! - - - </p><p> -  - </p><p> -  - </p><p> -  - </p><p> <img src="img//1653363075355.png" /></p><p> <img src="img//1653363135635.png" /></p><p><img src="img//1653363226407.png" /> </p><h3id=""></h3><p><img src="img//1653363336935.png" /></p><h3id="convolutionalneuralnetwork">ConvolutionalNeuralNetwork</h3><p><img src="img//1653363363386.png" /></p><h3id=""></h3><p></p><h2 id=""></h2><h3 id=""></h3><p></p><p></p><p></p><p>ImageNet <img src="img//1653363470620.png" />20122015</p><h3 id=""></h3><p></p><p></p><h3 id="semantic-segmentation">SemanticSegmentation</h3><p></p><p></p><h3 id="instance-segmentation">InstanceSegmentation</h3><p>+ -</p><h3 id="object-tracking">Object Tracking</h3><p>-- </p><h3 id="pose-estimation">PoseEstimation</h3><p>--</p><h2 id="-1"></h2><p><img src="img//1653364299657.png" /></p><h1 id=""></h1><h2 id=""></h2><p></p><p><strong></strong></p><ul><li>1960sHoneywellIDSIntegrated DataStore</li><li>1970s</li><li>1980s</li><li>2000sNo</li></ul><p> - 1973 -1981 -1998 -2014</p><h3 id=""></h3><p> -OracleSAPSybase -IBMDB2DBMS -SQL-ServerAccess</p><p>web2.0NoSQL</p><p>Ingres, Paradox, Foxbase, FoxPro, dBase,</p><p> - Ocean - Polar DBMySQL- GaussDBPostgreSQL - TDSQLMySQL -TiDBPingCAPSpanner - SequoialDB</p><h2 id=""></h2><h3 id=""></h3><p>1970 -  - - </p><p></p><p> - -NULL</p><p>N=N</p><h3 id=""></h3><p></p><p> -SEQUENCE(SQL:2003) - AUTO_INCREMENT(MySQL)</p><p>SQLStructured Query Language</p><h3 id=""></h3><p> -  -</p><h3 id=""></h3><p> -</p><ul><li><span class="math inline">\(\sigma\)</span> Select</li><li><span class="math inline">\(\pi\)</span> Projection</li><li>$$ Union</li><li>$$ Intersection</li><li><span class="math inline">\(-\)</span> Difference</li><li>$$ Product</li><li><span class="math inline">\(\bowtie\)</span> Join</li></ul><h3 id="-">-</h3><p>Restriction - SELECT  -</p><p><span class="math inline">\(\sigma_{predicate}(R)\)</span></p><p><img src="img//1653304755316.png" /></p><h3 id="-">-</h3><p>R -  -</p><p><span class="math inline">\(\pi_{(A_1,A_2,\cdots,A_n)}(R)\)</span></p><p><img src="img//1653304818871.png" /></p><h3 id="-">-/</h3><p> <spanclass="math inline">\((R \cup S)\)</span> <imgsrc="img//1653304905601.png" /></p><p> <spanclass="math inline">\((R \cap S)\)</span> <imgsrc="img//1653304920391.png" /></p><h3 id="-">-/</h3><p><span class="math inline">\((R-S)\)</span> <imgsrc="img//1653305001145.png" /></p><p> <spanclass="math inline">\((R \bowtie S)\)</span> <imgsrc="img//1653305015167.png" /></p><h3 id="-">-</h3><p> <spanclass="math inline">\((R \times S)\)</span></p><p><img src="img//1653305248604.png" /></p><h3 id=""></h3><p><img src="img//1653305290164.png" /></p><h2 id=""></h2><p>(Data) </p><p>(Database,DB)</p><p> - -</p><h3 id="dbms">DBMS</h3><p>DBMSSQLServerOracleSybaseMysql</p><h3 id=""></h3><p>VisualBasicDelphiJava, Python</p><p> -  -  - - </p><p><img src="img//1653305529386.png" /></p><h3 id=""></h3><p> -  -MysqlSqlServer</p><p>NoSQL -NoSqlMongoDBRedisMemcache-NoSQL</p><p> <img src="img//1653305579475.png" /></p><h3 id=""></h3><ul><li>SIGMODACM Conference on Management of Data</li><li>SIGKDDACM Knowledge Discovery and Data Mining</li><li>VLDBInternational Conference on Very Large Data Bases</li><li>ICDEIEEE International Conference on Data Engineering</li><li>TKDEIEEE Transactions on Knowledge and Data Engineering</li></ul><h1 id=""></h1><p></p><p> - PB  EB - </p><p> -  -  - - </p><h3 id=""></h3><p><img src="img//1653305755769.png" /></p><p>   -  -1.  2. 3. </p><h2 id=""></h2><p> -  -  - Web - </p><h3 id="-">-</h3><p> Nondependency Oriented Data) - --  -</p><p> - QuantitativeMultidimensional- Categorical and MixedAttribute- Binary andSet-Text</p><h3 id="-">-</h3><p> (Dependency Oriented Data) --- </p><p>  -   1,,-  ,</p><p> -   1,,   -<img src="img//1653306234560.png" /></p><ul><li></li></ul><p>  -</p><p>  - -</p><p> -  - -  - </p><p> -  =(,) (.) - Web-- </p><h3 id=""></h3><p> -  -</p><p> -  -  -  - </p><h3 id=""></h3><p><strong></strong>  - -  -   -  -</p><p> - Python Pandas  fillna  interpolate() sklearn  - R Hmisc  missMDA</p><p><strong></strong> -Binning4,8,15,21,21,24,25,28,34  3  <imgsrc="img//1653306654887.png" /> - (Regression) -   </p><h3 id=""></h3><p>  -sklearn.feature_extraction  FeatureSelector  --</p><h3 id=""></h3><p><strong></strong> [1,1]  [0,1]</p><p><strong></strong></p><p><strong></strong> - min-max  A min-max  A __ - z score <spanclass="math inline">\(\bar{A}\)</span> <spanclass="math inline">\(s_{A}\)</span></p><p><img src="img//1653306891268.png" /></p><h3 id=""></h3><p><img src="img//1653306928667.png" /></p><h3 id=""></h3><p> n  d  D</p><p> -   - </p><p> -  - </p><p></p><h3 id=""></h3><p></p><p> -  - -</p><p> outlier score -- </p><p> -  - -</p><h2 id=""></h2><p></p><p> - -  --  -</p><h3 id=""></h3><p></p><p><img src="img//1653353050790.png" /></p><p><img src="img//1653353055945.png" /></p><p><img src="img//1653353059710.png" /></p><ul><li>(Support)</li><li> </li><li>  minsup =0.3 {Bread, Milk}</li><li> <strong></strong></li><li>  L L </li><li>11 </li><li> ( Confidence) A B<span class="math display">\[  conf(X \Rightarrow Y)=\frac{sup(X \cup Y)}{sup(X)}\]</span></li><li>  X  Y  X Y minsup  minconf  XY&gt;=minsup  XY  X Y&gt;=minconf</li></ul><h3 id=""></h3><p> -  minsup  -  minconf        = = X Y</p><p>  12  12c(22)(11)</p><p>  Lattice 5 <span class="math inline">\(2^{5}=32\)</span> </p><p><strong></strong></p><h3 id=""></h3><p><strong></strong> </p><p>  <spanclass="math inline">\(2^{\lvert U \rvert }-1\)</span>||</p><p> -  - -</p><p><strong>Apriori</strong> </p><p> - k=1  k  - -  k  k+1 - k=k+1 1-3</p><h2 id=""></h2><h3 id="-1"></h3><p></p><p> -  - - -</p><p> -  -</p><p> -  K  -   - - -  -</p><p></p><p><strong>K-Means</strong></p><p>L2   - = ( )K --  -23</p><p><img src="img//1653355224607.png" /></p><p> - K  - - - - NP Aloise 2009</p><p><strong></strong> K Medians  - L1  -  -  K Means</p><p>K Medoids  -  L1  - -</p><h3 id=""></h3><p></p><p><img src="img//1653356559283.png" /></p><p><strong></strong> -  - -</p><p><strong></strong> - - - -  KMeans</p><h3 id=""></h3><p>   - -</p><p>  <imgsrc="img//1653356810418.png" /> <imgsrc="img//1653356813864.png" /></p><p>DBSCAN   Eps   -     -    1  -    -<img src="img//1653356912924.png" /></p><h2 id="-1"></h2><p></p><p> |  |  |  | | -------------- |---------------------- | ---------------------- | |  | |  | |  | |  | |  |  | | |  |  | |</p><h3 id="-2"></h3><p> -    -</p><p>      - - -   </p><h3 id=""></h3><p></p><p>   a   (1)  (2) </p><p></p><p></p><p> - Error  1 -GiniIndexSlide1954-55</p><p> -100%- 20%20%</p><h1 id=""></h1><h2 id="-1"></h2><p>  </p><p></p><p>-&gt;</p><h3 id=""></h3><ul><li>1994</li><li>1997ResnickRecommender System</li><li>2003Linden </li><li>2006Netflix</li><li>2007ACM </li><li>2016YouTube</li><li>2019</li></ul><h2 id=""></h2><h3 id=""></h3><p></p><p> -  -  - </p><h3 id=""></h3><p> </p><h3 id=""></h3><ul><li></li><li>Profile</li><li></li></ul><h3 id=""></h3><p>(Item)<strong></strong><strong></strong></p><p>(Item) - -  -</p><h3 id=""></h3><p></p><p></p><p> </p><h3 id=""></h3><p></p><h3 id=""></h3><p><strong></strong><strong></strong>-   - </p><p>LSTMGRUSelf-AttentionTransformer</p><h3 id=""></h3><p>(Profile Learning) <imgsrc="img//1653282691306.png" /></p><h3 id="-1"></h3><p> -  - - </p><p> ---  Cold Start</p><h3 id=""></h3><p>(,)  </p><p>R  U I  U  I  R  U  I  R</p><p><img src="img//1653282954819.png" /></p><p>UserCF --1.2.</p><p>ItemCF - Netflix  Hulu  Youtube  -1.2.</p><p>UserCFItemCF</p><table><thead><tr class="header"><th></th><th>UserCF</th><th>ItemCF</th></tr></thead><tbody><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td></td><td></td><td></td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td></td><td></td><td></td></tr></tbody></table><h3 id=""></h3><ul><li></li><li> head relation tail</li><li></li></ul><p><img src="img//1653283243706.png" /> <imgsrc="img//1653283247924.png" /></p><h3 id=""></h3><p>-&gt;-&gt;-&gt;</p><p><img src="img//1653283296327.png" /></p><h2 id="-2"></h2><h3 id=""></h3><p>-&gt;-&gt;-&gt;</p><p><img src="img//1653283353587.png" /></p><h3 id=""></h3><p></p><p><img src="img//1653283382250.png" /></p><h1 id=""></h1><h2 id="-5"></h2><p> -  -  -- -  -  -COVID-19</p><h3 id="-2"></h3><p></p><p> - - -</p><h3 id="-2"></h3><p>//</p><h2 id=""></h2><h3 id="plot-line">PlotLine</h3><ul><li></li><li></li></ul><h3 id="bar-chart">BarChart</h3><ul><li></li><li></li></ul><h3 id="bar-chart">BarChart</h3><h3 id="stack-diagram">StackDiagram</h3><ul><li></li><li>ReversedStacks</li></ul><h3 id=""></h3><h3id="histogram">Histogram</h3><ul><li></li><li>/</li></ul><h3 id="pie-chart">(Pie Chart)</h3><ul><li></li><li>1.2.3.4.5.6.</li></ul><h3 id="radar-chart">(RadarChart)</h3><ul><li></li><li></li></ul><h3id="rose-chart">(RoseChart)</h3><ul><li></li><li></li></ul><h3 id=""></h3><h3 id=""></h3><h3id="scatter-plot">ScatterPlot</h3><ul><li></li><li></li></ul><h3 id="box-plot">(BoxPlot)</h3><ul><li></li><li></li></ul><h3id="violin-chart">(ViolinChart)</h3><ul><li>(ViolinChart)</li><li>95%</li></ul><h3id="bubble-chart">BubbleChart</h3><ul><li></li><li></li></ul><h3id="heat-map">/HeatMap</h3><ul><li></li><li></li></ul><h3 id="pivot-table">PivotTable</h3><ul><li>PivotTable</li><li></li><li>TableLens</li></ul><h3 id="matrix-view">Matrix View</h3><ul><li></li><li></li></ul><h3 id="word-cloud">Word Cloud</h3><ul><li></li><li></li></ul><h3 id="theme-river">ThemeRiver</h3><ul><li>,</li><li>value</li></ul><h3 id="map">Map</h3><ul><li></li><li></li><li></li></ul><h3 id="dot-map">DotMap</h3><ul><li>(DotMap)</li></ul><h3 id=""></h3><ul><li>(FlowMap)</li><li></li></ul><h3 id=""></h3><h3 id="poi">POI</h3><h3 id="sankey-diagram">SankeyDiagram</h3><ul><li></li><li>Flow</li></ul><h3id="arc-diagram">(ArcDiagram)</h3><ul><li>(ArcDiagram)</li><li>(Nodes)X</li><li></li></ul><h3 id=""></h3><p>(Generative Graph) -COVID-19 -</p><p><img src="img//1653301416444.png" /></p><h3 id="tree-map">TreeMap</h3><ul><li>(TreeMap)</li></ul><h3id="network">Network</h3><ul><li>(Networkplanning)</li><li></li></ul><h3id="flow-diagram">FlowDiagram</h3><ul><li></li><li></li><li></li></ul><h2 id=""></h2><p>Web </p><p>HTMLCSSJavaScriptHighChartsEchartsD3.js</p><h3 id=""></h3><p>PythonMatplotlibMatlabSeabornMatplotlibAPIPlotly3DMatlab/MathematicaMatlabR+ggplot2TikZLaTeX</p><h3 id=""></h3><p>GnuplotGephi</p><h3 id=""></h3><p> -  -  -  - -   - -   - - </p><p> 95% - -</p><p> -  - -</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag>SJTU</tag>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/05/17/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/05/17/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><h2 id=""></h2><p><span class="math inline">\(O(\log n)\)</span>n <span class="math display">\[ \lvert x_c-r \rvert&lt; \frac{b-a}{2^{n+1}}\]</span> <spanclass="math inline">\(0.5 \times10^{-p}\)</span>p</p><p><strong>1.5</strong> <spanclass="math inline">\(e_i\)</span>i<span class="math display">\[ \lim_{i \to\infty}\frac{e_{i+1}}{e_i}=S&lt;1\]</span><strong></strong><spanclass="math inline">\(S\)</span></p><p><strong>1.6</strong> <spanclass="math inline">\(g\)</span><spanclass="math inline">\(g(r)=r\)</span>,<spanclass="math inline">\(S=\lvert g&#39;(r)\rvert&lt;1\)</span><spanclass="math inline">\(r\)</span><spanclass="math inline">\(S\)</span><spanclass="math inline">\(r\)</span></p><p>1.6 <span class="math display">\[ e_{i+1}\approx Se_i\]</span> </p><p><strong>1.7</strong> <spanclass="math inline">\(r\)</span><spanclass="math inline">\(r\)</span><strong></strong><spanclass="math inline">\(r\)</span></p><p>1.6<span class="math inline">\(\lvert g&#39;(r)\rvert&lt;1\)</span></p><blockquote><p> <span class="math inline">\(0.5 \times10^{-p}\)</span> <spanclass="math inline">\(p\)</span>.</p></blockquote><h2 id=""></h2><p> <span class="math inline">\(x_i\)</span> <span class="math inline">\(x_{i+1}\)</span><span class="math display">\[    x_{i+1}=x_i- \frac{f(x_i)}{f&#39;(x_i)}\]</span></p><h3 id=""></h3><p><strong></strong>  <span class="math inline">\(e_i\)</span> <span class="math inline">\(i\)</span>  <spanclass="math display">\[    M= \lim_{i \to \infty} \frac{e_{i+1}}{e_i^{2}} &lt; \infty\]</span> <strong></strong></p><p><strong></strong>  <span class="math inline">\(f\)</span> <span class="math inline">\(f(r)=0\)</span> <span class="math inline">\(f&#39;(r)\neq 0\)</span>Newton <span class="math display">\[    \lim_{i \to \infty}=\frac{e_{i+1}}{e_i^{2}}=M=\frac{f&#39;&#39;(r)}{2f&#39;(r)}\]</span></p><h3 id="newton">Newton</h3><p>Newton <span class="math display">\[    f(x)=x^{m}\]</span></p><p><strong></strong> <span class="math inline">\(f \inC^{m+1}[a,b]\)</span><span class="math inline">\(f\)</span>  <spanclass="math inline">\(r\)</span>  <spanclass="math inline">\(m\)</span> Newton<span class="math inline">\(r\)</span> <spanclass="math inline">\(S=(m-1)/m\)</span></p><p><strong></strong>  <spanclass="math inline">\(m&gt;1\)</span> Newton <spanclass="math display">\[    x_{i+1}=x_i-\frac{mf(x_i)}{f&#39;(x_i)}         \]</span>  <span class="math inline">\(r\)</span>.  <spanclass="math inline">\(f(x)=(x-r)^{m}g(x)\)</span> <spanclass="math display">\[    \lim_{i \to \infty}\frac{e_{i+1}}{e_{i}^{2}}=\frac{g&#39;(r)}{mg(r)}\]</span></p><p> <span class="math inline">\(\displaystyle\mu(x)=\frac{f(x)}{f&#39;(x)}\)</span> <span class="math inline">\(f(x)\)</span> .</p><h3 id=""></h3><p>Newton.  <spanclass="math inline">\(x_0\)</span> <spanclass="math display">\[    x_{i+1}=x_i-\frac{f(x_i)(x_i-x_{i-1})}{f(x_i)-f(x_{i-1})}\]</span></p><p><strong></strong> <span class="math display">\[    e_{i+1} \thickapprox \lvert \frac{f&#39;&#39;(r)}{2f&#39;(r)} \rvert^{\alpha-1}e_i^{\alpha}\]</span>  <spanclass="math inline">\(\alpha=(1+\sqrt{5})/2\)</span></p><p> <span class="math inline">\(1&lt;\alpha&lt;2\)</span>.</p><p> <spanclass="math inline">\(f&#39;(r),f&#39;&#39;(r)\neq 0\)</span> <spanclass="math display">\[    e_{i+1}= \lvert f&#39;&#39;(r)/2f&#39;(r) \rvert e_i e_{i-1}\]</span> .  <spanclass="math inline">\(\alpha\)</span>.</p><h1 id=""></h1><h3 id=""></h3><p> <span class="math inline">\(n\)</span> <span class="math inline">\(n-1\)</span></p><p>Newton. <span class="math inline">\(f[x_1\cdotsx_n]\)</span>  <spanclass="math inline">\(x^{n-1}\)</span>  <spanclass="math display">\[    \]</span>  <span class="math display">\[    f[x_1\cdots x_k]= \sum_{j=1}^{k} \frac{f(x_j)}{(x_j-1)\cdots(x_j-x_{j-1})(x_j-x_{j+1})\cdots (x_j-x_k)}\]</span>  <span class="math display">\[    f[x_1\cdots x_n]=f[\sigma(x_1)\cdots \sigma(x_n)]\]</span>  <span class="math inline">\(f(x)\)</span> <span class="math inline">\(n-1\)</span> <span class="math inline">\(c\)</span> <span class="math display">\[    f[x_1\cdots x_n]=\frac{f^{(n-1)}(c)}{(n-1)!}\]</span></p><p>Newton <span class="math display">\[    P(x)=f[x_1]+f[x_1 x_2](x-x_1)+\cdots +f[x_1\cdots x_n](x-x_1)\cdots(x-x_{n-1})\]</span></p><h3 id=""></h3><p><span class="math display">\[    f(x)-P_{n-1}(x)=f[x_1\cdots x_nx](x-x_1)\cdots (x-x_n)=\frac{f^{(n)}(c)}{n!}(x-x_1)\cdots (x-x_n)\]</span></p><h3 id=""></h3><p>.</p><h3 id="hermite">Hermite</h3><p><span class="math inline">\(f(x) \in C^{1}[a,b]\)</span><spanclass="math inline">\(a\leqslant x_0&lt;x_1&lt;\cdots &lt;x_n\leqslantb\)</span>.  <span class="math display">\[    H_{2n+1}(x)= \sum_{i=0}^{n} f(x_i)A_i(x)+ \sum_{i=0}^{n}f&#39;(x_i)B_i(x)\]</span> <span class="math display">\[    A_i(x)=[1-2(x-x_i)l_i&#39;(x_i)]l_i^{2}(x)\]</span> <span class="math display">\[    B_i(x)=(x-x_i)l_i^{2}(x)\]</span>  <span class="math inline">\(l_i\)</span>Lagrange.</p><h3 id="chebyshev">Chebyshev</h3><p></p><p> <span class="math display">\[    x_i= \cos \frac{(2i-1)\pi}{2n}, i=1,\cdots,n\]</span>  <span class="math display">\[    \max_{-1\leqslant x\leqslant 1} \lvert (x-x_1)\cdots (x-x_n) \rvert= \frac{1}{2^{n-1}} T_n(x)\leqslant \frac{1}{2^{n-1}}\]</span> </p><p><strong>Chebyshev</strong>  <spanclass="math inline">\(T_n(x)=\cos (n \arccos x)\)</span><span class="math display">\[    T_0(x)=1, \quad T_1(x)=x\]</span> <span class="math display">\[    T_{n+1}(x)=2x T_n(x)- T_{n-1}(x)\]</span> - <span class="math inline">\(T_n(x)\)</span>  <spanclass="math inline">\(n\)</span> . -  <spanclass="math inline">\(2^{n-1}\)</span>. <spanclass="math inline">\(T_n(1)=1, T_n(-1)=(-1)^{n}\)</span>. - <span class="math inline">\(1\)</span>. - <spanclass="math inline">\(T_n(x)=\cos (n \arccos x)\)</span> <span class="math display">\[    x_i= \cos \frac{i\pi}{n}, i=0,\cdots ,n        \]</span></p><p> <span class="math inline">\(\displaystyley=\frac{b-a}{2}x+\frac{b+a}{2}, x\in [-1,1]\)</span> <spanclass="math display">\[    \lvert (y-y_1)\cdots (y-y_n) \rvert =(\frac{b-a}{2})^{n}\lvert(x-x_1)\cdots (x-x_n) \rvert \leqslant (\frac{b-a}{2})^{n}\frac{1}{2^{n-1}}\]</span></p><h3 id=""></h3><p><span class="math inline">\(n\)</span>  <spanclass="math inline">\((x_i,y_i)\)</span>  <spanclass="math inline">\(3n-5\)</span> <spanclass="math inline">\(3n-3\)</span> . .p152-153.</p><ul><li><spanclass="math inline">\(S_1&#39;&#39;(x_1)=S_{n-1}&#39;&#39;(x_n)=0\)</span>.</li><li><spanclass="math inline">\(S_1&#39;&#39;(x_1)=2c_1=v_1,S_{n-1}&#39;&#39;(x_n)=2c_n=v_n\)</span>.</li><li><span class="math inline">\(S_1&#39;(x_1)=v_1,S_{n-1}&#39;(x_n)=v_n\)</span>.</li><li><spanclass="math inline">\(d_1=0=d_{n-1}\)</span>.</li><li><spanclass="math inline">\(S_1=S_2,S_{n-2}=S_{n-1}\)</span></li></ul><p><strong></strong> <span class="math inline">\(f(x) \inC^{4}[a,b]\)</span> <spanclass="math inline">\(S(x)\)</span> <spanclass="math inline">\(\delta = \max_{i} \delta_i\)</span><span class="math display">\[    \max_{a\leqslant x\leqslant b} \lvert f(x)- S(x) \rvert \leqslant\frac{5}{384} \max_{a\leqslant x\leqslant b} \lvert f^{(4)}(x) \rvert\delta^{4}\]</span></p><h3 id="bezier">Bezier</h3><h1 id=""></h1><h3 id=""></h3><p> <span class="math inline">\(f\)</span> <spanclass="math inline">\(P_n(x)\)</span>  <spanclass="math display">\[    E=\frac{1}{m} \sum_{i=1}^{m} (y_i-P_{n-1}(x_i))^{2}\]</span>   <span class="math display">\[    A=    \begin{bmatrix}    1 &amp; x_1 &amp; \cdots &amp; x_1^{n-1} \\    \vdots &amp; &amp; &amp; \vdots \\    1 &amp; x_{m} &amp; \cdots &amp; x_m^{n-1} \\    \end{bmatrix}\]</span> <span class="math inline">\(a=(a_0,a_1,\cdots,a_{n-1})^{\mathsf{T}},y=(y_1,\cdots ,y_m)^{\mathsf{T}}\)</span> <spanclass="math display">\[    A^{\mathsf{T}}A a= A^{\mathsf{T}}y\]</span></p><p> <span class="math inline">\((t_1,y_1),\cdots,(t_n,y_n)\)</span> <spanclass="math inline">\(y=a_0+a_1t\)</span> <spanclass="math display">\[    \begin{bmatrix}    \sum_{i=1}^{m} 1 &amp; \sum_{i=1}^{m} t_i \\    \sum_{i=1}^{m} t_i &amp; \sum_{i=1}^{m} t_i^{2} \\    \end{bmatrix}    \begin{bmatrix}    a_0 \\    a_1    \end{bmatrix}    =    \begin{bmatrix}    \sum_{i=1}^{m} y_i \\    \sum_{i=1}^{m} t_i y_i \\    \end{bmatrix}\]</span></p><p> <span class="math inline">\(A^{\mathsf{T}}A\)</span>QR.  <spanclass="math inline">\(A=QR\)</span> <span class="math display">\[    \left\| QRx-b \right\|_{2}= \left\| Rx-Q^{\mathsf{T}}b \right\|_{2}\]</span>  <span class="math display">\[    R=\begin{bmatrix}    R_1 \\    0 \\    \end{bmatrix}\]</span>  <span class="math display">\[    Q^{\mathsf{T}}b=\begin{bmatrix}    b_1 \\    b_2 \\    \end{bmatrix}\]</span></p><p> <span class="math inline">\(R_1\)</span> <span class="math display">\[    R_1 x=b_1\]</span></p><p><strong></strong> <span class="math display">\[    y=c_1 \mathrm{e}^{c_2t} \Rightarrow \ln y= \ln c_1+ c_2t\]</span></p><h3 id=""></h3><p>Euclid <spanclass="math inline">\(f\)</span> <spanclass="math inline">\(P_n(x)\)</span>  <spanclass="math display">\[    E= \int_{a}^{b} [f(x)-P_n(x)]^{2} \mathrm{d}x\]</span>  <span class="math inline">\(P_n(x)\)</span><strong></strong> <spanclass="math inline">\(P_n(x)=a_nx^{n}+\cdots+a_1x+a_0\)</span> <span class="math display">\[    \frac{\partial E}{\partial a_j}=0   \]</span>  <span class="math display">\[    \sum_{k=0}^{n} a_k \int_{a}^{b} x^{k+j} \mathrm{d}x= \int_{a}^{b}f(x) x^{j} \mathrm{d}x\]</span>  <spanclass="math inline">\(\varphi_k=x^{k}\)</span>Euclid <spanclass="math display">\[    \begin{bmatrix}    (\varphi_0,\varphi_0) &amp; \cdots &amp; (\varphi_0,\varphi_n) \\    \vdots &amp; &amp; \vdots \\    (\varphi_n,\varphi_0) &amp; \cdots &amp; (\varphi_n,\varphi_n) \\    \end{bmatrix}    \begin{bmatrix}    a_0 \\    \vdots \\    a_n \\    \end{bmatrix}    =    \begin{bmatrix}    (f,\varphi_0) \\    \vdots \\    (f,\varphi_n) \\    \end{bmatrix}\]</span>  <span class="math inline">\(\varphi_k\)</span> <span class="math inline">\(f(x)=\sin \pi x\)</span>Hilbert.<strong></strong>.</p><h3 id=""></h3><p><strong></strong> <span class="math inline">\(w(x)\)</span> <span class="math inline">\(\forall x \in I=[a,b],(w(x)\geqslant0)\)</span> <span class="math inline">\(I\)</span>.  <span class="math display">\[    (f,g)_{w}= \int_{a}^{b} w(x)f(x)g(x) \mathrm{d}x\]</span>  <span class="math inline">\(\{\varphi_0,\cdots,\varphi_n\}\)</span>  <span class="math display">\[    \int_{a}^{b} w(x)\varphi_j(x)\varphi_k(x) \mathrm{d}x=    \begin{cases}        0,\quad j\neq k \\        \alpha_k&gt;0,\quad j=k    \end{cases}\]</span>  <spanclass="math inline">\(\alpha_k=1\)</span>.</p><h3 id=""></h3><p> <span class="math inline">\(f\)</span><spanclass="math inline">\(w(x)\)</span>  <spanclass="math inline">\(P_n(x)=\sum_{k=0}^{n} a_k\varphi_k(x)\)</span> <span class="math display">\[    E(a_0,\cdots ,a_n)=\int_{a}^{b} w(x)[f(x)-P_n(x)]^{2} \mathrm{d}x\]</span>  <span class="math inline">\(P_n(x)\)</span><span class="math inline">\(w(x)\)</span><strong></strong>.</p><p> <span class="math inline">\(\{\varphi_0,\cdots,\varphi_n\}\)</span>  <span class="math display">\[    a_k=\frac{(\varphi_k,f)_{w}}{(\varphi_k,\varphi_k)_{w}}\]</span></p><h3 id="gram-schmidt-">Gram-Schmidt </h3><p> <span class="math inline">\(\{1,x,\cdots ,x^{n}\}\rightarrow\{\varphi_0,\cdots ,\varphi _n\}\)</span> .</p><p> <span class="math inline">\(w(x)\)</span> <spanclass="math inline">\([a,b]\)</span>.  <spanclass="math inline">\(\varphi_0(x)=1\)</span><spanclass="math inline">\(\varphi_1(x)=x-B_1\)</span> <spanclass="math display">\[    B_1=\frac{(x\varphi_0,\varphi_0)_{w}}{(\varphi_0,\varphi_0)_{w}}\]</span>  <span class="math inline">\(w(x)\equiv1\)</span> <span class="math display">\[    B_1=\frac{\int_{-1}^{1} x \mathrm{d}x}{\int_{-1}^{1} 1\mathrm{d}x}=0\]</span></p><p> <span class="math inline">\(k\geqslant 2\)</span> <spanclass="math display">\[    \varphi_k(x)=(x-B_k)\varphi_{k-1}-C_k\varphi_{k-2}\]</span>  <span class="math display">\[    B_k=\frac{(x\varphi_{k-1},\varphi_{k-1})_{w}}{(\varphi_{k-1},\varphi_{k-1})_{w}},\qquadC_k=\frac{(x\varphi_{k-2},\varphi_{k-1})_{w}}{(\varphi_{k-2},\varphi_{k-2})_{w}}\]</span>  <span class="math inline">\(\{\varphi_0,\cdots,\varphi_n\}\)</span> .LegendreChebyshev.Legendre<span class="math inline">\(P_n(1)=1\)</span></p><p> <span class="math display">\[    (x\varphi_{k-2},\varphi_{k-1})_{w}=(\varphi_{k-1},\varphi_{k-1})_{w}\]</span></p><h2 id="qr">QR</h2><p><strong>QR</strong>  <span class="math inline">\(A\in\mathbb{F}^{m \times n}, m &gt; n\)</span>, <spanclass="math inline">\(A=QR\)</span>.  <span class="math inline">\(Q\in O(m)\)</span> <span class="math display">\[    R=    \begin{bmatrix}        \tilde{R} \\        0 \\    \end{bmatrix}\]</span></p><p> <span class="math inline">\(O(n^{3})\)</span></p><h3 id="gram-schimidt-">Gram-Schimidt </h3><p>p194</p><p> <span class="math inline">\(m^{3}\)</span> <span class="math inline">\(m^{3}\)</span> </p><p> <span class="math inline">\(r_{ij}=q_{i}^{\mathsf{T}}A_j\)</span>  <spanclass="math inline">\(m\)</span> <spanclass="math inline">\(m-1\)</span> <spanclass="math inline">\(y-y-r_{ij}q_i\)</span> <spanclass="math inline">\(m\)</span> <spanclass="math inline">\(m\)</span> <spanclass="math inline">\(r_{jj}\)</span> <spanclass="math inline">\(q_j\)</span>. <span class="math display">\[    \sum_{j=1}^{n} (j-1)\cdot 2m=m^{3}-m\]</span>  <span class="math inline">\(m^{3}\)</span><span class="math inline">\(m^{3}\)</span>.</p><h3 id="householder-">Householder </h3><p> <span class="math inline">\(w \in \mathbb{R}^{n}\)</span><span class="math inline">\(w^{\mathsf{T}}w=1\)</span> <spanclass="math display">\[    H=I-2ww^{\mathsf{T}}\]</span>  Householder <strong></strong></p><p> <span class="math inline">\(\left\| x \right\|_{2}= \left\| y\right\|_{2}\)</span> <span class="math inline">\(\displaystylew=\frac{x-y}{\left\| x-y \right\|_{2}}\)</span> <spanclass="math inline">\(H=I-2ww^{\mathsf{T}}\)</span>  <spanclass="math inline">\(Hx=y\)</span></p><p>.</p><p><strong>HouseholderQR</strong></p><p>  <spanclass="math inline">\(m \times n\)</span>  <spanclass="math inline">\(A\)</span> QR - R<span class="math display">\[    n^{2}(m-\frac{n}{3})\]</span> .</p><ul><li>Q <span class="math display">\[  m^{2}n-mn^{2}+\frac{n^{3}}{3}\]</span> .</li></ul><hr /><p>QR decompositon - Wikipediahttps://en.wikipedia.org/wiki/QR_decomposition</p><hr /><h1 id=""></h1><h2 id=""></h2><h3 id=""></h3><p><strong></strong> <span class="math display">\[    f&#39;(x) =\frac{f(x+h)-f(x)}{h} - \frac{h}{2} f&#39;&#39;(c)\]</span>  <span class="math inline">\(c\)</span>  <spanclass="math inline">\(x\)</span>  <spanclass="math inline">\(x+h\)</span> </p><p> <span class="math inline">\(O(h^{n})\)</span><span class="math inline">\(n\)</span> </p><p><strong></strong> <spanclass="math display">\[    f&#39;(x)= \frac{f(x+h)-f(x-h)}{2h}-\frac{h^{2}}{6}f&#39;&#39;&#39;(c)\]</span>  <span class="math inline">\(x-h&lt;c&lt;x+h\)</span></p><p><strong></strong> <span class="math display">\[    f&#39;(x)\thickapprox  \frac{f(x-2h)-8f(x-h)+8f(x+h)-f(x+2h)}{12h}\]</span>  <span class="math inline">\(O(h^{4})\)</span></p><p><strong></strong> <spanclass="math display">\[    f&#39;&#39;(x)= \frac{f(x+h)-2f(x)+f(x-h)}{h^{2}}-\frac{h^{2}}{12}f^{(4)}(c)\]</span>  <span class="math inline">\(x-h&lt;c&lt;x+h\)</span></p><h3 id=""></h3><p> <spanclass="math display">\[    \frac{h^{2}}{6} f&#39;&#39;&#39;(c)+ \frac{\varepsilon_{mach}}{h}\]</span></p><h3 id=""></h3><p> <span class="math display">\[    Q=F_n(h) + Kh^{n} +O(h^{n+1})\]</span>  <span class="math inline">\(\frac{h}{2}\)</span> <span class="math inline">\(h\)</span> <spanclass="math display">\[    Q\thickapprox \frac{2^{n}F(h/2)-F(h)}{2^{n}-1}\]</span> <span class="math inline">\(F_{n+1}(h)\)</span>  <spanclass="math inline">\(n+1\)</span>  <spanclass="math inline">\(Q\)</span> </p><p>. <span class="math inline">\(h\)</span> .</p><h2 id=""></h2><p>.</p><h3 id=""></h3><p><span class="math display">\[    \int_{x_0}^{x_1} P(x) \mathrm{d}x= \frac{h}{2}(f(x_0)+f(x_1))\]</span> <span class="math display">\[    \int_{x_0}^{x_1} E(x) \mathrm{d}x= -\frac{h^{3}}{12}f&#39;&#39;(c)\]</span></p><h3 id="simpson">Simpson</h3><p><span class="math display">\[    \int_{x_0}^{x_2} f(x)\mathrm{d}x=\frac{h}{3}(y_0+4y_1+y_2)-\frac{h^{5}}{90}f^{(4)}(c)\]</span>  <spanclass="math inline">\(h=x_2-x_1=x_1-x_0\)</span><spanclass="math inline">\(c\)</span> <spanclass="math inline">\(x_0\)</span> <spanclass="math inline">\(x_2\)</span>.</p><p><strong></strong></p><p> <span class="math display">\[    E_s= \int_{x_0}^{x_2}\frac{(x-x_0)(x-x_1)(x-x_2)}{3!}f&#39;&#39;&#39;(c(x)) \mathrm{d}x\]</span>  <spanclass="math inline">\(f&#39;&#39;&#39;(c(x))\)</span> </p><p><span class="math display">\[    f&#39;&#39;&#39;(c(x))=f&#39;&#39;&#39;(c(x_1))+(x-x_1)f^{(4)}(c(\bar{x}))c&#39;(\bar{x})\]</span>  <span class="math inline">\(E_s\)</span>  <spanclass="math inline">\(f&#39;&#39;&#39;(c(x_1))\)</span> <spanclass="math inline">\(f^{(4)}(x)\)</span> .</p><p>Newton <spanclass="math inline">\(x_1\)</span>  <span class="math display">\[    P_3(x)=P_2(x)+D(x-x_0)(x-x_1)(x-x_2)\]</span>  <span class="math display">\[    \int_{x_0}^{x_2} P_2(x)  \mathrm{d}x=\int_{x_0}^{x_2} P_3(x)\mathrm{d}x\]</span>  <span class="math display">\[    E_s=\int_{x_0}^{x_2} \frac{(x-x_0)(x-x_1)^{2}(x-x_2)}{4!}f^{(4)}(c)\mathrm{d}x\]</span></p><p> <span class="math display">\[    E_s=-\frac{h^{5}}{90} f^{(4)}(c)\]</span></p><p>Simpson3.</p><h3id="newton-cotessimpson38">3Newton-CotesSimpson3/8</h3><p><span class="math display">\[    \int_{x_0}^{x_3} f(x) \mathrm{d}x \thickapprox\frac{3h}{8}(f_0+3f_1+3f_2+f_3)\]</span>  <spanclass="math inline">\(O(h^{5})\)</span>.</p><h3 id=""></h3><p> <span class="math inline">\(\displaystyle x_j=x_0+jh, x_0=a,x_n=b, h=(b-a)/n\)</span> <spanclass="math inline">\(f&#39;&#39;\)</span>  <spanclass="math inline">\([a,b]\)</span>  <spanclass="math display">\[    \int_{a}^{b} f(x) \mathrm{d}x=\frac{h}{2}(f_0+2f_1+\cdots+2f_{n-1}+f_n)-\frac{(b-a)h^{2}}{12}f&#39;&#39;(c)\]</span></p><p>21.</p><h3 id="simpson">Simpson</h3><p> <span class="math inline">\(f^{(4)}\)</span>  <spanclass="math inline">\([a,b]\)</span>  <spanclass="math display">\[    \int_{a}^{b} f(x) \mathrm{d}x=\frac{h}{3}(y_0+y_{2m}+4\sum_{i=1}^{m}y_{2i-1}+2\sum_{i=1}^{m-1} y_{2i})-\frac{(b-a)h^{4}}{180}f^{(4)}(c)\]</span></p><p> <span class="math inline">\(m\)</span> Simpson<span class="math inline">\(2m\)</span> . <spanclass="math inline">\(h=(b-a)/2m\)</span></p><h3 id=""></h3><p><span class="math display">\[    \int_{a}^{b} f(x) \mathrm{d}x\thickapprox\frac{h}{2}(f_0+2f_1+\cdots +2f_{n-1}+f_n)+\varepsilon_R\]</span>  <span class="math display">\[    \varepsilon_R \thickapprox (b-a)\varepsilon_h\]</span></p><h3 id=""></h3><p> <span class="math inline">\(f \in C^{2}[a,b]\)</span>  <spanclass="math display">\[    \int_{x_0}^{x_1} f(x)\mathrm{d}x=hf(w)+\frac{h^{3}}{24}f&#39;&#39;(c)\]</span>  <span class="math inline">\(h=x_1-x_0\)</span><spanclass="math inline">\(w\)</span>  <spanclass="math inline">\(x_0+h/2\)</span><spanclass="math inline">\(x_0\leqslant c\leqslant x_1\)</span></p><p>Taylor..</p><h3 id=""></h3><p><span class="math display">\[    \int_{a}^{b} f(x) \mathrm{d}x= h\sum_{i=1}^{m}f(w_i)+\frac{(b-a)h^{2}}{24}f&#39;&#39;(c)\]</span>  <span class="math inline">\(h=(b-a)/m\)</span><spanclass="math inline">\(w\)</span>  <spanclass="math inline">\([a,b]\)</span>  <spanclass="math inline">\(m\)</span> </p><h3 id="newton-cotes">Newton-Cotes</h3><p><span class="math display">\[    \int_{x_0}^{x_4} f(x)\mathrm{d}x=\frac{4h}{3}[2f(x_1)-f(x_2)+2f(x_3)]+\frac{14h^{5}}{45}f^{(4)}(c)\]</span></p><h2 id="romberg">Romberg</h2><p> <span class="math display">\[    \begin{aligned}    &amp;R_{11} \\    &amp;R_{21} \quad R_{22} \\    &amp;R_{31} \quad R_{32} \quad R_{33} \\    &amp;\vdots    \end{aligned}\]</span>  <span class="math display">\[    h_j=\frac{1}{2^{j-1}}(b-a)\]</span></p><p> <span class="math display">\[    R_{j1}=\frac{1}{2}R_{j-1,1}+h_j\sum_{i=1}^{2^{j-2}} f(a+(2i-1)h_j)\]</span></p><p><span class="math display">\[    R_{jk}=\frac{4^{k-1}R_{j,k-1}-R_{j-1,k-1}}{4^{k-1}-1}\]</span></p><p> <span class="math inline">\(R_{jj}\)</span> <spanclass="math inline">\(2j\)</span>.</p><p>RombergSimpson.RombergSimpsonSimpson.</p><p>Romberg <spanclass="math inline">\(R_{jj}\)</span>.</p><h2 id=""></h2><p> -  - </p><h2 id=""></h2><p> <spanclass="math inline">\(2n+1\)</span> <spanclass="math inline">\(n+1\)</span>Newton-Cotes.</p><p>Gauss <span class="math display">\[    x_1=-\sqrt{\frac{3}{5}} \quad x_2=0 \quad x_3=\sqrt{\frac{3}{5}} \\    c_1=\frac{5}{9} \quad c_2=\frac{8}{9} \quad c_3=\frac{5}{9}\]</span></p><p></p><p> <spanclass="math inline">\(\{p_0,p_1,p_2,\cdots,p_n\}\)</span> <spanclass="math inline">\([a,b]\)</span>deg<span class="math inline">\(p_i=i\)</span>  <spanclass="math inline">\(p_i\)</span> <spanclass="math inline">\((a,b)\)</span> <spanclass="math inline">\(i\)</span></p><p></p><hr /><p> https://zhuanlan.zhihu.com/p/270620896</p><table style="width:6%;"><tbody><tr class="odd"><td>### </td></tr><tr class="even"><td>Legendre <spanclass="math inline">\([-1,1]\)</span> <spanclass="math inline">\(w(x)=1\)</span>. <span class="math display">\[(n+1)P_{n+1}(x)=(2n+1)x P_n(x)-nP_{n-1}(x)\]</span>  <span class="math display">\[p_i(x)=\frac{1}{2^{i}i!}\frac{\mathrm{d}^{i}}{\mathrm{d}x^{i}}[(x^{2}-1)^{i}]\]</span>  <span class="math inline">\(P_0(x)=1\)</span><spanclass="math inline">\(P_1(x)=x\)</span></td></tr><tr class="odd"><td>Legendre <spanclass="math inline">\(P_n(1)=1\)</span>.</td></tr><tr class="even"><td> <span class="math display">\[(P_n,P_m)=\int_{-1}^{1} P_n(x)P_m(x) \mathrm{d}x=\begin{cases}0, \quad m \neq n \\\frac{2}{2n+1}, m = n\end{cases}\]</span></td></tr><tr class="odd"><td><span class="math inline">\(P_{2k}(x)\)</span> <span class="math inline">\(P_{2k+1}\)</span></td></tr><tr class="even"><td> <span class="math inline">\(\tilde{P}_n(x)=\frac{2^{n}(n!)^{2}}{(2n)!}P_n(x)\)</span> <spanclass="math inline">\(\tilde{P}_n(x)\)</span>Legendre.</td></tr><tr class="odd"><td></td></tr></tbody></table><p> https://zhuanlan.zhihu.com/p/352724194</p><hr /><p>Laguerre $w(x)=^{-x} $</p><p>Legendre.</p><h3 id="-1"></h3><p> <span class="math inline">\(n\)</span>.</p><p><span class="math display">\[    \int_{-1}^{1} x^{k} \mathrm{d}x=\sum_{i=1}^{n} c_i x_i^{k}\]</span>  <spanclass="math inline">\(x_i,c_i,i=1,2,\cdots,n\)</span>  <spanclass="math inline">\(2n\)</span>  <spanclass="math inline">\(k=0,1,\cdots ,2n-1\)</span> Gauss <span class="math inline">\(2n-1\)</span>. </p><p><span class="math display">\[    \int_{-1}^{1} 1 \cdot f(x) \mathrm{d}x \thickapprox \sum_{i=1}^{n}c_if(x_i)\]</span>  <span class="math display">\[    c_i=\int_{-1}^{1} 1 \cdot L_i(x) \mathrm{d}x, \quad i=1,\cdots ,n\]</span> <span class="math inline">\(L_i\)</span>Lagrange.<span class="math inline">\(x_i\)</span>Legendre.1.</p><p><strong></strong>Gauss <spanclass="math inline">\([-1,1]\)</span> <spanclass="math inline">\(n\)</span>Legendre <spanclass="math inline">\(2n-1\)</span>.</p><blockquote><p>Gauss<spanclass="math inline">\(2n-1\)</span>.Lengdre.</p></blockquote><p> <spanclass="math inline">\([a,b]\)</span> <spanclass="math inline">\(t=(2x-a-b)/(b-a)\)</span> <spanclass="math display">\[    \int_{a}^{b} f(x) \mathrm{d}x=\int_{-1}^{1}f(\frac{(b-a)t+b+a}{2})\frac{b-a}{2} \mathrm{d}t        \]</span></p><p><strong>Gauss</strong> <span class="math display">\[    R=\int_{-1}^{1} f(x) \mathrm{d}x-\sum_{i=1}^{n}c_if(x_i)=\frac{f^{(2n)}(\xi)}{(2n)!}\int_{-1}^{1} q(x) \mathrm{d}x\]</span>  <span class="math inline">\(q(x)=[(x-x_1)\cdots(x-x_n)]^{2}\)</span></p><p> <spanclass="math inline">\(x_1,x_2,\cdots,x_n\)</span>Hermite.</p><h3 id="gauss-chebyshev-">Gauss-Chebyshev </h3><p> <span class="math inline">\(\displaystylew(x)=\frac{1}{\sqrt{1-x^{2}}}\)</span>Hilbert.</p><p> <span class="math display">\[    \int_{-1}^{1} f(x) \mathrm{d}x= \int_{-1}^{1} w(x)g(x) \mathrm{d}x\thickapprox  \sum_{i=1}^{n} c_i g(x_i)\]</span>  <span class="math inline">\(g(x)=\sqrt{1-x^{2}}f(x)\)</span>. Chebyshev<span class="math inline">\(p_i(x)\)</span> <spanclass="math display">\[    \int_{-1}^{1} \frac{1}{\sqrt{1-x^{2}}}p_m(x)p_n(x) \mathrm{d}x=0,m\neq n\]</span>  <span class="math inline">\(\displaystyle x_i=\cos\frac{2i-1}{2n}\pi,i=1,2,\cdots ,n\)</span> <spanclass="math inline">\(\displaystyle c_i=\int_{-1}^{1}\frac{L_i(x)}{\sqrt{1-x^{2}}} \mathrm{d}x= \frac{\pi}{n}\)</span></p><h3 id=""></h3><p> <span class="math display">\[    \int_{a}^{b} \frac{g(x)}{(x-a)^{p}} \mathrm{d}x\]</span>  <span class="math inline">\(0&lt;p&lt;1\)</span></p><p> <span class="math inline">\(x=a\)</span>. -Simpson - Newton-Cotes</p><p> <span class="math display">\[    \int_{a}^{b} \frac{g(x)-g(a)}{(x-a)^{p}} \mathrm{d}x+ \int_{a}^{b}\frac{g(a)}{(x-a)^{p}} \mathrm{d}x\]</span> . .Simpson  <spanclass="math inline">\(g(x)\)</span> Taylor 4 <spanclass="math inline">\(\displaystyleG(x)=\frac{g(x)-Q_4(x)}{(x-a)^{p}}\in C^{4}[a,b]\)</span>.  <spanclass="math inline">\(Q_4(x)\)</span>  <spanclass="math inline">\(g(x)\)</span>  <spanclass="math inline">\(x=a\)</span> Taylor5.  <spanclass="math display">\[    \int_{a}^{b} \frac{Q_4(x)}{(x-a)^{p}} \mathrm{d}x= \sum_{k=0}^{4}\frac{g^{(k)}(a)}{k!}\frac{(b-a)^{k+1-p}}{k+1-p}\]</span>  <span class="math display">\[    G(x)= \frac{g(x)-Q_4(x)}{(x-a)^{p}}\]</span>  <span class="math display">\[    E=-\frac{h^{4}}{180}(b-a) G^{(4)}(\xi)\]</span></p><h3 id=""></h3><p>.</p><h1 id=""></h1><p><strong>Gerschgorin </strong> </p><h2 id=""></h2><p></p><p><strong>Rayleigh</strong>  <spanclass="math inline">\(x\)</span>  <spanclass="math display">\[    \lambda= \frac{x ^{\mathsf{T}}Ax}{x ^{\mathsf{T}}x}\]</span></p><p><strong></strong> <span class="math inline">\(A\)</span> $_1 &gt; _2 _m $  <span class="math inline">\(m \timesm\)</span> . <span class="math inline">\(A\)</span>.  <spanclass="math inline">\(\lambda_1\)</span> $S=_2/_1 $.</p><p>. <span class="math inline">\(A\)</span> <span class="math inline">\(x_{k+1}= A ^{-1}x_k\)</span>  <span class="math inline">\(Ax_{k+1}=x_k\)</span>Gauss.</p><h3 id=""></h3><p>p469</p><h3 id="rayleighrqi">RayleighRQI</h3><p>RayleighRayleigh.</p><h3 id="qr">QR</h3><ul><li>Householder <span class="math inline">\(B=HAH\)</span>Hessenberg</li><li>QR.  <spanclass="math inline">\(A_j=Q_jR_j\)</span> <spanclass="math inline">\(A_{j+1}=R_{j}Q_{j}\)</span></li></ul><p>Hessenberg <span class="math inline">\(\displaystyle\frac{5}{3} n^{3}\)</span> .  <spanclass="math inline">\(n\)</span>  <spanclass="math inline">\(A\)</span> <spanclass="math inline">\(n-2\)</span> Householder <spanclass="math inline">\(A\)</span> Hessenberg.</p><h2 id="svd">SVD</h2><p><span class="math inline">\(A\)</span>  <spanclass="math inline">\(m \times n\)</span> <spanclass="math inline">\(\{u_1,\cdots ,u_m\}\)</span>  <spanclass="math inline">\(\{v_1,\cdots ,v_n\}\)</span> <span class="math inline">\(s_1\geqslants_2\geqslant \cdots \geqslant s_n\geqslant 0\)</span> <spanclass="math display">\[    Av_1=s_1u_1 \\    \vdots \\    A v_n= s_n u_n\]</span>  <span class="math inline">\(v_i\)</span>  <spanclass="math inline">\(A\)</span>  <spanclass="math inline">\(u_i\)</span>  <spanclass="math inline">\(A\)</span>  <spanclass="math inline">\(s_i\)</span>  <spanclass="math inline">\(A\)</span> .</p><p>SVD <span class="math inline">\(A= U ^{\mathsf{T}}SV\)</span> <span class="math inline">\(U\)</span> <span class="math inline">\(V\)</span> .</p><h3 id=""></h3><p><span class="math inline">\(A\)</span> <span class="math inline">\(p\)</span> <spanclass="math inline">\(p&lt;r\)</span>.</p><h3 id="svd">SVD</h3><p> <span class="math display">\[    B=\begin{bmatrix}    0 &amp; A ^{\mathsf{T}} \\    A &amp; 0 \\    \end{bmatrix}\]</span>  <span class="math inline">\(B\)</span>  <spanclass="math display">\[    \begin{bmatrix}    A ^{\mathsf{T}}w \\    Av \\    \end{bmatrix}    =    \begin{bmatrix}    0 &amp; A^{\mathsf{T}} \\    A &amp; 0 \\    \end{bmatrix}    \begin{bmatrix}    v \\    w \\    \end{bmatrix}    =\lambda    \begin{bmatrix}    v \\    w \\    \end{bmatrix}\]</span>  <span class="math inline">\(w\)</span>  <spanclass="math inline">\(A ^{\mathsf{T}}A\)</span> <spanclass="math inline">\(\lambda^{2}\)</span>.</p><h3 id="svd">SVD</h3><p><strong>Frobenius</strong><span class="math inline">\(\left\| A\right\|_{F}= (\sum_{i}^{} \sum_{j}^{} a_{ij})^{1/2}\)</span> <spanclass="math display">\[    \left\| A \right\|_{F}^{2}= s_1^{2}+\cdots +s_{r}^{2}\]</span> <span class="math display">\[    \left\| A \right\|_{2}=s_1\]</span></p><p> <span class="math inline">\(U^{\mathsf{T}}AV= diag\{s_1,\cdots,s_r\}\)</span> <span class="math inline">\(k&lt;r\)</span><span class="math inline">\(A_k=\sum_{i=1}^{k}s_iu_iv_i^{\mathsf{T}}\)</span> <span class="math display">\[    \min_{\operatorname{rank}(B)=k}\left\| A-B \right\|_{2}= \left\|A-A_k \right\|_{2}= s_{k+1}\]</span></p><h1 id=""></h1><h2id="lu">LU</h2><h3 id="gauss">Gauss</h3><p><span class="math inline">\(n\)</span> <spanclass="math inline">\(n\)</span> <spanclass="math inline">\(\displaystyle\frac{2}{3}n^{3}+\frac{1}{2}n^{2}-\frac{7}{6}n\)</span>.<span class="math inline">\(\displaystyle \frac{1}{3}n^{3}\)</span> <span class="math inline">\(\displaystyle\frac{1}{3}n^{3}\)</span> ..</p><p> <spanclass="math inline">\(O(n^{2})\)</span> <spanclass="math inline">\(n^{2}\)</span> .</p><blockquote><p>0Gauss.</p></blockquote><h3 id="lu">LU</h3><p><span class="math inline">\(A=LU\)</span> <spanclass="math display">\[    Ly=b \Rightarrow Ux=y\]</span></p><p> <span class="math inline">\(L\)</span>  <spanclass="math inline">\(U\)</span>. - <spanclass="math inline">\(l_{ii}=1\)</span>  Doolittle  - <spanclass="math inline">\(u_{ii}=1\)</span>  Crount  - <spanclass="math inline">\(l_{ii}=u_{ii}\)</span>  Choleski </p><p> LU  <spanclass="math inline">\(O(n^{3})\)</span> LU <span class="math inline">\(O(n)\)</span>.</p><p><strong>LU Gauss  <spanclass="math inline">\(b\)</span> </strong></p><h3 id=""></h3><ul><li></li><li> <span class="math display">\[  \left\| A \right\|_{\infty}= \max_{\left\| x \right\|_{\infty}=1}\left\| Ax \right\|_{\infty}\]</span>  <span class="math inline">\(\left\| A \right\|_{\infty}=\max_{1\leqslant i\leqslant n} \sum_{j=1}^{n} \lvert a_{ij}\rvert\)</span></li><li><span class="math inline">\(L_2\)</span>  <spanclass="math display">\[  \left\| A \right\|_{2}= \max_{\left\| x \right\|_{2}=1} \left\| Ax\right\|_{2}\]</span>  <span class="math inline">\(\left\| A \right\|_{2}=[\rho(A^{\mathsf{T}}A)]^{1/2}\)</span><spanclass="math inline">\(\rho(A)\leqslant \left\| A \right\|_{}\)</span></li></ul><h3 id=""></h3><p> <span class="math inline">\(x_{a}\)</span>  <spanclass="math inline">\(Ax=b\)</span><strong></strong> <spanclass="math inline">\(\left\| b-Ax_{a}\right\|_{\infty}\)</span> <spanclass="math inline">\(\left\| x-x_{a} \right\|_{\infty}\)</span>.</p><p><strong></strong> <span class="math display">\[    Q=\frac{}{}=\frac{\left\|x-x_{a} \right\|_{\infty}/ \left\| x \right\|_{\infty}}{\left\| r\right\|_{\infty}/ \left\| b \right\|_{\infty}}\]</span></p><ul><li><p> <span class="math inline">\(A\)</span>  cond(<spanclass="math inline">\(A\)</span>) .</p></li><li><p><span class="math inline">\(n\)</span>  <spanclass="math inline">\(A\)</span>  <spanclass="math display">\[  \text{cond}(A)= \left\| A \right\|_{} \left\| A^{-1} \right\|_{}    \]</span> </p></li></ul><hr /><p>- 0.2https://zhuanlan.zhihu.com/p/388053510</p><hr /><p><strong>Q:</strong>  <spanclass="math inline">\(A\)</span> <spanclass="math inline">\(\text{cond}(A)=\left\| A \right\|_{}\left\| A^{-1} \right\|_{}\)</span></p><p><strong>:</strong>  <span class="math display">\[    Q=\frac{\left\| x-x_a \right\|_{}/\left\| x \right\|_{}}{\left\| b-Ax_a \right\|_{}/\left\| b \right\|_{}}\leqslant\]</span></p><p><span class="math display">\[    \exists b,x,r \quad \text{s.t.} \quad \left\| A\right\|_{}=\frac{\left\| Ax \right\|_{}}{\left\| x\right\|_{}}=\frac{\left\| b \right\|_{}}{\left\| x \right\|_{}}, \quad\left\| A ^{-1} \right\|_{}=\frac{\left\| A ^{-1} r \right\|_{}}{\left\|r \right\|_{}}\]</span></p><h3 id="palu-">PA=LU </h3><p><strong></strong> p80</p><p><strong></strong>  <spanclass="math inline">\(p\)</span>  <span class="math display">\[    \lvert a_{p1} \rvert \geqslant \lvert a_{i1} \rvert ,1\leqslanti\leqslant n\]</span>  <span class="math inline">\(1\)</span>  <spanclass="math inline">\(p\)</span> ..</p><p> <span class="math display">\[    Lc=Pb \Rightarrow Ux=c\]</span></p><h3 id=""></h3><p><strong></strong><spanclass="math inline">\(Ax=b\)</span>  <spanclass="math inline">\(x= Tx+b\)</span></p><p> <span class="math inline">\(A=D+L+U\)</span> - Jacobi <span class="math inline">\(T\)</span>: <spanclass="math display">\[    x=D ^{-1} [b-(L+U)x]\]</span> - Gauss-Seidel <spanclass="math inline">\(T\)</span> <span class="math display">\[    x=(L+D) ^{-1}(b-Ux)\]</span></p><p><strong>SOR</strong> -  <spanclass="math inline">\(\omega\)</span> <spanclass="math display">\[    [\omega(D+L)+(1-\omega)D]x=\omega(-Ux+b)+(1-\omega)Dx\]</span>  <span class="math inline">\(0&lt;\omega&lt;1\)</span><span class="math inline">\(\omega&gt;1\)</span></p><p><strong></strong>  <span class="math inline">\(x=Bx+f\)</span> <span class="math inline">\(x^{(0)}\)</span><span class="math inline">\(f\)</span> <spanclass="math inline">\(x^{(k+1)}=Bx^{(k)}+f\)</span>  <spanclass="math inline">\(\Leftrightarrow\)</span> <spanclass="math inline">\(\rho(B)&lt;1\)</span>.</p><p><strong></strong> <spanclass="math inline">\(\varepsilon^{(k)}=B^{k}\varepsilon^{(0)}\)</span> <span class="math inline">\(B\)</span> <spanclass="math inline">\(n\)</span> <spanclass="math inline">\(u_1,\cdots ,u_n\)</span> <spanclass="math inline">\(\lambda_1,\cdots ,\lambda_n\)</span> <spanclass="math inline">\(\varepsilon^{(0)}=\sum_{i=1}^{n}a_iu_i\)</span>,</p><p><span class="math display">\[    \varepsilon^{(k)}=\sum_{i=1}^{n} a_i\lambda_i^{k}u_i\]</span>  <span class="math inline">\(\rho(B)&lt;1\)</span>.</p><p><strong></strong> <span class="math display">\[    R(B)=-\ln \rho(B)\]</span> </p><p><strong></strong> <spanclass="math inline">\(x^{(k+1)}=Bx^{(k)}+f\)</span><span class="math inline">\(\left\| B \right\|_{}=q&lt;1\)</span> - - <span class="math inline">\(\displaystyle \left\|x^{(k)}-x^{*} \right\|_{}\leqslant \frac{q}{1-q}\left\|x^{(k)}-x^{(k-1)} \right\|_{}\)</span> - <spanclass="math inline">\(\displaystyle \left\| x^{(k)}-x^{*}\right\|_{}\leqslant \frac{q^{k}}{1-q}\left\| x^{(1)}-x^{(0)}\right\|_{}\)</span></p><h3 id=""></h3><p>1.  <spanclass="math inline">\(N\)</span> <spanclass="math inline">\(N\)</span> <spanclass="math inline">\(O(N^{2})\)</span><span class="math inline">\(2N\)</span> <spanclass="math inline">\(3N\)</span> .</p><p><strong></strong> <spanclass="math display">\[    AU=F\]</span> <span class="math inline">\(U\)</span>  <spanclass="math inline">\((n-1)^{2}\)</span> <spanclass="math inline">\(A\)</span>  <spanclass="math inline">\(2(n-1)+1\)</span></p><h3 id="cholesky-">Cholesky </h3><p> <span class="math inline">\(A\)</span>  <spanclass="math inline">\(n \times n\)</span> <span class="math inline">\(n \times n\)</span>  <spanclass="math inline">\(R\)</span>  <span class="math inline">\(A= R^{\mathsf{T}} R\)</span></p><p><strong></strong>  <span class="math display">\[    A=    \begin{bmatrix}    a &amp; b^{\mathsf{T}} \\    b &amp; C \\    \end{bmatrix}\]</span>  <span class="math inline">\(\displaystyleu=\frac{b}{\sqrt{a}}\)</span> <span class="math inline">\(A_1=C-uu^{\mathsf{T}}\)</span>  <span class="math display">\[    S=    \begin{bmatrix}    \sqrt{a} &amp; u^{\mathsf{T}} \\    0 &amp; I           \end{bmatrix}\]</span>  <span class="math display">\[    S ^{\mathsf{T}}    \begin{bmatrix}    1 &amp; 0 \\    0 &amp; A_1 \\    \end{bmatrix}    S=A     \]</span> <span class="math inline">\(A_1\)</span> <span class="math inline">\(A_1=V ^{\mathsf{T}}V\)</span><span class="math display">\[    R=    \begin{bmatrix}    \sqrt{a} &amp; u ^{\mathsf{T}} \\    0 &amp; V \\    \end{bmatrix}\]</span>  <span class="math inline">\(R\)</span>  <spanclass="math inline">\(A_1\)</span> .</p><h3 id=""></h3><p><strong></strong></p><p><strong></strong> <span class="math inline">\(A\)</span>  <spanclass="math inline">\(n\)</span> <spanclass="math inline">\(x\)</span>  <spanclass="math inline">\(Ax=b\)</span> <spanclass="math inline">\(\Leftrightarrow\)</span> <spanclass="math inline">\(x\)</span>  <spanclass="math inline">\(f(x)=\frac{1}{2}x^{\mathsf{T}}Ax-b^{\mathsf{T}}x\)</span>.  <span class="math display">\[    Ax^{*}=b \Leftrightarrow f(x^{*})= \min_{x\in \mathbb{R}^{n}}f(x)\]</span></p><p><strong></strong>  <span class="math inline">\(\nabla f(x)=Ax-b\)</span>  <span class="math inline">\(f(x+\alpha y)=f(x)+\alpha(Ax-b,y)+\frac{1}{2}\alpha^{2}(Ay,y)\)</span></p><p></p><h3 id=""></h3><ul><li><p> <span class="math inline">\(d_k\)</span><span class="math display">\[  x_{k+1}=x_k+\alpha_k d_k\]</span>  <spanclass="math inline">\(\alpha_k\)</span>.</p></li><li><p> <span class="math inline">\(d_k=-\nablaf(x_k)=b-Ax_k\)</span><strong></strong>. <span class="math inline">\(d_k=r_k\)</span> <spanclass="math inline">\(d_k\)</span> <spanclass="math inline">\(d_{k+1}\)</span>.</p></li></ul><p> <span class="math display">\[    \alpha_k=\frac{(r_k,r_k)}{(Ar_k,r_k)}\]</span> &gt;  <spanclass="math inline">\(d_k\)</span>  <spanclass="math inline">\(d_{k+1}\)</span><strong></strong> <spanclass="math inline">\((d_i,d_j)=0, i\neq j\)</span></p><p> <span class="math display">\[    \left\| x_k-x^{*} \right\|_{A}\leqslant\left(\frac{\lambda_1-\lambda_n}{\lambda_1+\lambda_n}\right)^{k}\left\|x_0-x^{*} \right\|_{A}\]</span></p><p><strong></strong> Kantorovich.Kantorovich</p><hr /><p>(Kantorovich)https://zhuanlan.zhihu.com/p/271983329</p><hr /><blockquote><p>### </p></blockquote><blockquote><p><span class="math inline">\(d_k\)</span>  <spanclass="math inline">\(r_k\)</span> <spanclass="math inline">\(x_k\)</span>  <spanclass="math inline">\(k\)</span> </p></blockquote><ul><li><p><span class="math inline">\(f(x)\)</span> <spanclass="math inline">\(d_k\)</span> <span class="math display">\[  x_{k+1}=x_{k}+\alpha_k d_k\]</span>  <span class="math inline">\(\alpha\)</span></p></li><li><p> <span class="math display">\[  d_{k+1}=r_k+\beta_k d_k\]</span> .</p></li></ul><p><strong></strong>  <span class="math inline">\(\left\| x\right\|_{A}=(Ax,x)^{\frac{1}{2}}\)</span>   <spanclass="math inline">\(d_i\)</span>  <spanclass="math inline">\(d_j\)</span>  <span class="math display">\[    (d_i,Ad_j)=0\]</span>  <span class="math inline">\(A\)</span>.  <span class="math inline">\(A\)</span>.</p><p>K <spanclass="math display">\[    \left\| x_k-x^{*} \right\|_{A}\leqslant2\left(\frac{\sqrt{K}-1}{\sqrt{K}+1}\right)^{k} \left\| x_0-x^{*}\right\|_{A}\]</span></p><p><strong></strong>  <spanclass="math inline">\({d_1,\cdots ,d_n}\)</span>  <spanclass="math inline">\(A\)</span>  <spanclass="math display">\[    \alpha_k=\frac{(d_k,r_{k-1})}{(d_k,Ad_k)},  x_k=x_{k-1}+\alpha_kd_k,  k=1,\cdots ,n\]</span> <strong></strong>. <span class="math inline">\(n\)</span> <span class="math display">\[    A x_n=b\]</span></p><p><strong></strong>  <spanclass="math inline">\(Ax_n=Ax_0+\alpha_1 Ad_1+\cdots +\alpha_n Ad_n\)</span></p><p><spanclass="math inline">\((d_k,r_{k-1})=(d_k,b-Ax_0)\)</span>.</p><p> <spanclass="math inline">\((b-Ax_n,d_k)=(b-Ax_0,d_k)-\alpha_k(Ad_k,d_k)=0\)</span><span class="math inline">\(\alpha_k\)</span> </p><p> <span class="math inline">\(r_n\)</span>  <spanclass="math inline">\(d_k\)</span>  <spanclass="math inline">\(Ax_n=b\)</span>.</p><p><strong></strong>. <span class="math inline">\(r_k\)</span><spanclass="math inline">\(\beta_{k-1}\)</span><spanclass="math inline">\(\alpha_k\)</span>  <spanclass="math display">\[    (r_i,r_j)=0, i \neq j\]</span></p><p> <span class="math display">\[    r_{k-1}=b-Ax_{k-1}, \quad(r_{k-1},d_i)=0, \quad i=1,\cdots ,k-1\]</span>  <span class="math display">\[    d_k=r_{k-1}+\beta _{k-1}d_{k-1}\]</span>  <span class="math inline">\(\beta_{k-1}\)</span> <span class="math display">\[    (d_{k-1},Ad_k)=0\]</span>  <span class="math display">\[    \beta_{k-1}=-\frac{(d_{k-1},Ar_{k-1})}{(d_{k-1},Ad_{k-1})}\]</span></p><p> <span class="math inline">\(d_k\)</span> <spanclass="math inline">\(d_j\)</span><spanclass="math inline">\(j&lt;k\)</span> <spanclass="math display">\[    d_{j-1} ^{\mathsf{T}}A d_k=d_{j-1} ^{\mathsf{T}} Ar_{k-1}+\beta_{k-1} d_{j-1} ^{\mathsf{T}}A d_{k-1}\]</span>  <span class="math inline">\(d_{j-1} ^{\mathsf{T}}A d_{k-1}=0\)</span>  <span class="math display">\[    A d_{j-1}=\frac{1}{\alpha_{j-1}}(r_{j-2}-r_{j-1}) \Rightarrowd_{j-1} ^{\mathsf{T}} Ar_{k-1}=\frac{1}{\alpha_{j-1}}(r_{j-2}-r_{j-1})^{\mathsf{T}} r_{k-1}=0\]</span></p><p> <span class="math inline">\(O(n^{3})\)</span>.</p><blockquote><p> <span class="math inline">\(r_0=b-Ax_0, \quadd_1=r_0\)</span> For <span class="math inline">\(k=1,\cdots ,n\)</span><span class="math inline">\(\displaystyle\alpha_k=\frac{(r_{k-1},r_{k-1})}{(d_k,Ad_k)}\)</span> <spanclass="math inline">\(x_k=x_{k-1}+\alpha_k d_k\)</span> <spanclass="math inline">\(r_k=r_{k-1}-\alpha_k A d_k\)</span> <span class="math inline">\(\displaystyle\beta_k=\frac{(r_k,r_k)}{(r_{k-1},r_{k-1})}=\frac{-(d_k, A r_k)}{(d_k,Ad_k)}\)</span> <span class="math inline">\(d _{k+1}=r_k+\beta_kd_k\)</span> End For</p></blockquote><p></p><p><strong></strong>  <spanclass="math inline">\(A\)</span><span class="math inline">\(\sqrt{n}\)</span> .</p><p> <span class="math inline">\(M\)</span><span class="math display">\[    M ^{-1} Ax= M ^{-1}b\]</span> . <span class="math inline">\(M\)</span><strong></strong>.  - <spanclass="math inline">\(M\)</span> <spanclass="math inline">\(A\)</span>  - <spanclass="math inline">\(M\)</span> </p><p><strong></strong> <spanclass="math inline">\(M=D\)</span>  <spanclass="math inline">\(D\)</span>  <spanclass="math inline">\(A\)</span> .  <spanclass="math inline">\((v,w)_{M}\)</span> .  <spanclass="math inline">\(M ^{-1}A\)</span>.</p><p><strong>-</strong>SSOR <span class="math display">\[    M=(D+\omega L)D ^{-1}(D+\omega U)\]</span>  <span class="math inline">\(A=L+D+U\)</span>. <spanclass="math inline">\(\omega \in [0,2]\)</span> <spanclass="math inline">\(\omega=1\)</span> -</p><blockquote><p> <spanclass="math inline">\(x_0=\)</span>  <spanclass="math inline">\(r_0=b-Ax_0, \quad d_1=z_0=M ^{-1}r_0\)</span> For<span class="math inline">\(k=1,\cdots ,n\)</span> <spanclass="math inline">\(\displaystyle\alpha_k=\frac{(r_{k-1},z_{k-1})}{(d_k,Ad_k)}\)</span> <spanclass="math inline">\(x_k=x_{k-1}+\alpha_k d_k\)</span> <spanclass="math inline">\(r_k=r_{k-1}-\alpha_k A d_k\)</span> <span class="math inline">\(z_k=M ^{-1}r_k\)</span> <spanclass="math inline">\(\displaystyle\beta_k=\frac{(r_k,z_k)}{(r_{k-1},z_{k-1})}\)</span> <spanclass="math inline">\(d _{k+1}=z_k+\beta_k d_k\)</span> End For</p></blockquote><p>SSOR <span class="math inline">\(M^{-1}\)</span> <span class="math inline">\(z_{k}= M ^{-1}r_k\)</span>  <span class="math inline">\(M=(I+\omega LD^{-1})(D+\omega U)\)</span>. <spanclass="math display">\[    (I+\omega LD ^{-1})c=v \\    (D+ \omega U)z=c \\\]</span></p><p>SSOR.</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/05/06/%E6%B0%B8%E4%B9%85%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <url>/2022/05/06/%E6%B0%B8%E4%B9%85%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p></p>          </div><p></p><p></p><p></p><p>2022224122202256</p><p></p><div class="note note-info">            <p></p>          </div><p></p><p>55</p><p></p><p></p><p>202220222019201520192021</p><p></p><p></p>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>8</title>
    <link href="/2022/05/01/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%888%EF%BC%89/"/>
    <url>/2022/05/01/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%888%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><p>The following videos are helpful to understand this chapter. Thanksto <span class="citation" data-cites="inversioner">@inversioner</span>at Zhihu.</p><hr /><p>[2008]https://www.bilibili.com/video/av13923799?from=search&amp;seid=12319283859508785318</p><hr /><hr /><p>2-https://www.bilibili.com/video/av4194600?from=search&amp;seid=12319283859508785318</p><hr /><h2 id=""></h2><p>Recall <strong>autonomous</strong> differential equations:differential equations whose independent variables are not explicitlyincluded in the equation.</p><p>Considering a moving particle in a n-dimensional space. We know itsspeed at <span class="math inline">\(\mathbf{x}\)</span> is <spanclass="math inline">\(\mathbf{v}(\mathbf{x})=(v_1(\mathbf{x}),\cdots,v_n(\mathbf{x}))\)</span>. The equation of motion is <spanclass="math display">\[    \frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t}=\mathbf{v}(\mathbf{x})\tag{1}\]</span> If <span class="math inline">\(\mathbf{v}(\mathbf{x})\)</span>satisfies the condition of existence and uniqueness theorem, then forall initial condition <span class="math display">\[    \mathbf{x}(t_0)=\mathbf{x}_0\]</span> (1) has unique solution satisfying (2) <spanclass="math display">\[    \mathbf{x}=\mathbf{\phi}(t,t_0,\mathbf{x}_0) \tag{3}\]</span></p><p>We call the space for <span class="math inline">\(\mathbf{x}\)</span>(namely <span class="math inline">\(\mathbb{R}^{n}\)</span> )<strong>phase space</strong>, the space for <spanclass="math inline">\((t,\mathbf{x})\)</span> (namely <spanclass="math inline">\(\mathbb{R}^{1} \times \mathbb{R}^{n}\)</span> )<strong>augmented phase space</strong>.</p><ol type="1"><li>defines a <strong>vector field</strong> in the phase space. <spanclass="math display">\[\mathbf{v}(\mathbf{x})=(v_1(\mathbf{x}),\cdots ,v_n(\mathbf{x})) \tag{4}\]</span></li></ol><p>The solution (3) gives a <strong>trajectory</strong> in the phasespace.</p><div class="note note-info">            <p>Trajectory is the projection of integral curve on the phasespace.</p>          </div><p>If <span class="math inline">\(\mathbf{x}_0\)</span> is a zero pointof (4), namely <span class="math inline">\(v(\mathbf{x}_0)=0\)</span>,then (1) has a constant solution <spanclass="math inline">\(\mathbf{x}=\mathbf{x}_0\)</span>. We call <spanclass="math inline">\(\mathbf{x}_0\)</span> is a <strong>balancepoint</strong> to (1). The balance points to (1) is called<strong>singularities</strong>.</p><p>If (3) is an unsteady periodic motion, namely <spanclass="math inline">\(\exists T&gt;0\)</span>, s.t. <spanclass="math display">\[    \mathbf{\varphi}(t+T,t_0,\mathbf{x}_0)\equiv\mathbf{\varphi}(t,t_0,\mathbf{x}_0)\]</span> Then its trajectory in the phase space is a closed curve,denoted as <strong>close orbit</strong>.</p><p>Any autonomous differential equation has the form of (1). Thisintroduces the concept of <strong>dynamic system</strong>.</p><p><strong>Basic properties of dynamic system</strong>: - translationinvariance of integral curves: for all constant <spanclass="math inline">\(C\)</span>, <spanclass="math inline">\(\mathbf{x}=\mathbf{\varphi}(t+C)\)</span> is asolution to (1). - uniqueness of trajectories passing through each pointin phase space. We can only consider the solution corresponding to theinitial time <span class="math inline">\(t_0=0\)</span>. Define <spanclass="math display">\[    \mathbf{\varphi}(t,\mathbf{x}_0)=\mathbf{\varphi}(t,0,\mathbf{x}_0)\]</span> - properties of group: <spanclass="math inline">\(\mathbf{\varphi}(t,\mathbf{x}_0)\)</span>satisfies <span class="math display">\[    \mathbf{\varphi}(t_2,\mathbf{\varphi}(t_1,\mathbf{x}_0))=\mathbf{\varphi}(t_2+t_1,\mathbf{x}_0)\tag{5}\]</span></p><h2 id=""></h2><p>.</p><p> <span class="math display">\[    \frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t}= \mathbf{f}(t,\mathbf{x})\tag{6}\]</span>  <spanclass="math inline">\(\mathbf{f}(t,\mathbf{x})\)</span>  <spanclass="math inline">\(\mathbf{x} \in G \subset \mathbb{R}^{n}\)</span> <span class="math inline">\(t \in (-\infty,\infty)\)</span> <span class="math inline">\(\mathbf{x}\)</span> Lipschitz .  <spanclass="math inline">\(\mathbf{x}= \mathbf{\varphi}(t)\)</span>  <spanclass="math inline">\(t_0\leqslant t&lt; \infty\)</span> . <spanclass="math inline">\(\varepsilon&gt;0\)</span> <spanclass="math inline">\(\delta=\delta(\varepsilon)&gt;0\)</span> <spanclass="math display">\[    \lvert \mathbf{x}_0- \mathbf{\varphi}(t_0) \rvert &lt; \delta\]</span>  <spanclass="math inline">\(\mathbf{x}(t_0)=\mathbf{x}_0\)</span> <span class="math inline">\(\mathbf{x}(t,t_0,\mathbf{x}_0)\)</span> <span class="math inline">\(t\geqslant t_0\)</span> <span class="math display">\[    \lvert \mathbf{x}(t,t_0,\mathbf{x}_0)-\mathbf{\varphi}(t) \rvert&lt;\varepsilon, \forall t\geqslant t_0\]</span>  <spanclass="math inline">\(\mathbf{x}=\mathbf{\varphi}(t)\)</span>Lyapunov<strong></strong>.  <spanclass="math inline">\(\mathbf{x}= \mathbf{\varphi}(t)\)</span> <spanclass="math inline">\(\delta_1(0&lt;\delta_1\leqslant\delta)\)</span> <span class="math display">\[    \lvert \mathbf{x}_0-\mathbf{\varphi}(t_0) \rvert &lt;\delta_1\]</span>  <span class="math display">\[    \lim_{t \to\infty}(\mathbf{x}(t,t_0,\mathbf{x}_0)-\mathbf{\varphi}(t))=0\]</span>  <span class="math inline">\(\mathbf{x}=\mathbf{\varphi}(t)\)</span>Lyapunov<strong></strong>.  <spanclass="math inline">\(\mathbf{x}=\mathbf{\varphi}(t)\)</span><strong></strong>.</p><p><strong></strong><strong></strong>. <spanclass="math inline">\(\mathbf{x}=\mathbf{\varphi}(t)\)</span><strong></strong>.</p><p> <span class="math inline">\(\mathbf{y}= \mathbf{x}-\mathbf{\varphi}(t)\)</span> <spanclass="math inline">\(\mathbf{f}(t,\mathbf{0})=\mathbf{0}\)</span>.</p><h3 id=""></h3><p>6 <spanclass="math inline">\(\mathbf{f}(t,\mathbf{x})\)</span> <spanclass="math inline">\(\mathbf{f}(t,\mathbf{0})=\mathbf{0}\)</span> <spanclass="math inline">\(\mathbf{x}\)</span> <span class="math display">\[    \frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t}= A(t) \mathbf{x}+N(t,\mathbf{x}) \tag{7}\]</span>  <span class="math inline">\(A(t)\)</span>  <spanclass="math inline">\(n\)</span>  <spanclass="math inline">\(t\geqslant t_0\)</span>  <spanclass="math inline">\(N(t,\mathbf{x})\)</span>  <spanclass="math inline">\(t\)</span>  <spanclass="math inline">\(\mathbf{x}\)</span>  <spanclass="math display">\[    G: \quad t\geqslant t_0, \lvert x \rvert \leqslant M\]</span>  <span class="math inline">\(\mathbf{x}\)</span> Lipschitz  <spanclass="math inline">\(N(t,\mathbf{0})=\mathbf{0}(t\geqslantt_0)\)</span>  <span class="math display">\[    \lim_{\lvert \mathbf{x} \rvert  \to 0} \frac{\lvert N(t,\mathbf{x})\rvert }{\lvert \mathbf{x} \rvert }=0 \quad( t\geqslant t_0 )\]</span></p><p><strong>8.1</strong> <span class="math display">\[    \frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t}= A(t) \mathbf{x} \tag{8}\]</span>  <span class="math inline">\(A(t)\)</span> -  <spanclass="math inline">\(\Leftrightarrow\)</span>  <spanclass="math inline">\(\mathbf{A}\)</span>  - <span class="math inline">\(\Leftrightarrow\)</span> <span class="math inline">\(\mathbf{A}\)</span> Jordan -  <spanclass="math inline">\(\Leftrightarrow\)</span>  <spanclass="math inline">\(\mathbf{A}\)</span> Jordan.</p><p><strong>8.2</strong>7 <spanclass="math inline">\(\mathbf{A}(t)=\mathbf{A}\)</span> <span class="math inline">\(\mathbf{A}\)</span>7.</p><p><strong>8.3</strong>7 <spanclass="math inline">\(\mathbf{A}(t)=\mathbf{A}\)</span> <span class="math inline">\(\mathbf{A}\)</span>7</p><p>Gronwall@inversioner9.GronwallGronwall.</p><p><strong>Gronwall</strong> <spanclass="math inline">\(u_{n}(t),v(t)\)</span> </p><p><span class="math display">\[    u_0(t)\leqslant c_0,u_n(t)\leqslant c_n+ \int_{t_0}^{t}v(s)u_{n-1}(s) \mathrm{d}s     \]</span></p><p> <span class="math inline">\(c_n\)</span> </p><p><span class="math display">\[    u_{n}(t)\leqslant \frac{c_0&#39;+\cdots +c_n&#39;}{n+1} \mathrm{e}^{\int_{t_0}^{t} v(s) \mathrm{d}s }\]</span></p><p> <span class="math inline">\(c_k&#39;:= \sup_{n\geqslantk}\{c_n\}\)</span>.</p><p></p><p><strong></strong> <spanclass="math inline">\(u_n(t),v(t)\)</span>  <spanclass="math display">\[    u_n(t)\leqslant u_0(t)+ \int_{t_0}^{t} v(s)u_{n-1}(s) \mathrm{d}s\]</span></p><p> <span class="math display">\[    u_n(t)\leqslant u_0(t) \mathrm{e}^{\int_{t_0}^{t} v(s) \mathrm{d}s}\]</span></p><p><strong></strong> $^{x} $Taylor.</p><p></p><p><strong></strong>7 <spanclass="math display">\[    \mathbf{x}(t)= \mathrm{e}^{(t-t_0)\mathbf{A}} \mathbf{x}_0+\mathrm{e}^{t \mathbf{A}} \int_{t_0}^{t} \mathrm{e}^{-s \mathbf{A}}\mathbf{N}(s,\mathbf{x}(s)) \mathrm{d}s\]</span>  <spanclass="math inline">\(\mathbf{x}_n(t)\)</span>  <spanclass="math inline">\(\mathbf{x}_0(t)=\mathrm{e}^{(t-t_0)\mathbf{A}}\mathbf{x}_0\)</span> <span class="math display">\[    \mathbf{x}_n(t)= \mathrm{e}^{(t-t_0)\mathbf{A}} \mathbf{x}_0+\mathrm{e}^{t \mathbf{A}} \int_{t_0}^{t} \mathrm{e}^{-s \mathbf{A}}\mathbf{N}(s, \mathbf{x}_{n-1}(s)) \mathrm{d}s\]</span></p><p> <span class="math inline">\(\mathbf{A}\)</span> <span class="math inline">\(\lim_{t \to\infty}\mathrm{e}^{t \mathbf{A}} = 0\)</span>.  <spanclass="math inline">\(\mathbf{N}(s,\mathbf{x})\)</span>Lipschitz <span class="math display">\[    \lvert \mathbf{N}(s,\mathbf{x})- \mathbf{N}(s,\mathbf{y})\rvert\leqslant L \lvert \mathbf{x}- \mathbf{y} \rvert\]</span>  <spanclass="math inline">\(\mathbf{N}(s,\mathbf{0})=\mathbf{0}\)</span>. <span class="math display">\[    \lvert \mathbf{x}_n(t) \rvert\leqslant\mathrm{e}^{(t-t_0)\mathbf{A}} \mathbf{x}_0+ \int_{t_0}^{t}L\mathrm{e}^{(t-s) \mathbf{A}} \lvert \mathbf{x}_{n-1}(s) \rvert\mathrm{d}s\]</span></p><p> <span class="math inline">\(\leqslant\)</span> <spanclass="math inline">\(\leqslant\)</span></p><p><span class="math display">\[    \lvert \mathbf{x}_n(t)- \mathbf{x}_{n-1}(t) \rvert \leqslant\int_{t_0}^{t} L\mathrm{e}^{(t-s) \mathbf{A}} \lvert\mathbf{x}_{n-1}(t)-\mathbf{x}_{n-2}(t) \rvert  \mathrm{d}s\]</span></p><p> <span class="math display">\[    \lvert \mathbf{x}_{n}(t)-\mathbf{x}_{n-1}(t) \rvert \leqslant\frac{(L(t-t_0))^{n}\mathrm{e}^{(t-t_0)\mathbf{A}}\mathbf{x}_0 }{n!}\]</span></p><p> $_0 $  <spanclass="math inline">\(\mathbf{x}_n(t)\)</span>  <spanclass="math inline">\(t\geqslant t_0\)</span>  <spanclass="math inline">\(\mathbf{x}(t)\)</span> <spanclass="math display">\[    \lvert \mathbf{x}(t) \rvert \leqslant \mathrm{e}^{L(t-t_0)}\mathrm{e}^{(t-t_0)\mathbf{A}} \mathbf{x}_0\]</span></p><div class="note note-info">            <p>.  $_0 $  $_1(t) $ $_n(t) $  <spanclass="math inline">\(L\)</span>Gronwall.</p>          </div><p> $_0 $  <spanclass="math inline">\(L=o(\mathbf{x}_0)\)</span> $_0 $ <span class="math display">\[    \lim_{t \to \infty}\mathrm{e}^{L(t-t_0)}\mathrm{e}^{(t-t_0)\mathbf{A}} \mathbf{x}_0= \mathbf{0}\]</span></p><div class="note note-info">            <p> $_0 $ .</p>          </div><p>8.3 <spanclass="math inline">\(\mathbf{x}_0\)</span>.</p><p></p><p><strong></strong>.</p><h3 id="lyapunov-">Lyapunov </h3><p> <span class="math display">\[    \frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t}=\mathbf{f}(\mathbf{x})\tag{9}\]</span></p><p> <span class="math inline">\(\mathbf{x} \in\mathbb{R}^{n}\)</span> <spanclass="math inline">\(\mathbf{f}(\mathbf{x})=(f_1(\mathbf{x}),\cdots,f_{n}(\mathbf{x}))\)</span> .</p><p> <spanclass="math inline">\(V(\mathbf{x})\)</span> <spanclass="math inline">\(\lvert \mathbf{x} \rvert \leqslant M\)</span>. </p><p><strong> I</strong> <span class="math display">\[    V(\mathbf{0})=0; V(\mathbf{x})&gt;0,  \mathbf{x} \neq \mathbf{0}\]</span>  <span class="math inline">\(V\)</span><strong></strong></p><p><strong> II</strong> <span class="math display">\[    \frac{\mathrm{d}V}{\mathrm{d}t}\bigg |_{(9)}=\frac{\partialV}{\partial x_1} f_1+\cdots +\frac{\partial V}{\partial x_n}f_n&lt;0,\mathbf{x} \neq \mathbf{0}\]</span>  <spanclass="math inline">\(\frac{\mathrm{d}V}{\mathrm{d}t}|_{(9)}\)</span><strong></strong></p><p><strong> II</strong>* <span class="math display">\[    \frac{\mathrm{d}V}{\mathrm{d}t} \bigg|_{(9)}\leqslant 0\]</span>  <spanclass="math inline">\(\frac{\mathrm{d}V}{\mathrm{d}t}|_{(9)}\)</span><strong></strong></p><p><strong> III</strong> <span class="math display">\[    \frac{\mathrm{d}V}{\mathrm{d}t} \bigg|_{(9)}&gt; 0, \mathbf{x}\neq\mathbf{0}\]</span>  <spanclass="math inline">\(\frac{\mathrm{d}V}{\mathrm{d}t}|_{(9)}\)</span></p><p><strong>8.4</strong> Lyapunov  -III9 -III*9 -IIII9.</p><div class="note note-info">            <p>IIII8.4.</p>          </div><div class="note note-info">            <p>IIII9.</p>          </div><p><strong></strong></p><hr /><p>https://www.zhihu.com/question/38156489</p><hr /><p>. </p><p>1 <spanclass="math inline">\(\frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t}=\mathbf{A} \mathbf{x}\)</span></p><p> <span class="math inline">\(V(\mathbf{x})= \mathbf{x}^{\mathsf{T}} \mathbf{B} \mathbf{x}\)</span><spanclass="math inline">\(\mathbf{B}\)</span> <span class="math inline">\(2\mathbf{x} ^{\mathsf{T}}\mathbf{BAx}=\mathbf{x}^{\mathsf{T}}(\mathbf{A}^{\mathsf{T}}\mathbf{B}+\mathbf{B}\mathbf{A})\mathbf{x}\)</span>. <spanclass="math inline">\(\mathbf{B}\)</span>  <spanclass="math inline">\(\mathbf{A}^{\mathsf{T}}\mathbf{B}+\mathbf{BA}\)</span>//. // <spanclass="math inline">\(\mathbf{C}\)</span> <spanclass="math inline">\(\mathbf{A}^{\mathsf{T}}\mathbf{B}+\mathbf{BA}=\mathbf{C}\)</span></p><p>2 <span class="math inline">\(\frac{\mathrm{d}x}{\mathrm{d}t}=y-x f(x, y), \frac{\mathrm{d}y}{\mathrm{d}t}= -x-y f(x, y)\)</span>.<span class="math inline">\(f\)</span> </p><p> <span class="math inline">\(V (x, y)= x^{2}+y^{2}\)</span><span class="math display">\[    \frac{1}{2}\frac{\mathrm{d}(x^{2}+y^{2})}{\mathrm{d}t}=-(x^{2}+y^{2}) f(x, y)\]</span>  <span class="math inline">\(f(x,y)\)</span> .</p><p>3 <span class="math inline">\(x&#39;&#39;+g(x)=0\)</span><span class="math inline">\(xg(x)&gt;0, \forall x \neq 0\)</span></p><p> <spanclass="math inline">\(\frac{\mathrm{d}x}{\mathrm{d}t}=y,\frac{\mathrm{d}y}{\mathrm{d}t}=-g(x)\)</span>.Lyapunov <spanclass="math inline">\(E=\frac{1}{2}y^{2}+\int_{0}^{x} g(s)\mathrm{d}s\)</span>.  <span class="math inline">\(E\)</span><span class="math inline">\(E\)</span>.</p><hr /><p>https://zhuanlan.zhihu.com/p/410896233</p><hr /><hr /><p>https://www.zhihu.com/question/38006572</p><hr /><h2id=""></h2><p> <span class="math display">\[    \frac{\mathrm{d}x}{\mathrm{d}t}=X(x,y), \quad\frac{\mathrm{d}y}{\mathrm{d}t}= Y(x,y) \tag{10}\]</span></p><p> <span class="math inline">\(X (x, y)\)</span>  <spanclass="math inline">\(Y (x, y)\)</span>  <spanclass="math inline">\((x, y)\)</span>.</p><p> <span class="math inline">\(t\)</span>  <spanclass="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}= \frac{Y(x,y)}{X(x,y)} \tag{11}\]</span></p><h3 id=""></h3><p> <span class="math inline">\((0,0)\)</span>  <spanclass="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t} \begin{pmatrix}    x \\    y \\    \end{pmatrix}    =    \mathbf{A}    \begin{pmatrix}    x \\    y \\    \end{pmatrix} \tag{12}\]</span>  <span class="math inline">\(\mathbf{A}\)</span>.</p><ul><li><span class="math inline">\(\mathbf{A}\)</span>  <spanclass="math inline">\((0,0)\)</span> <strong></strong>.</li><li><span class="math inline">\(\mathbf{A}\)</span>  <spanclass="math inline">\((0,0)\)</span> <strong></strong>.</li></ul><p>.</p><p> <span class="math inline">\(\mathbf{A}\)</span>Jordan. .</p><p><strong>8.5</strong>12<span class="math display">\[    p=-\text{tr}(\mathbf{A})=-(a+d)  q= \det(\mathbf{A})=ad-bc\]</span>  -  <span class="math inline">\(q&lt;0\)</span><span class="math inline">\((0,0)\)</span>  -  <spanclass="math inline">\(q&gt;0\)</span>  <spanclass="math inline">\(p^{2}&gt;4q\)</span>  <spanclass="math inline">\((0,0)\)</span>  -  <spanclass="math inline">\(q&gt;0\)</span>  <spanclass="math inline">\(p^{2}=4q\)</span>  <spanclass="math inline">\((0,0)\)</span>  -  <spanclass="math inline">\(q&gt;0\)</span>  <spanclass="math inline">\(0&lt;p^{2}&lt;4q\)</span>  <spanclass="math inline">\((0,0)\)</span>  -  <spanclass="math inline">\(q&gt;0\)</span>  <spanclass="math inline">\(p=0\)</span> <spanclass="math inline">\((0,0)\)</span> </p><p>24 <spanclass="math inline">\(p&gt;0\)</span>  <spanclass="math inline">\((0,0)\)</span>  <spanclass="math inline">\(p&lt;0\)</span> .</p><p> <span class="math inline">\(\mathbf{A}\)</span> jordan p266.</p><p>10.  <span class="math inline">\((0,0)\)</span> <spanclass="math display">\[    \begin{cases}        \frac{\mathrm{d}x}{\mathrm{d}t}= ax+by+\varphi(x,y) \\        \frac{\mathrm{d}y}{\mathrm{d}t}= cx+dy+ \psi (x, y) \\    \end{cases} \tag{13}\]</span></p><p> <spanclass="math inline">\(r=\sqrt{x^{2}+y^{2}}\)</span><strong>A</strong> <span class="math inline">\(\varphi (x, y), \psi(x, y)=o(r), r\to 0\)</span>. <strong>A</strong>* <spanclass="math inline">\(\varphi (x, y),\psi (x, y)=o(r^{1+\varepsilon}),r\to 0\)</span> <spanclass="math inline">\(\varepsilon\)</span> .<strong>B</strong> <span class="math inline">\(\varphi (x,y)\)</span>  <span class="math inline">\(\psi (x, y)\)</span> <span class="math inline">\(x\)</span>  <spanclass="math inline">\(y\)</span> </p><p><strong>8.6</strong> p268</p><p>1012<strong></strong>.</p><p>. </p><p><strong>8.7</strong>13 <spanclass="math inline">\((0,0)\)</span><strong></strong> <spanclass="math inline">\((0,0)\)</span>.</p><div class="note note-info">            <p>8.7 <span class="math inline">\(\mathbb{R}^{n}\)</span>Hartman-Grobman</p>          </div><h3 id=""></h3><p>10 <span class="math inline">\(\Gamma\)</span> <spanclass="math inline">\(\Gamma\)</span>10<strong></strong>.  <spanclass="math inline">\(\Gamma\)</span> <spanclass="math inline">\(t \to +\infty\)</span>  <spanclass="math inline">\(-\infty\)</span>  <spanclass="math inline">\(\Gamma\)</span>.  <spanclass="math inline">\(\Gamma\)</span>  <spanclass="math inline">\(t \to +\infty\)</span>  <spanclass="math inline">\(-\infty\)</span>  <spanclass="math inline">\(\Gamma\)</span> <spanclass="math inline">\(\Gamma\)</span><strong></strong><strong></strong><strong></strong><span class="math inline">\(t \to +\infty\)</span>  <spanclass="math inline">\(\Gamma\)</span> <spanclass="math inline">\(-\infty\)</span> <spanclass="math inline">\(\Gamma\)</span><strong></strong>.</p><p> <span class="math inline">\(\Gamma\)</span><strong></strong>.</p><p><strong>Poincar-Bendixson </strong> <spanclass="math inline">\(D\)</span>  <spanclass="math inline">\(L_1\)</span>  <spanclass="math inline">\(L_2\)</span>  <spanclass="math inline">\(\bar{D}=L_1 \cup D \cup L_2\)</span>10 <span class="math inline">\(L_1\)</span> <span class="math inline">\(L_2\)</span> <spanclass="math inline">\(\bar{D}\)</span>.  <spanclass="math inline">\(L_1\)</span>  <spanclass="math inline">\(L_2\)</span> 10 <spanclass="math inline">\(D\)</span>  <spanclass="math inline">\(\Gamma\)</span> <spanclass="math inline">\(\Gamma\)</span>  <spanclass="math inline">\(D\)</span> .</p><p>10<span class="math inline">\(D\)</span>  <spanclass="math inline">\(D\)</span> <spanclass="math inline">\(D\)</span>  <spanclass="math inline">\(D\)</span> . <spanclass="math inline">\(\Gamma\)</span>.</p><p><strong>Linard </strong> <span class="math display">\[    x&#39;&#39;+f(x)x&#39;+g(x)=0 \tag{14}\]</span></p><p> <span class="math inline">\(f(x)\)</span>  <spanclass="math inline">\(g(x)\)</span>  <spanclass="math inline">\(xg(x)&gt;0\)</span> <spanclass="math inline">\(x \neq 0\)</span>.  <spanclass="math display">\[    \frac{\mathrm{d}x}{\mathrm{d}t}= y-F(x), \quad\frac{\mathrm{d}y}{\mathrm{d}t}= -g(x) \tag{15}\]</span></p><p> <span class="math inline">\(F(x)= \int_{0}^{x} f(x)\mathrm{d}x\)</span>.  <span class="math inline">\(g(x)=-x\)</span>15 <spanclass="math inline">\((y-F(x),-x)\)</span>  <spanclass="math inline">\(P(x,y)\)</span> .</p><h3 id="linard-">Linard </h3><h3 id="poincar-">Poincar </h3><p><strong>8.9</strong>10 <spanclass="math inline">\(\Gamma\)</span>  <spanclass="math inline">\(\varepsilon&gt;0\)</span>  <spanclass="math inline">\(\Gamma\)</span>  <spanclass="math inline">\(\mathcal{U}\)</span>10 <spanclass="math inline">\(\varepsilon-\)</span>  <spanclass="math inline">\(\mathcal{U}\)</span> <span class="math inline">\(\Gamma\)</span> .</p><h2 id=""></h2><h3 id=""></h3><h3 id=""></h3><h3 id="hopf-">Hopf </h3><h3 id="poincar-">Poincar </h3><h3 id=""></h3><h3 id=""></h3><h3 id=""></h3>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (2)</title>
    <link href="/2022/05/01/Neuronal-Dynamics-2/"/>
    <url>/2022/05/01/Neuronal-Dynamics-2/</url>
    
    <content type="html"><![CDATA[<h1 id="ion-channels-and-the-hodgkin-huxley-model">Ion Channels and theHodgkin-Huxley Model</h1><p>The online version of this chapter:</p><hr /><p>Chapter 2 Ion Channels and the Hodgkin-Huxley Modelhttps://neuronaldynamics.epfl.ch/online/Ch2.html</p><hr /><p>In the previous description, spikes are formal events that aregenerated at the moment of threshold crossing.</p><p>Now we will talk about how a spike can be generated based on thebiophysics of cell membranes.</p><h2 id="equilibrium-potential">2.1 Equilibrium potential</h2><h3 id="nernst-potential">2.1.1 Nernst potential</h3><p>The probability of a molecule to take a state of energy <spanclass="math inline">\(E\)</span> is proportional to the Boltzman factor<span class="math inline">\(p(t) \propto \exp (-E/kT)\)</span> where<span class="math inline">\(k\)</span> is the Boltzman constant and<span class="math inline">\(T\)</span> the temperature.</p><p>The probality to find an ion in the region around location <spanclass="math inline">\(x\)</span> proportional to <spanclass="math inline">\(\exp [-qu(x)/kT]\)</span>.</p><p>Write <span class="math inline">\(n(x)\)</span> for the ion densityat point <span class="math inline">\(x\)</span>. <spanclass="math display">\[    \frac{n(x_1)}{n(x_2)}=\exp [-\frac{qu(x_1)-qu(x_2)}{kT}] \tag{2.1}\]</span></p><blockquote><p><strong>Q:</strong> What's the meaning of "Since this is a statementabout an equilibrium state, the reverse must also be true"?</p></blockquote><p><strong>Nernst potential</strong> () <spanclass="math display">\[    \Delta u=\frac{kT}{q}\ln \frac{n_2}{n_1} \tag{2.2}\]</span></p><h3 id="reversal-potential">2.1.2 Reversal Potential</h3><p>The cell membrane bilayer lipids - a nearly perfect electricalinsulator specific proteins as ion gates - ion pumps: actively transportions from one side to the other. - ion channels: passively</p><p><strong>Resting Potential</strong> The value <spanclass="math inline">\(u_{rest}\)</span> is determined by the dynamicequilibrium between the ion flow through the channels (permeability ofthe membrane) and active ion transport (efficiency of the ion pump inmaintaining the concentration difference).</p><h2 id="hodgkin-huxley-model">2.2 Hodgkin-Huxley Model</h2><p>Four equations <span class="math display">\[    \begin{aligned}        C &amp;\frac{\mathrm{d}u}{\mathrm{d}t}=-g_Kn^{4}(u-E_K)-g_{Na}m^{3}h(u-E_{Na})-g_l(u-E_l)+I(t) \\        &amp;\frac{\mathrm{d}}{\mathrm{d}t}n=-\frac{n-n_0(u)}{\tau_n(u)} \\        &amp;\frac{\mathrm{d}}{\mathrm{d}t}m=-\frac{m-m_0(u)}{\tau_m(u)} \\        &amp;\frac{\mathrm{d}}{\mathrm{d}t}h=-\frac{h-h_0(u)}{\tau_h(u)}    \end{aligned}\]</span></p><p>Gating variables <span class="math display">\[    I_{ion}=-g_{ion}r^{n_1}s^{n_2}\]</span> <span class="math inline">\(r\)</span> represents activationvariables, <span class="math inline">\(r_0(u)\to 1, u\to\infty\)</span>. <span class="math inline">\(s\)</span> representsinactivation variables, <span class="math inline">\(s_0(u)\to 0, u\to\infty\)</span> &gt; There could be 2 gating variables for some ions.&gt; It turns out to be useful to distinguish between a deactivatedchannel (<span class="math inline">\(m\)</span> close to zero and <spanclass="math inline">\(h\)</span> close to one) and an inactivatedchannel ( <span class="math inline">\(h\)</span> close to zero)</p><h3 id="threshold-in-the-hodgkin-huxley-model">Threshold in the HodgkinHuxley Model</h3><p><strong>Here, threshold relates to the paradigm of thecurrent.</strong></p><p><strong>threshold for repetitive firing</strong> current threshold(constant current)</p><p>threshold current: <span class="math inline">\(\theta_i\)</span>. Ifwe are above this minimal current, the model generates regular firing.If not, then the model shows constant firing. <strong>threshold foraction potential</strong> current threshold (step current)(uninformative)</p><p><strong>pulse current</strong> Consider <spanclass="math inline">\(I(t)=q\delta(t-t_0)\)</span>. It causes a jump ofthe membrane potential.</p><p>Fixing duration of the current, we have a current threshhold. If thecurrent is above the threshhold, then the neuron gives an actionpotential.</p><p>SAP: spike after potential</p><p><strong>step current input</strong> <spanclass="math inline">\((I_1,\Delta I,I_2)\)</span></p><p><img src="img/neu_dyn/2022-05-18-15-19-28.png" /></p><p>The final current as well as the step size that come intoaccount.</p><p><strong>ramp input</strong></p><p>Type II/Class II Behavior(Hodgking-Huxley model with standardparameters, giant axon of squid): Using a very slow ramp input on aHodgkin-Huxley model, we find an f-I curve with a jump</p><p>Type I/Class I Behavior(Hodgking-Huxley model with other parameters,e.g. for cortical pyramidal neuron): Using the frame variable ofHodgking-Huxley models but change parameters so it's more adapted tocortical neurons, then you make it a smooth response with very lowfrequencies.</p><h3 id="stochastic-channel-opening">Stochastic Channel Opening</h3><p>Channels open stochastically.</p><blockquote><center>Example: Time Constants, Transition Rates, and Channel Kinetics</center><p>The activation and inactivation dynamics of each channel type can bedescribed in terms of voltage-dependent transition rates <spanclass="math inline">\(\alpha\)</span> and <spanclass="math inline">\(\beta\)</span>, <span class="math display">\[\frac{\mathrm{d}m}{\mathrm{d}t}=\alpha_m(u)(1-m)-\beta_m(u)m\]</span> <span class="math display">\[\frac{\mathrm{d}n}{\mathrm{d}t}=\alpha_n(u)(1-n)-\beta_n(u)n\]</span> <span class="math display">\[\frac{\mathrm{d}h}{\mathrm{d}t}=\alpha_h(u)(1-h)-\beta_h(u)h\]</span> The asymptotic value <spanclass="math inline">\(x_0(u)\)</span> and the time constant <spanclass="math inline">\(\tau_x(u)\)</span> are given by <spanclass="math inline">\(x_0(u)=\alpha_x(u)/[\alpha_x(u)+\beta_x(u)]\)</span>and <spanclass="math inline">\(\tau_x(u)=[\alpha_x(u)+\beta_x(u)]^{-1}\)</span>.</p><p><span class="math inline">\(n\)</span> can be intepreted as theprobability of finding a single potassium channel open. In a patch with<span class="math inline">\(K\)</span> channels, approximately <spanclass="math inline">\(k \thickapprox (1-n)K\)</span> channels areexpected to be closed. We may interpret <spanclass="math inline">\(\alpha_n(u)\Delta t\)</span> as the probabilitythat in a short time interval <span class="math inline">\(\Deltat\)</span> one of the momentarily closed channels switches to the openstate.</p></blockquote><h2 id="the-zoo-of-ion-channels">2.3 The Zoo of Ion Channels</h2><h3 id="framework-for-biophysical-neuron-models">Framework forbiophysical neuron models</h3><h3 id="sodium-ion-channels-and-the-type-i-regime">Sodium Ion Channelsand the Type-I Regime</h3><h3 id="adaptation-and-refractoriness">Adaptation andRefractoriness</h3><p>I have a constant input current, an adaptation means that in thespike intervals, get longer and longer.</p><p>Ex: <span class="math display">\[    I_{M}= g_{M} m(u-E_{k})\]</span> - Potassium current - Kv7 subunits - slow time constant</p><p>A current such as <span class="math inline">\(I_{M}\)</span> is oneof the potentially many sources of adaptation. It works by lowering themembrane potential, by lowering the spike after potential(SAP).</p><p>Another way of generating adaptation: not by changing SAP, but byincreasing the firing threshold.</p><p><span class="math display">\[    I_{NaP} = g_{NaP} mh(u-E_{Na})      \]</span> - persistent sodium current - fast activation time constant -slow inactivation (~1s)</p><h3 id="subthreshold-effects">Subthreshold Effects</h3><h3 id="calcium-spikes-and-postinhibitory-rebound">Calcium spikes andpostinhibitory rebound</h3><h2 id="summary">2.4 Summary</h2>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ascoli-Arzela Theorem</title>
    <link href="/2022/04/29/Ascoli-Arzela-Theorem/"/>
    <url>/2022/04/29/Ascoli-Arzela-Theorem/</url>
    
    <content type="html"><![CDATA[<h1 id="ascoli-arzela-theorem">Ascoli-Arzela Theorem</h1><p>Ascoli-ArzelaODEMontelRiemannMapping TheoremPeter-WeylTheorem......</p><p></p><p><strong>Theorem (Ascoli-Arzela)</strong>:  <spanclass="math inline">\(D \subset \mathbb{R}^{d}\)</span><spanclass="math inline">\(\mathcal{F} \subset C(D)\)</span>  <spanclass="math inline">\(\mathcal{F}\)</span> <spanclass="math inline">\(\iff\)</span> <spanclass="math inline">\(\mathcal{F}\)</span> .</p><p><spanclass="math inline">\(C(D)\)</span></p><p> <strong>$C(D) $</strong> <spanclass="math inline">\(D\)</span> <spanclass="math inline">\(D\)</span>.</p><p><strong></strong><spanclass="math inline">\(\forall x_0 \in D, \forall \varepsilon&gt;0,\exists \delta&gt;0, \forall f \in \mathcal{F}, \forall \lvertx-x_0 \rvert &lt;\delta,(\lvert f(x)-f(x_0) \rvert&lt;\varepsilon)\)</span></p><p> <spanclass="math inline">\(\delta\)</span> <spanclass="math inline">\(x_0\)</span></p><p> <span class="math inline">\(D\)</span><spanclass="math inline">\(\lVert \cdot \rVert \colon C(D) \to \mathbb{R}, f\mapsto \max_{x \in D} \lvert f(x) \rvert\)</span> <spanclass="math inline">\(C(D)\)</span>.</p><p><strong></strong><spanclass="math inline">\(C(D)\)</span>Banach.</p><p><strong></strong><spanclass="math inline">\(C(D)\)</span>Cauchy <spanclass="math inline">\(\{f_n\}\)</span> <spanclass="math inline">\(f\)</span> <spanclass="math inline">\(f\)</span> <spanclass="math inline">\(C(D)\)</span>.</p><p><span class="math inline">\(f_n(x)=x^{n}, x \in[0,1]\)</span>.</p><p> <spanclass="math inline">\((X,d_{X})\)</span><spanclass="math inline">\(E\)</span> <spanclass="math inline">\(X\)</span>.</p><p><strong>Lemma 1</strong><span class="math inline">\(E\)</span><span class="math inline">\(\Rightarrow\)</span> <spanclass="math inline">\(E\)</span>. <strong></strong> <spanclass="math inline">\(\{x_k\}\)</span> <spanclass="math inline">\(E\)</span> <spanclass="math inline">\(E\)</span> <spanclass="math inline">\(F\)</span>. <spanclass="math inline">\(F\)</span> <spanclass="math inline">\(F\)</span> <spanclass="math inline">\(E\)</span> <spanclass="math inline">\(F\)</span><spanclass="math inline">\(E\)</span><span class="math inline">\(F\)</span> <spanclass="math inline">\(F\)</span> <spanclass="math inline">\(E\)</span></p><p><strong>Lemma 2</strong> <span class="math inline">\(E\)</span><span class="math inline">\(\Rightarrow\)</span> <spanclass="math inline">\(E\)</span> </p><p><strong></strong> <spanclass="math inline">\(\varepsilon &gt; 0\)</span> <spanclass="math inline">\(E\)</span> <spanclass="math inline">\(x_1,\cdots ,x_m\)</span> $E B(x_1,)B(x_m,)$ <spanclass="math inline">\(E\)</span><strong></strong><span class="math inline">\(x_1,\cdots ,x_m\)</span><strong><span class="math inline">\(\varepsilon\)</span>-</strong></p><p><strong>Lemma3</strong><span class="math inline">\(E\)</span><span class="math inline">\(\Rightarrow\)</span> <spanclass="math inline">\(E\)</span> <strong></strong> <spanclass="math inline">\(\varepsilon_{0}\)</span>Cauchy<span class="math inline">\(E\)</span>.</p><p> <span class="math inline">\(X\)</span></p><p><strong>Lemma4</strong><span class="math inline">\(E\)</span><span class="math inline">\(\Leftrightarrow\)</span> <spanclass="math inline">\(E\)</span><strong></strong> <spanclass="math inline">\(E\)</span> <spanclass="math inline">\(E\)</span> <spanclass="math inline">\(E\)</span>.Cauchy <spanclass="math inline">\(E\)</span>.</p><blockquote><p></p></blockquote><p><strong>Lemma5</strong><span class="math inline">\(E\)</span><span class="math inline">\(\Rightarrow\)</span> <spanclass="math inline">\(E\)</span> <spanclass="math inline">\(\Rightarrow\)</span> <spanclass="math inline">\(E\)</span><strong></strong>CauchyCauchy.</p><p><strong>Lemma6</strong><spanclass="math inline">\(E\)</span> <spanclass="math inline">\(\Rightarrow\)</span> <spanclass="math inline">\(E\)</span><strong></strong> <spanclass="math inline">\(\{x_k\}\)</span><spanclass="math inline">\(E\)</span>Cauchy<span class="math inline">\(E\)</span> <spanclass="math inline">\(E\)</span>.</p><p><strong>Lemma7</strong><span class="math inline">\(E\)</span><span class="math inline">\(\Rightarrow\)</span> <spanclass="math inline">\(E\)</span>Lebesgue <spanclass="math inline">\(E\)</span> <spanclass="math inline">\(\{U_{\alpha}\}\)</span> <spanclass="math inline">\(\varepsilon&gt;0\)</span> <spanclass="math inline">\(x \in E\)</span> <spanclass="math inline">\(U_{\alpha}\)</span> $B(x,) U_{} $.</p><p><strong></strong> <spanclass="math inline">\(\displaystyle\varepsilon_k=\frac{1}{k}\)</span><span class="math inline">\(U_{\alpha}\)</span> .</p><p><strong>Lemma8</strong><spanclass="math inline">\(E\)</span>Lebesgue <spanclass="math inline">\(\Rightarrow\)</span> <spanclass="math inline">\(E\)</span><strong></strong> <spanclass="math inline">\(\varepsilon-\)</span>Lebesgue..</p><p>Lemma2Lemma3LebesgueLemma7Lemma8.</p><p> <strong>Theorem1</strong> <spanclass="math inline">\((X,d_{X})\)</span><spanclass="math inline">\(E\)</span> <spanclass="math inline">\(X\)</span> - <spanclass="math inline">\(E\)</span> - <spanclass="math inline">\(E\)</span> - <spanclass="math inline">\(E\)</span></p><p></p><p><strong>Theorem2</strong><span class="math inline">\(X\)</span><span class="math inline">\(E\)</span> - <spanclass="math inline">\(E\)</span> - <spanclass="math inline">\(\bar{E}\)</span><strong></strong><strong></strong>- <span class="math inline">\(E\)</span> - <spanclass="math inline">\(E\)</span> <spanclass="math inline">\(E\)</span></p><p><strong></strong>12 <spanclass="math inline">\(\bar{E}\)</span> <spanclass="math inline">\(E\)</span> <spanclass="math inline">\(\varepsilon\)</span>. 23.34 <span class="math inline">\(E\)</span> <spanclass="math inline">\(F\)</span> <spanclass="math inline">\(E\)</span> <spanclass="math inline">\(F\)</span>. 41Lemma3.</p><blockquote><p>Theorem2..</p></blockquote><p> <span class="math inline">\(\forall\varepsilon&gt;0\)</span>  <spanclass="math inline">\(\varepsilon\)</span>- <spanclass="math inline">\(\forall \varepsilon&gt;0\)</span>  <spanclass="math inline">\(\varepsilon\)</span>- <spanclass="math inline">\(\varepsilon\)</span></p><p> <spanclass="math inline">\(C(D)\)</span>.</p><p><strong>Theorem (Ascoli-Arzela)</strong>:  <spanclass="math inline">\(D \subset \mathbb{R}^{d}\)</span><spanclass="math inline">\(\mathcal{F} \subset C(D)\)</span>  <spanclass="math inline">\(\mathcal{F}\)</span> <spanclass="math inline">\(\iff\)</span> <spanclass="math inline">\(\mathcal{F}\)</span> .</p><blockquote><p>..</p></blockquote><p><strong>Lemma1</strong> <spanclass="math inline">\(D\)</span> <spanclass="math inline">\(\mathcal{F}\)</span> <spanclass="math inline">\(\Leftrightarrow\)</span> <spanclass="math inline">\(\mathcal{F}\)</span>.</p><p><strong></strong>.</p><p><strong>Lemma2</strong> <spanclass="math inline">\(D\)</span>$ C(D) $ <spanclass="math inline">\(\mathcal{F}\)</span> <spanclass="math inline">\(\Leftrightarrow\)</span> <spanclass="math inline">\(\mathcal{F}\)</span>.</p><p><strong></strong>. <spanclass="math inline">\(x_1,\cdots ,x_m\)</span> <spanclass="math inline">\(\delta-\)</span> <spanclass="math inline">\(f(x_1),\cdots ,f(x_m)\)</span> <spanclass="math inline">\(\varepsilon-\)</span><span class="math inline">\(\varepsilon\)</span>.</p><p><strong>Ascoli-Arzela</strong> -  <spanclass="math inline">\(\Rightarrow\)</span>  <spanclass="math inline">\(\mathcal{F}\)</span> <spanclass="math inline">\(1-\)</span>.</p><ul><li> <span class="math inline">\(\Rightarrow\)</span></li></ul><p><strong>Lemma3</strong> <span class="math inline">\(f_k\rightrightarrows f\)</span> <spanclass="math inline">\(\{f_k\}\)</span>.</p><p><strong></strong><spanclass="math inline">\(\Rightarrow\)</span><spanclass="math inline">\(\{f_k\}\)</span> <spanclass="math inline">\(C(D)\)</span>Cauchy.</p><p>.  <spanclass="math inline">\(d(f_n(x),f(x))\)</span><spanclass="math inline">\(f\)</span> <spanclass="math inline">\(d(f(x),f(y))\)</span> <spanclass="math inline">\(n&gt;N\)</span> <spanclass="math inline">\(d(f_n(x),f_n(y))\)</span>.  <spanclass="math inline">\(f_k\)</span> <spanclass="math inline">\(k\)</span>. Lemma3.</p><p>Ascoli-Arzela.  <spanclass="math inline">\(\mathcal{F}\)</span> <spanclass="math inline">\(\varepsilon_0&gt;0\)</span> <spanclass="math inline">\(\delta&gt;0\)</span>  <spanclass="math inline">\(x_{\delta},y_{\delta} \in D, f_{\delta} \in\mathcal{F}\)</span> <spanclass="math inline">\(d(x_{\delta},y_{\delta})&lt;\delta\)</span><spanclass="math inline">\(d(f_{\delta}(x_{\delta}),f_{\delta}(y_{\delta}))&gt;\varepsilon_0\)</span>.</p><p> <span class="math inline">\(\displaystyle\delta_k=\frac{1}{k}, k=1,2,..\)</span> <spanclass="math inline">\(x_k,y_k,f_k\)</span>.  <spanclass="math inline">\(\mathcal{F}\)</span> <spanclass="math inline">\(\{f_k\}\)</span> <spanclass="math inline">\(\{f_{k_j}\}\)</span>. Lemma3<spanclass="math inline">\(\{f_{k_j}\}\)</span> <spanclass="math inline">\(\varepsilon_0&gt;0\)</span> <spanclass="math inline">\(\delta&gt;0\)</span> <spanclass="math inline">\(x,y \in D\)</span> <spanclass="math inline">\(d(x,y)&lt;\delta\)</span> <spanclass="math inline">\(d(f_{k_j}(x),f_{k_j}(y))&lt;\varepsilon_0, \forallj \in \mathbb{N}\)</span>.</p><p> <span class="math inline">\(j\)</span> <spanclass="math inline">\(\frac{1}{k_j}&lt;\delta\)</span> <spanclass="math inline">\(d(x_{k_j},y_{k_j})&lt;\frac{1}{k_j}&lt;\delta\)</span><spanclass="math inline">\(d(f_{k_j}(x_{k_j}),f_{k_j}(y_{k_j}))\geqslant\varepsilon_0\)</span></p><p><span class="math inline">\(\Leftarrow\)</span> <spanclass="math inline">\(D\)</span> <spanclass="math inline">\(n\)</span> <spanclass="math inline">\(D\)</span> <spanclass="math inline">\(C(D)=\mathbb{R}^{n}\)</span> <spanclass="math inline">\(\Rightarrow\)</span> .</p><p> <span class="math inline">\(D\)</span><spanclass="math inline">\(\Rightarrow\)</span><span class="math inline">\(\mathcal{F}\)</span>. Theorem2<span class="math inline">\(\mathcal{F}\)</span>.</p><p> <span class="math inline">\(\{f_n\}\)</span> <spanclass="math inline">\(\mathcal{F}\)</span><spanclass="math inline">\(D\)</span>.  $_k=,k=1,2,$ <span class="math inline">\(\varepsilon_k-\)</span><span class="math inline">\(\{x_{k,1},\cdots ,x_{k,m_k}\}\)</span>. <span class="math inline">\(I\)</span> <spanclass="math inline">\(\varepsilon_k-\)</span> <spanclass="math inline">\(I\)</span> <spanclass="math inline">\(I=\{x_1,x_2,\cdots \}\)</span>.</p><p> <span class="math inline">\(x_k \in I\)</span> <spanclass="math inline">\(\mathcal{F}\)</span> <spanclass="math inline">\(f_n(x_k)\)</span> <spanclass="math inline">\(\{f_n^{(k)}(x_k)\}\)</span> <spanclass="math inline">\(\{f_n^{(k)}(x_{k+1})\}\)</span><spanclass="math inline">\(\{f_n^{(k+1)}(x_{k+1})\}\)</span><span class="math inline">\(\{f_{n}^{(k)}\}\)</span>.  <spanclass="math inline">\(g_n=f_n^{(n)}\)</span> <spanclass="math inline">\(x_k \in I\)</span><spanclass="math inline">\(\{g_n(x_k)\}=\{f_n^{(n)}(x_k)\}\)</span>. <span class="math inline">\(\{g_n\}\)</span> <spanclass="math inline">\(I\)</span>.</p><p> <spanclass="math inline">\(n_0\)</span> <spanclass="math inline">\(\varepsilon_{n_0}-\)</span> <spanclass="math inline">\(\{\xi_1,\cdots ,\xi_k\}\)</span> <spanclass="math inline">\(\{g_n(\xi_i)\}\)</span>. <span class="math inline">\(\{g_n\}\)</span> <spanclass="math inline">\(D\)</span> <spanclass="math inline">\(\mathcal{F}\)</span>.</p><p> <strong></strong><spanclass="math inline">\(D=[a,b], X=C[a,b]\)</span>$ X $<span class="math inline">\(\lvert f(x) \rvert \leqslant M, \forall x,\forall f\)</span>.  $X X, f _{a}^{x} f(t) t $ <spanclass="math inline">\(\lvert \varphi(f)(x) \rvert \leqslantM(b-a)\)</span> <spanclass="math inline">\(\varphi(\mathcal{F})\)</span>.$(f)(x)-(f)(y) M x-y $  <spanclass="math inline">\(\varphi(\mathcal{F})\)</span>.Ascoli-Arzela<spanclass="math inline">\(\varphi(\mathcal{F})\)</span><spanclass="math inline">\(\varphi\)</span><strong></strong>Ascoli-Arzela.</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (1)</title>
    <link href="/2022/04/28/Neuronal-Dynamics-1/"/>
    <url>/2022/04/28/Neuronal-Dynamics-1/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction-neurons-and-mathematics">Introduction: Neurons andMathematics</h1><p>The online version of the book:</p><hr /><p>Neuronal Dynamics - a neuroscience textbook by Wulfram Gerstner,Werner M. Kistler, Richard Naud and Liam Paninskihttps://neuronaldynamics.epfl.ch/index.html</p><hr /><h2 id="elements-of-neuronal-systems">Elements of Neuronal Systems</h2><h3 id="the-ideal-spiking-neuron">The Ideal Spiking Neuron</h3><p>dendrites(), soma(), axon() synapse() &gt; synapseslocate in dendrites.</p><h3 id="spike-trains">Spike Trains</h3><p>action potentials or spikes, have an amplitude of about 100mV andtypically a durationn of 1-2 ms.</p><p>Spike train: a chain of action potentials emitted by a single neuron,a sequence of stereotyped events which occur at regular or irregularintervals.</p><p>The number and the timing of spikes matters.</p><p>Action potentials in a spike train are ususally well seperated.</p><h3 id="synapses">Synapses</h3><p>synaptic cleft</p><p>chemical synapses, electrical synapses(gap juntions),</p><h3 id="neurons-are-part-of-a-big-system">Neurons are part of a bigsystem</h3><p>neuron's receptive field(): the limited zone where a neuron issensitive to stimuli.(out of brain)</p><h2 id="elements-of-neuronal-dynamics">Elements of NeuronalDynamics</h2><p>membrane potential synapse: excitatory(the change is positive),inhibitory(the change is negative)</p><p>At rest, the cell membrane has already a strongly negativepolarization of about -65mV.</p><p>An input at an excitatory synapse reduces the negative polarizationof the membrane and is therefore called depolarizing. An input theincrease the negative polarization of the membrane even further iscalled hyperpolarizing.</p><h3 id="postsynaptic-potentials">Postsynaptic Potentials</h3><p>postsynaptic potential(PSP), excitatory postsynaptic potential(EPSP),inhibitory postsynaptic potential(IPSP)</p><p>The membrane potential responds linearly to input spikes. Linearitybreaks dwon if too many input spikes arrive during a short interval.</p><p><strong>spike-afterpotential</strong>: afer the pulse the membranepotential does not directly return to the resting potential, but passes,for many neuron types, through a phase of hyperpolarization below theresting value.</p><p>About 20-50 presynaptic spikes have to arrive within a short timewindow to trigger a postsynaptic action potential.</p><blockquote><p>when the voltage hits the threshold, the neuron can enter a state ofrefractoriness.</p></blockquote><h2 id="integrate-and-fire-models">Integrate-And-Fire Models</h2><p>The moment of threshold crossing defines the firing time <spanclass="math inline">\(t_i^{(f)}\)</span>.</p><p>'Leaky-Integrate-and-Fire' Model</p><h3 id="integration-of-inputs">Integration of Inputs</h3><p>The equation of a passive membrane: <span class="math display">\[    \tau_m \frac{\mathrm{d}u}{\mathrm{d}t}=-[u(t)-u_{rest}]+RI(t)\tag{1.5}\]</span> We refer to <span class="math inline">\(u\)</span> as themembrane potential and to <span class="math inline">\(\tau_m\)</span> asthe <strong>membrane time constant</strong> of the neuron. The aboveequation is called the equation of a passive membrane.</p><p>The solution of the differential equation with initial condition<span class="math inline">\(u(t_0)=u_{rest}+\Delta u\)</span> and $I(t)=0 $ is <span class="math display">\[    u(t)-u_{rest}=\Delta u \exp(-\frac{t-t_0}{\tau_m})\quad \text{for}\quad t&gt;t_0\]</span></p><p>This indicates that, in the absence fo input, the membrane potentialdecays exponentiallly to its resting value. The membrane time constant<span class="math inline">\(\tau_m =RC\)</span> is the characteristictime of the decay. For a typical neuron it is in the range of 10ms, andhence rather long compared to the duration of a spike which is on theorder of 1ms.</p><h3 id="pulse-input">Pulse Input</h3><p>Suppose that the passive membrane is stimulated by a constant inputcurrent <span class="math inline">\(I(t)=I_0\)</span> which starts at<span class="math inline">\(t=0\)</span> and ends at time <spanclass="math inline">\(t=\Delta\)</span>. For the sake of simplicity weassume that the membrane potential at time <spanclass="math inline">\(t=0\)</span> is at its resting value <spanclass="math inline">\(u(0)=u_{rest}\)</span>.</p><p>Now, the solution to the differential equation for <spanclass="math inline">\(0&lt;t&lt;\Delta\)</span> is <spanclass="math display">\[u(t)=u_{rest}+RI_0[1-\exp(-\frac{t}{\tau_m})]\]</span></p><p>If the input current never stopped, the mambrane potential wouldapproach for <span class="math inline">\(t\to \infty\)</span> theasymptotic value <spanclass="math inline">\(u(\infty)=u_{rest}+RI_0\)</span>. Once a steadystate is reached, the charge on the capacitor no longer changes. Allinput current must then flow thorugh the resistor. The steady-statevoltage at the resistor is therefore <spanclass="math inline">\(RI_0\)</span> so that the total membrane voltageis <span class="math inline">\(u_{rest}+RI_0\)</span>.</p><h4 id="example-short-pulses-and-the-dirac-function">Example: Shortpulses and the Dirac $ $ function</h4><p>For pulse duration <span class="math inline">\(\Delta \ll\tau_m\)</span>, expand the exponential term into a Taylor series, wefind:</p><p><span class="math display">\[u(\Delta)=u_{rest}+RI_0\frac{\Delta}{\tau_m}\]</span></p><p>Namely, the voltage deflection depends linearly on the amplitude andthe duration of the pulse. As long as <spanclass="math inline">\(q=I_0\Delta\)</span> stay the same, the voltagechange induced by a short current pulse is always the same, whenever theduration of the pulse <span class="math inline">\(\Delta\)</span> ismuch shorter than the time constant <spanclass="math inline">\(\tau_m\)</span>. Thus, the exact duration of thepulse is irrelevant, as long as it is short enough.</p><p>We no longer have to worry about the time course of the membranepotential during the application of the current pulse: the membranepotential simply jumps at time <span class="math inline">\(t=0\)</span>by an amount <span class="math inline">\(q/C\)</span>.</p><p>In conclusion, the solution of <span class="math display">\[        \tau_{m}\frac{\mathrm{d}u}{\mathrm{d}t}=-[u(t)-u_{rest}]+Rq\delta(t)    \]</span> is <span class="math inline">\(u(t)=u_{rest}\)</span> for<span class="math inline">\(t\leqslant 0\)</span> and given by <spanclass="math display">\[        u(t)-u_{rest}=q \frac{R}{\tau_m}\exp (-\frac{t}{\tau_m}) \quad\text{for} \quad t&gt;0    \]</span></p><p>We call it <strong>the impulse-response function or Green's functionof the linear differential equation</strong>.</p><h3 id="nonlinear-integrate-and-fire-model">Nonlinear Integrate-and-fireModel</h3><p><span class="math display">\[    \tau \frac{\mathrm{d}u}{\mathrm{d}t}=F(u)+RI(t)\]</span></p><p>where <span class="math inline">\(F(u)\)</span> can be a non-linearfunction. For example, <spanclass="math inline">\(F(u)=c_2(u-c_1)^{2}+c_0\)</span> (the quadraticintegrate-and-fire models) or <spanclass="math inline">\(F(u)=-(u-u_{rest})+c_0\exp (u-\theta)\)</span>(the exponential integrate-and-fire model)</p><h3 id="the-threshold-for-spike-firing">The Threshold for SpikeFiring</h3><p><strong>firing time</strong>: the moment when a given neuron emits anaction potential <span class="math inline">\(t ^{(f)}\)</span>. In theleaky-and-fire model is defined as <span class="math display">\[    t^{(f)}: u(t^{(f)})=\theta\]</span> immediately after <span class="math inline">\(t^{(f)}\)</span>the potential is reset to a new value <spanclass="math inline">\(u_r&lt;\theta\)</span> <spanclass="math display">\[    \lim_{\delta \to 0;\delta&gt;0} u(t^{(f)}+\delta)=u_r\]</span> <span class="math inline">\(u_r\)</span>refers to thespike-afterpotential</p><p>For <span class="math inline">\(t&gt; t^{(f)}\)</span>, the dynamicsis as usual until the next threshold crossing occurs.</p><p>The combination of leaky integration and reset defines the leakyintegrate-and-fire model.</p><p><strong>Q</strong>: What's the meaning of the notes under Fig. 1.9"Units of input current are chosen so that <spanclass="math inline">\(I_0=1\)</span> corresponds to a trajectory thatreaches the threshold for <span class="math inline">\(t \in\infty\)</span>"</p><h3 id="time-dependent-input">Time-dependent Input</h3><p>This subsection study a leaky integrate-and-fire model which isdriven by an arbitrary time-dependent input current <spanclass="math inline">\(I(t)\)</span>;.</p><p><strong>spike train of a neuron <spanclass="math inline">\(i\)</span></strong> <span class="math display">\[    S_i(t)=\sum_{f}^{} \delta(t-t_i^{(f)})  \tag{1.14}\]</span></p><p>In the absence of a threshold, the linear differential equation (1.5)has a solution <span class="math display">\[    u(t)=u_{rest}+\frac{R}{\tau_m}\int_{0}^{\infty} \exp(-\frac{s}{\tau_m})I(t-s) \mathrm{d}s \tag{1.15}\]</span> (Check ODE by Ding Tongren p33) where <spanclass="math inline">\(I(t)\)</span> is an arbitrary input current and<span class="math inline">\(\tau_m=RC\)</span> is the membrane timeconstant. Assume here that the input curreent is defined for a long timeback into the past: <span class="math inline">\(t \to -\infty\)</span>to avoid initial condition.</p><p>Consider adding a threshold <spanclass="math inline">\(\theta\)</span> to (1.15). The reset of thepotential corresponds to removing a charge <spanclass="math inline">\(q_r=C(\theta-u_r)\)</span> from the capacitor/adding a negative charge <span class="math inline">\(-q_r\)</span> ontothe capacitor. Therefore, the reset corresponds to a short current pulse<span class="math inline">\(I_r=-q_r\delta(t-t^{(f)})\)</span> at themoment of the firing <span class="math inline">\(t^{(f)}\)</span>. Thereset current is <span class="math display">\[    I_r=-q_r \sum_{f}^{} \delta(t-t^{(f)})=-C(\theta-u_r)S(t)\]</span> where <span class="math inline">\(S(t)\)</span> denotes thespike train.</p><p>The total current <span class="math inline">\(I(t)+I_r(t)\)</span>,consisting of the stimulating current and the reset current. The finalresult <span class="math display">\[    u(t)=u_{rest}+\sum_{f}^{} (u_r-\theta)\exp(-\frac{t-t^{(f)}}{\tau_m})+\frac{R}{\tau_m}\int_{0}^{\infty} \exp(-\frac{s}{\tau_m})I(t-s) \mathrm{d}s\]</span> (1.17) where the firing times <spanclass="math inline">\(t^{(f)}\)</span> are defined by the thresholdcondition <span class="math display">\[    t^{(f)}=\{t|u(t)=\theta\} \tag{1.18}\]</span> &gt; The second term of (1.17) describes the ' discharging' ofneurons at the moment of the firing/ the effect of the dischargingcurrent pulses at the moment of the reset. &gt; The last term decribesthe sum of the impulse response functions since the electricity existsat ant moment.</p><h3id="linear-differential-equation-vs.-linear-filter-two-equivalent-pictures">LinearDifferential Equation vs. Linear Filter: Two Equivalent Pictures</h3><p>Rewrite the solution (1.17) in the form <span class="math display">\[    u(t)=\int_{0}^{\infty} \eta(s)S(t-s) \mathrm{d}s+\int_{0}^{\infty}\kappa(s)I(t-s) \mathrm{d}s  \tag{1.22}\]</span> where the filter <span class="math inline">\(\displaystyle\eta(s)=(u_r-\theta)\exp (-\frac{s}{\tau_m})\)</span> , <spanclass="math inline">\(\displaystyle \kappa(s)=\frac{1}{C}\exp(-\frac{s}{\tau_m})\)</span> and <span class="math display">\[    S_i(t)=\sum_{f}^{} \delta(t-t_i^{(f)})\]</span></p><blockquote><p>(1.22) is much more general than the leaky integrate-and-fire model,because the filters do not need to be expoenntial but could have anyarbitrary shape.</p></blockquote><blockquote><p>The <strong>filter</strong> <span class="math inline">\(\eta\)</span>describes the reset of the membrane potential and, more generally,accounts for <strong>neuronal refractoriness</strong>. The<strong>filter</strong> <span class="math inline">\(\kappa\)</span>summarizes the linear electrical properties of the membrane.</p></blockquote><blockquote><p>(1.22) also tells us how to write a sum to the form of convolution,which relys on the propertiy of <spanclass="math inline">\(\delta\)</span> function.</p></blockquote><p><strong>Q</strong>: What is a filter?</p><h3 id="periodic-drive-and-fourier-transform">Periodic drive and Fouriertransform</h3><p>recall <span class="math display">\[    \hat{f}(\omega)=\int_{-\infty}^{\infty} f(t)\mathrm{e}^{-i\omegat}  \mathrm{d}t=\left\vert \hat{f}(\omega) \right\vert \mathrm{e}^{i\phi_f(\omega)}\]</span> where <span class="math inline">\(\left\vert \hat{f}(\omega)\right\vert\)</span> and <spanclass="math inline">\(\phi_f(\omega)\)</span> are called<strong>amplitude</strong> and <strong>phase</strong> of the Fouriertransform at frequency <span class="math inline">\(\omega\)</span>.</p><p>Consider a system <span class="math display">\[    u(t)=\int_{-\infty}^{\infty} \kappa(s)I(t-s) \mathrm{d}s\]</span> then we have <spanclass="math inline">\(\hat{u}(\omega)=\hat{\kappa}(\omega)\hat{I}(\omega)\)</span></p><p>In the above system, let <span class="math display">\[    I(t)=I_0\mathrm{e}^{i \omega t}\]</span> we focus on the real part. If the input is periodic atfrequency <span class="math inline">\(\omega\)</span> the output <spanclass="math display">\[    u(t)= \biggl[ \int_{-\infty}^{\infty} \kappa(s)\mathrm{e}^{-i\omegas}  \mathrm{d}s \biggr] I_0 \mathrm{e}^{i\omega t} \tag{1.27}\]</span> write <span class="math inline">\(u(t)=u_0 \mathrm{e}^{i\phi_k(\omega)+i\omega t}\)</span>. We have <span class="math display">\[    \frac{u_0}{I_0}=\left\vert \hat{\kappa}(\omega) \right\vert\tag{1.28}\]</span></p><p>calculate $() $ <span class="math display">\[    \left\vert \hat{\kappa}(\omega) \right\vert=\frac{1}{C}\left\vert\frac{\tau_m}{1+i\omega \tau_m} \right\vert  \]</span> Therefore the amplitude of the response to a periodic inputdecreases at high frequencies.</p><h2 id="limitations-of-the-leaky-integrate-and-fire-model">Limitationsof the Leaky Integrate-and-Fire Model</h2><h3 id="adaption-bursting-and-inhibitory-rebound">Adaption, Bursting,and Inhibitory Rebound</h3><p><strong>Adaption</strong>: Most neurons will respond to the currentstep with a spike train where intervals between spikes increasesuccessively until a steady state of periodic firing is reached.</p><p><strong>Fast-spiking neurons</strong>: neurons showing no adaption can be well approximated by non-adapting integrate-and-fire models. &gt;Many inhibitory neurons are fast-spiking neurons.</p><p><strong>Bursting and stuttering neurons</strong>: respond to constantstimulation by a sequence of spikes that is periodically (bursting) oraperiodically (stuttering) interrupted by rather long interals.</p><blockquote><p>changing 'filters' in (1.22), we can describe bursting.</p></blockquote><p><strong>Post-inhibitory rebound</strong>: when an inhibitory input isswitched off, many neurons respond with one or more 'rebound spikes',even the release of inhibition can trigger action potentials.</p><h3 id="shunting-inhibition-and-reversal-potential">Shunting Inhibitionand Reversal Potential</h3><p>postsynaptic current, PSC <span class="math display">\[    \text{PSC} \quad \propto \quad [u_0-E_{syn}]\]</span> where <span class="math inline">\(u_0\)</span> is the membranepotential and <span class="math inline">\(E_{syn}\)</span> is the'reversal potential' of the synapse.</p><blockquote><center>Ex: Shunting Inhibition</center></blockquote><p><strong>Q</strong>: I'm not clear about shunting inhibition</p><h3 id="conductance-changes-after-a-spike">Conductance Changes after aSpike</h3><p>The shape of the postsynaptic potentials does not only depend on thelevel of depolarization but also on the internal state of the neuron,e.g., on the timing relative to previous action potentials.</p><h3 id="spatial-structure">Spatial Structure</h3><p>The form of postsynaptic potentials also depends on the location ofthe synapse on the dendritic tree.</p><p><strong>Fig.1.12</strong> shows that a presynaptic spike that arrivesat time <span class="math inline">\(t_j^{(f)}\)</span> shortly after thespike of the postsynaptic neuron has a smaller effect than a spike thatarrives much later.</p><p>In the situation of successive inputs, the first input will causelocal changes of the membrane potential, changing the response of spikesthat arrive later.</p><p><strong>hot spots</strong>: small regions on the dendrite where astrong nonlinear boosting of synaptic currents occurs. The boosting canlead to dendritic spikes which last longer (tens of milliseconds).</p><h2 id="what-can-we-expect-from-integrate-and-fire-models">What Can WeExpect from Integrate-And-Fire Models?</h2><p>By adding adaptation and refractoriness to the neuron model, theprediction of leaky integrate-and-fire model works surprisinglywell.</p><p>The first way to add adaptation is: after each spike the threshold<span class="math inline">\(\theta\)</span> is increased by an amount<span class="math inline">\(\theta\)</span>, while during a quiescentperiod the threshold approaches its stationary value <spanclass="math inline">\(\theta_0\)</span>. Use the Dirac <spanclass="math inline">\(\delta\)</span>-function to express this idea<span class="math display">\[    \tau_{\text{adapt}}\frac{\mathrm{d}}{\mathrm{d}t}\theta(t)=-[\theta(t)-\theta_0]+\theta\sum_{f}^{} \delta(t-t^{(f)}) \tag{1.34}\]</span> where <span class="math inline">\(\tau_{\text{adapt}}\)</span>is the time constant of adaptation (a few hundred milliseconds) and$t<sup>{(f)}=t</sup>{(1)},t^{(2)}$ are the firing times of theneuron.</p><h2 id="summary">Summary</h2><p>The whole paragraph is valuable. For me, the most important sentenceis: The simple leaky integrate-and-fire model does not account for<strong>long-lasting refractoriness</strong> or<strong>adaptation</strong>.</p><h3 id="exercise">Exercise</h3><p><strong>Synaptic current pulse</strong> Synaptic inputs can beapproximated by an exponential current <spanclass="math inline">\(\displaystyle I(t)=\frac{q}{\tau_s}\exp[-\frac{t-t^{(f)}}{\tau_s}]\)</span> for <spanclass="math inline">\(t&gt;t^{(f)}\)</span> where <spanclass="math inline">\(t^{(f)}\)</span> is the moment when the spikearrives at the synapse.</p><ol type="a"><li><p>Calculate the response of a passive membrane with time constant<span class="math inline">\(\tau_m\)</span> to an input spike arrivingat time <span class="math inline">\(t^{(f)}\)</span>.</p></li><li><p>In the solution resulting from (a), take the limit <spanclass="math inline">\(\tau_s \to \tau_m\)</span> and show that in thislimit the response is proportional to <spanclass="math inline">\(\propto [t-t^{(f)}] \exp[-\frac{t-t^{(f)}}{\tau_s}]\)</span>. A function of this form issometimes called an <spanclass="math inline">\(\alpha\)</span>-function.</p></li><li><p>In the solution resulting from (a), take the limit <spanclass="math inline">\(\tau_s \to 0\)</span>. Can you relate your resultto the discussion of the Dirac- <spanclass="math inline">\(\delta\)</span> function?</p></li></ol><p><strong>Solution</strong> (a) <span class="math display">\[    \begin{aligned}        u(t)&amp;=u_{rest}+\exp{[-\frac{t}{\tau_m}]} \int_{t^{(f)}}^{t}\frac{qR}{\tau_m \tau_s} \exp[\frac{t^{(f)}}{\tau_s}+\frac{t}{\tau_m}-\frac{t}{\tau_s}]\mathrm{d}t \\        &amp;= u_{rest}+\frac{qR}{\tau_s-\tau_m}[\exp(\frac{t^{(f)}-t}{\tau_s})-\exp (\frac{t^{(f)}-t}{\tau_m}) ]\quad(t&gt;t^{(f)})   \\    \end{aligned}\]</span></p><ol start="2" type="a"><li>Verifying the equicontinuity, we can exchange the order oflimitation and integration. <span class="math display">\[\lim_{\tau_s \to \tau_m}u(t)=u_{rest}+\frac{qR}{\tau_m^2}[t-t^{(f)}]\exp[-\frac{t-t^{(f)}}{\tau_m}]\]</span></li></ol><p><img src="img/neu_dyn/2022-05-08-10-50-17.png" /></p><ol start="3" type="a"><li><span class="math display">\[\lim_{\tau_s \to 0^{+}}u(t)=u_{rest}+\frac{qR}{\tau_m}\exp(-\frac{t-t^{(f)}}{\tau_m}) \quad (t&gt;t^{(f)})\]</span></li></ol><p>It's exact the solution of the Dirac-<spanclass="math inline">\(\delta\)</span> function. That's because <spanclass="math display">\[    \int_{t^{(f)}}^{\infty} I(t) \mathrm{d}t=q\]</span> and <span class="math display">\[    \lim_{\tau_s \to 0^{+}} \frac{I(t)}{q} =\delta(t-t^{(f)})\]</span></p><p><strong>Time-dependent solution</strong> Show that (1.15) is asolution of (1.5) for time-dependent input <spanclass="math inline">\(I(t)\)</span>.</p><p><strong>Chain of linear equations</strong> Suppose that arrival of aspike at time <span class="math inline">\(t^{(f)}\)</span> releasesneurotransmitter into the synaptic cleft. The amount of availableneurotransmitter at time <span class="math inline">\(t\)</span> is <spanclass="math inline">\(\displaystyle \tau_x\frac{\mathrm{d}x}{\mathrm{d}t}=-x+\delta(t-t^{(f)})\)</span>. Theneurotransmitter binds to the postsynaptic membrane and opens channelsthat enable a synaptic current <span class="math inline">\(\displaystyle\tau_s \frac{\mathrm{d}I}{\mathrm{d}t}=-I+I_0 x(t)\)</span>. Finally,the current charges the postsynaptic membrane according to <spanclass="math inline">\(\displaystyle \tau_m\frac{\mathrm{d}u}{\mathrm{d}t}=-u+RI(t)\)</span>. Write the voltageresponse to a single current pulse as an integral.</p><p><strong>solution</strong> I'm not very sure. I omited some constantsin calculating.</p><p><span class="math display">\[    u=\frac{\tau_x^{2}}{(\tau_x-\tau_s)(\tau_x-\tau_m)}I_0R \exp(-\frac{t-t^{(f)}}{\tau_x})\]</span></p>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>7</title>
    <link href="/2022/04/19/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%887%EF%BC%89/"/>
    <url>/2022/04/19/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%887%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><h2 id=""></h2><p>. <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y)\]</span>  <span class="math inline">\(f(x, y)\)</span> <spanclass="math inline">\(G \in\mathbb{R}^{2}\)</span><strong></strong></p><p> <span class="math display">\[    (E): \quad \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y),\quad y(x_0)=y_0\]</span></p><p> <span class="math inline">\(y=y(x)\)</span> <spanclass="math inline">\(x_0\)</span><strong></strong></p><p><strong></strong><strong></strong><strong></strong><strong></strong>.</p><p><strong>7.1</strong> <span class="math inline">\(f(x,y)\)</span> <span class="math display">\[    R: \quad \left\vert x-x_0 \right\vert &lt;\alpha,\quad \left\verty-y_0 \right\vert &lt; \beta\]</span>  <span class="math inline">\((x-x_0)\)</span><spanclass="math inline">\((y-y_0)\)</span><span class="math inline">\(M&gt;0\)</span> <spanclass="math display">\[    F(x, y)=\frac{M}{(1-\displaystyle \frac{x-x_0}{a})(1-\displaystyle\frac{y-y_0}{b})} \tag{1}\]</span>  <span class="math display">\[    R_0: \quad \left\vert x-x_0 \right\vert &lt;a, \quad \left\verty-y_0 \right\vert &lt;b\]</span>  <span class="math inline">\(f(x,y)\)</span> <span class="math inline">\(a&lt;\alpha\)</span> <span class="math inline">\(b&lt;\beta\)</span>.</p><p><strong></strong> <span class="math inline">\(F (x,y)\)</span> <span class="math inline">\(M\)</span>.</p><p><strong>7.2</strong> <spanclass="math inline">\(R_0\)</span>1 <spanclass="math inline">\(F(x, y)\)</span> <spanclass="math display">\[    (\hat{E}): \quad \frac{\mathrm{d}y}{\mathrm{d}x}=F(x, y), \quady(x_0)=y_0      \]</span>  <span class="math inline">\(\left\vert x-x_0\right\vert &lt;\rho\)</span> <spanclass="math inline">\(y=\hat{y}(x)\)</span> <spanclass="math inline">\(\rho=a(1-\mathrm{e}^{-b/2aM} )\)</span><span class="math inline">\(a,b\)</span> <spanclass="math inline">\(M\)</span>.</p><p><strong></strong> <spanclass="math inline">\((\hat{E})\)</span> <spanclass="math inline">\(y=\hat{y}(x)\)</span> <spanclass="math inline">\(\hat{y}(x)\)</span> <spanclass="math inline">\(\displaystyle \ln(1-\frac{x-x_0}{a})\)</span> <spanclass="math inline">\(\sqrt{1+\cdot}\)</span> <spanclass="math inline">\(\hat{y}(x)\)</span> <spanclass="math inline">\((\hat{E})\)</span>.</p><p><strong>7.1</strong>Cauchy <spanclass="math inline">\(f(x, y)\)</span> <spanclass="math inline">\(R\)</span> <spanclass="math inline">\((x-x_0)\)</span> <spanclass="math inline">\((y-y_0)\)</span><span class="math inline">\((E)\)</span> <spanclass="math inline">\(x_0\)</span> <spanclass="math inline">\(\left\vert x-x_0 \right\vert&lt;\rho\)</span> <spanclass="math inline">\(y=y(x)\)</span>.  <spanclass="math inline">\(R\)</span> <spanclass="math inline">\(\rho\)</span>.</p><p><strong></strong> <spanclass="math inline">\(f(x,y)\)</span> <span class="math display">\[    f(x, y)=\sum_{i,j=0}^{\infty} a_{ij}(x-x_0)^{i}(y-y_0)^{j} \tag{2}\]</span>  <span class="math display">\[    y=y_0+\sum_{n=1}^{\infty} C_n(x-x_0)^{n} \tag{3}\]</span>  <span class="math inline">\(C_n\)</span><span class="math inline">\(a_{ij}\)</span> <spanclass="math inline">\(P_n\)</span><spanclass="math inline">\(P_n\)</span><strong></strong> <spanclass="math inline">\(P_n\)</span> <span class="math inline">\(f(x,y)\)</span>.3.</p><p> <span class="math display">\[    (\hat{E}): \quad \frac{\mathrm{d}y}{\mathrm{d}x}=F(x, y), \quady(x_0)=y_0      \]</span>  <span class="math inline">\(F(x, y)\)</span> <spanclass="math inline">\(a,b,M\)</span>.  <spanclass="math inline">\(R_0\)</span> <span class="math inline">\(f(x,y)\)</span>.  <spanclass="math inline">\((\hat{E})\)</span> <spanclass="math display">\[    y=\hat{y}(x)=y_0+\sum_{n=1}^{\infty} \hat{C_n}(x-x_0)^{n}\]</span>  $C_n $.</p><div class="note note-success">            <p>. <spanclass="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=\frac{y-x}{x}, y(0)=0\]</span></p>          </div><div class="note note-success">            <p>. <span class="math display">\[    x^{2}\frac{\mathrm{d}y}{\mathrm{d}x}=y-x, \quad y(0)=0\]</span>  <span class="math display">\[    y=x+x^{2}+2!x^{3}+\cdots +n! x^{n+1}+\cdots\]</span>  <span class="math inline">\(x \neq0\)</span>.</p>          </div><p>.</p><p>Cauchy  <spanclass="math display">\[    \frac{\mathrm{d}y^{(k)}}{\mathrm{d}x}=f_k(x,y^{(1)},\cdots,y^{(n)}), \quad y^{(k)}(x_0)=y_k\]</span> <span class="math inline">\((k=1,2,\cdots ,n)\)</span>. <span class="math inline">\(f_k\)</span> <spanclass="math display">\[    \left\vert x-x_0 \right\vert \leqslant \alpha, \quad \left\verty^{(1)}-y_1 \right\vert \leqslant \beta,\quad \cdots ,\quad \left\verty^{(n)}-y_n \right\vert \leqslant \beta      \]</span>  <span class="math inline">\(x-x_0\)</span><spanclass="math inline">\(y^{(k)}-y_k\)</span><span class="math inline">\(x_0\)</span> <spanclass="math inline">\(\left\vert x-x_0 \right\vert&lt;\rho\)</span> <spanclass="math inline">\(y^{(k)}=y^{(k)}(x)\)</span>. <span class="math display">\[    \rho=a(1-\mathrm{e}^{-b/2aM} )\]</span>  <span class="math inline">\(M\)</span>.</p><h2 id="-1"></h2><p> <span class="math display">\[    A(x)y&#39;&#39;+B(x)y&#39;+C(x)y=0 \tag{4}\]</span>  <span class="math inline">\(A(x),B(x)\)</span> <spanclass="math inline">\(C(x)\)</span> $x-x_0 &lt;r $ <span class="math inline">\((x-x_0)\)</span>.- <span class="math inline">\(A(x_0) \neq 0\)</span>.  <spanclass="math inline">\(x_0\)</span>4<strong></strong>. <span class="math display">\[    y&#39;&#39;+p(x)y&#39;+q(x)y=0 \tag{5}\]</span></p><ul><li><span class="math inline">\(A(x_0)=0\)</span> <spanclass="math inline">\(x_0\)</span>4<strong></strong>.</li></ul><p><strong>7.2</strong>5 <spanclass="math inline">\(p(x)\)</span> <spanclass="math inline">\(q(x)\)</span> $x-x_0 &lt;r $ <spanclass="math inline">\((x-x_0)\)</span>. 5<span class="math inline">\(\left\vert x-x_0 \right\vert&lt;r\)</span> <span class="math display">\[    y=\sum_{n=0}^{\infty} C_n(x-x_0)^{n}\]</span>  <span class="math inline">\(C_0=y_0\)</span> <spanclass="math inline">\(C_1=y_0&#39;\)</span>.</p><p><strong></strong>Cauchy.</p><h2 id="legendre-">Legendre </h2><p>Legendre <span class="math display">\[    (1-x^{2})y&#39;&#39;-2xy&#39;+n(n+1)y=0 \tag{6}\]</span>  <span class="math display">\[    y=C_0y_1(x)+C_1y_2(x), \quad (-1&lt;x&lt;1) \tag{7}\]</span>  <span class="math inline">\(C_0\)</span> <spanclass="math inline">\(C_1\)</span> <spanclass="math display">\[    y_1(x)=1-\frac{n(n+1)}{2!}x^{2}+\frac{(n-2)n(n+1)(n+3)}{4!}x^{4}-\cdots\]</span> <span class="math display">\[    y_2(x)=x-\frac{(n-1)(n+2)}{3!}x^{3}+\frac{(n+3)(n-1)(n+2)(n+4)}{5!}x^{5}-\cdots\]</span>  <span class="math inline">\(y_1(x)\)</span> <spanclass="math inline">\(y_2(x)\)</span>.  <spanclass="math inline">\(n\)</span>, <spanclass="math inline">\(y_1(x)=P_n(x)\)</span> <spanclass="math inline">\(n\)</span> <spanclass="math inline">\(y_2(x)\)</span> <spanclass="math inline">\(n\)</span> <spanclass="math inline">\(y_2(x)=P_n(x)\)</span> <spanclass="math inline">\(n\)</span>. <spanclass="math inline">\(P_n(x)\)</span><span class="math display">\[    P_n(x)=\frac{1}{2^{n}}\sum_{k=0}^{[\frac{n}{2}]}\frac{(-1)^{k}(2n-2k)!}{k!(n-k)!(n-2k)!}x^{n-2k} \tag{8}       \]</span>  <spanclass="math inline">\(n\)</span>8 <spanclass="math inline">\(n\)</span> <spanclass="math inline">\(P_n(x)\)</span>Legendre.<strong>Legendre</strong>.</p><p> <span class="math display">\[    P_n(-x)=(-1)^{n}P_n(x)\]</span> Legendre <span class="math display">\[    P_0(x),P_1(x), P_2(x),\cdots  \tag{9}\]</span>  <strong>1</strong> <spanclass="math inline">\(P_n(x)\)</span>Rodrigues <spanclass="math display">\[    P_n(x)=\frac{1}{2^{n}n!}\frac{\mathrm{d}^{n}}{\mathrm{d}x^{n}}(x^{2}-1)^{n}\tag{10}\]</span>  <span class="math display">\[    P_n(1)=1, \quad P_n(-1)=(-1)^{n} \tag{11}\]</span> <strong></strong>.</p><p><strong>2</strong>Legendre<span class="math display">\[    \int_{-1}^{1} P_n(x)P_m(x) \mathrm{d}x =    \begin{cases}        0,\quad m \neq n \\        \sigma_n&gt;0, \quad m=n    \end{cases}\]</span>  <span class="math inline">\(\sigma_n=\displaystyle\frac{2}{2n+1}\)</span> <strong></strong> <spanclass="math inline">\(n \neqm\)</span>11 <spanclass="math inline">\((P_n,P_m)=0\)</span>.  <spanclass="math inline">\(n=m\)</span><span class="math inline">\(\displaystyle \int_{0}^{\frac{\pi}{2}} \cos^{2n+1}\theta \mathrm{d}\theta\)</span> .</p><p> <span class="math inline">\(f(x)\)</span> <spanclass="math inline">\([-1,1]\)</span> <spanclass="math inline">\(f(x)\)</span> <spanclass="math inline">\(P_n(x)\)</span><strong>Fourier</strong><span class="math display">\[    f(x) \approx \sum_{n=0}^{\infty} a_nP_n(x) \tag{12}\]</span>  <span class="math display">\[    a_n=\frac{2n+1}{2} \int_{-1}^{1} f(x)P_n(x) \mathrm{d}x\]</span> Fourier.</p><p> <spanclass="math inline">\((1-x^{2})^{-1/4}f(x)\)</span> <spanclass="math inline">\(-1\leqslant x\leqslant1\)</span> -<strong>Dirichlet</strong><spanclass="math inline">\(f(x)\)</span> <spanclass="math inline">\(x_0\)</span>.- <strong>Dini</strong> <spanclass="math inline">\(h&gt;0\)</span> <spanclass="math display">\[    \int_{0}^{h} \frac{\left\vert f(x_0+t)-f(x_0+0)+f(x_0-t)-f(x_0-0)\right\vert }{t} \mathrm{d}t\]</span>  - <strong>Holder</strong><spanclass="math inline">\(f(x)\)</span> <spanclass="math inline">\(x_0\)</span> <spanclass="math inline">\(t&gt;0\)</span> <spanclass="math display">\[    \left\vert f(x_0 \pm t)-f(x_0) \right\vert \leqslant Lt ^{\alpha}\]</span>  <span class="math inline">\(L\)</span> <spanclass="math inline">\(\alpha\)</span> <spanclass="math inline">\(\alpha\leqslant 1\)</span></p><p>12 $x=(-1&lt;&lt;1) $ <spanclass="math display">\[    \frac{1}{2}[f(\xi +0)+f(\xi -0)]\]</span>  <span class="math inline">\(f(x)\)</span> <spanclass="math inline">\(x=\xi\)</span> <spanclass="math inline">\(x=\xi\)</span> <spanclass="math inline">\(f(\xi)\)</span>.</p><h2 id=""></h2><p>4.</p><p> <spanclass="math inline">\(x^{2}y&#39;&#39;-2y=0\)</span> <spanclass="math inline">\(x=0\)</span>4<span class="math inline">\(x_0\)</span>.</p><p> $x<sup>{2}y''+xy'+(x</sup>{2}-y)=0$<strong></strong>.</p><p><span class="math display">\[    \sum_{n=0}^{\infty} C_n(x-x_0)^{n+\rho}\quad(C_0 \neq 0)\]</span>  <spanclass="math inline">\(\rho\)</span><strong></strong>.</p><p><strong></strong>4 <spanclass="math display">\[    (x-x_0)^{2}P(x)y&#39;&#39;+(x-x_0)Q(x)y&#39;+R(x)y=0 \tag{13}\]</span>  <span class="math inline">\(P(x)\)</span><spanclass="math inline">\(Q(x)\)</span>  <spanclass="math inline">\(R(x)\)</span> <spanclass="math inline">\(x_0\)</span> <spanclass="math inline">\((x-x_0)\)</span>  <spanclass="math inline">\(P(x)\neq 0\)</span>  <spanclass="math inline">\(Q(x_0)\)</span> <spanclass="math inline">\(R(x_0)\)</span> <spanclass="math inline">\(x_0\)</span>4<strong></strong>.</p><p><strong>7.3</strong>4 <spanclass="math inline">\(x_0\)</span> <spanclass="math display">\[    y=\sum_{k=0}^{\infty} C_k(x-x_0)^{k+\rho} \quad (C_0\neq 0) \tag{14}\]</span>  <span class="math inline">\(\rho\)</span> <spanclass="math inline">\(C_k\)</span>.</p><p><strong></strong> <spanclass="math inline">\(x_0\)</span> <spanclass="math inline">\(\left\vert x-x_0 \right\vert &lt;r\)</span> <span class="math inline">\(Q(x)\)</span> <spanclass="math inline">\(R(x)\)</span> <spanclass="math display">\[    Q(x)=\sum_{k=0}^{\infty} a_k(x-x_0)^{k} \quadR(x)=\sum_{k=0}^{\infty} b_k(x-x_0)^{k}       \]</span> 14.</p><p><strong></strong> <span class="math display">\[    \rho(\rho-1)+a_0\rho+b_0=0\]</span></p><p> <span class="math inline">\(\rho_1\)</span> <spanclass="math inline">\(\rho_2\)</span><strong></strong><span class="math inline">\(\rho_1\geqslant \rho_2\)</span> <spanclass="math inline">\(\rho_1\)</span>.</p><p> $C_1,C_2,,C_k,$.</p><p>14 <spanclass="math inline">\(x_0\)</span> <spanclass="math inline">\(x_0\)</span>.</p><p> <spanclass="math inline">\(0&lt;r_1&lt;r\)</span> <spanclass="math inline">\(M&gt;0\)</span> <spanclass="math inline">\(M\geqslant 1\)</span> $a_k $ $b_k $ $_1a_k+b_k $ <span class="math inline">\(\displaystyle\frac{M}{r_1^{k}}\)</span>.</p><p> $C_k ()^{k} $.</p><p>14 <span class="math inline">\(\displaystyle0&lt;\left\vert x-x_0 \right\vert \leqslant\frac{r_2}{M}\)</span>.</p><div class="note note-success">            <p> <spanclass="math inline">\(\rho_1\)</span>14</p>          </div><div class="note note-success">            <p> <span class="math inline">\(\rho_1\)</span> <spanclass="math inline">\(\rho_2\)</span> <spanclass="math inline">\(\rho_1-\rho_2=m \in\mathbb{N}\)</span> <spanclass="math inline">\(\rho_2\)</span>14.Liouville <spanclass="math inline">\(\rho_1\)</span>14.</p>          </div><p><strong>Bessel</strong> <span class="math display">\[    x^{2}y&#39;&#39;+xy&#39;+(x^{2}-n^{2})y=0 \tag{15}\]</span>  <span class="math inline">\(n\geqslant 0\)</span> <span class="math inline">\(\pm n\)</span> <spanclass="math inline">\(\rho_1=n\)</span> <spanclass="math inline">\(C_0\)</span> <span class="math display">\[    C_0=\frac{1}{2^{n}\Gamma(n+1)}\]</span>  <span class="math display">\[    y=J_n(x)=\sum_{k=0}^{\infty}\frac{(-1)^{k}}{\Gamma(n+k+1)\Gamma(k+1)}(\frac{x}{2})^{2k+n} \tag{16}\]</span><strong>Bessel</strong>.</p><p> <span class="math inline">\(\rho_2=-n\)</span> -  <spanclass="math inline">\(2n\)</span>  <spanclass="math display">\[    y=J_{-n}(x)=\sum_{k=0}^{\infty}\frac{(-1)^{k}}{\Gamma(-n+k+1)\Gamma(k+1)}(\frac{x}{2})^{2k-n} \tag{17}\]</span> <strong>Bessel</strong>. - <spanclass="math inline">\(2n=N\in \mathbb{Z}\)</span> 1 <spanclass="math inline">\(2n=2s+1\)</span> <spanclass="math inline">\(C_{2s+1}=0\)</span>17 2<span class="math inline">\(n \in \mathbb{Z}\)</span>  <spanclass="math inline">\(\rho_2=-n\)</span>. <span class="math inline">\(\alpha \neq n\)</span><span class="math inline">\(\alpha \rightarrow n\)</span> <spanclass="math inline">\(J_{\alpha}(x)\)</span> <spanclass="math inline">\(J_{-\alpha}(x)\)</span> <spanclass="math display">\[    y_{\alpha}(x)=\frac{J_{\alpha}\cos \alpha\pi-J_{-\alpha}(x)}{\sin\alpha\pi} \quad (\sin \alpha\pi\neq 0)\]</span> Bessel <spanclass="math inline">\(n=\alpha\)</span> <spanclass="math inline">\(x=0\)</span>.  <spanclass="math display">\[    Y_{n}(x)=\lim_{\alpha \to n}y_{\alpha}(x)\]</span>  <spanclass="math inline">\(y=Y_{n}(x)\)</span>BesselBessel<spanclass="math inline">\(J_{n}(x)\)</span><strong>Neumann</strong>.</p><p> -  <span class="math inline">\(n \in\mathbb{Z}\)</span> <spanclass="math inline">\(J_{-n}(x)=(-1)^{n}J_n(x)\)</span>. <span class="math inline">\(\Gamma(-n+k+1)\to \infty\)</span> <spanclass="math display">\[    \begin{aligned}        J_{-n}(x)&amp;= \sum_{k=n}^{\infty}\frac{(-1)^{k}}{\Gamma(-n+k+1)\Gamma(k+1)}(\frac{x}{2})^{2k-n} \\        &amp;= \sum_{k=0}^{\infty}\frac{(-1)^{k+n}}{\Gamma(n+k+1)\Gamma(k+1)}(\frac{x}{2})^{2k+n} \\        &amp;= (-1)^{n}J_n(x)    \end{aligned}\]</span> -  <span class="math display">\[    J_{v}J_{-v}&#39;-J_{-v}J_{v}&#39;=-\frac{2\sin v\pi}{\pi x}\]</span> <strong></strong> -  <spanclass="math inline">\(Y_n(x)\)</span>Bessel. <spanclass="math inline">\(Y_n(x)\)</span>Watson(1944), pp. 58~59.</p><h2 id="bessel-">Bessel </h2><p> <span class="math inline">\(n\)</span>.  <spanclass="math inline">\(J_n(x)\)</span> <spanclass="math inline">\(n\)</span><strong>Bessel</strong> <spanclass="math inline">\(Y_n(x)\)</span> <spanclass="math inline">\(n\)</span><strong>Neumann</strong>.</p><p><strong>1</strong> <span class="math inline">\(x \to\infty\)</span> <span class="math inline">\(J_n(x)\)</span> <spanclass="math inline">\(Y_n(x)\)</span><strong></strong><span class="math display">\[    J_n(x)=\frac{A_n}{\sqrt{x}}[\sin (x+\alpha_n)+o(1)] \tag{18}        \]</span>  <span class="math display">\[    Y_n(x)=\frac{B_n}{\sqrt{x}}[\cos (x+\beta_n)+o(1)] \tag{19}\]</span></p><p> <span class="math inline">\(o(1)\)</span><span class="math inline">\(A_n,B_n,\alpha_n,\beta_n\)</span><span class="math inline">\(n\)</span> <spanclass="math inline">\(A_n&gt;0,B_n&gt;0\)</span>.</p><p><strong></strong> <span class="math display">\[    J_n(x)=\frac{u(x)}{\sqrt{x}} \quad (x&gt;0)\]</span> Bessel <spanclass="math display">\[    u&#39;&#39;(x)+  \biggl( 1-\frac{n^{2}-\frac{1}{4}}{x^{2}}\biggr)u(x)=0\]</span>  <span class="math inline">\(u(x)\)</span><span class="math inline">\(u&#39;(x)\)</span> <spanclass="math display">\[    r(x)=\sqrt{[u(x)]^{2}+[u&#39;(x)]^{2}}&gt;0\]</span>  <span class="math inline">\(u(x)\)</span> <spanclass="math inline">\(u&#39;(x)\)</span> <spanclass="math display">\[    \begin{cases}        u(x)=r(x)\sin \theta(x) \\        u&#39;(x)=r(x)\cos \theta(x)        \end{cases}\]</span>  <span class="math inline">\(r(x)\)</span><span class="math inline">\(\theta(x)\)</span>.</p><p><span class="math display">\[    \begin{cases}        r&#39;(x) =\frac{n^{2}-\frac{1}{4}}{x^{2}}r(x)\sin \theta(x)\cos \theta(x) \\        \theta&#39;(x)=1-\frac{n^{2}-\frac{1}{4}}{x^{2}}\sin^{2}\theta(x)    \end{cases}\]</span></p><p> <spanclass="math inline">\(\theta(x)=x+\varphi(x)\)</span>1 <span class="math inline">\(x\)</span> <spanclass="math inline">\(\lim_{x \to\infty}\varphi(x)=\alpha_n\)</span>.  <spanclass="math display">\[    \theta(x)=x+\alpha_n+o(1)\]</span>  <spanclass="math display">\[    \lim_{x \to \infty}r(x)=A_n&gt;0\]</span> .  <span class="math inline">\(r(x)\)</span><span class="math display">\[    r(x)=A_n+o(!)\]</span>  <span class="math display">\[    \begin{cases}        u(x)=A_n\sin (x+\alpha_n) +o(1) \\        u&#39;(x)=A_n\cos (x+\alpha_n) +o(1)            \end{cases}\]</span> ..</p><p>. <spanclass="math inline">\(J_n(x)\)</span> <spanclass="math inline">\(Y_n(x)\)</span><strong></strong>. - <span class="math inline">\(J_0(0)=1\)</span> - <spanclass="math inline">\(J_n(0)=0 \quad n\geqslant 1\)</span> - <spanclass="math inline">\(\lim_{x \to o^{+}}Y_n(x)=-\infty\)</span></p><p> <span class="math inline">\(J_n(x)\)</span> <spanclass="math inline">\(Y_n(x)\)</span>.</p><p> <spanclass="math inline">\(J_n(x)\)</span> <spanclass="math display">\[    0&lt;\beta_1&lt;\beta_2&lt;\cdots &lt;\beta_k&lt;\cdots (\to \infty)\]</span>  <span class="math inline">\(n\)</span>.</p><p> <span class="math inline">\([0,1]\)</span> <spanclass="math display">\[    J_n(\beta_1t),J_n(\beta_2t),\cdots ,J_n(\beta_kt),\cdots \tag{20}\]</span>  <strong>2</strong>20 <spanclass="math inline">\([0,1]\)</span> <spanclass="math inline">\(t\)</span><strong></strong> <spanclass="math display">\[    \int_{0}^{1} t J_n(\beta_jt)J_n(\beta_kt) \mathrm{d}t=    \begin{cases}        0,\quad j\neq k \\        \tau_{n,k}&gt;0,\quad j=k    \end{cases} \tag{21}\]</span>  <spanclass="math inline">\(\tau_{n,k}=\frac{1}{2}[J_n&#39;(\beta_k)]^{2}\)</span></p><p><strong></strong> <span class="math display">\[    u=J_n(at), \quad v=J_n(bt)\]</span> Bessel <spanclass="math inline">\(n^{2}\)</span> <span class="math display">\[    t^{2}(vu&#39;&#39;-uv&#39;&#39;)+t(vu&#39;-u&#39;v)+t^{2}(a^{2}-b^{2})uv=0\]</span>  <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}[t(vu&#39;-uv&#39;)]+t(a^{2}-b^{2})uv=0\]</span>  <spanclass="math inline">\(u(0),v(0),u&#39;(0),v&#39;(0)\)</span><span class="math display">\[    (a^{2}-b^{2})\int_{0}^{1} tuv\mathrm{d}t=bJ_n(a)J_n&#39;(b)-aJ_n(b)J_n&#39;(a) \tag{22}\]</span> 21. 22 <spanclass="math inline">\(b=\beta_k\)</span> <spanclass="math inline">\(a\neq \beta_k\)</span> <spanclass="math inline">\(a\to \beta_k\)</span>L'Hospital21.</p><p> <span class="math inline">\([0,1]\)</span> <spanclass="math inline">\(f(x)\)</span>Bessel <spanclass="math display">\[    f(x) ~ \sum_{k=1}^{\infty} a_kJ_n(\beta_kx) \tag{23}\]</span> Fourier <span class="math display">\[    a_k=\frac{2}{[J_n&#39;(\beta_k)]^{2}}\int_{0}^{1} tf(t)J_n(\beta_kt)\mathrm{d}t\]</span></p><div class="note note-success">            <p> <span class="math inline">\(\sqrt{x}f(x)\)</span> <spanclass="math inline">\(0\leqslant x\leqslant1\)</span>Dirichlet/Dini/HolderFourier<spanclass="math inline">\(x=x_0\)</span><span class="math inline">\(f(x)\)</span> <spanclass="math inline">\(x=x_0\)</span> <spanclass="math inline">\(f(x_0)\)</span></p>          </div>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>6</title>
    <link href="/2022/04/12/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%886%EF%BC%89/"/>
    <url>/2022/04/12/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%886%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><h2 id=""></h2><p> <span class="math inline">\(n\)</span><span class="math display">\[    \frac{\mathrm{d}y_i}{\mathrm{d}x}=\sum_{j=1}^{n} a_{ij}(x)y_j+f_i(x)\quad (i=1,2,\cdots ,n)\]</span>  <span class="math inline">\(a_{ij}(x)\)</span><span class="math inline">\(f_i(x)(i,j=1,2,\cdots ,n)\)</span><span class="math inline">\(a&lt;x&lt;b\)</span>. <span class="math display">\[    \mathbf{A}(x)=(a_{ij}(x))_{n \times n}\]</span>  <span class="math display">\[    \mathbf{y}=\begin{pmatrix}        y_1 \\        y_2 \\        \vdots \\        y_n    \end{pmatrix}    , \quad    \mathbf{f}(x)=\begin{pmatrix}        f_1(x) \\        f_2(x) \\        \vdots \\        f_n(x)    \end{pmatrix}\]</span>  <spanclass="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{A}(x)\mathbf{y}+\mathbf{f}(x)\tag{1}\]</span>  <spanclass="math inline">\(\mathbf{f}(x)\)</span><spanclass="math inline">\((a&lt;x&lt;b)\)</span>1<strong></strong><span class="math inline">\(\mathbf{f}(x)\equiv\mathbf{0}\)</span> <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{A}(x)\mathbf{y}\tag{2}\]</span> <strong></strong>.</p><p> <spanclass="math inline">\(\mathbf{A}(x)\)</span> <spanclass="math inline">\(x\)</span> <spanclass="math inline">\(\mathbf{A}(x)\mathbf{y}\)</span> <spanclass="math inline">\(\mathbf{y}\)</span>Lipschitz.</p><p><strong></strong>1<span class="math display">\[    \mathbf{y}(x_0)=\mathbf{y}_0 \tag{3}\]</span>  <spanclass="math inline">\(\mathbf{y}=\mathbf{y}(x)\)</span> <spanclass="math inline">\(a&lt;x&lt;b\)</span><span class="math inline">\(x_0 \in (a,b)\)</span> <spanclass="math inline">\(\mathbf{y}_0 \in\mathbb{R}^n\)</span>.</p><h3 id=""></h3><p><strong>6.1</strong>.</p><p>2 <spanclass="math inline">\((a,b)\)</span> <spanclass="math inline">\(S\)</span>.  <spanclass="math inline">\(S\)</span>.</p><p><strong>6.2</strong> <spanclass="math inline">\(S\)</span> <spanclass="math inline">\(n\)</span><spanclass="math inline">\(n\)</span>2.</p><p><strong></strong> <spanclass="math inline">\(S\)</span>.</p><p><strong>6.1</strong>2 <spanclass="math inline">\(a&lt;x&lt;b\)</span> <spanclass="math inline">\(n\)</span> <spanclass="math display">\[    \mathbf{\varphi}_1,\mathbf{\varphi}_2,\cdots,\mathbf{\varphi}_n\tag{4}\]</span>  <span class="math display">\[    \mathbf{y}=C_1\mathbf{\varphi}_1(x)+\cdots +C_n\mathbf{\varphi}_n(x)\tag{5}\]</span>  <span class="math inline">\(C_1,\cdots,C_n\)</span>.</p><p><strong></strong>6.2 <spanclass="math inline">\(S\)</span><span class="math inline">\(S\)</span>.</p><p>2 <spanclass="math inline">\(n\)</span><strong></strong>. <span class="math display">\[    \mathbf{y}_1(x),\cdots ,\mathbf{y}_n(x) \tag{6}\]</span> 2 <span class="math inline">\(n\)</span>.</p><p><strong>6.2</strong>6 <spanclass="math display">\[    \mathbf{y}_1(x)=\begin{pmatrix}    y_{11}(x) \\    y_{21}(x) \\    \vdots \\    y_{n1}(x)    \end{pmatrix},    \cdots    \mathbf{y}_n(x)=\begin{pmatrix}    y_{1n}(x) \\    y_{2n}(x) \\    \vdots \\    y_{nn}(x)    \end{pmatrix}\]</span>  <span class="math display">\[    \begin{vmatrix}    y_{11}(x) &amp; y_{12}(x) &amp; \cdots  &amp; y_{1n}(x) \\    \vdots    &amp; \vdots    &amp;         &amp; \vdots \\    y_{n1}(x) &amp; y_{n2}(x) &amp; \cdots  &amp; y_{nn}(x) \\    \end{vmatrix}\]</span> 6<strong>(Wronsky)</strong>.</p><p><strong>6.3</strong>Wronsky<strong>Liouville</strong><span class="math display">\[    W(x)=W(x_0) \mathrm{e}^ {\int_{x_0}^{x} \mathrm{tr} [\mathbf{A}(x)]\mathrm{d}x} \quad (a&lt;x&lt;b) \tag{7}\]</span>  <span class="math inline">\(x_0 \in (a,b)\)</span><strong></strong> <spanclass="math display">\[    \mathbf{W}&#39;=\mathrm{tr}[\mathbf{A}(x)]W\]</span>  <spanclass="math inline">\(W\)</span>.  <spanclass="math inline">\(W\)</span>.</p><p>LiouvilleWronsky <spanclass="math inline">\(W(x)\)</span> <spanclass="math inline">\(a&lt;x&lt;b\)</span>.</p><p><strong>6.2</strong>26<span class="math display">\[    W(x) \neq 0 \quad (a&lt;x&lt;b) \tag{8}\]</span></p><p><strong></strong>6.2.</p><p><strong>6.1</strong>6 <spanclass="math display">\[    W(x) \equiv 0 \quad (a&lt;x&lt;b)\]</span></p><p> <span class="math inline">\(x_0\)</span><span class="math inline">\(W(x_0)=0\)</span>.</p><p>6 <spanclass="math inline">\(\mathbf{Y}(x)=(y_{ij}(x))_{n \timesn}\)</span>2<strong></strong>.  <spanclass="math inline">\(\mathbf{Y}(x)\)</span>2.</p><p>6 <spanclass="math inline">\(\mathbf{Y}(x)\)</span><strong></strong>.2 <spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>6.1<span class="math display">\[    \mathbf{y}=\mathbf{\Phi}(x)\mathbf{c} \tag{9}\]</span>  <span class="math inline">\(\mathbf{c}\)</span> <spanclass="math inline">\(n\)</span>.</p><p><strong>6.2</strong>1 <spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>2<span class="math inline">\(n\)</span> <spanclass="math inline">\(\mathbf{C}\)</span> <spanclass="math display">\[    \mathbf{\Psi}(x)=\mathbf{\Phi}(x) \mathbf{C} \tag{10}\]</span> 2</p><p>2 <span class="math inline">\(\mathbf{\Phi}(x)\)</span> <spanclass="math inline">\(\mathbf{\Psi}(x)\)</span>2<span class="math inline">\(n\)</span> <spanclass="math inline">\(\mathbf{C}\)</span>10.</p><p><strong></strong>.</p><h3 id=""></h3><p><strong>6.4</strong>2 <spanclass="math inline">\(\mathbf{\Phi}(x)\)</span><spanclass="math inline">\(\mathbf{\phi}^*(x)\)</span>11<spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x)\)</span><span class="math display">\[    \mathbf{\varphi}(x)=\mathbf{\Phi}(x)\mathbf{c}+\mathbf{\varphi}^*(x)\]</span>  <span class="math inline">\(\mathbf{c}\)</span><spanclass="math inline">\(\mathbf{\varphi}(x)\)</span>.</p><p><strong></strong> <spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>.</p><p> <span class="math display">\[    \mathbf{\varphi}^*(x)=\mathbf{\Phi}(x)\int_{x_0}^{x}\mathbf{\Phi}^{-1}(s)\mathbf{f}(s) \mathrm{d}s \tag{11}\]</span> 1.</p><p><strong>6.5</strong> <spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>2111.</p><p><strong>6.3</strong>1 <spanclass="math display">\[    \mathbf{y}=\mathbf{\Phi}(x) \biggl( \mathbf{c}+\int_{x_0}^{x}\mathbf{\Phi}^{-1}(s)\mathbf{f}(s) \mathrm{d}s \biggr) \tag{12}\]</span>  <span class="math inline">\(\mathbf{c}\)</span> <spanclass="math inline">\(n\)</span>1<span class="math inline">\(\mathbf{y}(x_0)=\mathbf{y}_0\)</span> <span class="math display">\[    \mathbf{y}=\mathbf{\Phi}(x)\mathbf{\Phi}^{-1}(x_0)\mathbf{y}_0+\mathbf{\Phi}(x)\int_{x_0}^{x} \mathbf{\Phi}^{-1}(x)\mathbf{f}(s) \mathrm{d}s \tag{13}\]</span>  <span class="math inline">\(x_0 \in (a,b)\)</span>.</p><div class="note note-success">            <p> <spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>..</p>          </div><h2 id=""></h2><p><strong></strong> <spanclass="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{A}\mathbf{y}+\mathbf{f}(x)\tag{14}\]</span>  <span class="math inline">\(\mathbf{A}\)</span><span class="math inline">\(n\)</span><strong></strong><span class="math inline">\(\mathbf{f}(x)\)</span> <spanclass="math inline">\(a&lt;x&lt;b\)</span>.</p><p> <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{A}\mathbf{y}\tag{15}\]</span></p><p> <span class="math inline">\(n\)</span>=1 <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=ay\]</span>  <spanclass="math inline">\(y=C\mathrm{e}^{ax}\)</span>.</p><h3 id=""></h3><p><strong>1</strong> <spanclass="math inline">\(\mathbf{A}\)</span> <spanclass="math display">\[    \mathbf{E}+\mathbf{A}+\frac{1}{2!}\mathbf{A}^{2}+\cdots+\frac{1}{k!}\mathbf{A}^{k}+\cdots\]</span> .  <spanclass="math inline">\(\mathrm{e}^{\mathbf{A}}\)</span> <spanclass="math inline">\(\exp \mathbf{A}\)</span>..</p><p><strong>2</strong> -  <spanclass="math inline">\(\mathbf{AB}=\mathbf{BA}\)</span> <spanclass="math display">\[    \mathrm{e}^{\mathbf{A}+\mathbf{B}}=\mathrm{e}^{\mathbf{A}}+\mathrm{e}^{\mathbf{B}}    \]</span> -  <spanclass="math inline">\(\mathbf{A}\)</span><spanclass="math inline">\(\mathrm{e}^{\mathbf{A}}\)</span> <spanclass="math display">\[    (\mathrm{e}^{\mathbf{A}})^{-1}=\mathrm{e}^{-\mathbf{A}}     \]</span> -  <spanclass="math inline">\(\mathbf{P}\)</span> <spanclass="math inline">\(n\)</span> <span class="math display">\[    \mathrm{e}^{\mathbf{PAP^{-1}}}=\mathbf{P}\mathrm{e}^{\mathbf{A}}\mathbf{P}^{-1}\]</span> </p><h3id=""></h3><p><strong>6.4</strong><spanclass="math inline">\(\mathbf{\Phi}(x)=\mathrm{e}^{x\mathbf{A}}\)</span>15<strong></strong><span class="math inline">\(\mathbf{\Phi}(0)=\mathbf{E}\)</span></p><p></p><p><strong>6.3</strong>14<span class="math inline">\((a,b)\)</span> <spanclass="math display">\[    \mathbf{y}=\mathrm{e}^{x \mathbf{A}}\mathbf{c}+\int_{x_0}^{x}\mathrm{e}^{(x-s) \mathbf{A}} \mathbf{f}(s) \mathrm{d}s \tag{16}\]</span></p><p> <spanclass="math inline">\(\mathbf{c}\)</span>14<span class="math inline">\(\mathbf{y}(x_0)=\mathbf{y}_0\)</span><span class="math display">\[    \mathbf{y}=\mathrm{e}^{(x-x_0)\mathbf{A}}\mathbf{y}_0+\int_{x_0}^{x} \mathrm{e}^{(x-s) \mathbf{A}}\mathbf{f}(s) \mathrm{d}s \tag{17}\]</span>  <span class="math inline">\(x_0 \in (a,b)\)</span></p><h3 id="jordan">Jordan</h3><p> <span class="math inline">\(n\)</span> <spanclass="math inline">\(\mathbf{A}\)</span> <spanclass="math inline">\(n\)</span> <spanclass="math inline">\(\mathbf{P}\)</span> <spanclass="math display">\[    \mathbf{A}=\mathbf{PJP}^{-1}\]</span>  <span class="math inline">\(J\)</span>Jordan</p><p> <span class="math display">\[    \mathrm{e}^{x \mathbf{A}}=\mathrm{e}^{x\mathbf{PJP}^{-1}}=\mathbf{P} \mathrm{e}^{x\mathbf{J}}\mathbf{P}^{-1}      \]</span>  <span class="math inline">\(\mathrm{e}^{x\mathbf{A}}\mathbf{P} = \mathbf{P} \mathrm{e}^{x\mathbf{J}}\)</span>.</p><p> <span class="math inline">\(J\)</span> <spanclass="math inline">\(P\)</span>.</p><h3 id=""></h3><p>16<strong></strong></p><p><span class="math inline">\(A\)</span> <span class="math display">\[    \mathbf{\Phi}(x)=\mathrm{e}^{x \mathbf{A}} \mathbf{P}=\mathbf{P}    \begin{pmatrix}        \mathrm{e}^{\lambda_1x} &amp;  &amp;  &amp;  \\         &amp; \mathrm{e}^{\lambda_2x} &amp; &amp; \\         &amp; &amp; \ddots &amp; \\         &amp; &amp; &amp; \mathrm{e}^{\lambda_nx}    \end{pmatrix}\]</span>  <spanclass="math inline">\(\mathbf{\Phi}(0)=\mathbf{P}\)</span>. <span class="math display">\[    \mathrm{e}^{x \mathbf{A}}=\mathbf{\Phi}(x)\mathbf{\Phi}^{-1}(0)\tag{18}\]</span>  <span class="math inline">\(\mathbf{r}_i\)</span> <spanclass="math inline">\(\mathbf{P}\)</span> <spanclass="math inline">\(i\)</span> <spanclass="math display">\[    \mathbf{\Phi}(x)=(\mathrm{e}^{\lambda_1x}\mathbf{r}_1,\mathrm{e}^{\lambda_2x}\mathbf{r}_2,\cdots,\mathrm{e}^{\lambda_nx}\mathbf{r}_n)\]</span>  <spanclass="math inline">\(\mathbf{r}_i\)</span></p><p><strong></strong>15 <spanclass="math inline">\(\mathbf{y}=\mathrm{e}^{\lambdax}\mathbf{r}\)</span> <spanclass="math inline">\(\lambda\)</span> <spanclass="math inline">\(\mathbf{A}\)</span> <spanclass="math inline">\(\mathbf{r}\)</span> <spanclass="math inline">\(\lambda\)</span>.</p><p><strong>6.5</strong> <spanclass="math inline">\(n\)</span> <spanclass="math inline">\(\mathbf{A}\)</span> <spanclass="math inline">\(n\)</span> <spanclass="math inline">\(\lambda_1,\lambda_2,\cdots,\lambda_n\)</span><span class="math display">\[    \mathbf{\Phi}(x)=(\mathrm{e}^{\lambda_1x}\mathbf{r}_1,\mathrm{e}^{\lambda_2x}\mathbf{r}_2,\cdots,\mathrm{e}^{\lambda_nx}\mathbf{r}_n)\]</span> 15 <spanclass="math inline">\(\mathbf{r}_i\)</span> <spanclass="math inline">\(\mathbf{A}\)</span> <spanclass="math inline">\(\lambda_i\)</span>.</p><p><strong></strong>.</p><p>6.5 <strong>6.5</strong>* <spanclass="math inline">\(\mathbf{r}_1,\mathbf{r}_2,\cdots,\mathbf{r}_n\)</span><span class="math inline">\(\mathbf{A}\)</span> <spanclass="math inline">\(n\)</span>.</p><p>6.5 <spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>18<span class="math inline">\(\mathrm{e}^{x \mathbf{A}}\)</span></p><p>15 <span class="math display">\[    \mathbf{y}_1=\mathbf{u}(x)+i \mathbf{v}(x)\]</span>  <span class="math display">\[    \mathbf{y}_2=\mathbf{u}(x)-i \mathbf{v}(x)\]</span>  (15). 15.</p><p> <span class="math inline">\(\mathbf{A}\)</span>15 <span class="math inline">\(\mathrm{e}^{x\mathbf{A}}\mathbf{P}\)</span> <spanclass="math inline">\(\lambda_i\)</span> <spanclass="math inline">\(n_i\)</span> <spanclass="math display">\[    \mathbf{y}=e^{\lambda_i x}\biggl(\mathbf{r}_0+\frac{x}{1!}\mathbf{r}_1+\cdots+\frac{x^{n_i-1}}{(n_i-1)!}\mathbf{r}_{n_i-1} \biggr) \tag{19}\]</span>  <span class="math inline">\(\mathbf{r}_j(j=0,1,\cdots,n_i-1)\)</span> <spanclass="math inline">\(n\)</span>.</p><p><strong>6.7</strong> <spanclass="math inline">\(\lambda_i\)</span> <spanclass="math inline">\(\mathbf{A}\)</span> <spanclass="math inline">\(n_i\)</span>1519<span class="math inline">\(\mathbf{r}_0\)</span><span class="math display">\[    (\mathbf{A}-\lambda_i \mathbf{E})^{n_i}\mathbf{r}=\mathbf{0}\tag{20}\]</span> 19 <spanclass="math inline">\(\mathbf{r}_1,\mathbf{r}_2,\cdots,\mathbf{r}_n\)</span><span class="math display">\[    \begin{cases}        \mathbf{r}_1=(\mathbf{A}-\lambda_i \mathbf{E})\mathbf{r}_0 \\        \mathbf{r}_2=(\mathbf{A}-\lambda_i \mathbf{E})\mathbf{r}_1 \\        \cdots \cdots  \\        \mathbf{r}_{n_i-1}=(\mathbf{A}-\lambda_i\mathbf{E})\mathbf{r}_{n_i-2}    \end{cases} \tag{21}\]</span></p><p></p><p><strong>6.6</strong> <spanclass="math inline">\(n\)</span> <spanclass="math inline">\(\mathbf{A}\)</span> <spanclass="math inline">\(\mathbb{C}\)</span> <spanclass="math inline">\(\lambda_1,\lambda_2,\cdots,\lambda_s\)</span><spanclass="math inline">\(n_1,n_2,\cdots,n_s\)</span>15<span class="math inline">\(\mathbf{\Phi}(x)\)</span> <spanclass="math display">\[    \biggl( \mathrm{e}^{\lambda_1x}\mathbf{P}_1^{(1)}(x),\cdots,\mathrm{e}^{\lambda_1x}\mathbf{P}_{n_1}^{1}(x);\cdots;\mathrm{e}^{\lambda_s x}\mathbf{P}_1^{(s)}(x),\cdots,\mathrm{e}^{\lambda_s x}\mathbf{P}_{n_s}^{(s)}(x) \biggr) \tag{22}\]</span>  <span class="math display">\[    \mathbf{P}_j^{(i)}(x)=\mathbf{r}_{j0}^{(i)}+\frac{x}{1!}\mathbf{r}_{j1}^{(i)}+\frac{x^{2}}{2!}\mathbf{r}_{j2}^{(i)}+\cdots+\frac{x^{n_i-1}}{(n_i-1)!}\mathbf{r}_{jn_{i-1}}^{(i)} \tag{23}\]</span>  <span class="math inline">\(\lambda_i\)</span><span class="math inline">\(j\)</span> <spanclass="math inline">\((i=1,2,\cdots ,s;j=1,2,\cdots ,n_i)\)</span><span class="math inline">\(\mathbf{r}_{10}^{(i)},\cdots,\mathbf{r}_{n_i 0}^{(i)}\)</span>20 <spanclass="math inline">\(n_i\)</span> <spanclass="math inline">\(\mathbf{r}_{jk}^{(i)}(i=1,2,\cdots ,s;j=1,2,\cdots,n_i;k=1,2,\cdots ,n_i-1)\)</span> <spanclass="math inline">\(\mathbf{r}_{j0}^{(i)}\)</span>21 <spanclass="math inline">\(\mathbf{r}_0\)</span> <spanclass="math inline">\(\mathbf{r}_k\)</span>.  <spanclass="math inline">\(\mathbf{r}_{j,k}^{(i)}=(\mathbf{A-\lambda_iE})\mathbf{r}_{j,k-1}^{(i)}\)</span></p><p> <spanclass="math inline">\(\mathbf{\Phi}(x)\)</span><spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>.</p><p> <spanclass="math inline">\(r\)</span><span class="math inline">\(x^{n_i-1}\)</span> <spanclass="math inline">\(n_i\)</span> <spanclass="math inline">\(\lambda_i\)</span> <spanclass="math inline">\(n_i-1\)</span> <spanclass="math inline">\(\lambda_i\)</span>.</p><p><strong></strong>6.72215. <spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>. <span class="math display">\[    \mathbf{\Phi}(0)= \biggl(\mathbf{r}_{10}^{(1)},\cdots,\mathbf{r}_{n_1 0}^{(1)};\cdots ;\mathbf{r}_{10}^{(s)},\cdots,\mathbf{r}_{n_s 0}^{(s)} \biggr)\]</span>  <span class="math inline">\({\mathbf{r}_{j0}^{(i)}}\)</span>Wronsky<spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>15</p><h2 id=""></h2><p> <spanclass="math inline">\(y=y(x)\)</span> <spanclass="math inline">\(n\)</span> <spanclass="math display">\[    y^{(n)}+a_1(x)y^{(n-1)}+\cdots +a_{n-1}y&#39;+a_n(x)y=f(x) \tag{24}\]</span>  <span class="math inline">\(a_1(x),\cdots,a_n(x)\)</span> <span class="math inline">\(f(x)\)</span><span class="math inline">\(a&lt;x&lt;b\)</span>.  <spanclass="math inline">\(f(x)\)</span>24<strong></strong><strong></strong><span class="math display">\[    y^{(n)}+a_1(x)y^{(n-1)}+\cdots +a_{n-1}y&#39;+a_n(x)y=0 \tag{25}\]</span>  <span class="math display">\[    y_1=y, y_2=y&#39;,\cdots ,y_n=y^{(n-1)} \tag{26}\]</span> 24 <spanclass="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{A}(x)\mathbf{y}+\mathbf{f}(x)\tag{27}\]</span>  <span class="math inline">\(\mathbf{y}\)</span> <spanclass="math inline">\(\mathbf{y}_i\)</span><spanclass="math inline">\(\mathbf{f}\)</span> <spanclass="math inline">\(f(x)\)</span><spanclass="math inline">\(\mathbf{A}(x)\)</span>Frobenius</p><p><span class="math display">\[    \mathbf{A}(x)=      \begin{pmatrix}        0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\        0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \\        \vdots &amp; \vdots &amp; \vdots &amp; &amp; \vdots \\        0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 \\        -a_n(x) &amp; -a_{n-1}(x) &amp; -a_{n-2}(x) &amp; \cdots &amp;-a_1(x)    \end{pmatrix}\]</span></p><p>25 <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{A}(x)\mathbf{y}\tag{28}\]</span> 24 <spanclass="math inline">\(a&lt;x&lt;b\)</span>.</p><h3 id=""></h3><p> <span class="math display">\[    \varphi_1(x),\varphi_2(x),\cdots,\varphi_n(x) \tag{29}      \]</span> 28 <spanclass="math inline">\(n\)</span>29Wronsky.</p><p></p><p><strong>6.1</strong>*25 <spanclass="math inline">\(a&lt;x&lt;b\)</span> <spanclass="math inline">\(n\)</span>2925.</p><p><strong>6.2</strong>*29Wronsky<span class="math inline">\(a&lt;x&lt;b\)</span>.</p><div class="note note-success">            <p>29.Wronsky29</p>          </div><p> <spanclass="math inline">\(\mathbf{A}(x)\)</span>Liouville<span class="math display">\[    W(x)=W(x_0)\mathrm{e}^{-\int_{x_0}^{x} a_1(s) \mathrm{d}s} \quad(a&lt;x&lt;b) \tag{30}        \]</span>  <spanclass="math inline">\(W(x)\)</span>25Wronsky.</p><p>25.</p><p> <span class="math inline">\(y=\varphi(x)\)</span> <spanclass="math display">\[    y&#39;&#39;+p(x)y&#39;+q(x)y=0 \tag{31}\]</span>  <span class="math inline">\(p(x)\)</span><span class="math inline">\(q(x)\)</span> <spanclass="math inline">\(a&lt;x&lt;b\)</span>31<span class="math display">\[    y=\varphi(x)\biggl[ C_1+C_2 \int_{x_0}^{x}\frac{1}{\varphi(s)^{2}}\mathrm{e}^{-\int_{x_0}^{x} p(t) \mathrm{d}t}\mathrm{d}s \biggr] \tag{32}   \]</span>  <span class="math inline">\(C_1\)</span> <spanclass="math inline">\(C_2\)</span>.</p><p>Liouville.</p><p> <strong>6.3</strong>*  <spanclass="math inline">\(\varphi_1(x),\varphi_2(x),\cdots,\varphi_n(x)\)</span>25<spanclass="math inline">\(a&lt;x&lt;b\)</span>24<span class="math display">\[    y=C_1\varphi_1(x)+\cdots +C_n \varphi_n(x) +\varphi^*(x) \tag{33}\]</span>  <span class="math inline">\(C_i\)</span> <spanclass="math display">\[    \varphi^*(x)=\sum_{k=1}^{n} \varphi_k(x) \cdot \int_{x_0}^{x}\frac{W_k(s)}{W(s)}f(s) \mathrm{d}s \tag{34}\]</span></p><p> <span class="math inline">\(W(x)\)</span> <spanclass="math inline">\(\varphi_i(x)\)</span>Wronsky <spanclass="math inline">\(W_k(x)\)</span> <spanclass="math inline">\(W(x)\)</span> <spanclass="math inline">\(n\)</span> <spanclass="math inline">\(k\)</span>.</p><p><strong></strong> <span class="math display">\[    \mathbf{\Phi}(x) \int_{x_0}^{x} \mathbf{\Phi}^{-1}(s)\mathbf{f}(s)\mathrm{d}s\]</span>  <spanclass="math inline">\(\varphi^*(x)\)</span>.</p><p>.</p><p>  <spanclass="math inline">\(y=\varphi(x)\)</span> <spanclass="math display">\[    y&#39;&#39;+p(x)y&#39;+q(x)y=f(x) \tag{35}\]</span>  <spanclass="math inline">\(y=\varphi_1(x)\)</span> <spanclass="math inline">\(y=\varphi_2(x)\)</span>. <spanclass="math inline">\(p(x),q(x),f(x)\)</span> <spanclass="math inline">\(a&lt;x&lt;b\)</span>.</p><div class="note note-success">            <p>...</p>          </div><h3 id=""></h3><p> <span class="math inline">\(n\)</span> <spanclass="math display">\[    y^{(n)}+a_1y^{(n-1)}+\cdots +a_{n-1}y&#39;+a_ny=f(x) \tag{36}\]</span>  <span class="math display">\[    y^{(n)}+a_1y^{(n-1)}+\cdots +a_{n-1}y&#39;+a_ny=0 \tag{36}\]</span>  <span class="math inline">\(a_1,\cdots,a_n\)</span> <spanclass="math inline">\(f(x)\)</span> <spanclass="math inline">\(a&lt;x&lt;b\)</span>.</p><p>36 <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{Ay}\]</span>  <span class="math display">\[    \mathbf{A}=      \begin{pmatrix}        0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\        0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \\        \vdots &amp; \vdots &amp; \vdots &amp; &amp; \vdots \\        0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 \\        -a_n &amp; -a_{n-1} &amp; -a_{n-2} &amp; \cdots &amp; -a_1    \end{pmatrix} \tag{37}\]</span>  <spanclass="math inline">\(\mathbf{A}\)</span><strong></strong><span class="math display">\[    \lambda^{n}+a_1\lambda^{n-1}+\cdots +a_{n-1}\lambda+a_n=0 \tag{38}\]</span> 36.</p><p><strong>6.6</strong>*3638 <spanclass="math inline">\(\mathbb{C}\)</span> <spanclass="math inline">\(s\)</span> <spanclass="math inline">\(\lambda_1,\lambda_2,\cdots,\lambda_s\)</span><span class="math inline">\(n_i\)</span>.  <spanclass="math display">\[    \begin{cases}        \mathrm{e}^{\lambda_1x} , x\mathrm{e}^{\lambda_1x} ,\cdots,x^{n_1-1}\mathrm{e}^{\lambda_1x} ; \\        \cdots \\        \mathrm{e}^{\lambda_sx} ,x\mathrm{e}^{\lambda_sx},\cdots,x^{n_s-1}\mathrm{e}^{\lambda_sx} \tag{39}    \end{cases}\]</span> 36.  <spanclass="math inline">\(n\)</span></p><p><strong></strong>39. <spanclass="math inline">\(\mathbf{A}\)</span>Jordan <spanclass="math inline">\(\mathbf{J}\)</span> <spanclass="math inline">\(\lambda_k\)</span>Jordan.</p><p> <spanclass="math inline">\(\mathbf{P}=(p_{ij})\)</span><span class="math display">\[    p_{1m_j} \neq 0 \quad (j=1,2,\cdots ,s) \tag{40}\]</span>  <span class="math display">\[    m_1=1,m_2=n_1+1,\cdots ,m_s=n_1+\cdots +n_{s-1}+1\]</span>  <spanclass="math inline">\(p_{1m_j}=0\)</span> <spanclass="math inline">\(\mathbf{AP}\)</span> <spanclass="math inline">\(m_j\)</span> <spanclass="math inline">\(n-1\)</span> <spanclass="math inline">\(\mathbf{PJ}\)</span> <spanclass="math inline">\(\mathbf{P}\)</span> <spanclass="math inline">\(m_j\)</span> <spanclass="math inline">\(\mathbf{P}\)</span>.</p><p> $^{x }= $40 <spanclass="math inline">\(\mathbf{PJ}\)</span>39.</p><p>.</p><p> <spanclass="math inline">\(f(x)\)</span> <spanclass="math inline">\(\varphi^*(x)\)</span> <strong></strong> <spanclass="math display">\[    f(x)=P_m(x)\mathrm{e}^{\mu x}\]</span>  <span class="math inline">\(P_m(x)\)</span> <spanclass="math inline">\(x\)</span> <spanclass="math inline">\(m\)</span>.  <spanclass="math inline">\(\mu\)</span>3635<span class="math display">\[    \varphi^*(x)=Q_m(x)\mathrm{e}^{\mu x}\]</span>  <span class="math inline">\(m\)</span> <spanclass="math inline">\(Q_m(x)\)</span>.</p><p> <span class="math inline">\(\mu\)</span> <spanclass="math inline">\(k\)</span> <spanclass="math display">\[    \varphi^*(x)=x^kQ_m(x)\mathrm{e}^{\mu x}\]</span></p><p><strong></strong> <span class="math display">\[    f(x)=[A_m(x)\cos (\beta x)+B_l(x) \sin (\beta x)] \mathrm{e}^{\alphax}\]</span>  <span class="math inline">\(A_m(x)\)</span> <spanclass="math inline">\(B_l(x)\)</span> <spanclass="math inline">\(x\)</span> <spanclass="math inline">\(m\)</span> <spanclass="math inline">\(l\)</span>.  <spanclass="math display">\[    \varphi^*(x)=x^{k}[C_n(x)\cos (\beta x)+D_n(x) \sin (\beta x)]\mathrm{e}^{\alpha x}\]</span>  <span class="math inline">\(k\)</span> <spanclass="math inline">\(\alpha \pmi\beta\)</span>36<spanclass="math inline">\(k\)</span>0<spanclass="math inline">\(n= \max\{m,l\}\)</span> <spanclass="math inline">\(n\)</span> <spanclass="math inline">\(C_n(x)\)</span> <spanclass="math inline">\(D_n(x)\)</span>.</p><p><strong>Euler</strong> <span class="math display">\[    x^{n}y^{(n)}+a_1x^{n-1}y^{(n-1)}+\cdots +a_{n-1}xy&#39;+a_ny=0      \]</span>  <span class="math inline">\(a_1,a_2,\cdots,a_n\)</span> <span class="math inline">\(x&gt;0\)</span>.</p><p> $x=^{t} $ <spanclass="math inline">\(D=\frac{\mathrm{d}}{\mathrm{d}t}\)</span></p><p> <span class="math display">\[    x^{k}y^{(k)}=D(D-1)\cdots (D-k+1)y\]</span> Euler <spanclass="math inline">\(t\)</span> <spanclass="math inline">\(t= \ln x\)</span>.</p><p><strong></strong> <span class="math display">\[    y&#39;&#39;+p(x)y&#39;+q(x)y=0 \tag{41}\]</span>  <span class="math inline">\(p(x)\)</span> <spanclass="math inline">\(q(x)\)</span> <spanclass="math inline">\(I:a&lt;x&lt;b\)</span>.  <spanclass="math inline">\(y=\varphi(x)\)</span>41 <spanclass="math inline">\(I\)</span> <spanclass="math inline">\(\varphi(x)\)</span>  <spanclass="math inline">\(I\)</span> <spanclass="math inline">\(\varphi(x)\)</span>.</p><p><strong></strong>.</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>5</title>
    <link href="/2022/04/09/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%885%EF%BC%89/"/>
    <url>/2022/04/09/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%885%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><p><strong></strong>..</p><h2 id=""></h2><p><strong></strong><strong></strong>.n <span class="math display">\[    F\biggl(y,\frac{\mathrm{d}y}{\mathrm{d}x},\cdots,\frac{\mathrm{d}^ny}{\mathrm{d}x^n}\biggr)=0 \tag{1}\]</span>  <spanclass="math inline">\(z=\frac{\mathrm{d}y}{\mathrm{d}x}\)</span><span class="math display">\[    \begin{cases}        \displaystyle        \frac{\mathrm{d}^2y}{\mathrm{d}x^2}=\frac{\mathrm{d}z}{\mathrm{d}x}=\frac{\mathrm{d}z}{\mathrm{d}y}\frac{\mathrm{d}y}{\mathrm{d}x}=z\frac{\mathrm{d}z}{\mathrm{d}y}\\        \displaystyle        \frac{\mathrm{d}^3y}{\mathrm{d}x^3}=\frac{\mathrm{d}}{\mathrm{d}x}\biggl(z\frac{\mathrm{d}z}{\mathrm{d}y}\biggr)=z^2\frac{\mathrm{d}^2z}{\mathrm{d}y^2}+z        \biggl(\frac{\mathrm{d}z}{\mathrm{d}y}\biggr) \\        \cdots \\        \displaystyle        \frac{\mathrm{d}^ny}{\mathrm{d}x^n}=\varphi\biggl(z,\frac{\mathrm{d}z}{\mathrm{d}y},\cdots,\frac{\mathrm{d}^{n-1}z}{\mathrm{d}y^{n-1}}\biggr)    \end{cases}\]</span> 1 <spanclass="math inline">\(n-1\)</span> <spanclass="math display">\[    F_1\biggl(y,z,\frac{\mathrm{d}z}{\mathrm{d}y},\cdots,\frac{\mathrm{d}^{n-1}z}{\mathrm{d}y^{n-1}}\biggr)=0\]</span>  <span class="math inline">\(z\)</span><span class="math inline">\(y\)</span>.</p><p><strong></strong><strong></strong><strong></strong>.1<strong></strong> <spanclass="math display">\[    \frac{\mathrm{d}^2x}{\mathrm{d}t^2}+a^{2}\sin x=0\]</span>  <spanclass="math inline">\(a=\sqrt{g/l}&gt;0\)</span>.</p><p> <span class="math inline">\(\displaystyle\frac{\mathrm{d}x}{\mathrm{d}t}\)</span> <spanclass="math display">\[    \frac{1}{2} \biggl(\frac{\mathrm{d}x}{\mathrm{d}t}\biggr)^{2}-a^{2}\cos x=-\frac{1}{2}C_1  \]</span><strong></strong> <spanclass="math inline">\(\sin x \thickapproxx\)</span>.</p><p>2<strong></strong>. <span class="math display">\[    y&#39;&#39;=a \sqrt{1+(y&#39;)^{2}} \tag{2}\]</span>  <span class="math inline">\(a=\gamma/H_0\)</span>.<strong></strong> <span class="math display">\[    y(x_1)=y_1, \quad y(x_2)=y_2 \tag{3}\]</span> <strong></strong>.  <spanclass="math inline">\(z=y&#39;\)</span> <spanclass="math display">\[    z&#39;=a \sqrt{1+z^{2}}\]</span> .  <spanclass="math display">\[    z= \sinh a(x+C_1)\]</span>  <span class="math inline">\(C_1\)</span>.<br /><span class="math display">\[    y=\frac{1}{a} \cosh a(x+C_1) +C_2 \tag{4}\]</span>  <spanclass="math inline">\(C_2\)</span>.</p><p>23 <spanclass="math inline">\(C_1\)</span> <spanclass="math inline">\(C_2\)</span>. ...</p><p>3<strong></strong>Newton'sLaw6<span class="math inline">\(z=0\)</span>. 4..</p><h2 id="n">n</h2><p> <span class="math inline">\(n\)</span> <spanclass="math display">\[    \frac{\mathrm{d}^ny}{\mathrm{d}x^n}=F\biggl(x,y,\frac{\mathrm{d}y}{\mathrm{d}x},\cdots,\frac{\mathrm{d}^{n-1}y}{\mathrm{d}x^{n-1}} \biggr) \tag{5}\]</span>  <span class="math inline">\(x\)</span> <spanclass="math inline">\(y\)</span>  <spanclass="math display">\[    y_1=y,y_2=\frac{\mathrm{d}y}{\mathrm{d}x},\cdots,y_n=\frac{\mathrm{d}^{n-1}y}{\mathrm{d}x^{n-1}}       \]</span> 5 <spanclass="math inline">\(n\)</span> <spanclass="math display">\[    \begin{cases}        \displaystyle \frac{\mathrm{d}y_1}{\mathrm{d}x}=y_2 \\        \cdots \\        \displaystyle \frac{\mathrm{d}y_{n-1}}{\mathrm{d}x}=y_n \\        \displaystyle\frac{\mathrm{d}y_n}{\mathrm{d}x}=F(x,y_1,y_2,\cdots ,y_n)      \end{cases}\]</span> <span class="math display">\[    \begin{cases}        \displaystyle\frac{\mathrm{d}y_1}{\mathrm{d}x}=f_1(x,y_1,y_2,\cdots,y_n), \\        \displaystyle\frac{\mathrm{d}y_2}{\mathrm{d}x}=f_2(x,y_1,y_2,\cdots,y_n), \\        \cdots  \\        \displaystyle\frac{\mathrm{d}y_n}{\mathrm{d}x}=f_n(x,y_1,y_2,\cdots,y_n),    \end{cases} \tag{6}\]</span>  <spanclass="math inline">\(f_1,f_2,\cdots,f_n\)</span> <spanclass="math inline">\((x,y_1,y_2,\cdots,y_n)\)</span> <spanclass="math inline">\(D\)</span>.</p><p> <span class="math inline">\(n\)</span> <spanclass="math display">\[    y=(y_1,y_2,\cdots,y_n) \in  \mathbb{R}^{n}      \]</span>  <span class="math display">\[    f_i(x,\mathbf{y})=f_i(x,y_1,y_2,\cdots,y_n)     \]</span> <span class="math inline">\(i=1,2,\cdots ,n\)</span> <spanclass="math display">\[    \mathbf{f}(x, \mathbf{y})=(f_1(x,\mathbf{y}),\cdots,f_n(x,\mathbf{y})) \in \mathbb{R}^{n}          \]</span>  <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}= \biggl(\frac{\mathrm{d}y_1}{\mathrm{d}x},\frac{\mathrm{d}y_2}{\mathrm{d}x},\cdots,\frac{\mathrm{d}y_n}{\mathrm{d}x}\biggr)\]</span> 6 <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{f}(x,\mathbf{y})  \tag{7}\]</span>  <span class="math inline">\(\mathbf{f}(x,\mathbf{y})\)</span> <span class="math inline">\((x,\mathbf{y})\in D\)</span> <spanclass="math inline">\(n\)</span>.  <spanclass="math display">\[    \mathbf{y}(x_0)=\mathbf{y}_0\]</span>  <span class="math inline">\((x_0,\mathbf{y}_0)\in D \subset \mathbb{R}^{n+1}\)</span>.</p><div class="note note-success">            <p> <spanclass="math inline">\(\mathbb{R}^{n}\)</span>PicardPeano</p>          </div><p>6 <spanclass="math inline">\(f_1,f_2,\cdots,f_n\)</span> <spanclass="math inline">\(y_1,y_2,\cdots,y_n\)</span> <spanclass="math display">\[    f_k(x,y_1,y_2,\cdots,y_n)=\sum_{i=1}^{n} a_{ik}(x)y_i+e_k(x)\]</span> <span class="math inline">\((k=1,2,\cdots,n)\)</span>67<strong></strong><strong></strong>.</p><p> <span class="math display">\[    \frac{\mathrm{d}y_k}{\mathrm{d}x}=\sum_{i=1}^{n} a_{ik}(x)y_i+e_k(x)\]</span> <span class="math inline">\((k=1,2,\cdots,n)\)</span> <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{y}\mathbf{A}(x)+\mathbf{e}(x)\]</span>  <spanclass="math inline">\(\mathbf{y}=(y_1,y_2,\cdots,y_n)\)</span> <spanclass="math inline">\(\mathbf{e}(x)=e_1(x),e_2(x),\cdots,e_n(x)\)</span><span class="math inline">\(\mathbf{A}(x)=(a_{ik}(x))_{n \timesn}\)</span>.  <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{A}(x)\mathbf{y}+\mathbf{e}(x)\tag{8}\]</span> .</p><p> <span class="math inline">\(\mathbf{A}(x)\)</span> <spanclass="math inline">\(\mathbf{e}(x)\)</span> <spanclass="math inline">\(a&lt;x&lt;b\)</span>8<span class="math display">\[    \mathbf{y}(x_0)=y_0 \quad (a&lt;x_0&lt;b), \mathbf{y}_0 \in\mathbb{R}^{n}\]</span>  <spanclass="math inline">\(\mathbf{y}=\mathbf{y}(x)\)</span> <spanclass="math inline">\(a&lt;x&lt;b\)</span>.</p><h2 id=""></h2><p> <span class="math display">\[    (E_{\lambda}): \quad\frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{f}(x,\mathbf{y},\mathbf{\lambda}),\quad\mathbf{y}(0)=\mathbf{0}\]</span>  <spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x,\mathbf{\lambda})\)</span><span class="math inline">\(\mathbf{\lambda}\)</span><span class="math inline">\(\mathbf{\lambda}\)</span> <spanclass="math inline">\(m\)</span>.</p><p><strong>5.1</strong> <spanclass="math inline">\(n\)</span> <spanclass="math inline">\(\mathbf{f}(x,\mathbf{y},\mathbf{\lambda})\)</span><span class="math display">\[    G: \quad \left\vert x \right\vert \leqslant a, \quad \left\vert\mathbf{y} \right\vert \leqslant b, \quad \left\vert\mathbf{\lambda}-\mathbf{\lambda}_0 \right\vert \leqslant c    \]</span>  <spanclass="math inline">\(\mathbf{y}\)</span>Lipschitz <spanclass="math display">\[    \left\vert\mathbf{f}(x,\mathbf{y}_1,\mathbf{\lambda})-\mathbf{f}(x,\mathbf{y}_2,\mathbf{\lambda})\right\vert \leqslant L\left\vert \mathbf{y}_1-\mathbf{y}_2\right\vert  \]</span>  <span class="math inline">\(L\geqslant 0\)</span>. <span class="math inline">\(M\)</span> $(x,,) $ <spanclass="math inline">\(G\)</span> <spanclass="math display">\[    h= \min \biggl(a,\frac{b}{M} \biggr)\]</span>  <spanclass="math inline">\((E_\lambda)\)</span> <spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x,\mathbf{\lambda})\)</span><span class="math display">\[    D: \quad \left\vert x \right\vert \leqslant h, \quad \left\vert\mathbf{\lambda}-\mathbf{\lambda}_0 \right\vert \leqslant c\]</span> .</p><p><strong></strong> <spanclass="math inline">\((E_\lambda)\)</span>Picard <spanclass="math inline">\(\{\mathbf{\varphi}_k(x,\mathbf{\lambda})\}\)</span><spanclass="math inline">\(\mathbf{\lambda}\)</span><spanclass="math inline">\(\mathbf{\varphi}_k(x,\mathbf{\lambda})\)</span><spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x,\mathbf{\lambda})\)</span> <span class="math inline">\((E_\lambda)\)</span>.</p><p><strong></strong> <spanclass="math inline">\(n\)</span> <spanclass="math inline">\(f(x,\mathbf{y})\)</span> <spanclass="math display">\[    R: \quad \left\vert x-x_0 \right\vert \leqslant a, \quad \left\verty-y_0 \right\vert \leqslant b\]</span>  <spanclass="math inline">\(\mathbf{y}\)</span>Lipschitz. <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{f}(x,\mathbf{y}),\quad\mathbf{y}(x_0)=\eta \tag{9}\]</span>  <spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x,\mathbf{\eta})\)</span><span class="math display">\[    Q: \quad \left\vert x-x_0 \right\vert \leqslant \frac{h}{2}, \quad\left\vert \mathbf{\eta}-\mathbf{y_0} \right\vert \leqslant \frac{b}{2}\]</span>  <span class="math display">\[    h=\min \biggl(a,\frac{b}{M} \biggr)\]</span>  <span class="math inline">\(M\)</span> $f(x,) $<span class="math inline">\(R\)</span>.</p><p>..</p><p><strong>5.2</strong> <spanclass="math inline">\(n\)</span> <spanclass="math inline">\(\mathbf{f}(x,\mathbf{y})\)</span> <spanclass="math inline">\((x,\mathbf{y})\)</span><span class="math inline">\(\mathbf{y}\)</span> Lipschitz. <span class="math inline">\(\mathbf{y}=\mathbf{\xi}(x)\)</span> <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{f}(x,\mathbf{y})\tag{10}\]</span>  <spanclass="math inline">\(J\)</span>.  <spanclass="math inline">\(J\)</span> <spanclass="math inline">\(a\leqslant x\leqslant b\)</span>.  <spanclass="math inline">\(\delta&gt;0\)</span> <spanclass="math inline">\((x_0,\mathbf{y}_0)\)</span> <spanclass="math display">\[    a\leqslant x_0\leqslant b, \quad \left\vert\mathbf{y}_0-\mathbf{\xi}(x_0) \right\vert \leqslant \delta\]</span> Cauchy <span class="math display">\[    (E): \quad\frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{f}(x,\mathbf{y}),\quad\mathbf{y}(x_0)=y_0\]</span>  <spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x;x_0,\mathbf{y}_0)\)</span><span class="math inline">\(a\leqslant x\leqslantb\)</span> <span class="math display">\[    D_\delta: \quad a\leqslant x\leqslant b, a\leqslant x_0 \leqslant b,\left\vert \mathbf{y}_0-\mathbf{\xi}(x_0) \right\vert \leqslant \delta\]</span> .</p><p><strong></strong>LipschitzLipschitzPicard.</p><p> <spanclass="math inline">\(\mathbf{f}(x,\mathbf{y})\)</span> <spanclass="math inline">\(R\)</span> <spanclass="math inline">\(\displaystyle\frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}\mathbf{f}(x,\mathbf{y})\)</span><spanclass="math inline">\(R\)</span>.</p><p> <span class="math inline">\((x_0,\mathbf{y}_0)\)</span><spanclass="math inline">\(\mathbf{y}=\mathbf{\phi}(x;x_0,\mathbf{y}_0)=\mathbf{\psi}_{x_0,\mathbf{y}_0}(x)\)</span>. <span class="math inline">\(\mathbf{\psi}\)</span> <spanclass="math inline">\(x\)</span> <spanclass="math inline">\(\mathbf{\psi}_{x_0,\mathbf{y}_0}(x_0)=\mathbf{y}_0\)</span>. <span class="math inline">\(\mathbf{\varphi}\)</span> <spanclass="math inline">\((x_0, \mathbf{y}_0)\)</span> <spanclass="math inline">\((x_0^*,\mathbf{y}_0^*\)</span>.  <spanclass="math inline">\(\varepsilon&gt;0\)</span><spanclass="math inline">\(d((\widetilde{x}_0,\mathbf{\widetilde{y}}_0),(x_0^*,\mathbf{y}_0^*))\)</span>.</p><h2 id=""></h2><p> <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{f}(x,\mathbf{y},\mathbf{\lambda})\tag{11}\]</span>  <span class="math display">\[    \mathbf{y}(0)=\mathbf{0}\]</span>  <spanclass="math inline">\(\mathbf{y}=\varphi(x,\mathbf{\lambda})\)</span><span class="math inline">\(\mathbf{\lambda}\)</span>.</p><p><strong>5.3</strong> <spanclass="math inline">\(\mathbf{f}(x.\mathbf{y}.\mathbf{\lambda})\)</span><span class="math display">\[    G: \quad \left\vert x \right\vert \leqslant a, \quad \left\vert\mathbf{y} \right\vert \leqslant b, \quad \left\vert\mathbf{\lambda}-\mathbf{\lambda}_0 \right\vert\leqslant c\]</span>  <span class="math inline">\(\mathbf{y}\)</span><span class="math inline">\(\lambda\)</span>.11 <spanclass="math inline">\(\mathbf{y}(0)=\mathbf{0}\)</span> <spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x,\mathbf{\lambda})\)</span><span class="math display">\[    D: \quad \left\vert x \right\vert \leqslant h, \quad \left\vert\mathbf{\lambda}-\mathbf{\lambda}_0 \right\vert \leqslant c\]</span>  <spanclass="math inline">\(h\)</span>5.1.</p><p><strong></strong> <span class="math display">\[    \mathbf{y}=\int_{0}^{x} \mathbf{f}(x,\mathbf{y},\mathbf{\lambda})\mathrm{d}x\]</span> Picard5.1Picard <spanclass="math inline">\(\mathbf{\varphi}_k(x,\mathbf{\lambda})\)</span><span class="math inline">\(D\)</span>11<spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x,\mathbf{\lambda})\)</span>. <spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x,\mathbf{\lambda})\)</span><span class="math inline">\(\mathbf{\lambda}\)</span><span class="math inline">\(\frac{\partial \mathbf{\varphi}}{\partial\mathbf{\lambda}}(x,\mathbf{\lambda})\)</span> <spanclass="math inline">\(\frac{\partial \mathbf{\varphi}_k}{\partial\mathbf{\lambda}}(x,\mathbf{\lambda})\)</span> <spanclass="math inline">\((x,\mathbf{\lambda}) \in D\)</span>. <span class="math display">\[    \left\vert \frac{\partial \mathbf{\varphi}}{\partial\mathbf{\lambda}} \right\vert \leqslant \exp (\alpha h)\]</span>  <span class="math inline">\(\alpha\)</span> <spanclass="math inline">\(G\)</span> <spanclass="math inline">\(\displaystyle \frac{\partial \mathbf{f}}{\partial\mathbf{y}}(x,\mathbf{y},\mathbf{\lambda})\)</span> <spanclass="math inline">\(\displaystyle \frac{\partial \mathbf{f}}{\partial\mathbf{\lambda}}(x,\mathbf{y},\mathbf{\lambda})\)</span>.</p><p>Cauchy <span class="math display">\[    v_{k,s}=\left\vert \frac{\partial \mathbf{\varphi}_{k+s}}{\partial\mathbf{\lambda}}-\frac{\partial \mathbf{\varphi}_k}{\partial\mathbf{\lambda}} \right\vert\]</span> 0.</p><p> <span class="math display">\[    \frac{\partial \mathbf{\varphi}}{\partialx}(x,\mathbf{\lambda})=\mathbf{f}(x,\mathbf{\varphi}(x,\mathbf{\lambda}),\mathbf{\lambda})\]</span>  <span class="math inline">\(\displaystyle \frac{\partial\mathbf{\varphi}}{\partial x}(x,\mathbf{\lambda})\)</span> <spanclass="math inline">\((x,\mathbf{\lambda}) \in D\)</span>.</p><p><strong></strong> <spanclass="math inline">\(n\)</span> <spanclass="math inline">\(\mathbf{f}(x,\mathbf{y})\)</span> <spanclass="math display">\[    R: \quad \left\vert x-x_0 \right\vert \leqslant a, \quad \left\verty-y_0 \right\vert \leqslant b\]</span>  <spanclass="math inline">\(\mathbf{y}\)</span> <spanclass="math inline">\(\mathbf{f}&#39;_{\mathbf{y}}(x,\mathbf{y})\)</span>. <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{f}(x,\mathbf{y}),\quad \mathbf{y}(x_0)=\mathbf{\eta}\]</span>  <spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x,\mathbf{\eta})\)</span><span class="math display">\[    D: \quad \left\vert x-x_0 \right\vert \leqslant \frac{h}{2}, \quad\left\vert \mathbf{\eta}-\mathbf{y}_0 \right\vert \leqslant\frac{b}{2}      \]</span> .</p><p> <span class="math inline">\(y\)</span> <spanclass="math inline">\(\lambda\)</span>5.3<span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y, \lambda), \quad y(x_0)=y_0\tag{12}\]</span>  <spanclass="math inline">\(y=\varphi(x;x_0,y_0,\lambda)\)</span> <spanclass="math inline">\(x_0\)</span><spanclass="math inline">\(y_0\)</span> <spanclass="math inline">\(\lambda\)</span> <spanclass="math inline">\(\displaystyle \frac{\partial \varphi}{\partialx_0}\)</span> <span class="math inline">\(\displaystyle \frac{\partial\varphi}{\partial y_0}\)</span> <spanclass="math inline">\(\displaystyle \frac{\partial \varphi}{\partial\lambda}\)</span>.</p><p> <span class="math display">\[    \varphi(x;x_0,y_0,\lambda)=y_0+\int_{x_0}^{x}f(x,\varphi(x;x_0,y_0,\lambda),\lambda) \mathrm{d}x \tag{13}\]</span>  <span class="math inline">\(x_0\)</span><spanclass="math inline">\(y_0\)</span> <spanclass="math inline">\(\lambda\)</span>12<span class="math inline">\(x_0\)</span><spanclass="math inline">\(y_0\)</span> <spanclass="math inline">\(\lambda\)</span><strong></strong></p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4</title>
    <link href="/2022/04/06/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%884%EF%BC%89/"/>
    <url>/2022/04/06/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%884%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><h2 id=""></h2><p> <span class="math display">\[    F(x, y,\frac{\mathrm{d}y}{\mathrm{d}x})=0 \tag{1}\]</span>  <spanclass="math inline">\(\frac{\mathrm{d}y}{\mathrm{d}x}\)</span><span class="math inline">\((x, y)\)</span>.</p><h3 id=""></h3><p>1 <span class="math display">\[    y=f(x, p) \tag{2}       \]</span>  <spanclass="math inline">\(p=\frac{\mathrm{d}y}{\mathrm{d}x}\)</span>. <span class="math inline">\(f(x,p)\)</span> <spanclass="math inline">\((x,p)\)</span>. 2 <spanclass="math inline">\(x\)</span> <spanclass="math display">\[    p=f&#39;_x(x,p)+f&#39;_p(x,p)\frac{\mathrm{d}p}{\mathrm{d}x}\]</span>  <span class="math display">\[    [f&#39;_(x,p)-p]\mathrm{d}x+f&#39;_p(x,p)\mathrm{d}p=0 \tag{3}\]</span>  <span class="math inline">\(x\)</span><span class="math inline">\(p\)</span>.</p><p>3 <spanclass="math inline">\(p=u(x,C)\)</span>2 <spanclass="math display">\[    y=f(x,u(x,C))\]</span>  <spanclass="math inline">\(C\)</span>3 <spanclass="math inline">\(p=w(x)\)</span>2 <spanclass="math display">\[    y=f(x,w(x))\]</span> 3 <spanclass="math inline">\(x=v(p,C)\)</span>2 <spanclass="math display">\[    \begin{cases}        x=v(p,C) \\        y=f(v(p,C),p)    \end{cases}\]</span>  <spanclass="math inline">\(p\)</span>3<span class="math inline">\(x=z(p)\)</span>2 <spanclass="math display">\[    \begin{cases}        x=z(p) \\        y=f(z(p),)         \end{cases}\]</span></p><h3 id=""></h3><p> <span class="math display">\[    F(y,p)=0 \quad \biggl(p=\frac{\mathrm{d}y}{\mathrm{d}x}\biggr)\tag{4}\]</span>  <span class="math inline">\(y\)</span> <spanclass="math inline">\(p\)</span>4 <spanclass="math inline">\((y,p)\)</span>.  <spanclass="math display">\[    y=g(t),\quad p=h(t) \tag{5}\]</span> . 54<strong></strong>. <span class="math inline">\(g(t)\)</span><spanclass="math inline">\(g&#39;(t)\)</span> <spanclass="math inline">\(h(t)\)</span> <spanclass="math inline">\(t\)</span> <spanclass="math inline">\(h(t) \neq 0\)</span>. <span class="math display">\[    \mathrm{d}x=\frac{1}{p}\mathrm{d}y=\frac{g&#39;(t)}{h(t)}\mathrm{d}t\]</span>  <span class="math display">\[    x=\int_{}^{} \frac{g&#39;(t)}{h(t)} \mathrm{d}t+C\]</span> 4 <span class="math display">\[    x=\int_{}^{} \frac{g&#39;(t)}{h(t)} \mathrm{d}t+C, \quad y=g(t)\tag{6}\]</span></p><p> <span class="math display">\[    F(x, y,p)=0 \quad \biggl( p=\frac{\mathrm{d}y}{\mathrm{d}x} \biggr)\tag{7}\]</span>  <span class="math inline">\((x, y, p)\)</span>. <span class="math display">\[    x=f(u,v), \quad y=g(u,v), \quad p=h(u,v)\]</span>  <span class="math inline">\(u\)</span> <spanclass="math inline">\(v\)</span> .  <spanclass="math inline">\(\mathrm{d}y=p\mathrm{d}x\)</span><span class="math display">\[    M(u,v)\mathrm{d}u+N(u,v)\mathrm{d}v=0 \tag{8}\]</span>  <span class="math display">\[    \begin{cases}        M(u,v)=g&#39;_u(u,v)-h(u,v)f&#39;_u(u,v) \\        N(u,v)=g&#39;_v(u,v)-h(u,v)f&#39;_v(u,v)    \end{cases}\]</span> 8 <spanclass="math display">\[    v=Q(u,C) \tag{9}\]</span> 7 <span class="math display">\[    x=f(u,Q(u,C)), \quad y=g(u,Q(u,C))\]</span>  <span class="math inline">\(u\)</span> <spanclass="math inline">\(C\)</span> <spanclass="math inline">\(v=S(u)\)</span> <span class="math display">\[    x=f(u,S(u)),\quad y=g(u,S(u))\]</span> 7.</p><h2 id="-1"></h2><p> <span class="math display">\[    F(x, y, \frac{\mathrm{d}y}{\mathrm{d}x})=0 \tag{10}\]</span>  <span class="math display">\[    \Gamma: y=\varphi(x) \quad(x\in J)\]</span>  <span class="math inline">\(Q\in\Gamma\)</span> <spanclass="math inline">\(Q\)</span>10 <spanclass="math inline">\(\Gamma\)</span> <spanclass="math inline">\(Q\)</span> <spanclass="math inline">\(\Gamma\)</span> <spanclass="math inline">\(\Gamma\)</span>10<strong></strong>.</p><p><strong>4.1</strong> <spanclass="math inline">\(F(x, y,p)\)</span> <spanclass="math inline">\((x, y, p)\in G\)</span> <spanclass="math inline">\(y\)</span> <spanclass="math inline">\(p\)</span> <spanclass="math inline">\(F&#39;_y\)</span> <spanclass="math inline">\(F&#39;_p\)</span> <spanclass="math inline">\(y=\varphi(x),(x\inJ)\)</span>10 <spanclass="math display">\[    (x,\varphi(x)\varphi&#39;(x))\in G \quad(x\in J)\]</span>  <spanclass="math inline">\(y=\varphi(x)\)</span><strong>p-</strong><span class="math display">\[    F(x, y, p)=0, \quad F&#39;_p(x, y, p)=0\quad(p=\frac{\mathrm{d}y}{\mathrm{d}x}) \tag{11}\]</span> 11 <spanclass="math inline">\(p\)</span> <span class="math display">\[    \Delta(x, y)=0 \tag{12}\]</span> 10<strong>P-</strong>.10p-.</p><p><strong></strong> <spanclass="math inline">\(\exists x_0\in J\)</span> <spanclass="math display">\[    F&#39;_p(x_0, y_0, p_0) \neq 0  \]</span> <span class="math inline">\(F\)</span><span class="math inline">\(p\)</span> <spanclass="math inline">\((x_0,y_0)\)</span> <spanclass="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y)\]</span>  <span class="math inline">\(f(x, y)\)</span> <spanclass="math inline">\((x_0, y_0)\)</span> <spanclass="math inline">\(y\)</span>Picard <spanclass="math inline">\((x_0,y_0)\)</span>.</p><p>p- <spanclass="math inline">\(y=\psi(x)\)</span>.</p><p>.</p><p><strong>4.2</strong> <span class="math inline">\(F(x, y,p)\)</span> <span class="math inline">\((x, y,p)\inG\)</span>. 10p- <spanclass="math display">\[    F(x, y, p)=0, \quad F&#39;_p(x, y, p)=0\]</span> <span class="math inline">\(p\)</span> <spanclass="math inline">\(y=\psi(x)(x\inJ)\)</span>10. <spanclass="math display">\[    F&#39;_y(x,\psi(x),\psi&#39;(x)) \neq 0,\quadF&#39;&#39;_{pp}(x,\psi(x),\psi&#39;(x)) \neq 0 \tag{13}\]</span>  <span class="math display">\[    F&#39;_p(x,\psi(x),\psi&#39;(x))=0 \tag{14}\]</span>  <span class="math inline">\(x\in J\)</span>.  <spanclass="math inline">\(y=\psi(x)\)</span>10.</p><p><strong></strong>.</p><p>1314.</p><p> <span class="math display">\[    \biggl( \frac{\mathrm{d}y}{\mathrm{d}x} \biggr)^{2}-y^{2}=0\]</span>  <span class="math inline">\(y=0\)</span> <spanclass="math inline">\(F&#39;_y(x,0,0)=0\)</span> <spanclass="math inline">\(y=x\exp {\pm x}\)</span>.</p><p> <span class="math display">\[    \sin (y \frac{\mathrm{d}y}{\mathrm{d}x})=y\]</span>  <span class="math inline">\(y=0\)</span> <spanclass="math inline">\(F&#39;&#39;_{pp}(x,0,0)=0\)</span><spanclass="math inline">\(y=0\)</span>.</p><p> <span class="math display">\[    y=2x+y&#39;-\frac{1}{3}(y&#39;)^3\]</span>  <spanclass="math inline">\(F&#39;_p(x,\psi(x),\psi&#39;(x))=0\)</span></p><p>2.2 <spanclass="math inline">\(E(y)\)</span> <spanclass="math display">\[    \begin{cases}        E(y)=0, \quad y=0\\        E(y)\neq 0, \quad 0&lt;y\leqslant 1    \end{cases}\]</span>  <span class="math inline">\(y=0\)</span> <spanclass="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=E(y)    \]</span>  <span class="math display">\[    \int_{0}^{1} \frac{\mathrm{d}y}{E(y)}       \]</span> . <strong></strong><spanclass="math inline">\(\Rightarrow\)</span> <spanclass="math inline">\(y=0\)</span> <spanclass="math inline">\(y\neq 0\)</span> <spanclass="math inline">\(\frac{\mathrm{d}y}{E}=\mathrm{d}x \Rightarrow\int_{0}^{1} \frac{\mathrm{d}y}{E}\)</span>.</p><p><span class="math inline">\(\Leftarrow\)</span>$ _{0}^{1}$ <span class="math inline">\(x(y)=x_0+\int_{0}^{y}\frac{\mathrm{d}y}{E}\)</span> <spanclass="math inline">\(y=0\)</span> <spanclass="math inline">\(y&#39;(0)=0\)</span> <spanclass="math inline">\(y=0\)</span>.</p><h2 id=""></h2><p>.</p><p> <span class="math inline">\(C\)</span> <spanclass="math display">\[    K(C): \qquad V(x,y,C)=0 \tag{15}\]</span>  <span class="math inline">\(V(x, y, C)\)</span><span class="math inline">\((x, y, C)\in D\)</span>.</p><p> <spanclass="math inline">\(\Gamma\)</span>.  <spanclass="math inline">\(q\in \Gamma\)</span>15<span class="math inline">\(K(C^*)\)</span> <spanclass="math inline">\(q\)</span> <spanclass="math inline">\(\Gamma\)</span> <spanclass="math inline">\(K(C^*)\)</span> <spanclass="math inline">\(q\)</span> <spanclass="math inline">\(\Gamma\)</span>.  <spanclass="math inline">\(\Gamma\)</span>15<strong></strong>.</p><p>.</p><p><strong>4.3</strong> <span class="math display">\[    F(x, y, \frac{\mathrm{d}y}{\mathrm{d}x})=0 \tag{16}\]</span>  <span class="math display">\[    U(x, y, C)=0 \tag{17}\]</span> 17 <span class="math display">\[    \Gamma: \qquad y=\varphi(x) \quad (x\in J)\]</span>  <spanclass="math inline">\(y=\varphi(x)\)</span>16.</p><p><strong></strong> <spanclass="math inline">\(\Gamma\)</span>.</p><div class="note note-success">            <p>...</p>          </div><p><strong>4.4</strong> <spanclass="math inline">\(\Gamma\)</span>15.<strong>C-</strong> <span class="math display">\[    V(x, y, C)=0, \quad V&#39;_C(x, y, C)=0 \tag{18}\]</span>  <spanclass="math inline">\(C\)</span> <spanclass="math display">\[    \Omega(x, y)=0 \tag{19}\]</span></p><p><strong></strong> <span class="math display">\[    \begin{cases}        x=f(C) \\        y=g(C)    \end{cases}\]</span>  <spanclass="math inline">\(V(x, y,C)=0\)</span> <spanclass="math inline">\(C\)</span>.</p><p>C- <spanclass="math display">\[    (y-1)^{2}\biggl(\frac{\mathrm{d}y}{\mathrm{d}x}\biggr)^{2}=\frac{4}{9}y  \]</span> C- <spanclass="math inline">\(y=0\)</span> <spanclass="math inline">\(y=3\)</span>.</p><p>.<strong>4.5</strong>15C- <spanclass="math display">\[    V(x, y,C)=0 \quad V&#39;_C(x, y, C)=0   \]</span> 15 <spanclass="math display">\[    \Lambda: \quad x=\varphi(C), \quad y=\psi(C) \quad (C\in J)\]</span> <strong></strong><span class="math display">\[    (\varphi&#39;(C),\psi&#39;(C))\neq (0,0), \quad(V&#39;_x,V&#39;_y)\neq (0,0)\]</span>  <spanclass="math inline">\(V&#39;_x=V&#39;_x(\varphi(C),\psi(C),C)\)</span><spanclass="math inline">\(V&#39;_y=V&#39;_y(\varphi(C),\psi(C),C)\)</span>. <span class="math inline">\(\Lambda\)</span>15.<strong></strong> <spanclass="math inline">\(\Lambda\)</span><span class="math inline">\(\Lambda\)</span>.</p><div class="note note-success">            <p>4.3C-.</p>          </div><h2 id=""></h2><p>4.2117-120  ---</p><p>4 https://zhuanlan.zhihu.com/p/97414951</p><hr /><p>.</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3</title>
    <link href="/2022/04/05/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%883%EF%BC%89/"/>
    <url>/2022/04/05/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%883%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><h2 id="picard-">Picard </h2><p><strong>3.1</strong>  <span class="math display">\[    (E):\qquad \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y),\quad y(x_0)=y_0\tag{1}\]</span>  <span class="math inline">\(f(x, y)\)</span><span class="math display">\[    R: \qquad \left\vert x-x_0 \right\vert \leqslant a,\quad \left\verty-y_0 \right\vert \leqslant b       \]</span>  <span class="math inline">\(y\)</span>Lipschitz <span class="math inline">\((E)\)</span><spanclass="math inline">\(I=[x_0-h,x_0+h]\)</span><span class="math display">\[    h= \min\biggl\{a,\frac{b}{M}\biggr\},\quad M&gt;\max_{(x, y)\in R}\left\vert f(x, y) \right\vert     \]</span></p><p><strong></strong>(1)Picard <spanclass="math display">\[    y_{n+1}(x)=y_0+\int_{x_0}^{x} f(x,y_n(x)) \mathrm{d}x  \]</span>  $y_{n+1}(x)-y_n(x) $ <spanclass="math inline">\(I\)</span>.Lipschitz.Lipschitz.</p><p>Picard <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y) \tag{2}\]</span>  <span class="math inline">\(f(x,y)\)</span> <span class="math inline">\(D\)</span> <spanclass="math inline">\(y\)</span>Lipschitz<span class="math inline">\(D\)</span>.</p><p><strong>Osgood </strong> <span class="math inline">\(f(x,y)\)</span> <spanclass="math inline">\(G\)</span> <spanclass="math display">\[    \left\vert f(x, y_1)-f(x, y-2)\right\vert\leqslant F(\left\verty_1-y_2 \right\vert)  \]</span>  <span class="math inline">\(F(r)&gt;0\)</span> <spanclass="math inline">\(r&gt;0\)</span> <spanclass="math display">\[    \int_{0}^{r_1} \frac{1}{F(r)} \mathrm{d}r=\infty\]</span> <span class="math inline">\(r_1&gt;0\)</span>. <span class="math inline">\(f(x, y)\)</span> <spanclass="math inline">\(G\)</span> <spanclass="math inline">\(y\)</span><strong>Osgood</strong>. <spanclass="math inline">\(F(r)=Lr\)</span>LipschitzOsgood.</p><p><strong>3.2</strong> <span class="math inline">\(f(x,y)\)</span> <span class="math inline">\(G\)</span> <spanclass="math inline">\(y\)</span>Osgood(2) <spanclass="math inline">\(G\)</span>.</p><p><strong></strong>.</p><p><strong>Muller</strong>Lipschitz3.1Picard.</p><p> <span class="math display">\[    (E_0):\qquad \frac{\mathrm{d}y}{\mathrm{d}x}=F(x, y),\quad y(0)=0\]</span>  <span class="math display">\[    F(x, y)=    \begin{cases}        0, \qquad x=0,-\infty&lt;y&lt;\infty; \\        2x,\qquad 0&lt;x\leqslant 1, -\infty&lt;y&lt;0; \\        2x-\displaystyle{\frac{4y}{x}},\qquad 0&lt;x\leqslant 1,0\leqslant y&lt;x^{2}; \\        -2x,\qquad 0&lt;x\leqslant 1,x^{2}\leqslant y&lt;\infty \\    \end{cases}\]</span>  <span class="math inline">\(F(x,y)\)</span> <span class="math display">\[    S:\qquad 0\leqslant x\leqslant 1,\quad -\infty&lt;y&lt;\infty\]</span>  <spanclass="math inline">\(y\)</span>Lipschitz.</p><p> <spanclass="math inline">\((E_0)\)</span>Picard <spanclass="math display">\[    y_{n+1}(x)=y_0+\int_{0}^{x} f(x,y_n(x)) \mathrm{d}x \quad(y_0(x)=0)\]</span> <span class="math inline">\((0\leqslant x\leqslant1;n=0,1,2,\cdots )\)</span>.  <span class="math display">\[    y_n(x)=(-1)^{n+1}x^2 \quad (0\leqslant x\leqslant 1)\]</span> <span class="math inline">\((n=1,2,\cdots )\)</span>. <spanclass="math inline">\((E_0)\)</span>Picard.  <spanclass="math inline">\(y=\frac{1}{3}x^2(0\leqslant x\leqslant1)\)</span> <span class="math inline">\((E_{0})\)</span>.</p><h2 id="peano-">Peano </h2><p>Picard <spanclass="math inline">\(f(x, y)\)</span> <spanclass="math inline">\(R\)</span>Euler<span class="math inline">\((E)\)</span> <spanclass="math inline">\(\left\vert x-x_0 \right\vert \leqslanth\)</span>.</p><h3 id="euler-">Euler </h3><p>.</p><p>Euler <span class="math inline">\(\gamma_n\)</span><span class="math display">\[    y=\varphi_n(x) \quad (\left\vert x-x_0 \right\vert &lt;h) \tag{3}\]</span></p><p>Euler <span class="math display">\[    \varphi_n(x)=y_0+\sum_{k=0}^{s-1} f(x_k, y_k)(x_{k+1}-x_k)+f(x_s,y_s)(x-x_s), \quad x_s&lt;x\leqslant x_{0}+h\]</span>  <span class="math display">\[    \varphi_n(x)=y_0+\sum_{k=0}^{-s+1} f(x_k,y_k)(x_{k-1}-x_k)+f(x_{-s}, y{-s})(x-x_{-s}), \quad x_0-h\leqslantx&lt;x_0\]</span> Euler <span class="math inline">\(\left\vertx-x_0 \right\vert \leqslanth\)</span>.</p><h3 id="ascoli-">Ascoli </h3><p> <span class="math display">\[    f_1(x),f_2(x),\cdots,f_n(x)\]</span>  <spanclass="math inline">\(I\)</span><span class="math display">\[    f_{n_1}(x),f_{n_2}(x),\cdots,f_{n_k}(x)\]</span>  <spanclass="math inline">\(I\)</span>.</p><h3 id="peano--1">Peano </h3><p><strong>3.1</strong> Euler(3) <spanclass="math inline">\(\left\vert x-x_0 \right\vert \leqslanth\)</span>.</p><p><strong></strong>Ascoli .</p><p><strong>3.2</strong> Euler <spanclass="math inline">\(y=\varphi_n(x)\)</span> <spanclass="math inline">\(\left\vert x-x_0 \right\vert \leqslanth\)</span> <span class="math display">\[    \varphi_n(x)=y_0+\int_{x_0}^{x} f(x,\varphi_n(x))\mathrm{d}x+\delta_n(x)\]</span>  <spanclass="math inline">\(\delta_n(x)\)</span> <spanclass="math display">\[    \lim_{n \to \infty}\delta_n(x)=0 \quad(\left\vert x-x_0 \right\vert\leqslant h)\]</span> <strong></strong>omitted.</p><p><strong>3.3Peano </strong> <spanclass="math inline">\(f(x, y)\)</span> <spanclass="math inline">\(R\)</span> <spanclass="math display">\[    (E):\qquad \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y),\quad y(x_0)=y_0\tag{4}\]</span>  <span class="math inline">\(\left\vert x-x_0\right\vert \leqslant h\)</span> <spanclass="math inline">\(y=y(x)\)</span> <spanclass="math inline">\(R\)</span> <spanclass="math inline">\(h\)</span>3.1.</p><p><strong></strong>3.13.2<span class="math inline">\(k \to \infty\)</span>.</p><p><strong></strong> <span class="math inline">\(f(x,y)\)</span> <spanclass="math inline">\((E)\)</span>.  <spanclass="math display">\[    f^*(x, y)=    \begin{cases}        1,\qquad 1\leqslant \left\vert x+y \right\vert &lt;\infty \\        (-1)^{n}, \qquad \frac{1}{n+1}\leqslant \left\vert x+y\right\vert \leqslant \frac{1}{n} \quad (n=1,2,\cdots ) \\        0, \qquad \left\vert x+y \right\vert =0    \end{cases}\]</span>  <span class="math display">\[    (E^*): \qquad \frac{\mathrm{d}y}{\mathrm{d}x}=f^*(x, y), \quady(0)=0\]</span> .</p><h2 id=""></h2><p>.</p><p> <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y) \tag{5}\]</span>  <span class="math inline">\(f(x, y)\)</span><span class="math inline">\(G\)</span>.</p><p><strong>3.4</strong> <spanclass="math inline">\(P_0\)</span> <spanclass="math inline">\(G\)</span> <spanclass="math inline">\(\Gamma\)</span>(5) <spanclass="math inline">\(P_0\)</span>.  <spanclass="math inline">\(\Gamma\)</span> <spanclass="math inline">\(G\)</span><span class="math inline">\(G(P_0 \in G_1 \subset G)\)</span><span class="math inline">\(\Gamma\)</span> <spanclass="math inline">\(G_1\)</span>.</p><p><strong></strong> <span class="math inline">\(f(x,y)\)</span> <span class="math inline">\(G\)</span><spanclass="math inline">\(y\)</span>Lipschitz(5)<span class="math inline">\(G\)</span> <spanclass="math inline">\(P_0\)</span> <spanclass="math inline">\(\Gamma\)</span> <spanclass="math inline">\(\Gamma\)</span> <spanclass="math inline">\(G\)</span>.</p><p>.  <spanclass="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=x^{2}+y^{2}\]</span> .</p><div class="note note-info">            <p>..</p>          </div><p>.<strong>3.5</strong> <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y) \tag{6}\]</span>  <span class="math inline">\(f(x,y)\)</span> <span class="math display">\[    S: \qquad \alpha&lt;x&lt;\beta,\quad -\infty&lt;y&lt;\infty\]</span>  <span class="math display">\[    \left\vert f(x, y) \right\vert \leqslant A(x)\left\vert y\right\vert +B(x)\]</span>  <span class="math inline">\(A(x)\geqslant 0\)</span><span class="math inline">\(B(x)\geqslant 0\)</span> <spanclass="math inline">\(\alpha&lt;x&lt;\beta\)</span>.(6) <spanclass="math inline">\(\alpha&lt;x&lt;\beta\)</span>.</p><h2 id=""></h2><p>.<strong>3.6</strong> <spanclass="math inline">\(f(x, y)\)</span> <spanclass="math inline">\(F(x, y)\)</span> <spanclass="math inline">\(G\)</span> <spanclass="math display">\[    f(x, y)&lt;F(x, y),\quad (x, y) \in G\]</span>  <span class="math inline">\(y=\varphi(x)\)</span><span class="math inline">\(y=\Phi(x)\)</span> <spanclass="math inline">\(a&lt;x&lt;b\)</span> <spanclass="math display">\[    (E_1):\qquad \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y),\quady(x_0)=y_0\]</span>  <span class="math display">\[    (E_2):\qquad \frac{\mathrm{d}y}{\mathrm{d}x}=F(x, y),\quady(x_0)=y_0\]</span>  <span class="math inline">\((x_0,y_0)\inG\)</span>.  <span class="math display">\[    \begin{cases}        \varphi(x)&lt;\Phi(x),\qquad x_0&lt;x&lt;b \\        \varphi(x)&gt;\Phi(x),\qquad a&lt;x&lt;x_0    \end{cases}\]</span></p><p><strong></strong>omitted</p><p>(E) <spanclass="math inline">\(y=Z(x)\)</span> <spanclass="math inline">\(y=W(x)\)</span>(E) <spanclass="math inline">\(y=y(x)\)</span> <spanclass="math display">\[    W(x)\leqslant y(x)\leqslant Z(x),\quad(\left\vert x-x_0 \right\vert\leqslant h)\]</span>  <span class="math inline">\(y=W(x)\)</span> <spanclass="math inline">\(y=Z(x)\)</span>(E)<strong></strong><strong></strong>.</p><p><strong>3.7</strong> <spanclass="math inline">\(\sigma&lt;h\)</span> <spanclass="math inline">\(\left\vert x-x_0 \right\vert \leqslant\sigma\)</span>(E).</p><p>(E) <spanclass="math inline">\((x_0,y_0)\)</span>(E).</p><p>(E).</p><p><strong>3.8</strong> <spanclass="math inline">\(f(x, y)\)</span> <spanclass="math inline">\(F(x, y)\)</span> <spanclass="math inline">\(G\)</span> <spanclass="math display">\[    f(x, y)\leqslant F(x, y),\quad (x, y) \in G\]</span>  <span class="math inline">\(y=\varphi(x)\)</span><span class="math inline">\(y=\Phi(x)\)</span> <spanclass="math inline">\(a&lt;x&lt;b\)</span> <spanclass="math display">\[    (E_1):\qquad \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y),\quady(x_0)=y_0\]</span>  <span class="math display">\[    (E_2):\qquad \frac{\mathrm{d}y}{\mathrm{d}x}=F(x, y),\quady(x_0)=y_0\]</span>  <span class="math inline">\(y=\varphi(x)\)</span><spanclass="math inline">\((E_1)\)</span><spanclass="math inline">\(y=\Phi(x)\)</span> <spanclass="math inline">\((E_2)\)</span><span class="math display">\[    \varphi(x)\leqslant \Phi(x),\qquad x_0\leqslant x&lt;b; \\    \varphi(x)\geqslant \Phi(x),\qquad a&lt;x\leqslant x_0\]</span> <strong></strong>3.63.7.</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2</title>
    <link href="/2022/04/01/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%882%EF%BC%89/"/>
    <url>/2022/04/01/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%882%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id=""></h1><h2 id=""></h2><p> <span class="math display">\[P(x,y)\mathrm{d}x+Q(x,y)\mathrm{d}y=0\]</span> <spanclass="math inline">\(\Phi(x,y)\)</span> <spanclass="math display">\[\mathrm{d}\Phi(x,y)=P(x,y)\mathrm{d}x+Q(x,y)\mathrm{d}y\]</span><strong></strong><strong></strong>. <span class="math display">\[\Phi(x,y)=C\]</span> . .</p><p><strong>2.1</strong> <spanclass="math inline">\(P(x,y)\)</span><spanclass="math inline">\(Q(x,y)\)</span> <spanclass="math display">\[R:\quad \alpha&lt;a&lt;\beta,\quad \gamma&lt;y&lt;\delta\]</span> <spanclass="math inline">\(\frac{\partial P}{\partial y}\)</span><spanclass="math inline">\(\frac{\partial Q}{\partial x}\)</span> <span class="math display">\[\frac{\partial P}{\partial y}(x,y)=\frac{\partial Q}{\partial x}(x,y)\]</span> <span class="math inline">\(R\)</span>. <span class="math display">\[\int_{x_0}^xP(x,y)\mathrm{d}x+\int_{y_0}^yQ(x_0,y)\mathrm{d}y=C\]</span>  <span class="math display">\[\int_{x_0}^xP(x,y_0)\mathrm{d}x+\int_{y_0}^yQ(x,y)\mathrm{d}y=C\]</span> <span class="math inline">\((x_0,y_0)\)</span><spanclass="math inline">\(R\)</span>.</p><h2 id=""></h2><p> <span class="math display">\[    P(x,y)\mathrm{d}x+Q(x,y)\mathrm{d}y=0\]</span> <span class="math inline">\(P(x,y)\)</span><spanclass="math inline">\(Q(x,y)\)</span><spanclass="math inline">\(x\)</span><spanclass="math inline">\(y\)</span><strong></strong>. <span class="math display">\[    P(x,y)=X(x)Y_1(y),\quad Q(x,y)=X_1(x)Y(y)\]</span> <spanclass="math inline">\(X_1(x)Y_1(y)\)</span><span class="math display">\[\int_{}^{}\frac{X(x)}{X_1(x)}\mathrm{d}x+\int_{}^{}\frac{Y(y)}{Y_1(y)}\mathrm{d}y=C\]</span> <span class="math display">\[x=a_i,\quad (i=1,2,\cdots),\text{$a_i$$X_1(x)=0$}\]</span>  <span class="math display">\[y=b_j,\quad (i=1,2,\cdots),\text{$b_j$$Y_1(y)=0$}\]</span></p><p>  <span class="math display">\[\frac{\mathrm{d}y}{\mathrm{d}x}=f(y)\]</span> <span class="math inline">\(f(y)\)</span><spanclass="math inline">\(y=a\)</span><spanclass="math inline">\(\lvert y-a\rvert \leqslant\varepsilon\)</span><spanclass="math inline">\(f(y)=0\)</span><spanclass="math inline">\(y=a\)</span>. <spanclass="math inline">\(y=a\)</span><span class="math display">\[\biggl\lvert \int_{a}^{a\pm \varepsilon}\frac{\mathrm{d}y}{f(y)}\biggr\rvert=\infty \text{}\]</span></p><p>proof <span class="math inline">\(y(x_0)=a\)</span>.<spanclass="math inline">\(y(x)=a\)</span>.</p><p><span class="math inline">\(\Leftarrow\)</span><spanclass="math inline">\(y=g(x)\neq a\)</span><spanclass="math inline">\(\exists x_1\)</span><spanclass="math inline">\(h=g(x_1)-a\neq 0\)</span> <spanclass="math display">\[    \biggl\lvert \int_{a}^{a+h}\frac{1}{f(y)}\mathrm{d}y\biggr\rvert=\biggl\lvert\int_{x_0}^{x_1}\mathrm{d}x\biggr\rvert          \]</span> .</p><p><span class="math inline">\(\Rightarrow\)</span><span class="math inline">\(y(x)=a\)</span> <spanclass="math inline">\(g(x)=\int_{a}^{x}\frac{1}{f(y)}\mathrm{d}y\)</span>  <span class="math display">\[    g&#39;(y)=\frac{1}{f(y)} ,y\neq a    \]</span>  <span class="math inline">\(y=h(g)\neqa\)</span> <span class="math display">\[    \frac{\mathrm{d}h(g)}{\mathrm{d}g}=\frac{\mathrm{d}y}{\mathrm{d}g(y)} =f(y)=f(h(g))\]</span>  <span class="math inline">\(y=h(g)\)</span> <span class="math inline">\(h(g)\neq a\)</span> </p><h2 id=""></h2><p><strong></strong> <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x} +p(x)y=q(x) \qquad (1)\]</span> <span class="math inline">\(p(x)\)</span><spanclass="math inline">\(q(x)\)</span> <spanclass="math inline">\(I=(a,b)\)</span><spanclass="math inline">\(q(x)\equiv 0\)</span> <spanclass="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x} +p(x)y=0 \qquad (2)\]</span> <spanclass="math inline">\(q(x)\)</span>(1)<strong></strong>(2)<strong></strong>.</p><p>(2) <span class="math display">\[    y=C\mathrm{e}^{-\int_{}^{}p(x)\mathrm{d}x}\]</span> <span class="math inline">\(C\)</span></p><p>(1) <span class="math display">\[    y=\mathrm{e}^{-\int_{}^{}p(x)\mathrm{d}x}\biggl(C+\int_{}^{}q(x)\mathrm{e}^{\int_{}^{}p(x)\mathrm{d}x}\mathrm{d}x    \biggr)\]</span> <spanclass="math inline">\(C\)</span>.<strong></strong>.  <spanclass="math display">\[    \mu(x)=\mathrm{e}^{\int_{}^{}p(x)\mathrm{d}x}\]</span> . <spanclass="math inline">\(y(x_0)=y_0\)</span> <spanclass="math display">\[    y=y_0\mathrm{e}^{-\int_{x_0}^{x}p(x)\mathrm{d}x}+\int_{x_0}^{x}q(s)\mathrm{e}^{-\int_{s}^{x}p(t)\mathrm{d}t}\mathrm{d}s\]</span> <span class="math inline">\(p(x)\)</span><spanclass="math inline">\(q(x)\)</span><spanclass="math inline">\(I\)</span>.</p><h2 id=""></h2><h3 id=""><strong></strong></h3><p> <span class="math display">\[    P(x,y)\mathrm{d}x+Q(x,y)\mathrm{d}y=0         \]</span> <span class="math inline">\(P(x)\)</span><spanclass="math inline">\(Q(x)\)</span><spanclass="math inline">\(x\)</span><spanclass="math inline">\(y\)</span><spanclass="math inline">\(m\)</span> <spanclass="math display">\[    P(tx,ty)=t^mP(x,y),\quad Q(tx,ty)=t^mQ(x,y),\]</span><strong></strong>. <span class="math display">\[    y=ux\]</span>  <span class="math display">\[    x^m[P(1,u)+uQ(1,u)]\mathrm{d}x+x^{m+1}Q(1,u)\mathrm{d}u=0     \]</span> .  <spanclass="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=\Phi(\frac{y}{x} )  \]</span> <spanclass="math inline">\(x=0\)</span><spanclass="math inline">\(x=0\)</span>.</p><h3 id="bernoulli">(Bernoulli)</h3><p> <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}+p(x)y=q(x)y^n  \]</span> <spanclass="math inline">\(n\)</span><spanclass="math inline">\(n\neq 0\)</span><spanclass="math inline">\(1\)</span>. <spanclass="math inline">\((1-n)y^{-n}\)</span>  <spanclass="math inline">\(z=y^{1-n}\)</span>  <spanclass="math display">\[    \frac{\mathrm{d}z}{\mathrm{d}x} +(1-n)p(x)z=(1-n)q(x)\]</span> <spanclass="math inline">\(z\)</span>.</p><h3 id="riccati">(Riccati)</h3><p> <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x} =f(x,y)\]</span> <spanclass="math inline">\(f(x,y)\)</span><spanclass="math inline">\(y\)</span><span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x} =p(x)y^2+q(x)y+r(x)\]</span> <span class="math inline">\(p(x)\)</span>.</p><p><strong></strong> <spanclass="math inline">\(y=\phi_1(x)\)</span>.</p><p><spanclass="math inline">\(y=u+\phi_1(x)\)</span>.</p><p><strong></strong>  <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x} +ay^2=bx^m\]</span> <span class="math inline">\(a\neq 0\)</span><spanclass="math inline">\(b,m\)</span>. <spanclass="math inline">\(x\neq 0\)</span><span class="math inline">\(y\neq 0\)</span>.  <span class="math display">\[    m=0,-2,\frac{-4k}{2k+1} ,\frac{-4k}{2k-1}\quad(k=1,2,\cdots)  \]</span> .</p><h2 id=""></h2><p> <span class="math display">\[    P(x,y)\mathrm{d}x+Q(x,y)\mathrm{d}y=0 \tag{1}    \]</span> <span class="math inline">\(\mu =\mu(x,y)\)</span> <spanclass="math display">\[    \mu (x,y)P(x,y)\mathrm{d}x+\mu(x,y)Q(x,y)\mathrm{d}y=0 \tag{2}\]</span>  <span class="math display">\[    \frac{\partial (\mu P)}{\partial y} = \frac{\partial (\muQ)}{\partial x} \tag{3}\]</span> <spanclass="math inline">\(\mu=\mu(x,y)\)</span><strong></strong>.(3) <span class="math display">\[    P\frac{\partial \mu}{\partial y} -Q\frac{\partial \mu}{\partial x}=\biggl(\frac{\partial Q}{\partial x} -\frac{\partial P}{\partial y}\biggr)\mu\]</span> <strong></strong> (1)<spanclass="math inline">\(x\)</span> <spanclass="math display">\[    \frac{1}{Q(x,y)} \biggl(\frac{\partial P(x,y)}{\partial y}-\frac{\partial Q(x,y)}{\partial x} \biggr) \tag{4}\]</span> <span class="math inline">\(x\)</span><spanclass="math inline">\(y\)</span>. (4)<spanclass="math inline">\(G(x)\)</span> <spanclass="math display">\[    \mu(x)=\mathrm{e}^{\int G(x)\mathrm{d}x}\]</span> (1).</p><p>(1)<spanclass="math inline">\(y\)</span> <spanclass="math display">\[    \frac{1}{P(x,y)} \biggl(\frac{\partial Q(x,y)}{\partial x}-\frac{\partial P(x,y)}{\partial y} \biggr) =H(y)\]</span>  <span class="math display">\[    \mu(y)=\mathrm{e}^{\int H(y)\mathrm{d}y}\]</span> (1).</p><p><strong></strong>. </p><p><strong></strong> <span class="math inline">\(\mu =\mu(x,y)\)</span>(1) <spanclass="math display">\[    \mu P(x,y)\mathrm{d}x+\mu Q(x,y)\mathrm{d}y=\mathrm{d} \Phi (x,y)\]</span> <spanclass="math inline">\(\mu(x,y)g(\Phi(x,y))\)</span>(1)<spanclass="math inline">\(g(\cdot)\)</span>.</p><p>.  <span class="math display">\[    \mu g(\Phi) P\mathrm{d}x+\mu g(\Phi)Q\mathrm{d}y=\mathrm{d}(\intg)(\Phi)\]</span></p><p><spanclass="math inline">\(P(x,y)\mathrm{d}x+Q(x,y)\mathrm{d}y=0\)</span><span class="math display">\[    \mu(x,y)=\frac{1}{xP(x,y)+yQ(x,y)}  \]</span> . Euler.</p><h2 id=""></h2><p> <span class="math inline">\((x, y)\)</span> <spanclass="math display">\[    \Phi(x,y,C)=0 \tag{5}   \]</span>  <spanclass="math inline">\(C\)</span>.  <spanclass="math display">\[    \Psi(x,y,K) \tag{6}\]</span>  <spanclass="math inline">\(K\)</span>(6)(5)<span class="math inline">\(\alpha\)</span> (<spanclass="math inline">\(-\frac{\pi}{2}&lt;\alpha&lt;\frac{\pi}{2}\)</span>,).(6)(5)<strong></strong>. <spanclass="math inline">\(\alpha=\frac{\pi}{2}\)</span>(6)(5)<strong></strong>.</p><p>(5) <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=H(x, y)\]</span>  <span class="math display">\[    H(x, y)=-\frac{\Phi&#39;_x (x, y,C(x, y))}{\Phi&#39;_y (x,y,C(x,y))}\]</span>  <span class="math inline">\(C=C(x, y)\)</span> <spanclass="math inline">\(\Phi(x,y,C)\)</span>.</p><p> <span class="math inline">\(\alpha\neq\frac{\pi}{2}\)</span> <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=\frac{H(x, y)+\tan\alpha}{1-H(x,y)\tan \alpha}\]</span>  <spanclass="math inline">\(\alpha=\frac{\pi}{2}\)</span> <spanclass="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=-\frac{1}{H(x, y)}\]</span>  <span class="math inline">\(H(x,y)\)</span>.</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1</title>
    <link href="/2022/04/01/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%881%EF%BC%89/"/>
    <url>/2022/04/01/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%881%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>ODENeuronalDynamicsODEArnoldOrdinaryDifferentialEquations</p><p>1.1 <span class="math display">\[F(x,y,y&#39;,\cdots,y^{(n)})\]</span> <spanclass="math inline">\(n\)</span>.</p><p> <span class="math display">\[y(x_0)=y_0,y&#39;(x_0)=y&#39;_0,\cdots,y^{(n-1)}(x_0)=y^{(n-1)}_0\]</span></p><p>1.2. <spanclass="math inline">\(n\)</span> <spanclass="math display">\[y=\varphi(x,C_1,\cdots,C_n)\]</span> <spanclass="math inline">\(n\)</span><spanclass="math inline">\(C_1,\cdots,C_n\)</span>Jacobi<span class="math display">\[\det\begin{bmatrix}    \frac{\partial \varphi}{\partial C_1} &amp; \frac{\partial\varphi}{\partial C_2} &amp; \cdots &amp; \frac{\partial\varphi}{\partial C_n} \\    \frac{\partial \varphi&#39;}{\partial C_1} &amp; \frac{\partial\varphi&#39;}{\partial C_2} &amp; \cdots &amp; \frac{\partial\varphi&#39;}{\partial C_n} \\    \vdots &amp; \vdots &amp; &amp; \vdots \\    \frac{\partial \varphi^{(n-1)}}{\partial C_1} &amp; \frac{\partial\varphi^{(n-1)}}{\partial C_2} &amp; \cdots &amp; \frac{\partial\varphi^{(n-1)}}{\partial C_n} \\\end{bmatrix}\]</span> <spanclass="math inline">\(y=\varphi(x)\)</span>..</p><p><spanclass="math inline">\(C_1,\cdots,C_n\)</span>..<spanclass="math inline">\(C_1,\cdots,C_n\)</span>. <spanclass="math inline">\(F\)</span><spanclass="math inline">\(C^1\)</span>ODE.</p><p> <span class="math display">\[\frac{\textrm{d}y}{\textrm{d}x}=f(x,y)\]</span> <span class="math inline">\(f(x,y)\)</span><spanclass="math inline">\(G\)</span>. <spanclass="math inline">\(y=\varphi(x)\)</span><spanclass="math inline">\((x,y)\)</span><spanclass="math inline">\(\Gamma\)</span><strong></strong>..<strong></strong><strong></strong><strong></strong>.</p><p><spanclass="math inline">\(f(x,y)=k\)</span><spanclass="math inline">\(L_k\)</span><strong></strong>..</p><p> <span class="math display">\[\frac{\textrm{d}y}{\textrm{d}x}=-\frac{P(x,y)}{Q(x,y)}\]</span> <spanclass="math inline">\(P(x_0,y_x)=Q(x_0,y_0)=0\)</span><spanclass="math inline">\((x_0,y_0)\)</span><strong></strong>.. .</p>]]></content>
    
    
    <categories>
      
      <category></category>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2</title>
    <link href="/2022/03/29/%E9%AB%98%E4%BB%A3%E8%AF%BE%E5%A4%96%E9%A2%982/"/>
    <url>/2022/03/29/%E9%AB%98%E4%BB%A3%E8%AF%BE%E5%A4%96%E9%A2%982/</url>
    
    <content type="html"><![CDATA[<h1 id="2">2</h1><h3id="let-abin-mathbbcntimes-n-be-such-that-i-aa-geqslant-0-i-bb-geqslant-0.-then-deti-ab2geqslant-deti-aa-cdot-deti-bb">1.Let<span class="math inline">\(A,B\in \mathbb{C}^{n\times n}\)</span> besuch that <span class="math inline">\(I-AA^{*} \geqslant 0\)</span>,<span class="math inline">\(I-BB^{*} \geqslant 0\)</span>. Then <spanclass="math display">\[ (\det(I-AB^*))^2\geqslant \det(I-AA^*) \cdot\det(I-BB^*)\]</span></h3><p><strong>proof1</strong><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="    ">[1]</span></a></sup></p><p>Lemma Hermite <span class="math inline">\(A\geqslant0\)</span><span class="math inline">\(B\geqslant 0\)</span>. <spanclass="math display">\[ \det(A+B) \geqslant \det(A) + \det(B)\]</span> <span class="math inline">\(\Longleftrightarrow\)</span> <spanclass="math inline">\(A=0\)</span><spanclass="math inline">\(B=0\)</span> <spanclass="math inline">\(\det(A+B)=0\)</span></p><p>proof <span class="math inline">\(P\)</span><span class="math inline">\(PBP^*=\Lambda=\text{diag}(\lambda_1,\cdots,\lambda_n), \lambda_1 \geqslant \cdots \geqslant\lambda_n \geqslant 0\)</span> <spanclass="math inline">\(B\)</span>.  <spanclass="math inline">\(\det(A+B)=\det(PAP^*+\Lambda)\)</span><spanclass="math inline">\(\det(A)=\det(PAP^*)\)</span><spanclass="math inline">\(\det(B)=\det(\Lambda)\)</span> <spanclass="math inline">\(PAP^*\geqslant 0\)</span><span class="math inline">\(B=\Lambda\)</span>.</p><p> <span class="math inline">\(\det(A+\Lambda)\)</span> <spanclass="math display">\[\det(A+\Lambda)=\det(A)+\sum_{i=1}^n\lambda_id_i+\cdots+\prod_{i=1}^n\lambda_i\]</span></p><p> <span class="math inline">\(d_{i_1\cdots i_k}\)</span><span class="math inline">\(A\)</span> <spanclass="math inline">\(i_1,\cdots,i_k\)</span><span class="math inline">\(A\geqslant 0\)</span> <spanclass="math inline">\(d_{i_1\cdots i_k}\geqslant 0\)</span>.  <spanclass="math inline">\(\det(B)=\prod_{i=1}^n\lambda_i\)</span> <spanclass="math display">\[\det(A+\Lambda)\geqslant\det(A)+\det(\Lambda)\]</span></p><p>. ..</p><p>(1) <span class="math inline">\(\det(B)\neq 0\)</span><span class="math inline">\(\lambda_i\neq 0\)</span>.<span class="math inline">\(d_{i_1\cdotsi_n}=0\)</span>. <spanclass="math inline">\(k=n-1\)</span><spanclass="math inline">\(A\)</span>.  <spanclass="math inline">\(A\geqslant 0\)</span><spanclass="math inline">\(A=0\)</span>.</p><p>(2) <span class="math inline">\(\det(B)=0\)</span> <spanclass="math inline">\(B\neq 0\)</span>.  <spanclass="math inline">\(\lambda_i \neq 0\)</span>.  <spanclass="math inline">\(d_{i_1\cdots i_k} \geqslant0\)</span> <spanclass="math inline">\(d_{i_1\cdots i_k} = 0\)</span>. <spanclass="math inline">\(A\)</span> <spanclass="math inline">\(\det(A)=0\)</span>. <spanclass="math inline">\(\det(A+B)=0\)</span>.</p><p><spanclass="math inline">\(I-AA^*,I-BB^*\)</span> <spanclass="math display">\[    (I-AB^*)(I-BB^*)^{-1}(I-AB^*)^*    =(I-AA^*)+(A-B)(I-B^*B)^{-1}(A-B)^*\]</span></p><p><span class="math inline">\(\det(I-BB^*)=\det(I-B^*B)\)</span><spanclass="math inline">\(I-AA^* &gt; 0\)</span><spanclass="math inline">\((A-B)(I-BB^*)^{-1}(A-B)^* &gt; 0\)</span>. <span class="math display">\[\begin{aligned}    &amp;\lvert \det(I-AB^*) \rvert^2[\det(I-BB^*)]^{-1} \\    =&amp;\det[(I-AA^*)+(A-B)(I-B*B)^{-1}(A-B)^*] \\    \geqslant&amp;\det(I-AA^*)+\det(A-B)(I-B^*B)^{-1}(A-B)^* \\    =&amp;\det(I-AA^*)+\lvert\det(A-B)\rvert^2[\det(I-BB^*)]^{-1}\end{aligned}\]</span> Lemma <span class="math display">\[(A-B)(I-B^*B)^{-1}(A-B)^*=[(A-B)(I-B^*B)^{-\frac{1}{2}}][(A-B)(I-B^*B)^{-1\frac{1}{2}}]^*=0\]</span>  <spanclass="math inline">\((A-B)(I-B^*B)^{-\frac{1}{2}}=0\)</span> <span class="math inline">\(A=B\)</span>. .</p><p>Lemma</p><p>prop. Hermite<span class="math inline">\(A\)</span><spanclass="math inline">\(B\)</span> <spanclass="math inline">\(A\geqslant B\)</span>.  <spanclass="math inline">\(A-B \geqslant 0\)</span> <spanclass="math display">\[\det(A)\geqslant\det(B)\]</span> .</p><p><strong>proof 2</strong> <span class="math display">\[    M=    \begin{bmatrix}        I &amp; A^* \\        B &amp; I \\    \end{bmatrix}      \begin{bmatrix}        I &amp; -B^* \\        -A &amp; I    \end{bmatrix}\]</span> .</p><h3id="let-h_1h_2in-mathbbcntimes-n-such-that-h_1h_1-h_2h_2-h_10-h_20.-then">2.Let <span class="math inline">\(H_1,H_2\in \mathbb{C^{n\timesn}}\)</span> such that <span class="math inline">\(H_1^*=H_1\)</span>,<span class="math inline">\(H_2^*=H_2\)</span>, <spanclass="math inline">\(H_1&gt;0\)</span>, <spanclass="math inline">\(H_2&gt;0\)</span>. Then</h3><h3 id="if-h_2-h_10-then-h_1-1-h_2-10.">(1) If <spanclass="math inline">\(H_2-H_1&gt;0\)</span>, then <spanclass="math inline">\(H_1^{-1}-H_2^{-1}&gt;0\)</span>.</h3><h3id="minkowski-lvert-h_1-rvertfrac1nlvert-h_2-rvertfrac1n-leqslant-lvert-h_1h_2-rvertfrac1n-and-the-equality-holds-iff-h_2ah_1-for-some-ain-mathbbr.">(2)(Minkowski) <span class="math inline">\(\lvert H_1\rvert^{\frac{1}{n}}+\lvert H_2 \rvert^{\frac{1}{n}} \leqslant \lvertH_1+H_2 \rvert^{\frac{1}{n}}\)</span> and the equality holds iff <spanclass="math inline">\(H_2=aH_1\)</span>, for some <spanclass="math inline">\(a\in \mathbb{R}^+\)</span>.</h3><p></p><p>Lemma <span class="math inline">\(A\)</span><spanclass="math inline">\(B\)</span><spanclass="math inline">\(n\)</span>Hermite. <spanclass="math inline">\(A&gt;0\)</span><spanclass="math inline">\(B\geqslant 0\)</span></p><ol type="1"><li><p><span class="math inline">\(A\geqslant B \Longleftrightarrow\lambda_1(BA^{-1})\leqslant 1\)</span></p></li><li><p><span class="math inline">\(A &gt; B \Longleftrightarrow\lambda_1(BA^{-1})&lt; 1\)</span>.</p></li></ol><p> <span class="math inline">\(\lambda_1(A)\)</span><spanclass="math inline">\(A\)</span>.</p><p><strong>proof</strong> (1) <span class="math display">\[\begin{aligned}    A\geqslant B &amp;\LongleftrightarrowA^{-\frac{1}{2}}(A-B)A^{-\frac{1}{2}}\geqslant 0 \\    &amp;\LongleftrightarrowI_n-A^{-\frac{1}{2}}BA^{-\frac{1}{2}}\geqslant 0 \\    &amp;\Longleftrightarrow\lambda_1(I_n-A^{-\frac{1}{2}}BA^{-\frac{1}{2}})\geqslant 0 \\    &amp;\Longleftrightarrow\lambda_1(A^{-\frac{1}{2}}BA^{-\frac{1}{2}})\leqslant1 \\    &amp;\Longleftrightarrow\lambda_1(BA^{-1})\leqslant 1\end{aligned}\]</span></p><p>(1). (2).</p><p><span class="math inline">\(H_2&gt; H_1\Longleftrightarrow\lambda_1(H_1H_2^{-1})&lt; 1\Longleftrightarrow\lambda_1(H_2^{-1}H_1)&lt; 1 \Longleftrightarrow H_1^{-1}&gt;H_2^{-1}\)</span></p><ol start="2" type="1"><li><spanclass="math inline">\(H_1=I\)</span> <spanclass="math display">\[(\det(I_n+H_2))^{\frac{1}{n}}\geqslant 1+(\det(B))^{\frac{1}{n}}\]</span></li></ol><p> <spanclass="math inline">\(\lambda_1\geqslant\cdots\geqslant\lambda_n&gt;0\)</span><spanclass="math inline">\(H_2\)</span> <spanclass="math display">\[\prod_{i=1}^n(1+\lambda_i)\geqslant(1+\sqrt[n]{\lambda_1\cdots\lambda_n})^n\]</span> Holder. <spanclass="math inline">\(\lambda_1=\cdots\lambda_n=a\)</span><spanclass="math inline">\(H_2=aH_1\)</span>.</p><h3id="assume-that-n-leqslant-m-and-ain-mathbbcntimes-m-bin-mathbbcmtimes-p-ran.-show-that-detbi_m-aaa-1ab-leqslant-detbb">3.Assume that <span class="math inline">\(n \leqslant m\)</span>, and<span class="math inline">\(A\in \mathbb{C}^{n\times m}\)</span>, <spanclass="math inline">\(B\in \mathbb{C^{m\times p}}\)</span>, <spanclass="math inline">\(r(A)=n\)</span>. Show that <spanclass="math display">\[\det[B^*(I_m-A^*(AA^*)^{-1}A)B] \leqslant\det(B^*B)\]</span></h3><p><strong>proof1</strong>Lemma. <spanclass="math inline">\(B^*B\)</span><spanclass="math inline">\(B^*A^*(AA^*)^{-1}AB\)</span><spanclass="math inline">\(I_m-A^*(AA^*)^{-1}A\)</span><spanclass="math inline">\(AA^*=\text{diag}(\lambda_1,\cdots,\lambda_n)\)</span><spanclass="math inline">\(\lambda_i\)</span>.  <spanclass="math display">\[A^*\text{diag}(\frac{1}{\lambda_1},\cdots\frac{1}{\lambda_n})A=\frac{1}{\lambda_1\cdots\lambda_n}A^*A\]</span>  <span class="math display">\[(A^*A)^2=A^*AA^*A=A^*\text{diag}(\lambda_1\cdots\lambda_n)A=\lambda_1\cdots\lambda_nA^*A\]</span> <spanclass="math inline">\(A^*(AA^*)^{-1}A\)</span>1. <spanclass="math inline">\(I_m-A^*(AA^*)^{-1}A\)</span>.</p><p><strong>proof 2</strong> <spanclass="math inline">\(A^*(AA^*)^{-1}A\)</span> <spanclass="math inline">\((AA^*)^{-1}AA^*=I_n\)</span><spanclass="math inline">\(A^*(AA^*)^{-1}A\)</span> <spanclass="math inline">\(1\)</span>.</p><h3id="let-hin-mathbbcntimes-n-be-such-that-hh-and-hgeqslant-0-then-there-exists-a-unique-semipositive-definite-hermite-matrix-s-such-that-hs2.-furthermore-s-is-a-polynomial-of-h.">4.Let <span class="math inline">\(H\in \mathbb{C}^{n\times n}\)</span> besuch that <span class="math inline">\(H^*=H\)</span> and <spanclass="math inline">\(H\geqslant 0\)</span>, then there exists a uniquesemipositive-definite Hermite matrix <spanclass="math inline">\(S\)</span> such that <spanclass="math inline">\(H=S^2\)</span>. Furthermore, <spanclass="math inline">\(S\)</span> is a polynomial of <spanclass="math inline">\(H\)</span>.</h3><p>proof<spanclass="math inline">\(H\)</span><spanclass="math inline">\(P\)</span><spanclass="math inline">\(H=P\text{diag}(\lambda_1,\cdots,\lambda_n)P^*\)</span><spanclass="math inline">\(\lambda_i\)</span>. <spanclass="math inline">\(S=P\text{diag}(\sqrt{\lambda_1},\cdots,\sqrt{\lambda_n})P^*\)</span><spanclass="math inline">\(H=S^2\)</span>. .</p><p><spanclass="math inline">\(\sqrt{\lambda_1},\cdots,\sqrt{\lambda_n}\)</span><spanclass="math inline">\(t\)</span><spanclass="math inline">\(\sqrt{\lambda_1},\cdots,\sqrt{\lambda_t}\)</span>Lagrange<span class="math display">\[f(\lambda)=\sum_{i=1}^t\sqrt{\lambda_i}\prod_{k\neqi}\frac{\lambda-\lambda_k}{\lambda_i-\lambda_k}\]</span>  <span class="math display">\[\text{diag}(\sqrt{\lambda_1},\cdots,\sqrt{\lambda_n})=f(\text{diag}(\lambda_1,\cdots,\lambda_n))\]</span>  <span class="math display">\[S=P\text{diag}(\sqrt{\lambda_1},\cdots,\sqrt{\lambda_n})P^*=f(H)\]</span></p><h3 id="let-h-in-mathbbc2n-be-such-that-hh-h0-and-hjjbarh-where">5. Let<span class="math inline">\(H \in \mathbb{C}^{2n}\)</span> be such that<span class="math inline">\(H^*=H\)</span>, <spanclass="math inline">\(H&gt;0\)</span>, and <spanclass="math inline">\(HJ=J\bar{H}\)</span>, where</h3><p><span class="math display">\[J=\begin{bmatrix}    0 &amp; I_n \\    -I_n &amp; 0 \\\end{bmatrix}\]</span></p><h3id="show-that-there-exists-qin-gl_2nmathbbc-such-that-qjjbarq-and-hqq.">showthat there exists <span class="math inline">\(Q\inGL_{2n}(\mathbb{C})\)</span> such that <spanclass="math inline">\(QJ=J\bar{Q}\)</span>, and <spanclass="math inline">\(H=Q^*Q\)</span>.</h3><p><strong></strong> <span class="math inline">\(Q\)</span><span class="math inline">\(H\)</span><spanclass="math inline">\(Q^*=Q\)</span>.  <spanclass="math inline">\(Q\)</span> <spanclass="math inline">\(H\)</span> <spanclass="math inline">\(Q=g(H)\)</span> <spanclass="math inline">\(g(x)\in \mathbb{C}[x]\)</span>.  <spanclass="math display">\[    QJ=g(H)J=Jg(\bar{H})=J \overline{g(H)}=J \bar{Q}\]</span></p><h3id="let-v-be-a-n-dimensional-inner-space-and-v_1cdotsv_s-are-finitely-many-proper-subspaces-of-v.-then-there-exists-an-orthonormal-basis-alpha_1cdotsalpha_n-of-v-such-that-alpha_inotin-bigcup_j1sv_j-i12cdotsn.">6.Let V be a n-dimensional inner space, and <spanclass="math inline">\(V_1,\cdots,V_s\)</span> are finitely many propersubspaces of <span class="math inline">\(V\)</span>. Then there existsan orthonormal basis <spanclass="math inline">\(\{\alpha_1,\cdots,\alpha_n\}\)</span> of <spanclass="math inline">\(V\)</span> such that <spanclass="math inline">\(\alpha_i\notin \bigcup_{j=1}^sV_j,i=1,2,\cdots,n\)</span>.</h3><p>proof<spanclass="math inline">\(\alpha_1\)</span><spanclass="math inline">\(\alpha_1\inV/\bigcup_{j=1}^sV_j/\bigcup_{j=1}^sV_j^\bot\)</span>.<span class="math inline">\(\langle\alpha_1\rangle^\bot\)</span><spanclass="math inline">\(V_j\)</span><spanclass="math inline">\(\langle\alpha_1\rangle ^\bot\)</span>.<spanclass="math inline">\(\alpha_2,\cdots\)</span><spanclass="math inline">\(\alpha_n\)</span>.</p><h3id="let-abin-on-i.e.-abin-mathbbrntimes-n-and-amathsft-ai-bmathsft-bi.-if-detadetb0-then-detab0.">7.Let <span class="math inline">\(A,B\in O(n)\)</span> i.e. <spanclass="math inline">\(A,B\in \mathbb{R}^{n\times n}\)</span>, and <spanclass="math inline">\(A^{\mathsf{T}} A=I\)</span>, <spanclass="math inline">\(B^{\mathsf{T}} B=I\)</span>. If <spanclass="math inline">\(\det(A)+\det(B)=0\)</span>, then <spanclass="math inline">\(\det(A+B)=0\)</span>.</h3><p>proof<spanclass="math inline">\(\det(A)=-\det(B)=1\)</span><spanclass="math inline">\((A+B)^\top=A^\top(A+B)B^\top\)</span>.</p><p><spanclass="math inline">\(A\)</span><spanclass="math inline">\(B\)</span><spanclass="math inline">\(A+B\)</span><spanclass="math inline">\(1\)</span><spanclass="math inline">\(-1\)</span><spanclass="math inline">\(2n\)</span><spanclass="math inline">\(\det(A+B)=0\)</span>.</p><h3id="assume-that-pin-on-and-atextdiaga_1cdotsa_n-a_iin-mathbbr-then-the-eigenvalues-lambda_j1leqslant-j-leqslant-n-of-pa-satisfies-mleqslantlvertlambda_j-rvert-leqslant-m-where-mmin_1leqslant-jleqslant-na_j-mmax_1leqslant-jleqslant-na_j.">8.Assume that <span class="math inline">\(P\in O(n)\)</span>, and <spanclass="math inline">\(A=\text{diag}(a_1,\cdots,a_n)\)</span>, <spanclass="math inline">\(a_i\in \mathbb{R}\)</span>, then the eigenvalues<span class="math inline">\(\lambda_j,1\leqslant j \leqslant n\)</span>of <span class="math inline">\(PA\)</span> satisfies <spanclass="math inline">\(m\leqslant\lvert\lambda_j \rvert \leqslantM\)</span>, where <span class="math inline">\(m=\min_{1\leqslantj\leqslant n}\{a_j\}\)</span>, <spanclass="math inline">\(M=\max_{1\leqslant j\leqslantn}\{a_j\}\)</span>.</h3><p>proof.</p><h3id="let-ain-on-and-b-a-square-submatrix-of-a.-then-any-complex-eigenvalue-lambda-of-b-satisfies-that-barlambdalambda-leqslant-1.">9.Let <span class="math inline">\(A\in O(n)\)</span>, and <spanclass="math inline">\(B\)</span> a square submatrix of <spanclass="math inline">\(A\)</span>. Then any complex eigenvalue <spanclass="math inline">\(\lambda\)</span> of <spanclass="math inline">\(B\)</span> satisfies that <spanclass="math inline">\(\bar{\lambda}\lambda \leqslant 1\)</span>.</h3><p>proof<spanclass="math inline">\(B\)</span><spanclass="math inline">\(A\)</span><spanclass="math inline">\(k\)</span>. A: <spanclass="math display">\[ A=\begin{bmatrix}    B &amp; A_{12} \\    A_{13} &amp; A_{14} \\\end{bmatrix}\]</span></p><p><span class="math inline">\(A^{\mathsf{T}}A=I\)</span> <span class="math display">\[B^{\mathsf{T}} B+A_{13}^{\mathsf{T}} A_{13}=I_k\]</span></p><p><span class="math inline">\(\lambda\)</span><spanclass="math inline">\(B\)</span><spanclass="math inline">\(\alpha\)</span>. <spanclass="math inline">\(\alpha^*\)</span><spanclass="math inline">\(\alpha\)</span> <spanclass="math display">\[    \alpha^*A_{13}^{\mathsf{T}}A_{13}\alpha=(1-\bar{\lambda}\lambda)\alpha^*\alpha\]</span> <spanclass="math inline">\(\bar{\lambda}\lambda\leqslant 1\)</span>.</p><h3id="give-the-maximal-r-such-that-there-exist-beta_1cdotsbeta_r-satisfying-beta_i-beta_j0-1leqslant-ij-leqslant-r.">10.Give the maximal <span class="math inline">\(r\)</span> such that thereexist <span class="math inline">\(\beta_1,\cdots,\beta_r\)</span>satisfying <span class="math inline">\((\beta_i | \beta_j)&lt;0,1\leqslant i&lt;j \leqslant r\)</span>.</h3><p>proof: <spanclass="math inline">\(n\)</span><spanclass="math inline">\(n+1\)</span>.</p><p><span class="math inline">\(n+1\)</span><spanclass="math inline">\(v_1=(n+\sqrt{1+n},-1,-1,\cdots,-1)\)</span>,<spanclass="math inline">\(v_2=(-1,n+\sqrt{1+n},-1,\cdots,-1)\)</span>,<spanclass="math inline">\(\cdots\)</span>,<spanclass="math inline">\(v_n=(-1,-1,\cdots,n+\sqrt{1+n})\)</span><spanclass="math inline">\(v_{n+1}=(-1-\sqrt{1+n},\cdots,-1-\sqrt{1+n})\)</span>.</p><p> <spanclass="math inline">\(n-1\)</span> <spanclass="math inline">\(\alpha_1,\alpha_2,\cdots,\alpha_n\)</span><span class="math inline">\(n\)</span>. <spanclass="math inline">\(\tilde{\beta}_1,\tilde{\beta}_2,\cdots,\tilde{\beta}_n\in L(\alpha_1,\alpha_2,\cdots,\alpha_{n-1})\)</span> <span class="math display">\[    \begin{cases}        \beta_i=\tilde{\beta}_i \quad 1\leqslant i\leqslant n-1\\        \beta_n=\tilde{\beta}_n+\alpha_n \\        \tilde{\beta}_{n+1}=\tilde{\beta}_n-2(\tilde{\beta}_n|\tilde{\beta}_n)\alpha_n    \end{cases}\]</span> .</p><p><span class="math inline">\(n+2\)</span><spanclass="math inline">\(a_1,\cdots,a_{n+2}\)</span><spanclass="math inline">\(k_1,k_2,\cdots,k_{n+1}\)</span> <spanclass="math display">\[k_1a_1+\cdots+k_{n+1}a_{n+1}=0\]</span> <spanclass="math inline">\(a_{n+2}\)</span><spanclass="math inline">\(k_1,\cdots,k_i\)</span><spanclass="math inline">\(k_{i+1},\cdots,k_j\)</span><span class="math display">\[k_1a_1+\cdots+k_ia_i=-k_{i+1}a_{i+1}-\cdots-k_ja_j=V\]</span>  <span class="math display">\[V\cdotV=(k_1a_1+\cdots+k_ia_i)(-k_{i+1}a_{i+1}-\cdots-k_ja_j)=-k_1k_{i+1}a_1\cdota_{i+1}-\cdots&lt;0\]</span> <spanclass="math inline">\(n+1\)</span>.</p><p>Hermite</p><p> <span class="math inline">\(n-1\)</span> <spanclass="math inline">\(n\)</span>.  <spanclass="math inline">\(n\)</span>.</p><p> <spanclass="math inline">\(\beta_1,\beta_2,\cdots,\beta_{n+2}\)</span><spanclass="math inline">\(\beta_1,\beta_2,\cdots,\beta_{n-1}\)</span><spanclass="math inline">\(\alpha_1,\alpha_2,\cdots,\alpha_{n-1}\)</span><spanclass="math inline">\(\alpha_1,\alpha_2,\cdots,\alpha_n\)</span><span class="math display">\[    \beta_n=\gamma_n+a_n \alpha_n \\    \beta_{n+1}=\gamma_{n+1}+a_{n+1}\alpha_n \\    \beta_{n+2}=\gamma_{n+2}+a_{n+2}\alpha_n \\\]</span>  <spanclass="math inline">\((\gamma_i|\beta_j)&lt;0,n\leqslant i\leqslantn+2,1\leqslant j\leqslant n-1\)</span>.  <spanclass="math inline">\(n-1\)</span> <spanclass="math display">\[    (\gamma_i|\gamma_j)\geqslant 0, \quad n\leqslant i,j \leqslant n+2\]</span>  <spanclass="math inline">\((\beta_i|\beta_j)&lt;0,q\leqslant i,j\leqslantn+2\)</span>.  <span class="math inline">\(a_n\bar{a}_{n+2}&lt;0,a_{n+1}\bar{a}_{n+2}&lt;0,a_n\bar{a}_{n+1}&lt;0\)</span>.</p><p><spanclass="math inline">\(n+1\)</span><spanclass="math inline">\(n\)</span><spanclass="math inline">\(n\)</span><spanclass="math inline">\(2^n\)</span>.</p><h3id="let-ain-mathbbcntimes-n-and-lambda-be-an-eigenvalue-of-a.-then-lvert-lambda-rvert-leqslant-lvert-a-rvert-for-any-matrix-norm-of-a.">11.Let <span class="math inline">\(A\in \mathbb{C}^{n\times n}\)</span>,and <span class="math inline">\(\lambda\)</span> be an eigenvalue of<span class="math inline">\(A\)</span>. Then <spanclass="math inline">\(\lvert \lambda \rvert \leqslant \lVert A\rVert\)</span>, for any matrix norm of <spanclass="math inline">\(A\)</span>.</h3><p><strong>proof</strong> <span class="math inline">\(\varepsilon&gt;0\)</span> <span class="math display">\[    B=\frac{A}{\left\| A \right\|_{}+\varepsilon }\]</span>  <span class="math inline">\(\left\| B\right\|_{}&lt;1\)</span>.  <span class="math inline">\(B^n \to0\)</span> <span class="math inline">\(B\)</span> <spanclass="math inline">\(\mu\)</span> <spanclass="math inline">\(\left\vert \mu \right\vert &lt;1\)</span>. <span class="math inline">\(A\)</span> <spanclass="math inline">\(\lambda\)</span> <span class="math display">\[    \frac{\left\vert \lambda \right\vert }{\left\| A\right\|_{}+\varepsilon }&lt;1\]</span>  <span class="math inline">\(\varepsilon \to0\)</span>.</p><h2 id=""></h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>   <a href="#fnref:1" rev="footnote" class="footnote-backref"></a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
    <tags>
      
      <tag></tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LaTeX</title>
    <link href="/2022/03/29/LaTeX%E5%85%AC%E5%BC%8F%E6%B5%8B%E8%AF%95/"/>
    <url>/2022/03/29/LaTeX%E5%85%AC%E5%BC%8F%E6%B5%8B%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<div class="markdown-body"><p><span class="math display">\[E=mc^2\]</span></p></div>]]></content>
    
    
    <categories>
      
      <category></category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
