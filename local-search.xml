<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Takens&#39;s Theorem</title>
    <link href="/2023/10/04/Takens-s-Theorem/"/>
    <url>/2023/10/04/Takens-s-Theorem/</url>
    
    <content type="html"><![CDATA[<h1 id="takenss-theorem">Takens's Theorem</h1><h2 id="box-counting-dimension">box counting dimension</h2><p>The Minkowski-Bouigand dimension, also known as Minkowski dimensionor box-counting dimension.</p><p>Denote <span class="math inline">\(N(\varepsilon)\)</span> is thenumber of boxes of side length <spanclass="math inline">\(\varepsilon\)</span> required to cover the set.Then the box-counting dimension is defined as <spanclass="math display">\[    \dim_{\text{box}}(S):=\lim_{\varepsilon\to 0}\frac{\logN(\varepsilon)}{\log 1 / \varepsilon}\]</span></p><p>If <span class="math inline">\(S\)</span> is a smooth space (amanifold) of integer dimension <span class="math inline">\(d\)</span>,then <span class="math inline">\(N(1 / n) \thickapprox Cn^{d}\)</span>,which corresponds with the normal definition of dimension.</p><p>When the above limit does not exist, upper and lower box-countingdimensions can be defined, and they are strongly related to theHausdorff dimension.</p><h2 id="reference">Reference</h2>]]></content>
    
    
    <categories>
      
      <category>dynamical system</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Specific details about some large scale model</title>
    <link href="/2023/09/30/Specific-details-about-some-large-scale-model/"/>
    <url>/2023/09/30/Specific-details-about-some-large-scale-model/</url>
    
    <content type="html"><![CDATA[<h1 id="specific-details-about-some-large-scale-model">Specific detailsabout some large scale model</h1><p>Large scale models are used to characterize the dynamics of largepopulations of neurons, particularly across different brain regions. Inthat sense, all neurons in each brain region are summarized as onesingle neuron population. These models are often comprised of a localcircuit system and interactions between different neuron populations,with stochasticity coming from noise.</p><p>In our project about network effects of neurostimulation, we careabout indirect effects of stimulation on the brain. That's reasonablesince - there is significant difference between SC and EC. - Clinically,N2 component of cortico-cortical evoked potentials (CCEPs) isobserved<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Lemaréchal, Jean-Didier et al. (2022) A brain atlas of axonal and synaptic delays based on modelling of cortico-cortical evoked potentials. Brain.">[6]</span></a></sup></p><blockquote><p>... In addition, we did not primarily consider indirectcortico-subcortico-cortical pathways that would rather be implicated inthe later N2 component ...</p></blockquote><p>We hope to build a large scale model of cortex, which shows similar2-hop network effectis with the real data when applying a virtualstimulus. Furthermore, the model is expected to be used to develop aclose-loop network control method.</p><h2 id="decos-first-model-shows-poor-propagation-ratio">Deco's firstmodel shows poor propagation ratio</h2><p>The model proposed byDeco<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Gustavo Deco, Adrián Ponce-Alvarez et al. (2013) Resting-State Functional Connectivity Emerges from structurally and Dynamically Shaped Slow Linear Flunctuations. The Journal of Neuroscience.">[3]</span></a></sup>used a dynamical mean field approach to reduce thecomplexity<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Kong-Fatt Wong, Xiao-Jing Wang. (2006) A Recurrent Network Mechanism of Time Integration in Perceptual Decisions. The Journal of Neuroscience.">[5]</span></a></sup>.The model is described by the following equations:</p><p><span class="math display">\[    \frac{\mathrm{d}S_{i}(t)}{\mathrm{d}t}  = -\frac{S_{i}}{\tau_{S}}+(1-S_i)\gamma H(x_i) +\sigma \nu_i(t) \tag{1}\]</span></p><p><span class="math display">\[    H(x_i) = \frac{ax_{i}-b}{1-\exp (-d(ax_{i}-b))} \tag{2}\]</span></p><p><span class="math display">\[    x_i = wJ_{N}S_i + GJ_{N}\sum_{j}^{} C_{ij}S_{j} + I_0. \tag{3}\]</span></p><p>Here <span class="math inline">\(H(x_i)\)</span> and <spanclass="math inline">\(S_{i}\)</span> denote the population firing rateand the average synaptic gating variable at the loccal cortical area<span class="math inline">\(i\)</span>, hence <spanclass="math inline">\(S_{i} \in [0,1]\)</span>.</p><p>Parameters in (1) - (3) are strongly correlated with dynamics andbifurcation diagram of the model. Heterogeneity across cortices has beenconsidered byKong<sup id="fnref:11" class="footnote-ref"><a href="#fn:11" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Xiaolu Kong, Ru Kong et al. (2021) Sensory-motor cortices shape functional connectivity dynamics in the human brain. nature communications.">[11]</span></a></sup>,who developed a spatially heterogeneous Deco model. Their aim is tominimize the difference between empirical and simulated FC as well assharp transitions in FC patterns, which characterized by FCD.</p><h3 id="hard-sigmoidal-function-issue">Hard sigmoidal functionissue</h3><p>When doing a simulation of (1) - (3), various numerical methods canbe used. We set <span class="math inline">\(\Delta t = 1 / 2000s\)</span>, which is small enough to avoid stability issue. Hence, wedon't worry about using Euler–Maruyama method:</p><p><span class="math display">\[    S_i(t+\Delta t) \thickapprox S_i(t) + \left(-\frac{S_i(t)}{\tau_{S}}+(1-S_i(t))\gamma H(x_i) \right) \Delta t +\sigma \xi \sqrt{\Delta t}, \tag{4}\]</span></p><p>where <span class="math inline">\(\xi \sim\mathcal{N}(0,1)\)</span>.</p><p>The RHS of (4) can goes negative or larger than 1 regardless of anynumerical method (Euler–Maruyama, Runge-Kutta, Exponential-Euler, etc).That is unreasonable, so we applied a hard sigmoidal function: <spanclass="math display">\[    S_i(t+\Delta t) = \min (\max (\text{RHS of (4)}, 0), 1).\tag{5}\]</span></p><p>However, whether to apply the hard sigmoidal function has asignificant impact on dynamics.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/large_scale_model/fc_s_no_sigmoidal.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/large_scale_model/fc_s_sigmoidal.png" /></div></div></div><p>We can see that the network is in a bistable state, one with lowactivity and the other with relatively high activity. The network makestransitions between these two states.</p><p>Multistability is widespread in such nonlinear dynamical systems, oneof which impressed me most is bistable states in low-rankRNN<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Adrian Valente, Jonanthan W. Pillow. (2022) Extracting computational mechanisms from neural data using low-rank RNNs. NeurIPS.">[1]</span></a></sup><sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Francesca Mastrogiuseppe, Srdjan Ostojic. (2018) Linking Connectivity, Dynamics, and Computations in Low-Rank Recurrent Neural Networks. Neuron.">[2]</span></a></sup><sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Manuel Beiran, Alexis Dubreuil et al. (2021) Shaping Dynamics With Multiple Populations in Low-Rank Recurrent Networks. Neural Computation.">[8]</span></a></sup>.If stochasticity is deleted, <span class="math inline">\(I_0=0, \gamma =0.641, a = 270, b=108, d=0.154\)</span> as in Deco's paper, RHS of (3)has the value <span class="math inline">\(6.46 \times 10^{-6}\)</span>when all <span class="math inline">\(S_i=0\)</span>, which isnegligible. Given that <spanclass="math inline">\(S_i(t)/\tau_{S}\)</span> is linear, <spanclass="math inline">\((1-S_i(t))\gamma H(x_i)\)</span> is sub-linearnear <span class="math inline">\(S_i=0\)</span>, there indeed exists alow activity stable state.</p><p>The real problem is that the high activity state does not correspondto reality since <span class="math inline">\(S_i\)</span> or <spanclass="math inline">\(H(x_i)\)</span> is too large for a resting-stateneural network.</p><p>Whether to apply the hard sigmoidal function is not decisive in theemergence of transition between two stable states. We found anotherparameter set that shows such transitions with applying the hardsigmoidal function.</p><div class="note note-info">            <p>It is mathematically interesting to investigate why adding a hardsigmoidal function reduces the likelihood that the network will switchbetween two stable states.</p>          </div><h2 id="low-propagation-ratio-in-decos-model">Low propagation ratio inDeco's model</h2><p>We choose to apply the hard sigmoidal function and trained parameters(<span class="math inline">\(w, \sigma, I_{0}, G\)</span>) based onKong'scode<sup id="fnref:11" class="footnote-ref"><a href="#fn:11" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Xiaolu Kong, Ru Kong et al. (2021) Sensory-motor cortices shape functional connectivity dynamics in the human brain. nature communications.">[11]</span></a></sup>to suit empirical FC and FCD. The result shows poor correspondencebetween simulated stimulus time series and real ones, and we think thereason is that the model has a low propagation ratio.</p><h3 id="toy-model-test">toy model test</h3><p>To illustrate the propagation ratio, we use a toy model with onlythree nodes: A-B-C. The connection for A-B and B-C are the same, whilethere is no connection between A and C. The structural connectivitymatrix is normalized to have spectral radius 0.999. Moreover, noises aredeleted, <span class="math inline">\(I_0\)</span> is set to <spanclass="math inline">\(0\)</span> to indicate the propagation moreclearly.</p><p>After the 3-node model becomes stable (denoted as time 0), a stimulusis applied to node A and we observe its propagation along the path. A→Cpropagation ratio is defined as <span class="math display">\[    \text{A→C propagation ratio} =\frac{\mathbb{E}(S_C(t),t&gt;0)-\mathbb{E}(S_{C}(t),t&lt;0)}{\mathbb{E}(S_{A}(t), t&gt;0)-\mathbb{E}(S_{A}(t),t&lt;0)}.\]</span></p><p><span class="math inline">\(G=0.3, w=0.9\)</span> as similar with ourtrained results:</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/large_scale_model/prop_ratio_1_1.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/large_scale_model/prop_ratio_1_2.png" /></div></div></div><p><span class="math inline">\(G=5.0, w=-2.0\)</span> which is notrealistic:</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/large_scale_model/prop_ratio_2_1.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/large_scale_model/prop_ratio_2_2.png" /></div></div></div><p>A phase diagram is available after about 24 hours:</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/large_scale_model/prop_ratio_3_1.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/large_scale_model/prop_ratio_3_2.png" /></div></div></div><p>Here, End-up value is the mean of <span class="math inline">\(S_A,S_B, S_C\)</span> in the end, so a clear phase transition can be seen.The propagation ratio is high only when <spanclass="math inline">\(G\)</span> is large and <spanclass="math inline">\(w&lt;0\)</span>.</p><h2 id="decos-second-model-shows-better-propagation-ratio">Deco's secondmodel shows better propagation ratio</h2><p>Deco proposed anothermodel<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Gustavo Deco, Adrián Ponce-Alvarez et al. (2014) How Local Excitation-Inhibition Ratio Impacts the Whole Brain Dynamics. The Journal of Neuroscience.">[4]</span></a></sup>similar to the first one, but with excitatory population and inhibitorypopulation. Heterogeneous version of the model is implemented byDemirtaş etal.<sup id="fnref:9" class="footnote-ref"><a href="#fn:9" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Murat Demirtaş, Joshua B. Burt et al. (2019) Hierarchical Heterogeneity across Human Cortex Shapes Large-Scale Neural Dynamics. Neuron.">[9]</span></a></sup>The model is described by the following equations:</p><p><span class="math display">\[    \frac{\mathrm{d}S_i^{E}(t)}{\mathrm{d}t} =-\frac{S_i^{E}(t)}{\tau_{E}}+(1-S_i^{E}(t))\gamma H^{E}(x_i^{E}(t))+\sigma \nu_i(t) \tag{6}\]</span></p><p><span class="math display">\[    \frac{\mathrm{d}S_i^{I}(t)}{\mathrm{d}t} =-\frac{S_i^{I}(t)}{\tau_{I}}+H^{I}(x_i^{I}(t)) +\sigma \nu_i(t) \tag{7}\]</span></p><p><span class="math display">\[    x_i^{E}(t) = w^{E}I_0 + w^{EE}S_i^{E}(t)+gJ_{N} \sum_{j}^{}C_{ij}S_j^{E}(t)-w^{IE}S_i^{I}(t) \tag{8}\]</span></p><p><span class="math display">\[  x_i^{I}(t) = w^{I}I_0 + w^{EI}S_i^{E}(t) - S_i^{I}(t) \tag{9}\]</span></p><p>and <span class="math inline">\(H^{E}(x_i^{E}),H^{I}(x_i^{I})\)</span> is the same as (2) with different <spanclass="math inline">\(a,b,d\)</span>.</p><p>Deco's model with E-I populations shows boost in propagation ratio inthe toy model.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/large_scale_model/prop_ratio_4_1.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/large_scale_model/prop_ratio_4_2.png" /></div></div></div><p>Phase diagrams are as follows (<span class="math inline">\(G\)</span>versus <span class="math inline">\(w^{EI}\)</span> as an example, otherkey parameters: <span class="math inline">\(w^{EE} = 5, w^{IE} =1\)</span>), where propagation ratio of high activity endings isdeleted:</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/large_scale_model/prop_ratio_5_1.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/large_scale_model/prop_ratio_5_2.png" /></div></div></div><p>Though this model exhibits better propagation ratio, the problem isdifficulty in model fitting. We are also trying other models.</p><h3id="a-possible-approach-to-simplify-decos-model-with-e-i-populations">Apossible approach to simplify Deco's model with E-I populations</h3><p>Demirtaş et al. use parameters as follows:</p><table><thead><tr class="header"><th></th><th>Excitatory Populations</th><th>Inhibitory Populations</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(I_{0}\)</span></td><td>0.382 nA</td><td>-</td></tr><tr class="even"><td><span class="math inline">\(J\)</span></td><td>0.15 nA</td><td>-</td></tr><tr class="odd"><td><span class="math inline">\(\gamma\)</span></td><td>0.641</td><td>-</td></tr><tr class="even"><td><span class="math inline">\(w^{E}\)</span></td><td>1.0</td><td>-</td></tr><tr class="odd"><td><span class="math inline">\(\tau_{E}\)</span></td><td>0.1 s</td><td>-</td></tr><tr class="even"><td><span class="math inline">\(a_{E}\)</span></td><td>310 nC<span class="math inline">\(^{-1}\)</span></td><td>-</td></tr><tr class="odd"><td><span class="math inline">\(b_{E}\)</span></td><td>125 Hz</td><td>-</td></tr><tr class="even"><td><span class="math inline">\(d_{E}\)</span></td><td>0.16 s</td><td>-</td></tr><tr class="odd"><td><span class="math inline">\(w^{I}\)</span></td><td>-</td><td>0.7</td></tr><tr class="even"><td><span class="math inline">\(\tau_{I}\)</span></td><td>-</td><td>0.01 s</td></tr><tr class="odd"><td><span class="math inline">\(a_{I}\)</span></td><td>-</td><td>615 nC<span class="math inline">\(^{-1}\)</span></td></tr><tr class="even"><td><span class="math inline">\(b_{I}\)</span></td><td>-</td><td>177 Hz</td></tr><tr class="odd"><td><span class="math inline">\(d_{I}\)</span></td><td>-</td><td>0.087 s</td></tr></tbody></table><p>Not so strictly, <span class="math inline">\(\tau_{I} \ll\tau_{E}\)</span>, so we can use adiabatic approximation for <spanclass="math inline">\(S_i^{I}(t)\)</span>, i.e., <spanclass="math inline">\(S_i^{I}(t) = \tau_{I}H(x_i^{I}(t))\)</span>. Withall assumptions aforementioned, (6)-(8) become:</p><p><span class="math display">\[    \frac{\mathrm{d}S_i^{E}(t)}{\mathrm{d}t} =-\frac{S_i^{E}(t)}{\tau_{E}}+(1-S_i^{E}(t))\gamma H^{E}(x_i^{E}(t))\tag{10}\]</span></p><p><span class="math display">\[  x_i^{I}(t) = w^{EI}S_i^{E}(t) - \tau_{I}H^{I}(x_i^{I}(t)) \tag{11}\]</span></p><p><span class="math display">\[    \begin{aligned}          x_i^{E}(t) &amp;= w^{EE}S_i^{E}(t)+gJ_{N} \sum_{j}^{}C_{ij}S_j^{E}(t)-w^{IE}\tau_{I}H^{I}(x_i^{I}(t)) \\          &amp;= (w^{EE} - w^{IE}w^{EI})S_i^{E}(t) + gJ_{N}\sum_{j}^{}C_{ij}S_{j}^{E}(t) + w^{IE}x_{i}^{I}(t)    \end{aligned} \tag{12}\]</span></p><p>Consider the function: <span class="math display">\[    g(x) = x + \tau_{I}\frac{a_{I}x - b_{I}}{1-\exp(-d_{I}(a_{I}x-b_{I}))} = x + 0.01 \frac{615x-177}{1-\exp(-0.087(615x-177))} \tag{13}\]</span></p><p><img src="/img/large_scale_model/activation.png" /></p><p>Apart from <span class="math inline">\(x \in (0.2, 0.32)\)</span>,<span class="math inline">\(g(x)\)</span> can be well approximated by<span class="math display">\[    \tilde{g}(x) =    \begin{cases}        x, \quad x &lt; 177 / 615 \\        x + 0.01 (615x-177), \quad x\geqslant 177 / 615    \end{cases}\]</span></p><p>Hence, (11) has an approximate solution:</p><p><span class="math display">\[    \begin{cases}        x_i^{I}(t) = w^{EI}S_i^{E}(t), \quadw^{EI}S_{i}^{E}(t)&lt;177/615 \thickapprox 0.288 \\        x_i^{I}(t) = 0.140(w^{EI}S_{i}^{E}(t) + 1.77), \quadw^{EI}S_{i}^{E}(t)\geqslant 0.288    \end{cases}\]</span></p><p>which corresponds to a non-continuous vector field in (12). So amodified Deco model without E-I populations is derived. We only need tocheck <span class="math inline">\(S_i^{E}(t)\)</span> every time stepsand update <span class="math inline">\(x_i^{E}(t)\)</span>accordingly.</p><p>Adding noise and <span class="math inline">\(I_0\)</span> back, themodel can be described by the following equations: <spanclass="math display">\[    \frac{\mathrm{d}S_i(t)}{\mathrm{d}t} = - \frac{S_i(t)}{\tau_{S}} +(1-S_i(t))\gamma H(x_i(t)) + \sigma \nu_i(t) \tag{14}\]</span></p><p><span class="math display">\[  H(x_i(t)) = \frac{a_{E}x - b_{E}}{1-\exp (-d_{E}(a_{E}x - b_{E}))}\tag{15}\]</span></p><p><span class="math display">\[  x_i(t) = w_0 I_0 + w_1 S_i(t) + gJ_{N}\sum_{j}^{} C_{ij}S_{j}(t) + w_2y_i(t) \tag{16}\]</span></p><p><span class="math display">\[  y_i(t) = \min(w_3S_i(t) + w_4I_0, (w_3S_{i}(t) + w_4I_0 + 1.77) /7.15) \tag{17}\]</span></p><p>The modified Deco Model shows good propagation ratio (<spanclass="math inline">\(w_0 = 1, w_1 = -3, w_2 = 1, w_3 = 7, w_4 = 0.7, G= 6\)</span>):</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/large_scale_model/prop_ratio_6_1.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/large_scale_model/prop_ratio_6_2.png" /></div></div></div><p>Further steps include specification of parameters. <spanclass="math inline">\(w_0 = w^{E}\)</span> and <spanclass="math inline">\(w_1 = w^{I}\)</span> represent strength ofbackground input, so they are negligible while <spanclass="math inline">\(I_0 = 0\)</span>. <span class="math inline">\(w_1= w^{EE} - w^{EI}w^{IE}, w_2=w^{IE}, w_3=w^{EI}, G\)</span> aretrainable parameters. Among them <spanclass="math inline">\(w_1\)</span> and <spanclass="math inline">\(G\)</span> is the key.</p><p>More experiments need to be done to verify the effectiveness of thisapproach.</p><h3 id="an-observation">An observation</h3><p>In (3), negative <span class="math inline">\(w\)</span> seems notrealistic. However, in (12), <span class="math inline">\(w^{EE} -w^{IE}w^{EI}\)</span> is reasonable to be negative, so we can regardnegative <span class="math inline">\(w\)</span> in (3) as a result ofE-I balance. That also corresponds with two phase diagrams above: when<span class="math inline">\(w\)</span> is negative and <spanclass="math inline">\(G\)</span> is large to compensate for <spanclass="math inline">\(w\)</span>, the network ends up in a reasonablelow activity state and shows high propagation ratio.</p><h2 id="other-models">Other models</h2><p>We also apply other models to our project, including vanilla RNN,Joglekar'smodel<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Madhura R. Joglekar, Jorge F. Mejias et al. (2018) Inter-areal Balanced Amplification Enhances Signal Propagation in a Large-Scale Circuit Model of the Primate Cortex. Neuron.">[7]</span></a></sup>,Chaudhuri'smodel<sup id="fnref:10" class="footnote-ref"><a href="#fn:10" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Rishidev Chaudhuri, Kenneth Knoblauch et al. (2015) A Large-Scalue Circuit Mechanism for Hierarchical Dynamical Processing in the Primate Cortex. Neuron.">[10]</span></a></sup>.Further experiments are expected.</p><h2 id="reference">Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Adrian Valente, Jonanthan W.Pillow. (2022) Extracting computational mechanisms from neural datausing low-rank RNNs. NeurIPS.<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Francesca Mastrogiuseppe,Srdjan Ostojic. (2018) Linking Connectivity, Dynamics, and Computationsin Low-Rank Recurrent Neural Networks. Neuron.<a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Gustavo Deco, AdriánPonce-Alvarez et al. (2013) Resting-State Functional ConnectivityEmerges from structurally and Dynamically Shaped Slow LinearFlunctuations. The Journal of Neuroscience.<a href="#fnref:3" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Gustavo Deco, AdriánPonce-Alvarez et al. (2014) How Local Excitation-Inhibition RatioImpacts the Whole Brain Dynamics. The Journal of Neuroscience.<a href="#fnref:4" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Kong-Fatt Wong, Xiao-JingWang. (2006) A Recurrent Network Mechanism of Time Integration inPerceptual Decisions. The Journal of Neuroscience.<a href="#fnref:5" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Lemaréchal, Jean-Didier etal. (2022) A brain atlas of axonal and synaptic delays based onmodelling of cortico-cortical evoked potentials. Brain.<a href="#fnref:6" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>Madhura R. Joglekar, JorgeF. Mejias et al. (2018) Inter-areal Balanced Amplification EnhancesSignal Propagation in a Large-Scale Circuit Model of the Primate Cortex.Neuron. <a href="#fnref:7" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>Manuel Beiran, AlexisDubreuil et al. (2021) Shaping Dynamics With Multiple Populations inLow-Rank Recurrent Networks. Neural Computation.<a href="#fnref:8" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:9" class="footnote-text"><span>Murat Demirtaş, Joshua B.Burt et al. (2019) Hierarchical Heterogeneity across Human Cortex ShapesLarge-Scale Neural Dynamics. Neuron.<a href="#fnref:9" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:10" class="footnote-text"><span>Rishidev Chaudhuri, KennethKnoblauch et al. (2015) A Large-Scalue Circuit Mechanism forHierarchical Dynamical Processing in the Primate Cortex. Neuron.<a href="#fnref:10" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:11" class="footnote-text"><span>Xiaolu Kong, Ru Kong et al.(2021) Sensory-motor cortices shape functional connectivity dynamics inthe human brain. nature communications.<a href="#fnref:11" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Neuronal Dynamics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Neuroscience</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>平均场朗之万动力学的数值模拟</title>
    <link href="/2023/06/21/%E5%B9%B3%E5%9D%87%E5%9C%BA%E6%9C%97%E4%B9%8B%E4%B8%87%E5%8A%A8%E5%8A%9B%E5%AD%A6%E7%9A%84%E6%95%B0%E5%80%BC%E6%A8%A1%E6%8B%9F/"/>
    <url>/2023/06/21/%E5%B9%B3%E5%9D%87%E5%9C%BA%E6%9C%97%E4%B9%8B%E4%B8%87%E5%8A%A8%E5%8A%9B%E5%AD%A6%E7%9A%84%E6%95%B0%E5%80%BC%E6%A8%A1%E6%8B%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="平均场朗之万动力学的数值模拟">平均场朗之万动力学的数值模拟</h1><p><strong>摘要</strong>平均场朗之万动力学（MFLD），一种具有非线性漂移的朗之万动力学的推广，在许多领域有显著的作用，比如描述多粒子极限下的噪声粒子梯度下降算法或证明深度学习中随机梯度下降算法的收敛性.过去的工作证明了 MFLD弱收敛到对应的不变分布，且该不变分布是某个自由能函数的最小化子.在一定条件下，该收敛是指数速率的. 本文从引入随机微分方程和其对应的Fokker-Planck方程出发，尝试了平均场朗之万方程的数值模拟和经验分布的收敛性.</p><h2 id="随机微分方程和-fokker-planck-方程">随机微分方程和 Fokker-Planck方程</h2><p>本节主要参考了应用随机分析的相关材料<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="E, W., Li, T. \&amp; Vanden-Eijinden, E. 2019. Applied Stochastic Analysis. RI: AMS.">[2]</span></a></sup><sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Lelièvre, T. \&amp; Stoltz, G. (2016) Partial differential equations and stochastic methods in molecular dynamics. Acta Numerica. Cambridge University Press, 25, pp. 681–880.">[4]</span></a></sup></p><h3 id="随机微分方程">随机微分方程</h3><p>一般的随机微分方程形如 <span class="math display">\[    \dot{X}_t=b(X_t,t)+\sigma(X_t,t)\dot{W}_t,\tag{1}\]</span> 其中 <span class="math inline">\(\dot{W}_t\)</span>为维纳过程的（在分布意义下的）导数，更广为人知的名字是白噪声.可以将其理解为具有均值 <span class="math inline">\(m(t)=0\)</span>和互相关函数 <span class="math inline">\(K(s,t)=\delta(t-s)\)</span>的一个高斯过程.</p><p>一般将（1）改写为如下形式： <span class="math display">\[    \mathrm{d}X_t=b(X_t,t)\mathrm{d}t+\sigma(X_t,t)\mathrm{d}W_t.\tag{2}\]</span> （2）式应从积分意义下理解，即 <span class="math display">\[    X_t=X_0+\int_{0}^{t} b(X_s,s) \mathrm{d}s+\int_{0}^{t} \sigma(X_s,s)\mathrm{d}W_s. \tag{3}\]</span> 如何定义（3）式最后一项是一个问题，伊藤将其定义为 <spanclass="math display">\[    \int_{0}^{t} \sigma(X_s,s) \mathrm{d}W_s=\lim_{\lvert \Delta\rvert  \to 0}\sum_{j}^{} \sigma(X_{t_j},t_j)\left(W_{t_{j+1}}-W_{t_{j}} \right)\]</span>这里省略与伊藤积分相关的一些结果，只是不加证明地给出伊藤积分最重要的性质之一：伊藤引理.</p><p><strong>伊藤引理</strong> 令 <spanclass="math inline">\(\bm{X}_t\)</span> 满足 <spanclass="math inline">\(\mathrm{d}\bm{X}_t=\bm{b}(t)\mathrm{d}t+\bm{\sigma}(t)\mathrm{d}\bm{W}_t\)</span>，其中<span class="math inline">\(\bm{X}_t\in \mathbb{R}^{n}\)</span>，<spanclass="math inline">\(\bm{\sigma}\in \mathbb{R}^{n\timesm}\)</span>，<span class="math inline">\(\bm{W}\in\mathbb{R}^{m}\)</span> 为一个 <span class="math inline">\(m\)</span>维的标准维纳过程. 定义 <spanclass="math inline">\(Y_t=f(\bm{X}_t)\)</span>，其中 <spanclass="math inline">\(f\in C^{\infty}(\mathbb{R}^{d})\)</span>. 则 <spanclass="math display">\[    \mathrm{d}Y_t=\left( \bm{b}\cdot \nablaf+\frac{1}{2}\bm{\sigma\sigma}^{\mathsf{T}}\colon \nabla ^{2}f \right)\mathrm{d}t+\nabla f\cdot \bm{\sigma}\cdot \mathrm{d}\bm{W}_t.\tag{4}\]</span></p><h3id="随机微分方程和偏微分方程的联系">随机微分方程和偏微分方程的联系</h3><p>对于一个 <span class="math inline">\(\mathbb{R}^{d}\)</span>中时齐的随机微分方程 <span class="math display">\[    \mathrm{d}\bm{X}_t=\bm{b}(\bm{X}_t)\mathrm{d}t+\bm{\sigma}(\bm{X}_t)\mathrm{d}\bm{W}_t,\tag{5}\]</span> 满足初值 <span class="math inline">\(\bm{X}_0\in\mathbb{R}^{d}\)</span>. <span class="math inline">\(\bm{W}_t\in\mathbb{R}^{m}\)</span> 是标准维纳过程，<spanclass="math inline">\(\bm{b}\colon \mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\)</span>， <span class="math inline">\(\bm{\sigma}\colon\mathbb{R}^{d} \rightarrow \mathbb{R}^{d\times m}\)</span>满足足够的光滑性，使得（5）存在唯一的强解.（5）自然地与一个微分算子相关： <span class="math display">\[    \mathcal{L}=\bm{b}\cdot \nabla +\frac{1}{2}\bm{\sigma\sigma}^{\mathsf{T}}\colon \nabla ^{2},\]</span> <span class="math inline">\(\mathcal{L}\)</span> 被称为马氏链<span class="math inline">\((\bm{X}_t)_{t\geqslant 0}\)</span>的无穷小生成算子. 这里符号 <span class="math inline">\(\colon\)</span>为 Frobenius 内积，即对于一个给定的 <spanclass="math inline">\(C^{\infty}\)</span> 试验函数 <spanclass="math inline">\(\varphi\colon \mathbb{R}^{d}\rightarrow\mathbb{R}\)</span>，有 <span class="math display">\[    \mathcal{L}\varphi=\sum_{i=1}^{d} b_i\partial_{x_i}\varphi+\frac{1}{2}\sum_{i=1}^{d}\sum_{j=1}^{d}\sum_{k=1}^{m} \sigma_{i,k}\sigma_{j,k}\partial_{x_i,x_j}\varphi.\]</span> （5）和（6）的关系由下面引理给出</p><p><strong>引理</strong> 任取 <span class="math inline">\(\varphi\inC^{\infty}_{c}(\mathbb{R}^{d})\)</span>. 则 <spanclass="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}[\mathbb{E}_{\bm{W}}(\varphi(\bm{X}_t))]\bigg|_{t=0}=\mathcal{L}\varphi(\bm{X})\bigg|_{\bm{X}=\bm{X}_0}.\tag{7}\]</span></p><p><strong>证明</strong> 由伊藤引理， <span class="math display">\[    \mathrm{d}\varphi(\bm{X}_t)=\mathcal{L}\varphi(\bm{X}_t)\mathrm{d}t+(\nabla\varphi(\bm{X}_t))^{\mathsf{T}}\sigma(\bm{X}_t)\mathrm{d}\bm{W}_t,\]</span> 因为 <span class="math inline">\(\varphi\inC_{c}^{\infty}(\mathbb{R}^{d})\)</span>，故有 <spanclass="math display">\[    \mathbb{E}_{\bm{W}}\left( \int_{0}^{t} (\nabla\varphi(\bm{X}_s))^{\mathsf{T}}\bm{\sigma}(\bm{X}_s) \mathrm{d}\bm{W}_s\right) =0\]</span> 这里用到期望是一个平方可积鞅的性质. 因此 <spanclass="math display">\[    \mathbb{E}_{\bm{W}}[\varphi(\bm{X}_t)]-\varphi(\bm{X}_0)=\int_{0}^{t}\mathbb{E}_{\bm{W}}[(\mathcal{L}\varphi)(\bm{X}_s) ]\mathrm{d}s, \tag{8}\]</span> 以及如下极限存在： <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}(\mathbb{E}_{\bm{W}}[\varphi(\bm{X}_t)])\bigg|_{t=0}=\lim_{t\to0}\frac{\mathbb{E}_{\bm{W}}[\varphi(\bm{X}_t)]-\varphi(\bm{X}_0)}{t}=\mathcal{L}\varphi(\bm{X})\bigg|_{\bm{X}=\bm{X}_0}.\]</span></p><p>这样，我们知道任何一个形如（5）的SDE，至少有一个与之密切相关的微分算子<span class="math inline">\(\mathcal{L}\)</span>，并且 <spanclass="math inline">\(\mathcal{L}\)</span> 不包含随机项.我们知道（5）其实是一个连续时间、连续状态的随机过程，可以考虑 <spanclass="math inline">\(x_t\)</span> 在不同状态的概率分布.进一步，（5）的不变分布自然地对应于某种“稳态”，因此非常重要. 如果认为<span class="math inline">\(t=0\)</span> 时刻 <spanclass="math inline">\(x\)</span> 所处状态满足分布 <spanclass="math inline">\(p(0,x)\)</span>，那么 <spanclass="math inline">\(p\)</span> 就满足（5）所对应的 Fokker-Planck方程，它与 <span class="math inline">\(\mathcal{L}\)</span>有着密切的关系，我们将在下一节给出证明.</p><p>在此之前，可以注意到（5）的不变分布与 <spanclass="math inline">\(\mathcal{L}\)</span> 有如下关系：概率分布 <spanclass="math inline">\(\pi\)</span> 为（5）的不变分布，当且仅当对于任意<span class="math inline">\(\varphi\inC_{c}^{\infty}(\mathbb{R}^{d})\)</span>，有 <spanclass="math display">\[    \int_{}^{} \mathcal{L}\varphi \mathrm{d}\pi=0. \tag{9}\]</span> 这可以由（7）两边关于 <span class="math inline">\(\pi\)</span>积分，并利用当 <span class="math inline">\(x_0\sim \pi\)</span> 时 <spanclass="math inline">\(\mathbb{E}(\varphi(x_t))=\mathbb{E}(\varphi(x_{0}))\)</span>的性质得到.</p><h3 id="fokker-planck-方程">Fokker-Planck 方程</h3><p>考虑形如（4）的一个 <spanclass="math inline">\(\mathbb{R}^{d}\)</span> 中的一个随机过程 <spanclass="math inline">\((\bm{X}_t)_{t\geqslant 0}\)</span>. 设 <spanclass="math inline">\(t\)</span> 时刻 <spanclass="math inline">\(\bm{X}_t\)</span> 服从的分布有密度函数 <spanclass="math inline">\(\psi(t,\bm{X})\)</span>，<spanclass="math inline">\(\psi(0,\bm{X})=\psi_{0}(\bm{X})\)</span>为初始时刻的密度函数.</p><p><strong>注</strong> 我们略去了 <spanclass="math inline">\(\bm{X}_t\)</span>在许多情况下是一个连续型随机变量的证明. 如果 <spanclass="math inline">\(\bm{X}_t\)</span> 没有密度函数，下面的Fokker-Planck 方程仍然具有弱形式.</p><p>在上面这些假设下，<span class="math inline">\(\psi\)</span> 满足Fokker-Planck 方程（也被称为 Kolmogorov 前向方程）： <spanclass="math display">\[    \partial_{t}\psi=\mathcal{L}^{\dag}\psi, \quad \psi(0)=\psi_0,\tag{10}\]</span> 其中 <span class="math inline">\(\mathcal{L}^{\dag}\)</span>表示算子 <span class="math inline">\(\mathcal{L}\)</span> 的 <spanclass="math inline">\(L^{2}\)</span> 伴随： <spanclass="math display">\[    \mathcal{L}^{\dag}=-\operatorname{div}(\bm{b}\\cdot)+\frac{1}{2}\nabla ^{2}\colon(\bm{\sigma\sigma}^{\mathsf{T}}\cdot).\]</span> 即对于任何 <span class="math inline">\(\varphi\inC^{\infty}(\mathbb{R}^{d})\)</span>， <span class="math display">\[    \mathcal{L}^{\dag}\psi=-\sum_{i=1}^{d}\partial_{x_i}(b_i\psi)+\frac{1}{2}\sum_{i,j=1}^{d} \sum_{k=1}^{m}\partial_{x_i,x_j}(\sigma_{i,k}\sigma_{j,k}\psi).\]</span> <span class="math inline">\(L^{2}\)</span>伴随的意思是，对于任何 <span class="math inline">\(f,g \inC^{\infty}_{c}(\mathbb{R}^{d})\)</span>，有 <spanclass="math display">\[    \int_{\mathbb{R}^{d}}^{} \mathcal{L}f(x)g(x)\mathrm{d}x=\int_{\mathbb{R}^{d}}^{} f(x)\mathcal{L}^{\dag}g(x)\mathrm{d}x.\]</span> （7）的弱形式可以通过（5）推导得到. 对任意 <spanclass="math inline">\(h&gt;0\)</span> 和 <spanclass="math inline">\(\varphi\inC^{\infty}_{c}(\mathbb{R}^{d})\)</span>， <span class="math display">\[    \frac{\mathbb{E}_{\bm{W}}[\varphi(\bm{X}_{t+h})]-\mathbb{E}_{\bm{W}}[\varphi(\bm{X}_t)]}{h}=\frac{1}{h}\int_{t}^{t+h}\mathbb{E}_{\bm{W}}[(\mathcal{L}\varphi)(\bm{X}_s)] \mathrm{d}s,\]</span> 对不同布朗运动取均值，密度函数为 <spanclass="math inline">\(\psi\)</span>，故 <span class="math display">\[    \begin{aligned}        \frac{1}{h}\left( \int_{\mathbb{R}^{d}}\varphi(\bm{X})\psi(t+h,\bm{X})\mathrm{d}\bm{X}-\int_{\mathbb{R}^{d}}^{} \varphi(\bm{X})\psi(t,\bm{X})\mathrm{d}\bm{X} \right)  \\        =\frac{1}{h}\int_{t}^{t+h} \int_{\mathbb{R}^{d}}^{}(\mathcal{L}\varphi)(\bm{X})\psi(s,\bm{X}) \mathrm{d}\bm{X} \mathrm{d}s.    \end{aligned}\]</span> 取极限 <span class="math inline">\(h\rightarrow0\)</span>，就有 <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}\left(\int_{\mathbb{R}^{d}}\varphi(\bm{X})\psi(t,\bm{X}) \mathrm{d}\bm{X}\right) = \int_{\mathbb{R}^{d}}^{}(\mathcal{L}\varphi)(\bm{X})\psi(t,\bm{X}) \mathrm{d}\bm{X}. \tag{11}\]</span> （11）是（10）的弱形式. 要求（5）的不变分布，只要在（10）中解<span class="math inline">\(\mathcal{L}^{\dag}\psi=0\)</span> 即可.</p><h3 id="不变分布是自由能的极小化子">不变分布是自由能的极小化子</h3><p>给定一个势函数 <span class="math inline">\(f\colon\mathbb{R}^{d}\rightarrow\mathbb{R}\)</span>，满足李普希茨连续性和合适的增长速率条件，过阻尼朗之万动力学定义为<span class="math display">\[    \mathrm{d}X_t=-\nabla f(X_t)\mathrm{d}t+\sigma \mathrm{d}W_t,\tag{12}\]</span> 它的 Fokker-Planck 方程形如 <span class="math display">\[    \partial_t m=\operatorname{div}(m\nabla f)+\frac{\sigma^{2}}{2}\Delta m, \tag{13}\]</span> 其中 <span class="math inline">\(\sigma\)</span> 是一个常数，<span class="math inline">\(W\)</span> 是一个 <spanclass="math inline">\(d\)</span> 维布朗运动. 此时，记（9）的不变分布为<span class="math inline">\(m^{\sigma,*}\)</span>，则它一定有如下形式<span class="math display">\[    m^{\sigma,*}(x)=\frac{1}{Z}\exp \left( -\frac{2}{\sigma^{2}}f(x)\right) , \quad \forall x\in \mathbb{R}^{d},\]</span> 其中 <span class="math inline">\(Z\)</span> 是归一化常数.</p><p>一个重要发现是，<span class="math inline">\(m^{\sigma,*}\)</span>是如下自由能函数的唯一最小化子： <span class="math display">\[    V^{\sigma}(m):=\int_{\mathbb{R}^{d}}^{}f(x)m(\mathrm{d}x)+\frac{\sigma^{2}}{2}H(m),\]</span> 其中 <span class="math inline">\(H(m)\)</span> 是 <spanclass="math inline">\(m\)</span> 与 Lebesgue测度的相对熵（即通常意义下的熵）： <span class="math display">\[    H(m):=\int_{\mathbb{R}^{d}}^{} m(x)\log \left( \frac{m(x)}{g(x)}\right)  \mathrm{d}x,\]</span> 这里 <span class="math inline">\(g(x)=1\)</span> 对应于Lebesgue 测度，<span class="math inline">\(g(x)\)</span>可以取为更广泛的概率测度.</p><h2 id="平均场朗之万动力学">平均场朗之万动力学</h2><p>记 <span class="math inline">\(\mathcal{P}(\mathbb{R}^{d})\)</span>为 <span class="math inline">\(\mathbb{R}^{d}\)</span>上所有概率测度构成的集合，<spanclass="math inline">\(\mathcal{P}_{p}(\mathbb{R}^{d})\)</span> 为 <spanclass="math inline">\(\mathbb{R}^{d}\)</span> 上所有 <spanclass="math inline">\(p\)</span> 阶矩有限的概率测度构成的集合. <spanclass="math inline">\(F\)</span> 是一个定义在 <spanclass="math inline">\(\mathcal{P}(\mathbb{R}^{d})\)</span> 上的凸泛函.<span class="math inline">\(U\in C^{\infty}(\mathbb{R}^{d})\)</span>且满足 <span class="math display">\[    \int_{\mathbb{R}^{d}}^{} \mathrm{e}^{-U(x)}  \mathrm{d}x=1,\]</span> 后文中记 <span class="math inline">\(g(x)=\exp(-U(x))\)</span>.</p><p><strong>定义（MFLD）</strong> <span class="math display">\[\mathrm{d}X_t=-\left(D_{m}F(m_t,X_t)+\frac{\sigma^{2}}{2}\nablaU(X_t)\right)\mathrm{d}t+\sigma \mathrm{d}W_t, \tag{14}\]</span> 其中 <span class="math inline">\(D_mF\)</span>被称为概率测度空间上的内变分，定义为 <spanclass="math inline">\(D_mF(m,x):=\nabla \frac{\delta F}{\deltam}(m,x)\)</span>.</p><p><strong>注</strong> 如果对于某个 <span class="math inline">\(f\inC^{1}(\mathbb{R}^{d},\mathbb{R})\)</span>，令 <spanclass="math inline">\(F(m)=\int_{\mathbb{R}^{d}}^{}f(x)m(\mathrm{d}x)\)</span>，则有 <spanclass="math inline">\(D_mF(m,x)=\nabla f(x)\)</span>. 那么 MFLD化为过阻尼的朗之万动力学（9）.</p><p>MFLD 有相应的 Fokker-Planck 方程，下面简称 MFLE：</p><p><span class="math display">\[    \partial_{t}m=\nabla \cdot \left( \left(D_mF(m,\cdot)+\frac{\sigma^{2}}{2}\nabla U \right)m+\frac{\sigma^{2}}{2}\nabla m \right) , \tag{15}\]</span> 其中 <span class="math inline">\(m_t\)</span> 是 <spanclass="math inline">\(X_t\)</span> 分布的密度函数.</p><p>之前的工作证明了，在满足适当条件的情况下，（10）的不变分布是如下自由能函数的最小化子<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Hu, K., Ren, Z., SISKA, D. \&amp; SZPRUCH, L. (2020) Mean-Field Langevin Dynamics and Energy Landscape of Neural Networks. arXiv preprint at arXiv:1905.07769.">[3]</span></a></sup><span class="math display">\[    V^{\sigma}(m):=F(m)+\frac{\sigma^{2}}{2}H(m), \tag{16}\]</span></p><p>记这个不变分布为 <span class="math inline">\(m^{*}\)</span>，则 <spanclass="math inline">\(m^{*}\in \mathcal{P}_{2}(\mathbb{R}^{d})\)</span>且满足下列变分方程： <span class="math display">\[    \frac{\delta F}{\delta m}(m^{*},\cdot)+\frac{\sigma^{2}}{2}\log(m^{*})+\frac{\sigma^{2}}{2}U \ \text{几乎处处是一个常数} \tag{17}\]</span></p><p>任何初始分布 <spanclass="math inline">\(\mathcal{W}_{2}\)</span>-收敛到 <spanclass="math inline">\(m^{*}\)</span>：<spanclass="math inline">\(\lim_{t \to\infty}\mathcal{W}_{2}(m_t,m^{*})=0\)</span>. 这里略去 Wasserstein度量的相关介绍，但我们知道至少在某种意义上任何初始分布会收敛到不变分布.</p><h3id="噪声粒子梯度下降noisy-particle-gradient-descent-npgd">噪声粒子梯度下降（NoisyParticle Gradient Descent, NPGD）</h3><p>噪声粒子梯度下降（Noisy Particle Gradient Descent,NPGD）是一种最小化带正则项概率空间上的泛函的算法.当粒子数趋于无穷时，NPGD 可以用 MFLD 描述. 为了用能量泛函 <spanclass="math inline">\(G\)</span> 的一阶变分描述动力学，我们对 <spanclass="math inline">\(G\)</span>规定一定的光滑性条件<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Chizat, L. (2022) Mean-Field Langevin Dynamics: Exponential Convergence and Annealing. arXiv preprint at arXiv: 2202.01009v3.">[1]</span></a></sup>.</p><p><strong>假设（<span class="math inline">\(G\)</span>的光滑性条件）</strong> 对任意 <span class="math inline">\(m \in\mathcal{P}_{2}(\mathbb{R}^{d})\)</span>，<spanclass="math inline">\(G\)</span> 在 <spanclass="math inline">\(m\)</span> 处有一阶变分 <spanclass="math inline">\(V[m]\in C^{1}(\mathbb{R}^{d})\)</span> 且满足<span class="math inline">\((m,x)\rightarrow \nabla V[m](x)\)</span>在如下意义下 Lipschitz 连续：存在 <spanclass="math inline">\(L&gt;0\)</span> 使得 <span class="math display">\[    \forall m,\nu \in \mathcal{P}_{2}(\mathbb{R}^{d}), \quad \forallx,y\in \mathbb{R}^{d}, \quad \left\| \nabla V[m](x)-\nabla V[\nu](y)\right\|_{2}\leqslant L(\left\| x-y \right\|_{2}+W_2(m,\nu)).\]</span> 其中 <span class="math inline">\(W_{2}(\cdot,\cdot)\)</span>为 Wasserstein 距离.</p><p>NPGD 的出发点是用经验分布来代替真实分布. 如果有 <spanclass="math inline">\(N\)</span> 个粒子 <spanclass="math inline">\(X_1,\cdots ,X_N \in \mathbb{R}^{d}\)</span>. 假设<span class="math inline">\(G\)</span>是一个服从上述假设的能量泛函，<span class="math inline">\(V\)</span>是其一阶变分. 独立同分布初始化 <span class="math inline">\(X_{i,0}\sim\mu_0\in \mathcal{P}_{2}(\mathbb{R}^{d}), i \in [N]\)</span> 并定义NPGD： <span class="math display">\[    \begin{cases}        X_{i,t+\mathrm{d}t}=X_{i,t}-\mathrm{d}t \nablaV[\hat{m}_{t}](X_{i,t})+\sigma\sqrt{\mathrm{d}t}Z_{i,t} \\        \hat{m}_{t}=\frac{1}{N}\sum_{i=1}^{N} \delta_{X_{i,t}},    \end{cases} \tag{18}\]</span> 其中 <span class="math inline">\(\mathrm{d}t&gt;0\)</span>为时间步长，<span class="math inline">\(Z_{i,t}\sim\mathcal{N}(0,1)\)</span> 为独立同分布的标准正态变量.</p><p>概率分布 <span class="math inline">\(m_{t}\)</span> 为下列Fokker-Planck 方程的近似解 <span class="math display">\[    \partial_{t} m=\nabla \cdot \left(m \nablaV[m]\right)+\frac{\sigma^{2}}{2} \Delta m, \tag{19}\]</span> 也即 <span class="math display">\[    \partial_{t} m=\nabla \cdot\left(D_{m}G(m,\cdot)m+\frac{\sigma^{2}}{2} \nabla  m\right). \tag{20}\]</span> 满足 <span class="math inline">\(G\)</span>的光滑性条件下，（19）有唯一解. 注意，（20）的形式与（15）略有差别，MFLD的核心在于 <span class="math inline">\(t\)</span> 时刻 <spanclass="math inline">\(X_{i}\)</span> 的动力学与所有粒子的分布 <spanclass="math inline">\(m_{t}\)</span>有关，从这个角度来看，（20）与（15）相差的正则项并不关键.</p><h2 id="过阻尼朗之万动力学的数值模拟">过阻尼朗之万动力学的数值模拟</h2><p>由上文知，过阻尼朗之万动力学是 MFLD的特殊情形，故我们先研究（10）的数值解的特性. 考虑方程 <spanclass="math display">\[    \mathrm{d}x=-\frac{x}{50}\mathrm{d}t+\sigma\mathrm{d}W_t, \tag{21}\]</span> 它的不变分布为 <span class="math display">\[    m^{\sigma,*}(x)=\frac{1}{5\sqrt{2\pi}\sigma}\exp(-\frac{x^{2}}{50\sigma^{2}}). \tag{22}\]</span></p><p><strong>这里省略了一幅大图，实在不知道怎么排版好</strong></p><p>利用 KL 散度和 Wasserstein 距离来衡量两个概率分布的差异.这里只考虑一维情况. KL 散度的计算是方便的.<spanclass="math inline">\(p\)</span>-Wasserstein距离的计算利用了下列事实：对任意 <spanclass="math inline">\(\mu_1,\mu_2\in\mathcal{P}_{p}(\mathbb{R})\)</span>，设它们的累积分布函数分别为 <spanclass="math inline">\(F_1(x), F_2(x)\)</span>，则 <spanclass="math inline">\(p\)</span>-Wasserstein 距离为 <spanclass="math display">\[    W_{p}(\mu_1,\mu_2)=\left( \int_{0}^{1} \lvert F_1 ^{-1}(q)-F_{2}^{-1}(q) \rvert ^{p} \mathrm{d}q \right) ^{1/p},\]</span> 其中 <span class="math inline">\(F_1^{-1}\)</span> 和 <spanclass="math inline">\(F_{2}^{-1}\)</span> 是逆累积分布函数. 特别的，当<span class="math inline">\(p=1\)</span> 时，结合几何意义不难看出 <spanclass="math display">\[    W_1(\mu_1,\mu_2)=\int_{\mathbb{R}}^{} \lvert F_1(x)-F_2(x)\rvert  \mathrm{d}x.\]</span></p><p>本节模拟均采用了大量独立样本以对分布函数作较好的近似.具体的，独立初始化 <span class="math inline">\(N=10000\)</span> 个样本<span class="math inline">\(X^{1}_{0},\cdots ,X^{N}_{0}\)</span>.不失一般性，<span class="math inline">\(X^{i}_{0}\sim \mathcal{N}(0,1),i=1,\cdots,N\)</span>. 时间步长 <spanclass="math inline">\(\mathrm{d}t=0.01\)</span> 并演化 <spanclass="math inline">\(10000\)</span> 步至 <spanclass="math inline">\(t=100\)</span> 处. 从 <spanclass="math inline">\(\mathcal{N}(0,25\sigma^{2})\)</span> 随机采样<span class="math inline">\(N=10000\)</span>个样本，以该样本的经验分布代替真实分布 <spanclass="math inline">\(\mathcal{N}(0,25\sigma^{2})\)</span> 计算每步的 KL散度和 Wasserstein 距离. 图1展示了取 <spanclass="math inline">\(\sigma=0.1,1,5\)</span> 时的模拟情况.从图中可以看出，KL 散度和 Wasserstein 距离收敛阶相同，且明显当 <spanclass="math inline">\(\sigma\)</span> 小时收敛速度快且稳定. 另外，在 KL散度和 Wasserstein距离下降的主要部分，二者都大致呈指数阶收敛，这与理论相符<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Chizat, L. (2022) Mean-Field Langevin Dynamics: Exponential Convergence and Annealing. arXiv preprint at arXiv: 2202.01009v3.">[1]</span></a></sup><sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="E, W., Li, T. \&amp; Vanden-Eijinden, E. 2019. Applied Stochastic Analysis. RI: AMS.">[2]</span></a></sup>.</p><h2id="平均场朗之万动力学的数值模拟核最大均值差异度量">平均场朗之万动力学的数值模拟——核最大均值差异度量</h2><p>Chizat 使用 NPGD 模拟了 MFLD. 具体来说，对于任何概率分布 <spanclass="math inline">\(\mu,\nu\in\mathcal{P}(\mathcal{X})\)</span>，定义核最大均值差异（kernel MaximumMean Discrepancy，kMMD）为 <span class="math display">\[    G^{2}(\mu,\nu)=\frac{1}{2}\int_{\mathcal{X}^{2}}^{} k(x,y)\mathrm{d}\mu(x)\mathrm{d}\mu(y)-\int_{\mathcal{X}^{2}}^{} k(x,y)\mathrm{d}\mu(x)\mathrm{d}\nu(y)+\frac{1}{2}\int_{\mathcal{X}^{2}}^{}k(x,y) \mathrm{d}\nu(x)\mathrm{d}\nu(y). \tag{23}   \]</span></p><p>其中 <span class="math inline">\(k\in C^{2}(\mathcal{X} \times\mathcal{X})\)</span> 是一个光滑半正定的再生核. 它可以看成 <spanclass="math inline">\(\mathcal{P}(\mathcal{X})\)</span> 上的能量泛函. 当<span class="math inline">\(k(x,y)=k(y,x)\)</span> 时，它还可以看成是<span class="math inline">\(\mathcal{P}(\mathcal{X})\)</span>上的一个度量.</p><p>kMMD 和 Wasserstein 距离之间的关系由 Kantorovich-Rubinstein 对偶给出.它告诉我们，对于一个可分的拓扑空间 <spanclass="math inline">\(\mathcal{X}\)</span> 和任意 <spanclass="math inline">\(\mu,\nu \in\mathcal{P}_{1}(\mathcal{X})\)</span>， <span class="math display">\[    W_{1}(\mu,\nu)=\sup_{\left\| f \right\|_{L}\leqslant 1} \left|\int_{}^{} f(\mu-\nu) \mathrm{d}x\right| \tag{24}\]</span> 而 <span class="math inline">\(G(\mu,\nu)\)</span> 可以写成<span class="math display">\[    G^{2}(\mu,\nu)=\frac{1}{2}\left\| \mu-\nu\right\|^{2}_{\mathcal{H}}=\frac{1}{2}\int_{}^{} k(x,y)(\mu(x)-\nu(x))(\mu(y)-\nu(y))\mathrm{d}x\mathrm{d}y \tag{25}\]</span> 如果 <span class="math inline">\(k(x,y)\)</span> 具有 <spanclass="math inline">\(h(x)h(y)\)</span> 的形式（比如 <spanclass="math inline">\(k(x,y)=xy\)</span>），那么 <spanclass="math display">\[    G(\mu,\nu)=\frac{\sqrt{2}}{2}\left| \int_{}^{} h(\mu-\nu)\mathrm{d}x \right| \tag{26}\]</span> 由此得到 <span class="math inline">\(G(\mu,\nu)\)</span> 和<span class="math inline">\(W_1(\mu,\nu)\)</span> 应当以同阶收敛，即若<span class="math inline">\(\mu_{n}\)</span> 依概率收敛到 <spanclass="math inline">\(\nu\)</span>，则有<spanclass="math inline">\(G(\mu_{n},\nu)=O_{p}(n^{-s})\LeftrightarrowW_1(\mu_{n},\nu)=O_{p}(n^{-s})\)</span>.</p><p>回到 MFLD 的构造，<span class="math inline">\(G\)</span>的一阶变分易求：对于 <span class="math inline">\(\mu\in\mathcal{P}(\mathcal{X})\)</span> 和 <span class="math inline">\(x\in\mathcal{X}\)</span>， <span class="math display">\[    V[\mu](x)=\int_{}^{} k(x,y) \mathrm{d}(\mu-\nu)(y).\]</span></p><p>在满足适当条件的情况下，（14）的解以指数阶收敛到 <spanclass="math inline">\(G\)</span> 的正则化 <spanclass="math inline">\(F_{\tau}=G+\frac{\sigma^{2}}{2} H\)</span>的极小化子. 并且，如果噪声系数 <spanclass="math inline">\(\sigma=\sigma_{t}\)</span> 随时间增长收敛到 <spanclass="math inline">\(0\)</span>，如 <spanclass="math inline">\(\sigma_{t}=\alpha/\sqrt{\log (t)}\)</span>，那么<span class="math inline">\(G(\mu_{t})\)</span> 就会收敛到无正则化 <spanclass="math inline">\(F_{0}=G\)</span> 的极小化子.</p><p>考虑方程（18），在本例中化为 <span class="math display">\[    \begin{cases}        X^{i}_{t+\mathrm{d}t}=X^{i}_{t}-\int_{}^{} k_{x}(x,y)\mathrm{d}(\mu_{t}-\nu)(y) +\sigma_{t} \sqrt{\mathrm{d}t}Z^{i}_{t}\\        \mu_{t}=\frac{1}{N}\sum_{i=1}^{N} \delta_{X^{i}_{t}},    \end{cases} \tag{27}\]</span>由于平均场效应的存在，每一步的更新都依赖于所有样本的状态，故计算量非常大.限于计算资源，我们只能考虑 <span class="math inline">\(N=100\)</span>的情况. 此时这 <span class="math inline">\(N\)</span>个样本无法较好近似一个连续分布，故取 <spanclass="math inline">\(\nu\)</span> 为离散分布为佳.为展示方便起见，后文取 <span class="math display">\[    \nu=\frac{1}{10} \sum_{i=1}^{10} \delta_{2i-11}\]</span> 这样（27）中的积分化为一个求和. 观察当 <spanclass="math inline">\(\sigma_{t}\to 0 (t\to \infty)\)</span>且收敛速度不同时，经验分布 <spanclass="math inline">\(\mu_{t}=\frac{1}{N}\sum_{i=1}^{N}\delta_{X^{i}_{t}}\)</span> 是否在如下意义下收敛到 <spanclass="math inline">\(\nu\)</span>： <span class="math display">\[    \lim_{t \to \infty}G(\mu_{t},\nu)=0.\]</span> 这里使用 Chizat 提到的退火算法. 他证明了当迭代第 <spanclass="math inline">\(k\)</span> 步的噪声系数 <spanclass="math inline">\(\sigma_{k}=\sigma/\sqrt{\log k}\)</span> 且 <spanclass="math inline">\(\sigma\)</span> 大于某个临界值 <spanclass="math inline">\(\sigma^{*}\)</span> 时，经过足够长的时间，经验分布<span class="math inline">\(\mu_{t}\)</span> 会以如下速率收敛到 <spanclass="math inline">\(G\)</span> 的极小化子（Chizat, Theorem 4.1）：<span class="math display">\[    G(\mu_{t})-\inf G\leqslant C&#39; \frac{\log \log t}{\log t}.\]</span></p><p><strong>这里也省略了一幅大图</strong></p><p>独立初始化 <span class="math inline">\(X^{i}_{0}\sim\mathcal{N}(0,25)\)</span>，初始噪声系数 <spanclass="math inline">\(\sigma=1\)</span>，时间步长 <spanclass="math inline">\(\mathrm{d}t=0.01\)</span>，迭代步数为 <spanclass="math inline">\(10000\)</span>，并在（27）中选用三种不同的核函数$k(x,y)k(x,y)=1+2_{k=1}^{5} (1+k)^{-1}(k(x-y)), k(x,y)=(-(x-y)^{2}/2),k(x,y)=xy $. 其中第一种是 Chizat的实验中使用的，它并不严格正定，但是在实验中表现良好.第二种是常用的高斯核，具有 <spanclass="math inline">\(k(x,y)=\tilde{k}(x-y)=\tilde{k}(y-x)\)</span>的性质. 第三种是线性核. 图2中展示了三种退火速度和三种核函数下的实验结果.由于 <span class="math inline">\(N\)</span> 和 <spanclass="math inline">\(t\)</span> 的限制，<spanclass="math inline">\(\sigma_{k}=\sigma/\sqrt{\log (k+1)}\)</span>时的收敛速度并不快. <spanclass="math inline">\(\sigma_{k}=\sigma/\sqrt{k}\)</span> 和 <spanclass="math inline">\(\sigma_{k}=\sigma/k\)</span>下降速率较快且速率相近. <spanclass="math inline">\(\sigma_{k}=\sigma/k\)</span> 更加稳定.但已有的结果表明，比 <spanclass="math inline">\(\sigma_{k}=\sigma/\sqrt{\log (k+1)}\)</span>更快的收敛速度已经会破坏收敛性（Holley, Sec.3）. 即使取 <spanclass="math inline">\(\sigma_{k}=\sigma/\sqrt{\log(k+1)}\)</span>，<span class="math inline">\(\sigma\)</span>较小时也会破坏收敛性. 这些情况下，经验分布 <spanclass="math inline">\(\mu_{t}\)</span> 会收敛到 <spanclass="math inline">\(G\)</span> 的局部极小值点，而非全局最小值点.</p><p>图2还表明 <span class="math inline">\(k(x,y)=1+2\sum_{k=1}^{5}(1+k)^{-1}\cos (k(x-y))\)</span> 的表现最好，而线性核 <spanclass="math inline">\(k(x,y)=xy\)</span>收敛很不稳定，这可能是因为线性核无界导致的.</p><p>最后，空间的紧性对于收敛性也有一定的影响. Chizat在实验中使用的底空间是 <spanclass="math inline">\((\mathbb{R}/2\pi\mathbb{Z})^{d}\)</span>. <spanclass="math inline">\(\sigma_{k}\)</span> 下降得较慢（对应于 <spanclass="math inline">\(\sigma_{k}=\sigma/\sqrt{\log (k+1)}\)</span>时，大量样本具有的遍历性能够让经验分布 <spanclass="math inline">\(\mu_{t}\)</span> 收敛到 <spanclass="math inline">\(G\)</span> 的全局最小值点. 本文中使用的空间是<spanclass="math inline">\(\mathbb{R}\)</span>，因此初始化对于能否收敛到全局最小值点有很大影响.图3展示了初始化为 <spanclass="math inline">\(X^{i}_{0}=0\)</span>，并增大初始噪声系数 <spanclass="math inline">\(\sigma=5\)</span> 时 <spanclass="math inline">\(G\)</span> 的收敛性情况. 在所有情况下，经验分布<span class="math inline">\(\mu_{t}\)</span> 不稳定包含 <spanclass="math inline">\(-9,-7,-5,5,7,9\)</span> 处的概率. <spanclass="math inline">\(\sigma_{k}=\sigma/\sqrt{\log (k+1)}, k(x,y)=\exp(-(x-y)^{2}/2)\)</span> 条件下 <span class="math inline">\(G\)</span>先降低，后增加的原因主要是从 <spanclass="math inline">\(X^{i}_{0}=0\)</span> 出发，经验分布 <spanclass="math inline">\(\mu_{t}\)</span> 的方差随时间增大而增大.当经验分布 <span class="math inline">\(\mu_{t}\)</span> 的方差与 <spanclass="math inline">\(\nu\)</span> 可比后，<spanclass="math inline">\(\mu_{t}\)</span> 的方差继续增大，<spanclass="math inline">\(G\)</span> 值增加.为得到收敛性，可能需要增加样本个数 <spanclass="math inline">\(N\)</span> 和模拟时间 <spanclass="math inline">\(t\)</span>.</p><h2 id="结论">结论</h2><p>平均场朗之万动力学（MFLD）的数值模拟是一个具有挑战性的问题.相比过阻尼朗之万动力学或欠阻尼朗之万动力学，MFLD的模拟对于计算资源的要求要高得多.在模拟过阻尼朗之万动力学或欠阻尼朗之万动力学时，使用离散分布近似连续分布是可能的，参考文献中提到的指数阶收敛性也容易复现.与之相反的是，用大量样本模拟 MFLD 通常是不可行的.本文取经验分布和目标分布（即 <span class="math inline">\(G\)</span>的最小化子）都为离散分布. 在使用 Chizat 提出的退火算法求 kMMD的最小化子时，核函数 <spanclass="math inline">\(k(x,y)\)</span>、退火速率 <spanclass="math inline">\(\sigma_{k}\)</span>、样本个数 <spanclass="math inline">\(N\)</span>、模拟时间 <spanclass="math inline">\(t\)</span>、初始化 <spanclass="math inline">\(X^{i}_{0}\)</span>甚至空间的紧性对于经验分布的收敛性和收敛速度都有影响.由于迭代步数较大，无论哪种退火速率，一定时间后噪声会非常小，可能使经验分布落在一个局部极小区域内，从而最终无法收敛到全局最小分布.</p><p>本文用到的所有代码均可在https://github.com/Newtonpula/Numecial-Simulation-of-MFLD 找到.</p><p><strong>这里也省略了一幅大图</strong></p><h2 id="reference">Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Chizat, L. (2022) Mean-FieldLangevin Dynamics: Exponential Convergence and Annealing. arXiv preprintat arXiv: 2202.01009v3.<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>E, W., Li, T. &amp;Vanden-Eijinden, E. 2019. Applied Stochastic Analysis. RI: AMS.<a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Hu, K., Ren, Z., SISKA, D.&amp; SZPRUCH, L. (2020) Mean-Field Langevin Dynamics and EnergyLandscape of Neural Networks. arXiv preprint at arXiv:1905.07769.<a href="#fnref:3" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Lelièvre, T. &amp; Stoltz,G. (2016) Partial differential equations and stochastic methods inmolecular dynamics. Acta Numerica. Cambridge University Press, 25, pp.681–880. <a href="#fnref:4" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>numerical analysis</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fisher Information, AIC &amp; BIC</title>
    <link href="/2023/03/02/Fisher-Information/"/>
    <url>/2023/03/02/Fisher-Information/</url>
    
    <content type="html"><![CDATA[<h2 id="fisher-information">Fisher Information</h2><p>希望估计参数 <span class="math inline">\(\theta\)</span>，有观测量<span class="math inline">\(x_1,\cdots ,x_n\)</span>，其中 <spanclass="math inline">\(x\sim D(\theta)\)</span>.</p><p>有似然函数 <span class="math display">\[    L(\theta)=\prod_{i=1}^{n} p(x_i|\theta)\]</span></p><p>取对数以方便计算，或者可以认为是一种 normalization. <spanclass="math display">\[    \ln L(\theta)=\sum_{i=1}^{n} \ln p(x_i|\theta)\]</span></p><p>定义 score 函数 <span class="math display">\[    S(\theta)=\frac{\partial }{\partial \theta}\lnL(\theta)=\frac{\frac{\partial }{\partial \theta}L(\theta)}{L(\theta)}\]</span></p><p>从最后一个等号可以看出 normalization 的意味.</p><p>一个值得注意的点是 <span class="math display">\[    \mathbb{E}(S(\theta))=\int_{\mathbb{R}^{n}} \frac{\frac{\partial}{\partial \theta}L(\theta)}{L(\theta)}L(\theta)     \mathrm{d}x_1\cdots\mathrm{d}x_n=\frac{\partial }{\partial \theta}\int_{\mathbb{R}^{n}}L(\theta) \mathrm{d}x_1\cdots \mathrm{d}x_n=\frac{\partial }{\partial\theta}1=0\]</span></p><p>定义 Fisher information <span class="math display">\[    I(\theta)=\operatorname{var}(S(\theta))\]</span></p><p><span class="math display">\[    I(\theta)=-\mathbb{E}\left(\frac{\partial ^{2}}{\partial\theta^{2}}\ln L(\theta)\right)\]</span></p><p>对于 <span class="math inline">\(\theta\)</span> 空间中的每个 <spanclass="math inline">\(\theta\)</span>，因为 <spanclass="math inline">\(\mathbb{E}(S(\theta))=0\)</span>，如果 <spanclass="math inline">\(I(\theta)\)</span>很小，则说明在样本空间中取到使得 <span class="math inline">\(\lvertS(\theta) \rvert\)</span> 很大的样本集合 <spanclass="math inline">\(\{x_1,\cdots x_n\}\)</span> 的概率很小；如果 <spanclass="math inline">\(I(\theta)\)</span>很大，则说明在样本空间中取到使得 <span class="math inline">\(\lvertS(\theta) \rvert\)</span> 很大的样本集合 <spanclass="math inline">\(\{x_1,\cdots x_n\}\)</span>的概率较大。对于该样本集合，<spanclass="math inline">\(L(\theta)\)</span> 对于 <spanclass="math inline">\(\theta\)</span>的变化十分敏感，因此这个样本集合能够对 <spanclass="math inline">\(\theta\)</span> 作出更好的估计.</p><p><span class="math inline">\(\displaystyle \frac{\frac{\partial}{\partial \theta}L(\theta)}{L(\theta)}\)</span> 可以认为是一种normalization，因为 <spanclass="math inline">\(L(\theta+\mathrm{d}\theta)\thickapproxL(\theta)+\frac{\partial }{\partial\theta}L(\theta)\mathrm{d}\theta\)</span>，重要的其实是 <spanclass="math inline">\(\displaystyle \frac{\frac{\partial }{\partial\theta}L(\theta)}{L(\theta)}\)</span> 这个比例.</p><p>对不同尺度的模型，不能直接比较 <spanclass="math inline">\(I(\theta)\)</span>，但在同一模型中，<spanclass="math inline">\(I(\theta)\)</span> 是一个绝对的量.</p><p><strong>例1</strong> <span class="math inline">\(X\sim\mathcal{N}(\theta,\sigma_0^{2})\)</span>，其中 <spanclass="math inline">\(\sigma_0\)</span> 是给定的，求采样 <spanclass="math inline">\(n\)</span> 次得到的 Fisher information.</p><p><span class="math display">\[    I(\theta)=\frac{n}{\sigma_0^{2}}\]</span></p><p>可以发现，<span class="math inline">\(I(\theta)\)</span> 与 <spanclass="math inline">\(\theta\)</span>无关，说明单纯对分布作一个平移不会影响信息量.</p><p><strong>例2</strong> <span class="math inline">\(X\sim\mathcal{N}(\mu_0,\theta^{2})\)</span>，其中 <spanclass="math inline">\(\mu_0\)</span> 是给定的，求采样 <spanclass="math inline">\(n\)</span> 次得到的 Fisher information. <spanclass="math display">\[    I(\theta)=\frac{2n}{\theta^{2}}\]</span></p><p>可以发现，<span class="math inline">\(I(\theta)\)</span> 随着 <spanclass="math inline">\(\theta\)</span> 的增大而减小. 事实上，如果 <spanclass="math inline">\(\theta\to 0\)</span>，那么分布成为一个 <spanclass="math inline">\(\delta\)</span> 函数，每次采样只能采到 <spanclass="math inline">\(\mu_0\)</span>，我们有很大的把握认为这时 <spanclass="math inline">\(\theta=0\)</span>. 如果 <spanclass="math inline">\(\theta\to\infty\)</span>，那么分布近似为一个均匀分布，采样得到的 <spanclass="math inline">\(x_i\)</span> 分布很广泛，我们有很大的把握认为<span class="math inline">\(\theta\)</span> 很大，但具体 <spanclass="math inline">\(\theta\)</span> 是多少几乎不可能估计出来.</p><p><strong>例3</strong> <span class="math inline">\(X\sim D\)</span> 与<span class="math inline">\(\theta\)</span>无关，此时无论取多少次，<span class="math inline">\(I(\theta)\)</span>都等于 <span class="math inline">\(0\)</span>.</p><p><strong>例4</strong> <span class="math inline">\(X\sim\mathcal{N}(1000+\exp (-\theta^{2}),\sigma_0^{2})\)</span>，其中 <spanclass="math inline">\(\sigma_0\)</span> 是给定的，求采样 <spanclass="math inline">\(n\)</span> 次得到的Fisher information. <spanclass="math display">\[    I(\theta)=\frac{4n\theta^{2}\exp (-2\theta^{2})}{\sigma_0^{2}}\]</span></p><p>这形式很奇妙吧，而且与 <span class="math inline">\(1000\)</span>并没有关系. 但是 <span class="math inline">\(\exp (-\theta^{2})\)</span>相比 <span class="math inline">\(1000\)</span>是很小的，感觉这背后有一种 Fisher information的缺陷（或许其实是优点），让我再问问.</p><h3 id="fisher-information-in-neural-activity---approach-i">Fisherinformation in neural activity - approach I</h3><p>类推到对刺激 <span class="math inline">\(\theta\)</span> 的neuralactivity <spanclass="math inline">\(\mathbf{r}\)</span>，假设它服从具有线性充分统计量的指数族分布（正态分布当然属于指数族分布）.<span class="math display">\[    p(\mathbf{r}|\theta)=g(\theta)\Phi(\mathbf{r})\exp(\mathbf{h}(\theta)^{\mathsf{T}}\mathbf{r}), \tag{1}\]</span></p><p>其中 <span class="math display">\[    g(\theta)=\frac{1}{\int \Phi(\mathbf{r})\exp(\mathbf{h}(\theta)^{\mathsf{T}}\mathbf{r})\mathrm{d}\mathbf{r}},\tag{2}\]</span></p><p>且 <spanclass="math inline">\(g(\theta),\Phi(\mathbf{r}),\mathbf{h}(\theta)\)</span>都是已知的函数. <span class="math inline">\(T(\mathbf{r}_1,\cdots,\mathbf{r}_n)=\mathbf{r}_1+\cdots +\mathbf{r}_n\)</span>是一个线性充分统计量.</p><p>为什么要选用指数族分布呢？指数族分布是唯一具有下述几条性质的分布： -如果有独立同分布的一组样本 <span class="math inline">\(x_1,\cdots,x_n\)</span> 取自某一族由未知参数 <spanclass="math inline">\(\theta\)</span> 刻画的分布，在一定条件下，如果有<span class="math inline">\(\mathbb{R}^{m}\)</span> 中的充分统计量 <spanclass="math inline">\(T(x_1,\cdots ,x_n)\)</span> 使得 <spanclass="math inline">\(m\)</span> 不随 <spanclass="math inline">\(n\)</span>的增大而增大，那么该族分布一定是指数族分布. - 共轭先验（不懂） -关于变分推断（不懂）</p><p>此时 score 函数 <span class="math display">\[    S(\theta)=\frac{\partial }{\partial \theta}\logp(\mathbf{r}|\theta)=\mathbf{h}&#39;(\theta)^{\mathsf{T}}(\mathbf{r}(\theta)-\mathbf{f}(\theta)),\tag{3}\]</span></p><p>其中 <spanclass="math inline">\(\mathbf{f}(\theta)=\mathbb{E}(\mathbf{r}(\theta))\)</span>是 population activity vector. 记 <spanclass="math inline">\(\mathbf{\Sigma}(\theta)=\mathbb{E}[(\mathbf{r}(\theta)-\mathbf{f}(\theta))(\mathbf{r}(\theta)-\mathbf{f}(\theta))^{\mathsf{T}}]\)</span>为 neural activity的协方差矩阵.</p><p>此时的 Fisher information 就是 <span class="math display">\[    I(\theta)=\mathbf{h}&#39;(\theta)^{\mathsf{T}}\mathbf{\Sigma}(\theta) \mathbf{h}&#39;(\theta), \tag{4}\]</span></p><p>另外可以计算得到 <span class="math display">\[    \mathbf{f}&#39;(\theta)=\frac{\mathrm{d}}{\mathrm{d}\theta}\int_{}^{}\mathbf{r}p(\mathbf{r}|\theta)\mathrm{d}\mathbf{r}=\mathbf{\Sigma}(\theta)\mathbf{h}&#39;(\theta).\tag{5}\]</span></p><p>代回（4），得到 <span class="math display">\[    I(\theta)=\mathbf{f}&#39;(\theta)^{\mathsf{T}}\mathbf{\Sigma}^{-1}(\theta)\mathbf{f}&#39;(\theta).\tag{6}\]</span></p><h3 id="fisher-information-in-neural-activity---approach-ii">Fisherinformation in neural activity - approach II</h3><p>线性 Fisher information还可以被认为是从一个最小方差、无偏的线性解码器中得到的信息.线性意味着解码器把 neural activity 投影到一个向量 <spanclass="math inline">\(\mathbf{w}\)</span> 上.</p><p>在一篇<a href="https://www.nature.com/articles/nn1321">2004年的natureneuroscience</a>上，实验是这么做的：对于十分靠近刺激 <spanclass="math inline">\(\theta_0\)</span> 的两个刺激 <spanclass="math inline">\(\theta_1=\theta_0-\delta \theta,\theta_2=\theta_0+\delta \theta\)</span>，训练两个参数 <spanclass="math inline">\(\mathbf{w}\)</span> 和 <spanclass="math inline">\(b\)</span>，使得可以从 neural activity 也就是<span class="math inline">\(\mathbf{r}\)</span> 中估计出受到的刺激是<span class="math inline">\(\theta_1\)</span> 还是 <spanclass="math inline">\(\theta_2\)</span> <span class="math display">\[    \hat{\theta}=\mathbf{w}^{\mathsf{T}}\mathbf{r}+b.\]</span></p><p>由于当 <span class="math inline">\(\delta \theta\to 0\)</span>时，估计的参数 <span class="math inline">\(\hat{\theta}\)</span> 应当在<span class="math inline">\(\mathbf{r}\)</span> 取平均时趋于 <spanclass="math inline">\(\theta_0\)</span>（无偏性），故实际上有 <spanclass="math display">\[    \hat{\theta}=\mathbf{w}^{\mathsf{T}}(\mathbf{r}-\mathbf{f}(\theta_0))+\theta_0.\tag{7}\]</span></p><p>如果我们不是通过实验+数据处理来得到 <spanclass="math inline">\(\mathbf{w}\)</span>，我们依然可以求得一个最佳的<span class="math inline">\(\mathbf{w}\)</span>（注意这个 <spanclass="math inline">\(\mathbf{w}\)</span> 是跟 <spanclass="math inline">\(\theta_0\)</span> 有关的），它应当满足对不同的<span class="math inline">\(\mathbf{r}\)</span> 方差最小. 这就是一个unbiased locally linear estimator.</p><p>在<a href="https://www.jstor.org/stable/2984603">关于locally unbiasedestimation的理论</a>中，提到了渐近无偏的条件： - 期望渐近无偏，即 <spanclass="math inline">\(\lim_{\theta_1 \to\theta_0}\mathbb{E}_{\theta_1}(\hat{\theta})=\mathbb{E}_{\theta_0}(\hat{\theta})\)</span>，- 方差渐近无偏，即 <span class="math inline">\(\lim_{\theta_1 \to\theta_0}\operatorname{var}(\hat{\theta}|\theta_1)=\operatorname{var}(\hat{\theta}|\theta_0)\)</span>.</p><p>因为 <span class="math display">\[    \mathbb{E}_{\mathbf{r}}(\hat{\theta})=\theta_0\]</span></p><p>我没管一些 regularity 的条件，那么认为期望渐近无偏就等价于（在每个<span class="math inline">\(\theta\)</span> 处都有） <spanclass="math display">\[    \frac{\mathrm{d}\mathbb{E}_{\mathbf{r}}(\hat{\theta})}{\mathrm{d}\theta}=1.\tag{8}\]</span></p><p>然后一定要用到 <spanclass="math inline">\(p(\mathbf{r}|\theta)\)</span>的具体表达式（1），代入（8），得到约束条件 <span class="math display">\[    \mathbf{w}^{\mathsf{T}}\mathbf{f}&#39;(\theta)=1. \tag{9}\]</span></p><p>为了寻找满足约束条件（9）下使得估计方差最小的 <spanclass="math inline">\(\mathbf{w}\)</span>，目标是寻找 <spanclass="math display">\[    \min _{\mathbf{w}}\mathbf{w}^{\mathsf{T}}\mathbf{\Sigma}\mathbf{w},\quad s.t.\ \mathbf{w}^{\mathsf{T}}\mathbf{f}&#39;(\theta)=1. \tag{10}\]</span></p><p>用拉格朗日乘子，得到 <span class="math display">\[    \mathbf{w}^{*}=\frac{\mathbf{\Sigma}^{-1}\mathbf{f}&#39;}{\mathbf{f}&#39;^{\mathsf{T}}\mathbf{\Sigma}^{-1}\mathbf{f}&#39;}\tag{11}\]</span></p><p>此时估计值的方差为 <span class="math display">\[    \operatorname{var}(\hat{\theta})=\frac{1}{\mathbf{f}&#39;^{\mathsf{T}}\mathbf{\Sigma}^{-1}\mathbf{f}&#39;}.\tag{12}\]</span></p><p>此时的 Fisher information就定义为 <spanclass="math inline">\(\operatorname{var}(\hat{\theta})\)</span>的倒数，即 <span class="math display">\[    I(\theta)=\mathbf{f}&#39;^{\mathsf{T}}\mathbf{\Sigma}^{-1}\mathbf{f}&#39;.\tag{13}\]</span></p><h2id="generalizing-fisher-information-beyond-fine-discrimination">GeneralizingFisher information beyond fine discrimination</h2><h3 id="approach-i">approach I</h3><p>记 <span class="math inline">\(C_1\)</span> 为受到 <spanclass="math inline">\(\theta_1\)</span> 刺激的实验， <spanclass="math inline">\(C_2\)</span> 为受到 <spanclass="math inline">\(\theta_2\)</span> 刺激的实验.假设在每种实验中，neural activity 服从正态分布， <spanclass="math display">\[    C_1\colonp(\mathbf{r}|\theta_1)=\mathcal{N}(\mathbf{r}|\mathbf{f}_1,\mathbf{\Sigma})\\    C_2\colonp(\mathbf{r}|\theta_2)=\mathcal{N}(\mathbf{r}|\mathbf{f}_2,\mathbf{\Sigma}),\tag{14}\]</span></p><p>（均值不同，但协方差矩阵相同）附加一些对称性条件、先验条件、对正确结果的偏好条件：记<span class="math inline">\(L_{ij}\)</span> 是当 <spanclass="math inline">\(C_i\)</span> 时选择 <spanclass="math inline">\(C_j\)</span> 的 loss，规定 <spanclass="math inline">\(L_{12}=L_{21}, L_{11}=L_{22},L_{11}&lt;L_{12}\)</span>，先验 <spanclass="math inline">\(p(C_1)=p(C_2)=\frac{1}{2}\)</span>. 这时，expectedBayesian risk（或者也可以叫 posterior expected loss/ Bayesian expectedloss）就是 <span class="math display">\[    \sum_{i\in \{1,2\}}^{} L_{iD(\mathbf{r})}p(C_i|\mathbf{r}).\]</span></p><p>这里 <span class="math inline">\(D(\mathbf{r})\)</span> 是一个decision rule. 最佳的 decision rule，使得 expected Bayesian risk 最小<span class="math display">\[    D(\mathbf{r})=\begin{cases}        2, \quad\Lambda(\mathbf{r})=\log\frac{p(\mathbf{r}|\theta_2)}{p(\mathbf{r}|\theta_1)}&gt;0, \\        1, \quad \Lambda(\mathbf{r})\leqslant 0,    \end{cases}\]</span> (15)</p><p><span class="math inline">\(\Lambda(\mathbf{r})\)</span> 是log-likelihood ratio. 在假设的正态分布中， <span class="math display">\[    \Lambda(\mathbf{r})=(\mathbf{f}_2-\mathbf{f}_1)^{\mathsf{T}}\mathbf{\Sigma}^{-1}(\mathbf{r}-\mathbf{f}_0),\tag{16}\]</span></p><p>其中 <spanclass="math inline">\(\mathbf{f}_0=\frac{1}{2}(\mathbf{f}_1+\mathbf{f}_2)\)</span>.令 <span class="math inline">\(\mathbf{w}=\mathbf{\Sigma}^{-1}\delta\mathbf{f}, \delta \mathbf{f}=\mathbf{f}_2-\mathbf{f}_1\)</span>. <spanclass="math display">\[    \Lambda(\mathbf{r})=\mathbf{w}^{\mathsf{T}}(\mathbf{r}-\mathbf{f}_0).\tag{17}\]</span></p><p>注意 <span class="math display">\[    \Lambda(\mathbf{r})|C_1\sim \mathcal{N}\left(-\frac{1}{2}\mathbf{w}^{\mathsf{T}}\mathbf{\Sigma w} ,\mathbf{w}^{\mathsf{T}} \mathbf{\Sigma w}\right), \quad\Lambda(\mathbf{r})|C_2\sim \mathcal{N}\left(\frac{1}{2}\mathbf{w}^{\mathsf{T}}\mathbf{\Sigma w},\mathbf{w}^{\mathsf{T}}\mathbf{\Sigma w} \right). \tag{18}\]</span></p><p>故我们可以计算 <span class="math inline">\(D(\mathbf{r})\)</span>下做出正确决策的概率 <span class="math display">\[    p(\text{correct})=\frac{1}{2}p(\Lambda(\mathbf{r})\leqslant0|C_1)+\frac{1}{2}p(\Lambda(\mathbf{r})&gt;0|C_2)=\Phi\left(\frac{1}{2}\sqrt{\mathbf{w}^{\mathsf{T}}\mathbf{\Sigmaw}}\right)=\Phi\left( \frac{1}{2}\sqrt{\delta\mathbf{f}^{\mathsf{T}}\mathbf{\Sigma}^{-1}\delta \mathbf{f}} \right)\]</span></p><p>其中 <span class="math inline">\(\Phi(\cdot)\)</span>是标准正态分布的累积分布函数，即 <span class="math display">\[    \Phi(y)=\frac{1}{\sqrt{2\pi}} \int_{-\infty}^{y} \exp(-\frac{x^{2}}{2}) \mathrm{d}x\]</span></p><p>与（6）比较，可以定义 <span class="math display">\[    I_{g}(\theta)=\frac{\delta\mathbf{f}^{\mathsf{T}}\mathbf{\Sigma}^{-1}\delta \mathbf{f}}{\delta\theta^{2}}, \tag{19}\]</span></p><h3 id="approach-ii">approach II</h3><p>第二种得到 generalized linear Fisher information 的办法也是通过一个optimal linear discriminator，但此时我们对 neural activity的分布没有像（1）那么高的要求.</p><p>我们可以这么做： <span class="math display">\[    \hat{\theta}=\mathbf{w}^{\mathsf{T}}\mathbf{r}. \tag{20}\]</span></p><p>给 <span class="math inline">\(\hat{\theta}\)</span> 一个阈值. 要使得<span class="math inline">\(\mathbf{w}\)</span>是最佳的，意味着最小化两个类内的方差，最大化两个类间的平均距离（类间的方差，对<span class="math inline">\(\mathbf{r}\)</span> 而言）. 即 <spanclass="math display">\[    \max_{\mathbf{w}}\frac{\mathbf{w}^{\mathsf{T}}\delta \mathbf{f}\delta \mathbf{f}^{\mathsf{T}}\mathbf{w}}{\mathbf{w}^{\mathsf{T}}\mathbf{\Sigma w}}, \quad s.t. \ \left\| \mathbf{w} \right\|_{}^{2}=1,\tag{21}\]</span></p><p>其中 <span class="math inline">\(\delta \mathbf{f} \delta\mathbf{f}^{\mathsf{T}}\)</span> 是类间协方差矩阵，<spanclass="math inline">\(\mathbf{\Sigma}\)</span> 是平均类内协方差矩阵<span class="math display">\[    \mathbf{\Sigma}=\frac{\mathbf{\Sigma}_1+\mathbf{\Sigma}_2}{2}.\tag{22}\]</span></p><p>利用拉格朗日乘子，得到 <span class="math display">\[    \mathbf{w}=\frac{\mathbf{\Sigma}^{-1}\delta \mathbf{f}}{\delta\mathbf{f}^{\mathsf{T}} \mathbf{\Sigma}^{-1}\delta \mathbf{f}}. \tag{23}\]</span></p><p>注意 <span class="math inline">\(\hat{\theta}\)</span>是许多相关的随机变量的线性组合，因此不能直接用中心极限定理.然而，还是可以近似认为 <span class="math display">\[    \hat{\theta}|C_1\sim\mathcal{N}(\mathbf{w}^{\mathsf{T}}\mathbf{f}_1, \mathbf{w}^{\mathsf{T}}\mathbf{\Sigma}_1 \mathbf{w}), \quad \hat{\theta}|C_2 \sim\mathcal{N}(\mathbf{w}^{\mathsf{T}}\mathbf{f}_2, \mathbf{w}^{\mathsf{T}}\mathbf{\Sigma}_2 \mathbf{w}). \tag{24}\]</span></p><p>另外，阈值的选取使得正确率最高，但算起来比较麻烦，这里就不算了.</p><h2 id="bic-bayesian-information-criterion">BIC (Bayesian informationcriterion)</h2><p>简略地说，<span class="math inline">\(BIC\)</span>依靠后验概率评估模型的效果. 如果有 <spanclass="math inline">\(r\)</span> 个候选模型 <spanclass="math inline">\(M_1,\cdots ,M_r\)</span>，每个模型有一个likelihood <spanclass="math inline">\(f_i(x|\mathbf{\theta}_{i})(\mathbf{\theta}_{i}\in\Theta_{i} \subset \mathbb{R}^{k_i})\)</span>，有先验 <spanclass="math inline">\(\pi_i(\mathbf{\theta}_i)\)</span> 和 <spanclass="math inline">\(P(M_i)\)</span>. 给定 <spanclass="math inline">\(n\)</span> 个观测量 <spanclass="math inline">\(\mathbf{x}_{n}=\{x_1,\cdots,x_n\}\)</span>，marginal likelihood 定义为 <spanclass="math display">\[    p(\mathbf{x}_n|M_i)=\int_{}^{}f_i(\mathbf{x}_n|\mathbf{\theta}_i)\pi_{i}(\mathbf{\theta}_{i})\mathrm{d}\mathbf{\theta}_{i}. \tag{25}\]</span></p><p>由 Bayes 定理 <span class="math display">\[    P(M_i|\mathbf{x}_n)=\frac{p(\mathbf{x}_n|M_i)P(M_i)}{\sum_{j=1}^{r}p(\mathbf{x}_n|M_j)P(M_j)}, \quad i=1,\cdots ,r. \tag{26}\]</span></p><p>选择使 <span class="math inline">\(P(M_i|\mathbf{x}_n)\)</span>最大的模型即可，由此我们导出 <span class="math inline">\(BIC\)</span>这个量.</p><p>一般我们会认为模型使用概率等可能，即 <spanclass="math inline">\(P(M_i)\)</span> 都相等. 此时</p><p><span class="math display">\[    B_{12}=\frac{P(M_1|\mathbf{x}_n)}{P(M_2|\mathbf{x}_n)}=\frac{p(\mathbf{x}_n|M_1)}{p(\mathbf{x}_n|M_2)}\frac{P(M_1)}{P(M_2)}=\frac{\int_{}^{}f_1(\mathbf{x}_n|\mathbf{\theta}_1)\pi_{1}(\mathbf{\theta}_1)\mathrm{d}\mathbf{\theta}_{1}}{\int_{}^{}f_2(\mathbf{x}_n|\mathbf{\theta}_{2})\pi_{2}(\mathbf{\theta}_{2})\mathrm{d}\mathbf{\theta}_{2}} \tag{27}\]</span></p><p>Bayes factor 定义为 <span class="math inline">\(\displaystyle\frac{p(\mathbf{x}_n|M_1)}{p(\mathbf{x}_n|M_2)}\)</span>.</p><h3 id="section"></h3><h3 id="laplace-approximation">Laplace Approximation</h3><p>Laplace Approximation是一种用二阶泰勒展开近似计算积分的方法，依赖于所谓的 Bernstein–von Mises定理. 目标是近似 <span class="math display">\[    \int_{}^{} \exp \{nq(\mathbf{\theta})\} \mathrm{d}\mathbf{\theta}\tag{28}\]</span></p><p>这里 <span class="math inline">\(\mathbf{\theta}\)</span> 是一个<span class="math inline">\(p\)</span> 维参数向量， <spanclass="math inline">\(q(\mathbf{\theta})\)</span> 一般属于 <spanclass="math inline">\(C_{c}^{2}(\mathbb{R}^{p})\)</span>且有一个全局最大值点 <spanclass="math inline">\(\hat{\mathbf{\theta}}\)</span>. 此时当然满足 <spanclass="math inline">\(\partial q(\mathbf{\theta})/\partial\mathbf{\theta}|_{\mathbf{\theta}=\hat{\mathbf{\theta}}}=\mathbf{0}\)</span>.在 <span class="math inline">\(\hat{\mathbf{\theta}}\)</span>作泰勒展开</p><p><span class="math display">\[    q(\mathbf{\theta})=q(\hat{\mathbf{\theta}})-\frac{1}{2}(\mathbf{\theta}-\hat{\mathbf{\theta}})^{\mathsf{T}}J_{q}(\hat{\mathbf{\theta}})(\mathbf{\theta}-\hat{\mathbf{\theta}})+\cdots, \tag{29}\]</span></p><p>Bernstein–von Mises 定理告诉我们 <span class="math display">\[    \int_{}^{} \exp \{nq(\mathbf{\theta})\}\mathrm{d}\mathbf{\theta}\thickapprox \exp\{nq(\hat{\mathbf{\theta}})\}\int_{}^{} \exp\left\{-\frac{n}{2}(\mathbf{\theta}-\hat{\mathbf{\theta}})^{\mathsf{T}}J_q(\hat{\mathbf{\theta}})(\mathbf{\theta}-\hat{\mathbf{\theta}})\right\}\mathrm{d}\mathbf{\theta}=\exp\{nq(\hat{\mathbf{\theta}})\}\frac{(2\pi)^{p/2}}{n^{p/2}\lvertJ_q(\hat{\mathbf{\theta}}) \rvert ^{1/2}}\]</span></p><h3 id="derivation-of-the-bic">Derivation of the BIC</h3><p>为方便起见，省略 <span class="math inline">\(M_i\)</span>，将marginal likelihood 写成 <span class="math display">\[    p(\mathbf{x}_n)=\int_{}^{}f(\mathbf{x}_n|\mathbf{\theta})\pi(\mathbf{\theta}) \mathrm{d}\mathbf{\theta}=\int_{}^{} \exp \{\logf(\mathbf{x}_n|\mathbf{\theta})\}\pi(\mathbf{\theta})\mathrm{d}\mathbf{\theta}=\int_{}^{} \exp\{l(\mathbf{\theta})\}\pi(\mathbf{\theta}) \mathrm{d}\mathbf{\theta},\tag{30}\]</span></p><p>即 <span class="math display">\[    l(\theta)=\log f(\mathbf{x}_n|\theta)=\sum_{i=1}^{n} \logf(x_n|\theta)\]</span></p><p>在极大似然估计（MLE）对应的 <spanclass="math inline">\(\hat{\mathbf{\theta}}\)</span> 附近， <spanclass="math display">\[    l(\mathbf{\theta})=l(\hat{\mathbf{\theta}})-\frac{n}{2}(\mathbf{\theta}-\hat{\mathbf{\theta}})^{\mathsf{T}}J(\hat{\mathbf{\theta}})(\mathbf{\theta}-\hat{\mathbf{\theta}})+\cdots, \tag{31}\]</span></p><p>其中 <span class="math display">\[    J(\hat{\mathbf{\theta}})=-\frac{1}{n}\frac{\partial^{2}l(\mathbf{\theta})}{\partial\mathbf{\theta} \partial\mathbf{\theta}^{\mathsf{T}}}\bigg|_{\mathbf{\theta}=\hat{\mathbf{\theta}}}\tag{32}\]</span></p><p>另外对 <span class="math inline">\(\pi(\mathbf{\theta})\)</span>作类似展开， <span class="math display">\[    \pi(\mathbf{\theta})=\pi(\hat{\mathbf{\theta}})+(\mathbf{\theta}-\hat{\mathbf{\theta}})^{\mathsf{T}}\frac{\partial\pi(\mathbf{\theta})}{\partial\mathbf{\theta}}\bigg|_{\theta=\hat{\mathbf{\theta}}}+\cdots  \tag{33}\]</span></p><p>将（31）（33）代入（30），且考虑到 <spanclass="math inline">\(\hat{\mathbf{\theta}}-\mathbf{\theta}=O_p(n^{-1/2})\)</span>（以<span class="math inline">\(n^{-1/2}\)</span> 的阶依概率收敛），就有<span class="math display">\[    p(\mathbf{x}_n)\thickapprox \exp\{l(\hat{\mathbf{\theta}})\}\pi(\hat{\mathbf{\theta}})(2\pi)^{p/2}n^{-p/2}\lvert J(\hat{\mathbf{\theta}}) \rvert ^{-1/2}. \tag{34}\]</span></p><p>故有 <span class="math display">\[    -2\log p(\mathbf{x}_n)\thickapprox -2l(\hat{\mathbf{\theta}})+p\logn + \log \lvert J(\hat{\mathbf{\theta}}) \rvert -p \log (2\pi)-2 \log\pi(\hat{\mathbf{\theta}}).\]</span></p><p>当 <span class="math inline">\(n\to \infty\)</span> 时，前两项dominate 后三项，故可以定义 <span class="math display">\[    BIC=-2\log f(\mathbf{x}_n|\hat{\mathbf{\theta}})+p\log n. \tag{35}\]</span></p><p>另外，<span class="math inline">\(BIC\)</span> 也可以看成当统计模型是regular 时，用 MLE 得到的 <spanclass="math inline">\(\hat{\mathbf{\theta}}\)</span> 代入后的 Bayes freeenergy.</p><h2 id="aic-akaike-information-criterion">AIC (Akaike informationcriterion)</h2><p>简略地说，AIC 利用 K-L 散度衡量了利用 MLE 得到的 <spanclass="math inline">\(f(x|\hat{\mathbf{\theta}})\)</span> 的预测准确度.利用 K-L 散度衡量预测分布 <span class="math inline">\(f\)</span>与真实分布 <span class="math inline">\(g\)</span> 之间的关系 <spanclass="math display">\[    I\{g(z);f(z|\hat{\mathbf{\theta}})\}=\mathbb{E}_{G}\left[ \log\left\{\frac{g(Z)}{f(Z|\hat{\mathbf{\theta}})}\right\}\right] \tag{36}\]</span></p><p>这里固定 <spanclass="math inline">\(\hat{\mathbf{\theta}}=\hat{\mathbf{\theta}}(\mathbf{x}_n)\)</span>.</p><p>我们当然希望 expected log-likelihood <span class="math display">\[    \mathbb{E}_{G}\left[ \logf(Z|\hat{\mathbf{\theta}})\right]=\int_{}^{} \logf(z|\hat{\mathbf{\theta}}) \mathrm{d}G(z)  \tag{37}\]</span></p><p>尽可能大. 一种对 log-likelihood 的估计是 <spanclass="math display">\[    \mathbb{E}_{\hat{G}}\left[ \logf(Z|\hat{\mathbf{\theta}})\right]=\int_{}^{} \logf(z|\hat{\mathbf{\theta}})\mathrm{d}\hat{G}(z)=\frac{1}{n}\sum_{\alpha=1}^{n} \logf(x_{\alpha}|\hat{\mathbf{\theta}})=\frac{1}{n}l(\hat{\mathbf{\theta}}).\tag{38}\]</span></p><h3 id="bias-of-the-log-likelihood">Bias of the Log-likelihood</h3><p>注意我们用观测到的样本 <spanclass="math inline">\(\mathbf{x}_n=\{x_1,\cdots x_n\}\)</span>对特定的模型进行MLE，然后得到一个 <spanclass="math inline">\(\hat{\mathbf{\theta}}\)</span>.在评估该模型时，我们重复利用 <spanclass="math inline">\(\mathbf{x}_n\)</span> 对 <spanclass="math inline">\(\mathbb{E}_{G}\left[ \logf(Z|\hat{\mathbf{\theta}})\right]\)</span> 进行了估计.这其中会有误差，感觉主要原因是采样不够多、不够好.</p><p><ahref="https://link.springer.com/book/10.1007/978-0-387-71887-3">两个日本人写的书3.4.3节</a>中定义了bias</p><p><span class="math display">\[    b(G)=\mathbb{E}_{G(\mathbf{x}_n)}\left[ \logf(\mathbf{X}_n|\hat{\mathbf{\theta}}(\mathbf{X}_n))-n\mathbb{E}_{G(z)}[\log f(Z|\hat{\mathbf{\theta}}(\mathbf{X}_n))]\right]\tag{39}\]</span></p><p>并计算得到</p><p><span class="math display">\[    b(G)=\operatorname{tr}\{I(\mathbf{\theta}_0)J(\mathbf{\theta}_0)^{-1}\}\]</span></p><p>其中 <span class="math display">\[    I(\mathbf{\theta}_0)=\mathbb{E}_{G(z)}\left[\frac{\partial \logf(Z|\mathbf{\theta})}{\partial \mathbf{\theta}} \frac{\partial \logf(Z|\mathbf{\theta})}{\partial\mathbf{\theta}^{\mathsf{T}}}\bigg|_{\mathbf{\theta}_0}\right]=\int_{}^{}g(z) \frac{\partial \log f(z|\mathbf{\theta})}{\partial \mathbf{\theta}}\frac{\partial \log f(z|\mathbf{\theta})}{\partial\mathbf{\theta}^{\mathsf{T}}}\bigg|_{\mathbf{\theta}_0} \mathrm{d}z\]</span></p><p><span class="math display">\[    J(\mathbf{\theta}_0)=-\mathbb{E}_{G(z)}\left[ \frac{\partial^{2}\log f(Z|\mathbf{\theta})}{\partial \mathbf{\theta} \partial\mathbf{\theta}^{\mathsf{T}}}\bigg|_{\mathbf{\theta}_0}\right]=-\int_{}^{}g(z) \frac{\partial ^{2}\log f(z|\mathbf{\theta})}{\partial\mathbf{\theta} \partial\mathbf{\theta}^{\mathsf{T}}}\bigg|_{\mathbf{\theta}_0} \mathrm{d}z\]</span></p><p>注意一下几点： - <span class="math inline">\(b(G)\)</span> 与观测次数<span class="math inline">\(n\)</span> 无关； - <spanclass="math inline">\(b(G)\)</span> 需要真实分布 <spanclass="math inline">\(G(z)\)</span>（我们并不知道）来计算，我们可以跟（38）一样估计<span class="math inline">\(G(z)\)</span>，虽然这真的很粗糙. - <spanclass="math inline">\(b(G)\)</span> 的导出并没有用到真实分布 <spanclass="math inline">\(g(x)\)</span> 包含于模型 ${f(x|);^{p}} $中这个假设. 当这个假设成立时，<spanclass="math inline">\(I(\mathbf{\theta}_0)=J(\mathbf{\theta}_0)\)</span>.</p><h3 id="akaike-information-criterion-aic">Akaike Information Criterion(AIC)</h3><p>假设真实分布 <span class="math inline">\(g(x)\)</span> 包含于模型${f(x|);^{p}} $ 中，即存在 <spanclass="math inline">\(\mathbf{\theta}_0\in \Theta\)</span> 使得 <spanclass="math inline">\(g(x)=f(x|\mathbf{\theta}_0)\)</span>. 此时 <spanclass="math display">\[    b(G)=\operatorname{tr}(I_p)=p, \tag{40}\]</span></p><p>恰好为模型的参数个数.如果认为采到不同样本集合的概率大致相同（感觉也不太靠谱），那么就有 <spanclass="math display">\[    p=b(G)\thickapprox \sum_{\alpha=1}^{n} \logf(X_{\alpha}|\hat{\mathbf{\theta}})-n \mathbb{E}_{G(z)}\logf(Z|\hat{\mathbf{\theta}})\]</span></p><p>乘个系数 <span class="math inline">\(-2\)</span> 让它长得跟 <spanclass="math inline">\(BIC\)</span> 像一点，就得到 <spanclass="math inline">\(AIC\)</span> 的定义 <span class="math display">\[    AIC= -2 \sum_{\alpha=1}^{n} \logf(X_{\alpha}|\hat{\mathbf{\theta}})+2p \tag{41}\]</span></p><h2 id="waic">WAIC</h2><p>对于一个统计模型，如果从参数空间到概率分布的映射是一一映射且<strong>它的Fisher information matrix 是正定的</strong>，就称该统计模型是 regular的. 否则，就称该统计模型是 singular 的.一般来说，如果一个统计模型包含分层结构、隐变量等结构，那么这个模型是singular 的，比如神经网络是一个 singular 的模型.</p><p><span class="math inline">\(WAIC\)</span> 是 <spanclass="math inline">\(AIC\)</span> 在 singular 的模型上的推广. 在singular 模型中，MLE 不满足渐近无偏性（asymptotic normality 我猜的）.从而，<span class="math inline">\(AIC\)</span> 不等于 averagegeneralization error，<span class="math inline">\(BIC\)</span> 不等于Bayes marginal likelihood，甚至也不能渐近地趋向.</p><p>对于 <span class="math inline">\(\mathbf{\Theta}\)</span>空间中的函数 <spanclass="math inline">\(f(\mathbf{\theta})\)</span>，给定一组样本集合<span class="math inline">\(\mathbf{x}_n=\{x_1,\cdots,x_n\}\)</span>，<span class="math inline">\(f(\mathbf{\theta})\)</span>关于后验分布的期望为</p><p><span class="math display">\[    \mathbb{E}_\mathbf{\theta}[f(\mathbf{\theta})]=\int_{}^{}f(\mathbf{\theta})p(\mathbf{\theta}|x_1,\cdots x_n)\mathrm{d}\mathbf{\theta}=\frac{\int_{}^{}f(\mathbf{\theta})\prod_{i=1}^{n}p(x_i|\mathbf{\theta})^{\beta}\pi(\mathbf{\theta})\mathrm{d}\mathbf{\theta}}{\int_{}^{} \prod_{i=1}^{n}p(x_i|\mathbf{\theta})^{\beta}\pi(\mathbf{\theta})\mathrm{d}\mathbf{\theta}}\]</span></p><p><span class="math inline">\(0&lt;\beta&lt;\infty\)</span> 称为inverse temperature，只关心 <span class="math inline">\(\beta=1\)</span>的情况（称为 strict Bayes estimation）. Watanabe 定义了 Bayes predictivedistribution <span class="math display">\[    p^{*}(x)\equiv \mathbb{E}_{\mathbf{\theta}}[p(x|\mathbf{\theta})].\]</span></p><p>Bayes generalization loss <spanclass="math inline">\(B_gL(n)\)</span> <span class="math display">\[    B_gL(n)=-\mathbb{E}_{X}[\log p^{*}(x)],\]</span></p><p>Bayes training loss <span class="math inline">\(B_{t}L(n)\)</span><span class="math display">\[    B_{t}L(n)=-\frac{1}{n} \sum_{i=1}^{n} \log p^{*}(x_{i}),\]</span></p><p>empirical variance <span class="math display">\[    V(n)=\sum_{i=1}^{n} \left\{ \mathbb{E}_{\mathbf{\theta}}[(\logp(x_i|\mathbf{\theta}))^{2}]-\mathbb{E}_{\mathbf{\theta}}[\logp(x_i|\mathbf{\theta})]^{2}\right\},\]</span></p><p>functional variance 体现了后验分布的波动. 然后可以定义 <spanclass="math display">\[    WAIC(n)\equiv B_{t}L(n)+\frac{\beta}{n}V(n),\]</span></p><p>我只关心 <span class="math inline">\(\beta=1\)</span>，那么 <spanclass="math display">\[    WAIC(n)=-\frac{1}{n}\sum_{i=1}^{n} \log p^{*}(x_i)+\frac{1}{n}\sum_{i=1}^{n} \left\{ \mathbb{E}_{\mathbf{\theta}}[(\logp(x_i|\mathbf{\theta}))^{2}]-\mathbb{E}_{\mathbf{\theta}}[\logp(x_i|\mathbf{\theta})]^{2}\right\}.\]</span></p><p>一般乘以系数 <span class="math inline">\(2n\)</span> 让它长得跟 <spanclass="math inline">\(AIC\)</span> 和 <spanclass="math inline">\(BIC\)</span> 更像 <span class="math display">\[    WAIC=-2\sum_{i=1}^{n} \log p^{*}(x_i)+2\sum_{i=1}^{n} \left\{\mathbb{E}_{\mathbf{\theta}}[(\logp(x_i|\mathbf{\theta}))^{2}]-\mathbb{E}_{\mathbf{\theta}}[\logp(x_i|\mathbf{\theta})]^{2}\right\}\]</span></p><p>注意计算 <span class="math inline">\(p^{*}(x)\)</span>时不需要真实分布，只需要样本和模型族.</p><h2 id="difference-among-information-criterions">Difference amonginformation criterions</h2><p><span class="math inline">\(BIC\)</span> 近似了 marginallikelihood，以选取最高的 posterior，这种做法其实并不好（Chapter7.4Bayesian Data Analysis, third edition, Andrew Gelman, et al.）.</p>]]></content>
    
    
    <categories>
      
      <category>Statistics</category>
      
      <category>Information Theory</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The Circular Law (2)</title>
    <link href="/2023/01/28/The-Circular-Law-2/"/>
    <url>/2023/01/28/The-Circular-Law-2/</url>
    
    <content type="html"><![CDATA[<h1 id="the-circular-law-2">The Circular Law (2)</h1><h2 id="gaussian-case">Gaussian Case</h2><p>Let <span class="math inline">\(X:=(X_{ij})_{1\leqslant i,j\leqslantn}\)</span> be an i.i.d. random matrix on <spanclass="math inline">\(\mathbb{C}\)</span> with variance <spanclass="math inline">\(1\)</span>. We consider <spanclass="math inline">\(X\)</span> as a random variable in <spanclass="math inline">\(\mathcal{M}_{n}(\mathbb{C})\)</span>. Sometimes wedenote <span class="math inline">\(G\)</span> instead of <spanclass="math inline">\(X\)</span> in order to distinguish the Gaussiancase from the generalcase.<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Charles Bordenave and Djalil Chafaï, The circular law">[1]</span></a></sup></p><h3 id="mean-circular-law">Mean Circular Law</h3><p>Recall that <spanclass="math inline">\(\mu_{G}=\frac{1}{n}\sum_{k=1}^{n}\delta_{\lambda_k(G)}\)</span>. The first result relies on the fact that<span class="math inline">\(\mathbb{E}\mu_{G}\)</span> has density <spanclass="math inline">\(\varphi_{n,1}\)</span>.</p><div class="note note-info">            <p><strong>Theorem</strong> <spanclass="math inline">\(\mathbb{E}\mu_{n^{-1/2}G}\rightsquigarrow\mathcal{C}_{1}\)</span> as <span class="math inline">\(n\to\infty\)</span>, where <spanclass="math inline">\(\mathcal{C}_{1}\)</span> is the uniform law on theunit disc of <span class="math inline">\(\mathbb{C}\)</span> withdensity <span class="math inline">\(z\mapsto \pi^{-1}\mathbf{1}_{\{z\in\mathbb{C}\colon \lvert z \rvert \leqslant 1\}}\)</span>.</p>          </div><p><strong>proof</strong> From (2.35) with <spanclass="math inline">\(k=1\)</span>, we get that the density of <spanclass="math inline">\(\mathbb{E}\mu_{G}\)</span> is <spanclass="math display">\[    \varphi_{n,1}\colon \mapsto \gamma(z)\left( \frac{1}{n}\sum_{l=0}^{n-1} \lvert H_l(z) \rvert ^{2}\right)=\frac{1}{n\pi}\mathrm{e}^{-\lvert z \rvert ^{2}}\sum_{l=0}^{n-1} \frac{\lvert z \rvert ^{2l}}{l!}.     \tag{3.1}  \]</span></p><p>For <span class="math inline">\(r^{2}&lt;n\)</span>, <spanclass="math display">\[    \mathrm{e}^{r^{2}} -\sum_{l=0}^{n-1}\frac{r^{2l}}{l!}=\sum_{l=n}^{\infty}\frac{r^{2l}}{l!}\leqslant\frac{r^{2n}}{n!}\sum_{l=0}^{\infty}\frac{r^{2l}}{(n+1)^{l}}=\frac{r^{2n}}{n!}\frac{n+1}{n+1-r^{2}}   \tag{3.2}\]</span></p><p>while for <span class="math inline">\(r^{2}&gt;n\)</span>, <spanclass="math display">\[    \sum_{l=0}^{n-1} \frac{r^{2l}}{l!}\leqslant\frac{r^{2(n-1)}}{(n-1)!}\sum_{l=0}^{n-1} \left( \frac{n-1}{r^{2}}\right) ^{l}\leqslant \frac{r^{2(n-1)}}{(n-1)!}\frac{r^{2}}{r^{2}-n+1}.\tag{3.3}\]</span></p><p>By taking <span class="math inline">\(r^{2}=\lvert \sqrt{n}z \rvert^{2}\)</span> we obtain that for every compact <spanclass="math inline">\(C\subset \mathbb{C}\)</span> <spanclass="math display">\[    \lim_{n \to \infty}\sup _{z\in \mathbb{C}} \lvertn\varphi_{n,1}(\sqrt{n}z)-\pi^{-1} \mathbf{1}_{[0,1]}(\lvert z \rvert )\rvert =0. \tag{3.4}\]</span></p><p>The <span class="math inline">\(n\)</span> in front of <spanclass="math inline">\(\varphi_{n,1}\)</span> is due to the fact that weare on <span class="math inline">\(\mathbb{C}\)</span>: <spanclass="math inline">\(\mathrm{d}\sqrt{n}x\mathrm{d}\sqrt{n}y=n\mathrm{d}x\mathrm{d}y\)</span>.</p><h3 id="strong-circular-law">Strong Circular Law</h3><div class="note note-info">            <p><strong>Theorem (Silverstein, 1984)</strong> a.s. <spanclass="math inline">\(\mu_{n^{-1/2}G}\rightsquigarrow\mathcal{C}_{1}\)</span> as <span class="math inline">\(n\to\infty\)</span>, where <spanclass="math inline">\(\mathcal{C}_{1}\)</span> is the uniform law on theunit disc of <span class="math inline">\(\mathbb{C}\)</span> withdensity <span class="math inline">\(z\mapsto \pi^{-1}\mathbf{1}_{\{z\in\mathbb{C}\colon \lvert z \rvert \leqslant 1\}}\)</span></p>          </div><p><strong>proof</strong> Let us pick a compactly supported continuousbounded function <span class="math inline">\(f\)</span> and set <spanclass="math display">\[    S_n:=\int_{\mathbb{C}}^{} f \mathrm{d}\mu_{n^{-1/2}G}, \quadS_{\infty}:=\pi^{-1}\int_{\lvert z \rvert \leqslant 1}^{} f(z)\mathrm{d}x\mathrm{d}y. \tag{3.5}\]</span></p><p>Suppose for now that we have <span class="math display">\[    \mathbb{E}[(S_n-\mathbb{E}S_n)^{4}]=O(n^{-2}). \tag{3.6}\]</span></p><p>By monotone convergence (or by the Fubini-Tonelli theorem), <spanclass="math display">\[    \mathbb{E}\sum_{n=1}^{\infty}(S_n-\mathbb{E}S_n)^{4}=\sum_{n=1}^{\infty}\mathbb{E}[(S_n-\mathbb{E}S_n)^{4}]&lt;\infty \tag{3.7}\]</span></p><p>and consequently <span class="math inline">\(\sum_{n=1}^{\infty}(S_n-\mathbb{E}S_n)^{4}&lt;\infty\)</span> a.s. which implies <spanclass="math inline">\(\lim_{n \to \infty}S_n-\mathbb{E}S_n=0\)</span>a.s. Since <span class="math inline">\(\lim_{n \to\infty}\mathbb{E}S_n=S_{\infty}\)</span> by the mean circular law, weget that a.s. <span class="math display">\[    \lim_{n \to \infty}S_n=S_{\infty}.\]</span></p><p>To establish (3.6), we set <span class="math display">\[    S_n-\mathbb{E}S_n=\frac{1}{n}\sum_{i=1}^{n} Z_i\]</span></p><p>with <span class="math display">\[    Z_i:=f(\lambda_i(n^{-1/2}G))-\mathbb{E}f(\lambda_i(n^{-1/2}G))\tag{3.8}\]</span></p><p>Now <span class="math display">\[    \begin{aligned}        \mathbb{E}[(S_n-\mathbb{E}S_{n})^{4}]&amp;=\frac{1}{n^{4}}\sum_{i_1}^{} \mathbb{E}[Z_{i_1}^{4}] \\        &amp;+\frac{4}{n^{4}}\sum_{i_1,i_2}^{}\mathbb{E}[Z_{i_1}Z_{i_2}^{3}] \\        &amp;+\frac{3}{n^{4}}\sum_{i_1,i_2}^{}\mathbb{E}[Z_{i_1}^{2}Z_{i_2}^{2}]\\        &amp;+\frac{6}{n^{4}}\sum_{i_1,i_2,i_3}^{}\mathbb{E}[Z_{i_1}Z_{i_2}Z_{i_3}^{2}] \\        &amp;+\frac{1}{n^{4}}\sum_{i_1,i_2,i_3,i_4}^{}\mathbb{E}[Z_{i_1}Z_{i_2}Z_{i_3}Z_{i_4}].    \end{aligned}\]</span></p><p>The first three terms of the RHS are <spanclass="math inline">\(O(n^{-2})\)</span> since <spanclass="math inline">\(\max_{1\leqslant i\leqslant n}\lvert Z_i \rvert\leqslant 2\left\| f \right\|_{\infty}\)</span>.</p><p>Note that the <span class="math inline">\(k\)</span>-correlation ofspectral density for <span class="math inline">\(n^{-1/2}G\)</span> is<span class="math display">\[    \tilde{\varphi}_{n,k}(z_1,\cdots,z_k)=\frac{(n-k)!}{n!}n^{k}\pi^{-k}\exp \left( -\sum_{i=1}^{k} n\lvertz_i \rvert^{2}  \right) \det \left[\sum_{l=0}^{n-1} n^{l}\frac{(z_i\bar{z_j})^{l}}{l!}\right]_{1\leqslant i,j\leqslant k}.  \tag{3.9}\]</span></p><p>For the fourth term, rewrite <span class="math display">\[    \tilde{\varphi}_{n,3}(z_1,z_2,z_3)=\frac{(n-3)!}{n!}n^{3}\prod_{i=1}^{3}\tilde{\varphi}_{n,1}(z_i)-\left(\frac{(n-3)!}{n!}n^{3}\prod_{i=1}^{3}\tilde{\varphi}_{n,1}(z_i)- \tilde{\varphi}_{n,3}(z_1,z_2,z_3) \right)\]</span></p><p>The function in the parenthesis is a density and has mass <spanclass="math inline">\(\displaystyle\frac{(n-3)!}{n!}n^{3}-1=\frac{3n-2}{(n-1)(n-2)}=O(n^{-1})\)</span>. And<span class="math inline">\(Z_1,Z_2,Z_3\)</span> are independent withrespect to <span class="math inline">\(\prod_{i=1}^{3}\tilde{\varphi}_{n,1}(z_i)\)</span>. Hence the contribution of thefourth term is <span class="math inline">\(O(n^{-2})\)</span>.</p><p>For the bound of the last term, note that <spanclass="math display">\[    \begin{aligned}        &amp;\exp \left( -\sum_{i=1}^{k} n\lvert z_i \rvert ^{2} \right)\det\left[ \sum_{l=0}^{n-1} \frac{n^{l}(z_i \bar{z_j})^{l}}{l!}\right]\\        &amp;=\det\left[ \exp (-\frac{n}{2}(\lvert z_i \rvert^{2}+\lvertz_j \rvert ^{2}))\sum_{l=0}^{n-1}\frac{n^{l}(z_i\bar{z_j})^{l}}{l!}\right] \\        &amp;=\det\left[\exp (n(-\frac{1}{2}\lvert z_i \rvert^{2}-\frac{1}{2}\lvert z_j \rvert ^{2}))\left( \exp (nz_i\bar{z_j})-\frac{n^{n}(z_i\bar{z_j})^{n}}{2\pi i}\int_{\xi=n\mathrm{e}^{i\theta}}^{} \frac{\mathrm{e}^{\xi}\mathrm{d}\xi}{(\xi-nz_i\bar{z_j})\xi^{n}}\right) \right].    \end{aligned}\]</span> (3.10)</p><p>Hence, if $S {z &lt;&lt;1} $, then <span class="math display">\[    \begin{aligned}        &amp;\operatorname{Pr}(z_1 \in S,\cdots ,z_k\in S)\\        &amp;= \frac{(n-k)!}{n!}n^{k}\pi^{-k}\int_{S^{k}}^{} \det \expn\left( -\frac{1}{2}\lvert z_i \rvert ^{2}-\frac{1}{2}\lvert z_j \rvert^{2}+z_i\bar{z_j} \right) \mathrm{d}z_1 \cdots\mathrm{d}z_k+O(\alpha^{n}). \\    \end{aligned}\]</span> (3.11)</p><p>The diagonal term in the determinant expansion gives <spanclass="math display">\[    \frac{(n-k)!}{n!}n^{k}\pi^{-k}\mu^{k}(S),\]</span></p><p>and this is the leading term. Let <span class="math display">\[    B=    \begin{cases}        0, \quad k=1 \\        -\frac{(n-k)!}{n!}n^{k}\pi^{-k}\tbinom{k}{2}\mu^{k-2}(S)\int_{S^{2}}^{}\exp \left( -\lvert z_1-z_2 \rvert ^{2}n\right)  \mathrm{d}z_1\mathrm{d}z_2, \quad k\geqslant 2.    \end{cases}\]</span></p><p>For <span class="math inline">\(k\geqslant 2\)</span>, <spanclass="math inline">\(B\)</span> is the contribution to (3.11) from allterms in the determinant containing all but two of the diagonalelements. The contributions from all other terms are <spanclass="math inline">\(O(n^{-2})\)</span>. (3.11) can be written as <spanclass="math display">\[    \frac{(n-k)!}{n!}n^{k}\left( \frac{\mu(S)}{\pi} \right)^{k}+B+O(n^{-2}).\]</span></p><p>Using this it is not hard to see the last term is also an <spanclass="math inline">\(O(n^{-2})\)</span>.</p><p>% note danger % I don't understand! % endnote %</p><p><strong>Theorem (Layers)</strong> We have the following equality indistribution: <span class="math display">\[    (\lvert \lambda_1(G)\rvert,\cdots ,\lvert \lambda_n(G)   \rvert)=(Z_{(1)},\cdots ,Z_{(n)})\]</span></p><p>where <span class="math inline">\(Z_{(1)},\cdots ,Z_{(n)}\)</span> isthe non-increasing reordering of a sequence <spanclass="math inline">\(Z_1,\cdots ,Z_n\)</span> of independent randomvariabes with <span class="math inline">\(Z_k^{2}\sim\Gamma(k,1)\)</span> for every <span class="math inline">\(1\leqslantk\leqslant n\)</span>. Here <spanclass="math inline">\(\Gamma(a,\lambda)\)</span> stands for the law on<span class="math inline">\(\mathbb{R}_{+}\)</span> with Lebesguedensity <span class="math inline">\(x\mapsto\lambda^{a}\Gamma(a)^{-1}x^{a-1}e^{-\lambda x}\)</span>.</p><p><strong>proof</strong> According to (2.24) and (3.9), for every <spanclass="math inline">\(r&gt;0\)</span>, <span class="math display">\[    \begin{aligned}        &amp;\operatorname{Pr}(\lvert \lambda_1(G) \rvert \leqslant\sqrt{n}r) \\        &amp;= \int_{\lvert z_1 \rvert \leqslant \sqrt{n}r}^{} \cdots\int_{\lvert z_n \rvert \leqslant \sqrt{n}r}^{} \frac{1}{n!\pi^{n}}\exp\left( -\sum_{i=1}^{n} \lvert z_i \rvert^{2}  \right) \det\left[\sum_{l=0}^{n-1} \frac{(z_i \bar{z_j})^{l}}{l!}\right]_{1\leqslanti,j\leqslant n}\mathrm{d}z_n^{r}\mathrm{d}z_n^{i}\cdots  \mathrm{d}z_1^{r}\mathrm{d}z_1^{i}\\        &amp;=\frac{1}{\pi^{n}}\det \left[ \int_{\lvert z \rvert\leqslant \sqrt{n}r}^{} \mathrm{e}^{-\lvert z \rvert ^{2}}z^{j}\bar{z}^{j} \mathrm{d}z^{r}\mathrm{d}z^{i} \right]=\prod_{j=1}^{n}\int_{0}^{nr^{2}} \mathrm{e}^{-t} t^{j} \mathrm{d}t=\prod_{j=1}^{n}\operatorname{Pr}\left( \frac{E_1+\cdots +E_j}{n}\leqslant r^{2} \right)    \end{aligned}\]</span></p><p>where <span class="math inline">\(E_1,\cdots ,E_k\)</span> are i.i.d.exponential random variables of unit mean.</p><p>The law of large numbers suggests that <spanclass="math inline">\(r=1\)</span> is a critical value. The centrallimit theorem suggests that <span class="math inline">\(\lvert\lambda_1(G) \rvert\)</span> behaves when <spanclass="math inline">\(n\gg 1\)</span> as the maximum of i.i.d.Gaussians, for which the fluctuations follow the Gumbel law.</p><h2 id="universal-case">Universal Case</h2><div class="note note-info">            <p><strong>Marchenko-Pastur quarter circular law</strong> a.s. <spanclass="math inline">\(\nu_{n^{-1/2}X}\rightsquigarrow\mathcal{Q}_{2}\)</span> as <span class="math inline">\(n\to\infty\)</span>, where <spanclass="math inline">\(\mathcal{Q}_{2}\)</span> is the quarter circularlaw on $[0,2]_{+} $ with density <span class="math display">\[    x\mapsto \frac{\sqrt{4-x^{2}}}{\pi}\mathbf{1}_{[0,2]}(x).\]</span></p>          </div><p>The <span class="math inline">\(n^{-1/2}\)</span> normalization iseasily understood from the law of large numbers:</p><p><span class="math display">\[    \int_{}^{} s^{2}\mathrm{d}\nu_{n^{-1/2}X}(s)=\frac{1}{n^{2}}\sum_{i=1}^{n}s_i(X)^{2}=\frac{1}{n^{2}}\operatorname{tr}(XX^{*})=\frac{1}{n^{2}}\sum_{i,j=1}^{n} \lvert X_{i,j} \rvert^{2}\rightarrow \mathbb{E}(\lvert X_{1,1} \rvert )^{2}. \tag{2.1}\]</span></p><div class="note note-info">            <p><strong>Girko circular law</strong> a.s. <spanclass="math inline">\(\mu_{n^{-1/2}X}\rightsquigarrow\mathcal{C}_{1}\)</span> as <span class="math inline">\(n\to\infty\)</span>, where <spanclass="math inline">\(\mathcal{C}_{1}\)</span> is the circular law whichis the uniform law on the unit disc of <spanclass="math inline">\(\mathbb{C}\)</span> with density <spanclass="math display">\[    z\mapsto \frac{1}{\pi}\mathbf{1}_{\{z\in \mathbb{C}\colon \lvert z\rvert \leqslant 1\}}.\]</span></p>          </div><p>The a.s. tightness of <spanclass="math inline">\(\mu_{n^{-1/2}X}\)</span> is easily understoodsince Weyl's inequality give <span class="math display">\[    \int_{}^{} \lvert \lambda^{2}\rvert  \mathrm{d}\mu_{n^{-1/2}X}(\lambda)=\frac{1}{n^{2}}\sum_{i=1}^{n}\lvert \lambda_i(X) \rvert ^{2}\leqslant \frac{1}{n^{2}}\sum_{i=1}^{n}s_i(X)^{2}=\int_{}^{} s^{2} \mathrm{d}\nu_{n^{-1/2}X}(s)\]</span></p><h2 id="reference">Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Charles Bordenave and DjalilChafaï, The circular law<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>R. A. Horn and Ch. R.Johnson, Topics in matrix analysis, Cambridge University Press,Cambridge, 1994, Corrected reprint of the 1991 original.<a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Madan Lal Mehta, Randommatrices, third ed. Pure and Applied Mathematics, vol.142, Acad. Press,2004. <a href="#fnref:3" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Rider, B. A limit theorem atthe edge of a non-Hermitian random matrix ensemble. Random matrixtheory. J. Phys. A 36 (2003), no. 12, 3401–3409.<a href="#fnref:4" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>概率论</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The Circular Law (1)</title>
    <link href="/2023/01/22/The-Circular-Law-1/"/>
    <url>/2023/01/22/The-Circular-Law-1/</url>
    
    <content type="html"><![CDATA[<h1 id="the-circular-law-1">The Circular Law (1)</h1><p>This page gives some basic results and prove the spectral law of anGaussian i.i.d randommatrix.<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Charles Bordenave and Djalil Chafaï, The circular law">[1]</span></a></sup></p><p>All random variables are defined on a unique common probability space<span class="math inline">\((\Omega,\mathcal{A},\mathbb{P})\)</span>. Atypical element of <span class="math inline">\(\Omega\)</span> isdenoted <span class="math inline">\(\omega\)</span>. We write <spanclass="math inline">\(a.s.\)</span>, <spanclass="math inline">\(a.a.\)</span>, and <spanclass="math inline">\(a.e.\)</span> for almost surely, Lebesgue almostall, and Lebesgue alomost everwhere respectively.</p><h2 id="two-kinds-of-spectra">Two kinds of spectra</h2><p>Label the eigenvalues of <span class="math inline">\(A \in\mathcal{M}_{n}(\mathbb{C})\)</span> as <spanclass="math inline">\(\lambda_1(A),\cdots ,\lambda_{n}(A)\)</span> sothat $<em>1(A) </em>{n}(A) $. The singular values of <spanclass="math inline">\(A\)</span> are defined by <spanclass="math display">\[    s_k(A):=\lambda_{k}(\sqrt{AA^{*}}).\]</span></p><p>We have <span class="math display">\[    s_1(A)\geqslant \cdots \geqslant s_{n}(A)\geqslant 0.\]</span></p><p>The matrices <spanclass="math inline">\(A,A^{\mathsf{T}},A^{*}\)</span> have the samesingular values. The Hermitian matrix <span class="math display">\[    H_{A}:=    \begin{pmatrix}        0 &amp; A \\        A^{*} &amp; 0 \\    \end{pmatrix}\]</span></p><p>has eigenvalues <span class="math inline">\(s_1(A),-s_1(A),\cdots,s_n(A),-s_n(A)\)</span>. Notice the mapping <spanclass="math inline">\(A \mapsto H_{A}\)</span> is linear in <spanclass="math inline">\(A\)</span>, in contrast with the mapping <spanclass="math inline">\(A \mapsto \sqrt{AA^{*}}\)</span>.</p><p>Define the operator norm or spectral norm of <spanclass="math inline">\(A\)</span> as <span class="math display">\[    \left\| A \right\|_{2\rightarrow 2}:=\max _{\left\| x\right\|_{2}=1}\left\| Ax \right\|_{2}=s_1(A)\]</span></p><p>Notice that if <span class="math inline">\(A\)</span> is non-singularthen <span class="math inline">\(s_i(A ^{-1})=s_{n-i}(A) ^{-1}\)</span>for <span class="math inline">\(1\leqslant i\leqslant n\)</span> and<span class="math inline">\(s_n(A)=s_1(A ^{-1}) ^{-1}=\left\| A^{-1}\right\|_{2\rightarrow 2} ^{-1}\)</span>.</p><div class="note note-info">            <p><strong>Courant-Fischer variational formulas</strong> Denoting <spanclass="math inline">\(\mathcal{G}_{n,i}\)</span> the Grassmannian of all<span class="math inline">\(i\)</span>-dimensional subspaces, we have<span class="math display">\[      \begin{aligned}        s_i(A)&amp;=\min _{E\in \mathcal{G}_{n,i-1}} \max _{\left\| x\right\|_{2}=1,x\perp E} \left\| Ax \right\|_{2}\\        &amp;=\max_{E \in \mathcal{G}_{n,n-i}} \min _{\left\| x\right\|_{2}=1,x\perp E}\left\| Ax \right\|_{2} \\        &amp;=\min _{E\in \mathcal{G}_{n,n-i+1}} \max_{\left\| x\right\|_{2}=1,x\in E}\left\| Ax \right\|_{2} \\        &amp;=\max _{E\in \mathcal{G}_{n,i}} \min _{\left\| x\right\|_{2}=1,x \in E} \left\| Ax \right\|_{2}    \end{aligned}\]</span></p>          </div><div class="note note-info">            <p><strong>Corollary</strong> Let <span class="math inline">\(A \in\mathcal{m,n}\)</span> be given, and let <spanclass="math inline">\(A_r\)</span> denote a submatrix of <spanclass="math inline">\(A\)</span> obtained by deleting a total of <spanclass="math inline">\(r\)</span> rows and/or columns from <spanclass="math inline">\(A\)</span>. Then <span class="math display">\[    s_k(A)\geqslant s_k(A_r)\geqslant s_{k+r}(A), k=1,\cdots,\min\{m,n\} \tag{1.1}\]</span></p><p>where for <span class="math inline">\(X \in\mathcal{M}_{p,q}\)</span> we set <spanclass="math inline">\(s_j(X)\equiv 0\)</span> if <spanclass="math inline">\(j&gt;\min\{p,q\}\)</span>.</p>          </div><p><strong>proof</strong> It suffices to consider the case <spanclass="math inline">\(r=1\)</span>, i.e., <spanclass="math inline">\(s_k(A)\geqslant s_{k}(A_1)\geqslants_{k+1}(A)\)</span>. If <span class="math inline">\(A_1\)</span> isformed from <span class="math inline">\(A\)</span> by deleting column<span class="math inline">\(s\)</span>, denote by <spanclass="math inline">\(e_s\)</span> the standard unit basis vector with a<span class="math inline">\(1\)</span> in position <spanclass="math inline">\(s\)</span>. If <span class="math inline">\(x\in\mathbb{C}^{n}\)</span>, denote by <span class="math inline">\(\xi \in\mathbb{C}^{n-1}\)</span> the vector obtained by deleting entry <spanclass="math inline">\(s\)</span> from <spanclass="math inline">\(x\)</span>. <span class="math display">\[    \begin{aligned}        s_k(A)&amp;=\min_{E\in \mathcal{G}_{n,k-1}} \max_{x\perpE,\left\| x \right\|_{2}=1}\left\| Ax \right\|_{2} \\        &amp;\geqslant \min_{E\in \mathcal{G}_{n,k-1}} \max_{x\perpE,x\perp e_s,\left\| x \right\|_{2}=1}\left\| Ax \right\|_{2}\\        &amp;= \min_{E\in \mathcal{G}_{n-1,k-1}} \max_{\xi\perpE,\left\| x \right\|_{2}=1}\left\| A_1\xi \right\|_{2}=s_k(A_1)    \end{aligned}\]</span></p><p><span class="math display">\[    \begin{aligned}        s_{k+1}(A)&amp;=\max _{\mathcal{G}_{n,n-k-1}}\min _{\left\| x\right\|_{2}=1,x\perp E} \left\| Ax \right\|_{2} \\        &amp;\leqslant \max _{\mathcal{G}_{n,n-k-1}}\min _{x\perpE,x\perp e_s,\left\| x \right\|_{2}=1} \left\| Ax \right\|_{2} \\        &amp;= \max _{\mathcal{G}_{n-1,n-k-1}}\min _{\xi\perp E,\left\|\xi \right\|_{2}=1} \left\| A_1\xi \right\|_{2}=s_k(A_1)    \end{aligned}\]</span></p><p>If a row of <span class="math inline">\(A\)</span> is deleted, applythe same argument to <span class="math inline">\(A^{*}\)</span>, whichhas the same singular values as <spanclass="math inline">\(A\)</span>.</p><div class="note note-info">            <p><strong>Lemma</strong> Let <span class="math inline">\(C \in\mathcal{M}_{m,n}\)</span>, <span class="math inline">\(V_{k} \in\mathcal{M}_{m,k}\)</span> and <span class="math inline">\(W_{k} \in\mathcal{M}_{n,k}\)</span> be given, where <spanclass="math inline">\(k\leqslant \min\{m,n\}\)</span> and <spanclass="math inline">\(V_k,W_k\)</span> have orthonormal columns. Then -<span class="math inline">\(s_i(V_k^{*}CW_{k})\leqslant s_i(C)\)</span>,<span class="math inline">\(i=1,\cdots,k\)</span> - <spanclass="math inline">\(\lvert \det V_k^{*}CW_{k} \rvert \leqslants_1(C)\cdots s_k(C)\)</span>.</p>          </div><p><strong>proof</strong> There are unitary matrices <spanclass="math inline">\(V\in \mathcal{M}_{m}\)</span> and <spanclass="math inline">\(W \in \mathcal{M}_{n}\)</span> such that <spanclass="math inline">\(V=[V_k \ *]\)</span> and <spanclass="math inline">\(W=[W_k \ *]\)</span>. Since <spanclass="math inline">\(V_k^{*}CW_{k}\)</span> is the upper left <spanclass="math inline">\(k\)</span>-by-<spanclass="math inline">\(k\)</span> submatrix of <spanclass="math inline">\(V^{*}CW\)</span>, (1.1) and unitary invariance ofsingular values ensure that <spanclass="math inline">\(s_i(V_k^{*}CW_{k})\leqslants_i(V^{*}CW)=s_i(C),i=1,\cdots ,k\)</span>, and hence <spanclass="math inline">\(\lvert \det V_k^{*}CW_{k} \rvert=s_1(V_k^{*}CW_{k})\cdots s_k(V_{k}^{*}CW_{k})\leqslant s_1(C)\cdotss_k(C)\)</span>.</p><div class="note note-info">            <p><strong>Weyl inequalities</strong> For every <spanclass="math inline">\(A \in \mathcal{M}_{n}(\mathbb{C})\)</span> and<span class="math inline">\(1\leqslant k\leqslant n\)</span>, <spanclass="math display">\[    \prod_{i=1}^{k} \lvert \lambda_{i}(A) \rvert \leqslant\prod_{i=1}^{k} s_i(A). \tag{1.2}\]</span></p>          </div><p><strong>proof</strong> By the Schur triangularization theorem, thereis a unitary <span class="math inline">\(U \in\mathcal{M}_{n}(\mathbb{C})\)</span> such that <spanclass="math inline">\(U^{*}AU=\Delta\)</span> is upper triangular and<span class="math inline">\(diag \Delta=(\lambda_1,\cdots,\lambda_n)\)</span>. Let <span class="math inline">\(U_k \in\mathcal{M}_{n,k}\)</span> denote the first <spanclass="math inline">\(k\)</span> columns of <spanclass="math inline">\(U\)</span>. It's easy to know that <spanclass="math inline">\(U_k^{*}AU_{k}=\Delta_{k}\)</span> is the upperleft <span class="math inline">\(k\)</span>-by-<spanclass="math inline">\(k\)</span> principal submatrix of <spanclass="math inline">\(\Delta\)</span>, and <spanclass="math inline">\(diag \Delta_k=(\lambda_1,\cdots,\lambda_n)\)</span>. Apply the lemma with <spanclass="math inline">\(C=A\)</span> and <spanclass="math inline">\(V_k=W_k=U_k\)</span> to conclude that <spanclass="math display">\[    \lvert \lambda_1(A)\cdots \lambda_k(A) \rvert =\lvert \det \Delta_k\rvert =\lvert \det U_k^{*}AU_{k} \rvert \leqslant s_1(A)\cdotss_k(A)      \]</span></p><p>It's easy to know that (1.2) holds quality when <spanclass="math inline">\(k=n\)</span>.</p><p>The reversed form $<em>{i=n-k+1}^{n} s_i(A)</em>{i=n-k+1}^{n} _i(A) $for every <span class="math inline">\(1\leqslant k\leqslant n\)</span>can be deduced easily.</p><div class="note note-info">            <p><strong>Theorem</strong> Let <span class="math inline">\(A\in\mathcal{M}_{m,p}\)</span> and <span class="math inline">\(B\in\mathcal{M}_{p,n}\)</span> be given, let <spanclass="math inline">\(q\equiv \min\{n,p,m\}\)</span>. Then <spanclass="math display">\[    \prod_{i=1}^{k} s_i(AB)\leqslant \prod_{i=1}^{k} s_i(A)s_i(B), \k=1,\cdots ,q \tag{1.3}\]</span></p><p>If <span class="math inline">\(n=p=m\)</span>, then equality holds in(1.3) for <span class="math inline">\(k=n\)</span>.</p>          </div><p><strong>proof</strong> Let <span class="math inline">\(AB=V\SigmaW^{*}\)</span>, and let <span class="math inline">\(V_k \in\mathcal{M}_{m,k}\)</span> and <span class="math inline">\(W_k\in\mathcal{M}_{n,k}\)</span> denote the first <spanclass="math inline">\(k\)</span> columns of <spanclass="math inline">\(V\)</span> and <spanclass="math inline">\(W\)</span>, respectively. Then <spanclass="math inline">\(V_k^{*}(AB)W_k=diag(s_1(AB),\cdots,s_k(AB))\)</span>. Since <span class="math inline">\(p\geqslantk\)</span>, use the polar decomposition to write the product <spanclass="math inline">\(BW\in \mathcal{M}_{p,k}\)</span> as <spanclass="math inline">\(BW_{k}=X_{k}Q\)</span>, where <spanclass="math inline">\(X_k\in \mathcal{M}_{p,k}\)</span> has orthonormalcolumns, <span class="math inline">\(Q\in \mathcal{M}_{k}\)</span> ispositive semidefinite, <span class="math inline">\(\det Q^{2}=\detW_k^{*}(B^{*}B)W_k\leqslant s_1(B^{*}B)\cdotss_k(B^{*}B)=s_1(B)^{2}\cdots s_k(B)^{2}\)</span> by lemma. Use lemmaagain to compute <span class="math display">\[    s_1(AB)\cdots s_k(AB)= \lvert \det V_k^{*}(AB)W_k \rvert = \lvert\det V_k^{*}AX_{k} \det Q \rvert \leqslant (s_1(A)\cdotss_k(A))(s_1(B)\cdots s_k(B))\]</span></p><p>If <span class="math inline">\(n=p=m\)</span>, then <spanclass="math inline">\(s_1(AB)\cdots s_n(AB)=\lvert \det AB \rvert=\lvert \det A \rvert \lvert \det B \rvert =s_1(A)\cdotss_k(A)s_1(B)\cdots s_k(B)\)</span>.</p><div class="note note-info">            <p><strong>(Strong) majorization inequalities</strong> If <spanclass="math inline">\(A\)</span> is nonsingular, then</p><p><span class="math display">\[    \sum_{i=1}^{k} \log \lvert \lambda_i(A) \rvert \leqslant\sum_{i=1}^{k} \log s_i(A), k=1,\cdots ,n,\]</span></p><p>with equality for <span class="math inline">\(k=n\)</span>.</p>          </div><p><strong>proof</strong> It's just a corollary of the previoustheorem.</p><p>Using some classic inequality trick, we can introduce Schur-convex orisotone functions and prove weak majorization inequality.</p><div class="note note-warning">            <p><strong>Definition</strong> Let <spanclass="math inline">\(x=[x_i],y=[y_i]\in \mathbb{R}^{n}\)</span> begiven vectors, <span class="math inline">\(x_{[1]}\geqslant \cdots\geqslant x_{[n]}\)</span> and <spanclass="math inline">\(y_{[1]}\geqslant \cdots \geqslanty_{[n]}\)</span>. We say that <span class="math inline">\(y\)</span>weakly majorizes <span class="math inline">\(x\)</span> if <spanclass="math display">\[    \sum_{i=1}^{k} x_{[i]}\leqslant \sum_{i=1}^{k} y_{[i]}, \quadk=1,2,\cdots,n \tag{1.4}\]</span></p><p>If (1.4) holds equality when <spanclass="math inline">\(k=n\)</span>, then we say <spanclass="math inline">\(y\)</span> majorizes <spanclass="math inline">\(x\)</span>, denoted as <spanclass="math inline">\(x \prec y\)</span>.</p>          </div><div class="note note-info">            <p><strong>Schur</strong> Suppose <spanclass="math inline">\(A=(a_{ij})\in \mathbb{C}^{n\times n}\)</span> isHermitian with eigenvalues <spanclass="math inline">\(\{\lambda_i\}_{1\leqslant i\leqslant n}\)</span>,then <span class="math inline">\(\{a_{ii}\}\prec\{\lambda_{i}\}\)</span>.</p>          </div><p><strong>proof</strong> <span class="math inline">\(\sum_{i=1}^{r}\lambda_{[i]}=\sup _{E\in\mathcal{G}_{n,r}}\operatorname{tr}(A|_{E})\geqslant \sum_{i=1}^{r}a_{[ii]}\)</span>.</p><div class="note note-info">            <p><strong>Horn</strong> Suppose <spanclass="math inline">\(\{d_i\}\prec \{\mu_i\}\)</span>, then there existsa symmetric matrix with primary diagonal entries <spanclass="math inline">\(\{d_i\}\)</span>, eigenvalues <spanclass="math inline">\(\{\mu_i\}\)</span>.</p>          </div><p><strong>proof</strong> We construct orthogonal matrix <spanclass="math inline">\(Q\)</span>, such that <spanclass="math inline">\(Q^{\mathsf{T}}diag(\mu_1,\cdots ,\mu_n)Q\)</span>has primary diagonal entries <spanclass="math inline">\(\{d_i\}\)</span>. Use induction to prove it, referto https://zhuanlan.zhihu.com/p/76422480.</p><div class="note note-info">            <p><strong>Hardy-Littlewood-Polya</strong> Suppose <spanclass="math inline">\(x,y\in \mathbb{R}^{n}\)</span>, then <spanclass="math inline">\(x\prec y\)</span> if and only if there is a doublysubstochastic <span class="math inline">\(A\in\mathcal{M}_{n}(\mathbb{R})\)</span> such that <spanclass="math inline">\(x=Ay\)</span>.</p>          </div><p><strong>proof</strong> If <span class="math inline">\(x\precy\)</span>, then the Horn theorem gives an orthogonal matrix <spanclass="math inline">\(Q=(q_{ij})\)</span> such that <spanclass="math inline">\(x_i=\sum_{j=1}^{n} q_{ij}^{2}y_j,\forall1\leqslant i\leqslant n\)</span>. <spanclass="math inline">\(A=(q_{ij}^{2})\)</span> satisfies the condition.For the rest, refer to https://zhuanlan.zhihu.com/p/76422480 aswell.</p><div class="note note-info">            <p><strong>Theorem</strong> <span class="math inline">\(x\)</span> and<span class="math inline">\(y\)</span> are two given vectors withnonnegative entries. Then <span class="math inline">\(y\)</span> weaklymajorizes <span class="math inline">\(x\)</span> if and only if there isa doubly substochastic <span class="math inline">\(Q\in\mathcal{M}_{n}(\mathbb{R})\)</span> such that <spanclass="math inline">\(x=Qy\)</span>.</p>          </div><p><strong>proof</strong> If <span class="math inline">\(Q\in\mathcal{M}_{n}(\mathbb{R})\)</span> is doubly substochastic and <spanclass="math inline">\(Qy=x\)</span> with <spanclass="math inline">\(x,y\geqslant 0\)</span>, let <spanclass="math inline">\(S\in \mathcal{M}_{n}(\mathbb{R})\)</span> bedoubly stochastic and such that <span class="math inline">\(0\leqslantQ\leqslant S\)</span>. If <spanclass="math inline">\((Qy)_{i_1}=(Qy)_{[1]},\cdots,(Qy)_{i_k}=(Qy)_{[k]}\)</span>, then <span class="math display">\[    \sum_{i=1}^{k} (Qy)_{[i]}=\sum_{j=1}^{k} (Qy)_{i_j}\leqslant\sum_{j=1}^{k} (Sy)_{i_j}\leqslant \sum_{i=1}^{k} (Sy)_{[i]}\leqslant\sum_{i=1}^{k} y_{[i]}\]</span></p><p>for <span class="math inline">\(k=1,2,\cdots,n\)</span>. Thus, <spanclass="math inline">\(y\)</span> weakly majorizes <spanclass="math inline">\(Qy=x\)</span>.</p><p>Conversely, suppose <span class="math inline">\(y\)</span> weaklymajorizes <span class="math inline">\(x\)</span> and <spanclass="math inline">\(x,y\geqslant 0\)</span>. If <spanclass="math inline">\(x=0\)</span>, let <spanclass="math inline">\(Q\equiv 0\)</span>. If <spanclass="math inline">\(x\neq 0\)</span>, let <spanclass="math inline">\(\varepsilon\)</span> denote the smallest positiveentry of <span class="math inline">\(x\)</span>, set <spanclass="math inline">\(\delta\equiv (y_1-x_1)+\cdots +(y_n-x_n)\geqslant0\)</span>, and let <span class="math inline">\(m\)</span> be anypositive integer such that <spanclass="math inline">\(\varepsilon\geqslant \delta/m\)</span>. Let <spanclass="math display">\[    \xi\equiv [x_1,\cdots ,x_n,\delta/m,\cdots,\delta/m]^{\mathsf{T}}\in \mathbb{R}^{n+m}\]</span></p><p>and let <span class="math display">\[    \eta=[y_1,\cdots ,y_n,0,\cdots ,0]^{\mathsf{T}}\in \mathbb{R}^{n+m}\]</span></p><p>Then <span class="math display">\[    \sum_{i=1}^{k} \xi_{[i]}\leqslant \sum_{i=1}^{k} \eta_{[i]}, \quadk=1,2,\cdots,m+n\]</span></p><p>with equality for <span class="math inline">\(k=m+n\)</span>. Thus,there is strong majorization relationship between <spanclass="math inline">\(\xi\)</span> and <spanclass="math inline">\(\eta\)</span>. By Hardy-Littlewood-Polya theorem,there is a doubly stochastic <span class="math inline">\(S\in\mathcal{M}_{m+n}(\mathbb{R})\)</span> such that <spanclass="math inline">\(\xi=S\eta\)</span>. If we let <spanclass="math inline">\(Q\)</span> denote the upper left <spanclass="math inline">\(n\)</span>-by-<spanclass="math inline">\(n\)</span> principal submatrix of <spanclass="math inline">\(S\)</span>, <span class="math inline">\(Q\)</span>is doubly substochastic and <spanclass="math inline">\(x=Qy\)</span>.</p><div class="note note-info">            <p><strong>Lemma</strong> Let <span class="math inline">\(x_1,\cdots,x_n\)</span>, <span class="math inline">\(y_1,\cdots ,y_n\)</span> be<span class="math inline">\(2n\)</span> given real numbers such that<span class="math inline">\(x_1\geqslant x_2\geqslant \cdots \geqslantx_n\)</span>,<span class="math inline">\(y_1\geqslant y_2\geqslant\cdots \geqslant y_n\)</span>, and <span class="math display">\[    \sum_{i=1}^{k} x_i\leqslant \sum_{i=1}^{k} y_i, \quad k=1,2,\cdots,n\tag{1.5}\]</span></p><p>If <span class="math inline">\(f(t)\)</span> is a given real-valuedincreasing convex function on the interval <spanclass="math inline">\([\min\{x_n,y_n\},y_1]\)</span>, then <spanclass="math inline">\(f(x_1)\geqslant \cdots \geqslant f(x_n)\)</span>,<span class="math inline">\(f(y_1)\geqslant \cdots f(y_n)\)</span>, and<span class="math display">\[    \sum_{i=1}^{k} f(x_i)\leqslant \sum_{i=1}^{k} f(y_i), \quadk=1,2,\cdots,n\]</span></p><p>If equality holds for <span class="math inline">\(k=n\)</span> in(1.4) and if <span class="math inline">\(f(\cdot)\)</span> is convex(but not necessarily increasing) on the interval <spanclass="math inline">\([y_n,y_1]\)</span>, then <spanclass="math display">\[    \sum_{i=1}^{n} f(x_i)\leqslant \sum_{i=1}^{n} f(y_i) \tag{1.6}\]</span></p>          </div><p><strong>proof</strong> The asserted inequality is trivial for <spanclass="math inline">\(k=1\)</span>. Let <spanclass="math inline">\(k\)</span> be a given integer with <spanclass="math inline">\(2\leqslant k\leqslant n\)</span>. Since <spanclass="math inline">\(x\)</span> is weakly majorized by <spanclass="math inline">\(y\)</span>, there is a doubly stochastic <spanclass="math inline">\(S\in \mathcal{M}_{k}(\mathbb{R})\)</span> suchthat <span class="math inline">\(x\leqslant Sy\)</span>.</p><p>Notice that the entries of <span class="math inline">\(Sy\)</span>lie in the interval <span class="math inline">\([y_n,y_1]\)</span> andhence are in the domain of <spanclass="math inline">\(f(\cdot)\)</span>. Using the monotonicity andconvexity of <span class="math inline">\(f(\cdot)\)</span>, we have</p><p><span class="math display">\[    \sum_{i=1}^{k} f(x_i)\leqslant \sum_{i=1}^{k} f\left( \sum_{j=1}^{k}s_{ij}y_j \right) \leqslant \sum_{i=1}^{k} \sum_{j=1}^{k}s_{ij}f(y_j)=\sum_{j=1}^{k} \left( \sum_{i=1}^{k} s_{ij} \right)f(y_j)=\sum_{j=1}^{k} f(y_j)\]</span></p><p>If equality holds for <span class="math inline">\(k=n\)</span>, thereis a doubly stochastic <span class="math inline">\(S\in\mathcal{M}_{n}(\mathbb{R})\)</span> such that <spanclass="math inline">\(x=Sy\)</span> and hence convexity of <spanclass="math inline">\(f(\cdot)\)</span> gives <spanclass="math display">\[    \sum_{i=1}^{n} f(x_i)=\sum_{i=1}^{n} f\left( \sum_{j=1}^{n}s_{ij}y_j \right) \leqslant \sum_{i=1}^{n} \sum_{j=1}^{n}s_{ij}f(y_j)=\sum_{j=1}^{n} f(y_j)\]</span></p><div class="note note-info">            <p><strong>Theorem</strong> Let <span class="math inline">\(A\in\mathcal{M}_{n}(\mathbb{C})\)</span>, then <span class="math display">\[    \sum_{i=1}^{k} \lvert \lambda_{i}(A) \rvert^{p}\leqslant\sum_{i=1}^{k} s_i(A)^{p} \ \text{for} \ k=1,\cdots ,n, \forall p&gt;0\]</span></p><p><span class="math display">\[    \lvert \operatorname{tr} \ A \rvert \leqslant \sum_{i=1}^{n} s_i(A)\]</span></p><p>We have a more general version of this theorem, please refer to [HJ,Theorem3.3.13].<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="R. A. Horn and Ch. R. Johnson, Topics in matrix analysis, Cambridge University Press, Cambridge, 1994, Corrected reprint of the 1991 original.">[2]</span></a></sup></p>          </div><p>We can deduce from Weyl's inequalities that <spanclass="math display">\[    \sum_{i=1}^{n} \lvert \lambda_i(A) \rvert ^{2}\leqslant\sum_{i=1}^{n} s_i(A)^{2}=\operatorname{tr}(AA^{*})=\sum_{i,j=1}^{n}\lvert A_{i,j} \rvert ^{2}. \tag{1.7}\]</span></p><p>Since <span class="math inline">\(s_1(\cdot)=\left\| \cdot\right\|_{2\rightarrow 2}\)</span> we have for any <spanclass="math inline">\(A,B\in \mathcal{M}_{n}(\mathbb{C})\)</span> that<span class="math display">\[    s_1(AB)\leqslant s_1(A)s_1(B), \quad s_1(A+B)\leqslants_1(A)+s_1(B). \tag{1.8}\]</span></p><p>We define tha empirical eigenvalues and singular values measures by<span class="math display">\[    \mu_{A}:=\frac{1}{n}\sum_{k=1}^{n} \delta_{\lambda_k(A)}, \quad\nu_{A}:=\frac{1}{n}\sum_{k=1}^{n} \delta_{s_k(A)}.\]</span></p><p><span class="math inline">\(\mu_{A}\)</span> ad <spanclass="math inline">\(\nu_{A}\)</span> are supported respectively in<span class="math inline">\(\mathbb{C}\)</span> and <spanclass="math inline">\(\mathbb{R}_{+}\)</span>. From (1.2) we get <spanclass="math display">\[    \begin{aligned}        \int_{}^{} \log \lvert \lambda\rvert  \mathrm{d}\mu_{A}(\lambda)&amp;= \frac{1}{n}\sum_{i=1}^{n} \log\lvert \lambda_i(A) \rvert \\        &amp;=\frac{1}{n}\sum_{i=1}^{n} \log (s_i(A)) \\        &amp;=\int_{}^{} \log (s) \mathrm{d}\nu_{A}(s).    \end{aligned}\]</span></p><p>The map <span class="math inline">\(A\mapsto (s_1(A),\cdotss_n(A))\)</span> is 1-Lipschitz: for any <spanclass="math inline">\(A,B\in \mathcal{M}_{n}(\mathbb{C})\)</span>,</p><p><span class="math display">\[    \max_{1\leqslant i\leqslant n}\lvert s_i(A)-s_i(B) \rvert \leqslants_1(A-B).\]</span></p><p>The Hilbert-Schimidt norm/ Schur norm/ Frobenius norm, is</p><p><span class="math display">\[    \left\| A \right\|_{2}^{2}=\operatorname{tr}(AA^{*})=\sum_{i=1}^{n}s_i(A)^{2}=n \int_{}^{} s^{2} \mathrm{d}\nu_{A}(s).\]</span></p><p>In the sequel, we say that a sequence of (possibly signed) measures<span class="math inline">\((\eta_{n})_{n\geqslant 1}\)</span> on <spanclass="math inline">\(\mathbb{C}\)</span> (respectively on <spanclass="math inline">\(\mathbb{R}\)</span>) tends weakly to a (possiblysigned) measure <span class="math inline">\(\eta\)</span>, and we denote<span class="math display">\[    \eta_{n} \rightsquigarrow \eta,\]</span></p><p>when for all continuous and bounded function <spanclass="math inline">\(f\colon \mathbb{C}\rightarrow \mathbb{R}\)</span>(respectively <span class="math inline">\(f\colon \mathbb{R}\rightarrow\mathbb{R}\)</span>), <span class="math display">\[    \lim_{n \to \infty}\int_{}^{} f \mathrm{d}\eta_n=\int_{}^{} f\mathrm{d}\eta.\]</span></p><h2 id="spectral-law">Spectral Law</h2><p>Since <span class="math inline">\(\mathbb{C}\equiv\mathbb{R}^{2}\)</span>, we now consider the case where <spanclass="math inline">\(X_{11}\sim \mathcal{N}(0,\frac{1}{2}I_2)\)</span>.We denote <span class="math inline">\(G\)</span> instead of <spanclass="math inline">\(X\)</span> in order to distinguish the Gaussiancase from the general case. We say <spanclass="math inline">\(G\)</span> belongs to the <strong>Complex GinibreEnsemble</strong>.</p><p>The Lebesgue density of the <span class="math inline">\(n\timesn\)</span> random matrix <span class="math inline">\(G=(G_{ij})\)</span>in <span class="math inline">\(\mathcal{M}_{n}(\mathbb{C})\equiv\mathbb{C}^{n\times n}\)</span> is <span class="math display">\[    A\in \mathcal{M}_{n}(\mathbb{C})\mapsto\pi^{-n^{2}}\mathrm{e}^{-\sum_{i,j=1}^{n} \lvert A_{ij} \rvert ^{2}} .\tag{2.1}\]</span></p><p>Notice that it's a Boltzmann distribution with energy <spanclass="math display">\[    A\mapsto \sum_{i,j=1}^{n} \lvert A_{ij} \rvert^{2}=\operatorname{tr}(AA^{*})=\left\| A \right\|_{2}^{2}=\sum_{i=1}^{n}s_i^{2}(A).\]</span></p><p>So this law is unitary invariant, in the sense that if <spanclass="math inline">\(U\)</span> and <spanclass="math inline">\(V\)</span> are <span class="math inline">\(n\timesn\)</span> unitary matrices then <spanclass="math inline">\(UGV\)</span> and <spanclass="math inline">\(G\)</span> are equally distributed.</p><p>We set <span class="math display">\[    \Delta_n:=\{(z_1,\cdots ,z_n)\in \mathbb{C}^{n}\colon \lvert z_1\rvert \geqslant \cdots \geqslant \lvert z_n \rvert \}.\]</span></p><p>Recall the Lebesgue density of the <spanclass="math inline">\(n\times n\)</span> random matrix <spanclass="math inline">\(G=(G_{ij})\)</span> in <spanclass="math inline">\(\mathcal{M}_{n}(\mathbb{C})\equiv\mathbb{C}^{n\times n}\)</span> is <span class="math display">\[    G\in \mathcal{M}_{n}(\mathbb{C})\mapsto\pi^{-n^{2}}\mathrm{e}^{-\sum_{i,j=1}^{n} \lvert G_{ij} \rvert ^{2}} .\]</span></p><div class="note note-info">            <p><strong>Theorem</strong> <spanclass="math inline">\((\lambda_1(G),\cdots ,\lambda_n(G))\)</span> hasdensity <span class="math inline">\(n!\varphi_{n}\mathbf{1}_{\Delta_n}\)</span> where <span class="math display">\[    \varphi_{n}(z_1,\cdots ,z_n)=\frac{\pi^{-n}}{1!2!\cdots n!}\exp\left( -\sum_{k=1}^{n} \lvert z_k \rvert ^{2} \right) \prod_{1\leqslanti&lt;j\leqslant n}^{} \lvert z_i-z_j \rvert ^{2}.\]</span></p><p>In particular, for every symmetric Borel function <spanclass="math inline">\(F\colon \mathbb{C}^{n}\rightarrow\mathbb{R}\)</span>, <span class="math display">\[    \mathbb{E}[F(\lambda_1(G),\cdots,\lambda_n(G))]=\int_{\mathbb{C}^{n}}^{} F(z_1,\cdots,z_n)\varphi_{n}(z_1,\cdots ,z_n) \mathrm{d}z_1\cdots \mathrm{d}z_n\]</span></p>          </div><div class="note note-danger">            <p>There are two ways to get <spanclass="math inline">\(\varphi_n\)</span>, using diagonalization or Schurdecompositionrespectively.<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Madan Lal Mehta, Random matrices, third ed. Pure and Applied Mathematics, vol.142, Acad. Press, 2004.">[3]</span></a></sup>We apply the first one since it is more understandable.</p>          </div><p>We may suppose that <span class="math inline">\(G\)</span> hasdistinct eigenvalues since matrices with multiple eigenvalues havemeasure <span class="math inline">\(0\)</span>. Let <spanclass="math inline">\(X\)</span> be the <spanclass="math inline">\(N\times N\)</span> matrix whose columns are theeigenvectors of <span class="math inline">\(G\)</span> so that <spanclass="math inline">\(G=XEX^{-1}\)</span>. By differentiation, <spanclass="math display">\[    X^{-1}\mathrm{d}GX=\mathrm{d}E+\mathrm{d}BE-E\mathrm{d}B \tag{2.2}\]</span></p><p>with <span class="math display">\[    \mathrm{d}B=X^{-1}\mathrm{d}X.\]</span></p><p><span class="math display">\[    (X^{-1}\mathrm{d}GX)_{jj}^{r}=\mathrm{d}z_j^{r}=\mathrm{d}x_j, \quad(X^{-1}\mathrm{d}GX)_{jj}^{i}=\mathrm{d}z_j^{i}=\mathrm{d}y_j,\]</span></p><p><span class="math display">\[    (X^{-1}\mathrm{d}GX)_{jk}^{r}=(x_k-x_j)\mathrm{d}B_{jk}^{r}-(y_k-y_j)\mathrm{d}B_{jk}^{i},\quad j\neq k\]</span></p><p><span class="math display">\[    (X^{-1}\mathrm{d}GX)_{jk}^{i}=(y_k-y_j)\mathrm{d}B_{jk}^{r}+(x_k-x_j)\mathrm{d}B_{jk}^{i},\quadj\neq k\]</span></p><p>where <span class="math inline">\(x_j,y_j\)</span> are the real andimaginary parts of <span class="math inline">\(z_j\)</span>, thediagonal elements of <span class="math inline">\(E\)</span>. Denote<spanclass="math inline">\(\mathrm{d}\hat{G}=X^{-1}\mathrm{d}GX\)</span>.</p><p>The Jacobian <span class="math display">\[    \frac{\partial(\mathrm{d}\hat{G}_{jj}^{r},\mathrm{d}\hat{G}_{jj}^{i})}{\partial(x_j,y_j)}=1\]</span></p><p><span class="math display">\[    \frac{\partial (\mathrm{d}G_{jk}^{r},\mathrm{d}G_{jk}^{i})}{\partial(\mathrm{d}B_{jk}^{r},\mathrm{d}B_{jk}^{i})}=(x_k-x_j)^{2}+(y_k-y_j)^{2}=\lvertz_k-z_j \rvert ^{2}\]</span></p><p>The volume element is therefore given by <spanclass="math display">\[    \mu(\mathrm{d}G)=\mu(\mathrm{d}\hat{G})=\prod_{j\neq k}^{} \lvertz_k-z_j \rvert ^{2}\mathrm{d}B_{jk}^{r}\mathrm{d}B_{jk}^{i}\prod_{i}^{}\mathrm{d}x_i\mathrm{d}y_i. \tag{2.3}\]</span></p><p>We now evaluate the integral <span class="math display">\[    J=\int_{}^{} \exp [-\operatorname{tr}(G^{*}G)]\prod_{j\neqk}^{}  \mathrm{d}B_{jk}^{r}\mathrm{d}B_{jk}^{i}. \tag{2.4}\]</span></p><h3 id="evaluation-of-2.4">Evaluation of (2.4)</h3><p>Similar to QR decomposition, any complex nonsingular <spanclass="math inline">\(N\times N\)</span> matrix <spanclass="math inline">\(X\)</span> can be expressed in one and only oneway as <span class="math display">\[    X=UYV \tag{2.5}\]</span></p><p>where <span class="math inline">\(U\)</span> is a unitary matrix,<span class="math inline">\(Y\)</span> is a triangular matrix with alldiagonal elements equal to unity, <spanclass="math inline">\(y_{ij}=0,i&gt;j\)</span>, <spanclass="math inline">\(y_{ii}=1\)</span>, and <spanclass="math inline">\(V\)</span> is a diagonal matrix with real positivediagonal elements.</p><p>Using the fact that <span class="math inline">\(G=XEX^{-1}\)</span>,<span class="math inline">\(U^{*}U=I\)</span>, and <spanclass="math inline">\(EV=VE\)</span>, we can write <spanclass="math display">\[    \operatorname{tr}(G^{*}G)=\operatorname{tr}[E^{*}Y^{*}YE(Y^{*}Y)^{-1}],\tag{2.6}\]</span></p><p><span class="math display">\[    \mathrm{d}B=X^{-1}\mathrm{d}X=V^{-1}Y^{-1}(U^{-1}\mathrm{d}U)YV+V^{-1}Y^{-1}\mathrm{d}YV+V^{-1}\mathrm{d}V.\tag{2.7}\]</span></p><p>From (3.1.6) and the structure of <spanclass="math inline">\(Y\)</span> and <spanclass="math inline">\(V\)</span> we see that <spanclass="math display">\[    \prod_{j\neq k}^{}\mathrm{d}B_{jk}^{r}\mathrm{d}B_{jk}^{i}=\prod_{j&lt;k}^{}(Y^{-1}\mathrm{d}Y)_{jk}^{r}(Y^{-1}\mathrm{d}Y)_{jk}^{i}a, \tag{2.8}\]</span></p><p>where <span class="math inline">\(a\)</span> depends only on <spanclass="math inline">\(U\)</span> and <spanclass="math inline">\(V\)</span>.</p><p><span class="math display">\[    \int_{}^{} \exp [-\operatorname{tr}(E^{*}HEH^{-1})]\prod_{j&lt;k}^{}(Y^{-1}\mathrm{d}Y)_{jk}^{r}(Y^{-1}\mathrm{d}Y)_{jk}^{i}, \tag{2.9}\]</span></p><p>where <span class="math display">\[    H=Y^{*}Y. \tag{2.10}\]</span></p><p>The matrix <span class="math inline">\(H\)</span> is Hermitian. Anyof its upper left diagonal block of size <spanclass="math inline">\(m\)</span> is obtained from the upper leftdiagonal block of <span class="math inline">\(Y\)</span> of the samesize: <span class="math inline">\(H_n=Y_n^{*}Y_n\)</span>. Therefore,for every <span class="math inline">\(n\)</span>, <spanclass="math inline">\(\det H_n=1\)</span>.</p><p>We make a change of variables. First, because <spanclass="math inline">\(\det Y=1\)</span>, <span class="math display">\[    \prod_{j&lt;k}^{}(Y^{-1}\mathrm{d}Y)_{jk}^{r}(Y^{-1}\mathrm{d}Y)_{jk}^{i}=\prod_{j&lt;k}^{}\mathrm{d}Y_{jk}^{r}\mathrm{d}Y_{jk}^{i}. \tag{2.11}\]</span></p><p>Next, we take <span class="math inline">\(H_{jk}^{r}\)</span>, <spanclass="math inline">\(H_{jk}^{i}\)</span> for <spanclass="math inline">\(j&lt;k\)</span> as independent variables from<span class="math display">\[    H_{jk}=Y_{jk}+\sum_{l&gt;j}^{} Y_{jl}^{*}Y_{lk}, \quad j&lt;k,  \]</span></p><p>The Jacobian of the transformation from <spanclass="math inline">\(Y\)</span> to <spanclass="math inline">\(H\)</span> is <spanclass="math inline">\(1\)</span>. So <span class="math display">\[    J=C \int_{}^{} \exp[-\operatorname{tr}(E^{*}HEH^{-1})]\prod_{j&lt;k}^{}\mathrm{d}H_{jk}^{r} \mathrm{d}H_{jk}^{i}, \tag{2.12}\]</span></p><p>where <span class="math inline">\(C\)</span> is a constant.</p><p>The integration over <span class="math inline">\(H\)</span> is donein <span class="math inline">\(N\)</span> steps. At every step weintegrate over the variables of the last column and thus decrease by onethe size of the matrix, whose structure remains the same.</p><p>Let <span class="math inline">\(H&#39;=Y_n^{*}Y_n\)</span>, <spanclass="math inline">\(E&#39;=[z_i\delta_{jk}]_{j,k=1,2,\cdots,n}\)</span>, be the relevant matrices of order <spanclass="math inline">\(m\)</span> and <spanclass="math inline">\(H\)</span>, <span class="math inline">\(E\)</span>be those obtained from <span class="math inline">\(H&#39;\)</span>,<span class="math inline">\(E&#39;\)</span> by removing the last row andlast column. Let the Greek indices run from <spanclass="math inline">\(1\)</span> to <spanclass="math inline">\(n\)</span>, and the Latin indices from <spanclass="math inline">\(1\)</span> to <spanclass="math inline">\(n-1\)</span>. Let <spanclass="math inline">\(\Delta_{\alpha\beta}&#39;\)</span> be the cofactorof <span class="math inline">\(H_{\alpha\beta}&#39;\)</span> in <spanclass="math inline">\(H&#39;\)</span> and <spanclass="math inline">\(\Delta_{jk}\)</span>, the cofactor of <spanclass="math inline">\(H_{jk}\)</span> in <spanclass="math inline">\(H\)</span>. Let <spanclass="math inline">\(g_i=H_{in}&#39;\)</span>. Because <spanclass="math inline">\(\det H&#39;=\det H=1\)</span>, we have <spanclass="math display">\[    \Delta_{\alpha\beta}&#39;=(H&#39;^{-1})_{\beta\alpha}, \quad\Delta_{jk}=(H^{-1})_{kj}. \tag{2.13}\]</span></p><p>Expanding <span class="math inline">\(\det H&#39;\)</span>, <spanclass="math inline">\(\Delta_{in}&#39;\)</span>,<spanclass="math inline">\(\Delta_{jk}&#39;\)</span> by the last row and lastcolumn, we have <span class="math display">\[    1=H_{nn}&#39;-\sum_{j,k}^{} g_{j}^{*}g_{k}\Delta_{kj}, \tag{2.14}\]</span></p><p><span class="math display">\[    \Delta_{in}&#39;=-\sum_{l}^{} \Delta_{il}g_{l}^{*}, \tag{2.15}\]</span></p><p><span class="math display">\[    \Delta_{ij}&#39;=H_{nn}&#39;\Delta_{ij}-\sum_{l,k}^{}g_k^{*}g_l\Delta_{ij}^{lk}, \tag{2.16}\]</span></p><p>where <span class="math inline">\(\Delta_{ij}^{lk}\)</span> is thecofactor obtained from <span class="math inline">\(H\)</span> byremoving the <span class="math inline">\(i\)</span>th and <spanclass="math inline">\(l\)</span>th rows and the <spanclass="math inline">\(j\)</span>th and <spanclass="math inline">\(k\)</span>th columns. Sylvester's theorem (alsoknown as Desnanot–Jacobi identity) expresses <spanclass="math inline">\(\Delta_{ij}^{lk}\)</span> in terms of <spanclass="math inline">\(\Delta_{rs}\)</span> <span class="math display">\[    \Delta_{ij}^{lk}=\Delta_{ij}\Delta_{lk}-\Delta_{ik}\Delta_{lj}.\tag{2.17}\]</span></p><p>In writing (2.17), we have replaced <span class="math inline">\(\detH\)</span> by unity on the LHS. Let <span class="math display">\[    \phi_{n}=\operatorname{tr}(E&#39;^{*}H&#39;E&#39;H&#39;^{-1})=\sum_{\alpha,\beta}^{}z_{\alpha}^{*}z_{\beta}H_{\alpha\beta}&#39;\Delta_{\alpha\beta}&#39;.\tag{2.18}\]</span></p><p>Separating the last row and last column and making use of (2.13) to(2.17), we get <span class="math display">\[    \phi_{n}=\lvert z_n \rvert ^{2}+\phi_{n-1}+\langle g^{*} |H^{-1}(E^{*}-z_n^{*}I)H(E-z_nI)H^{-1}|g \rangle , \tag{2.19}\]</span></p><p>where <span class="math display">\[    \langle g^{*}|B|g \rangle =\sum_{i,j}^{} g_{i}^{*}B_{ij}g_j.\tag{2.20}\]</span></p><p>Substituting (2.19) for <span class="math inline">\(n=N\)</span> in(2.12), we get <span class="math display">\[    \begin{aligned}        J&amp;=C\mathrm{e}^{-\lvert z_{N} \rvert ^{2}} \int_{}^{}\mathrm{e}^{-\phi_{N-1}} \prod_{1\leqslant i&lt;j\leqslant N-1}^{}\mathrm{d}H_{ij}^{r}\mathrm{d}H_{ij}^{i} \\        &amp;\times \int_{}^{} \exp [-\langle g^{*}|H^{-1}(E^{*}-z_{N}^{*}I)H(E-z_{N}I)H^{-1}|g \rangle ]\prod_{1\leqslanti\leqslant N-1}^{}  \mathrm{d}g_i^{r}\mathrm{d}g_i^{i}.    \end{aligned}\]</span></p><div class="note note-info">            <p><span class="math inline">\(B\in\mathcal{M}_{N}(\mathbb{C})\)</span>, <spanclass="math inline">\(B\)</span> is Hermitian. Then <spanclass="math display">\[    \int_{}^{} \exp [-\langle g^{*}| B |g \rangle ] \prod_{j=1}^{N}\mathrm{d}g_{j}^{r}\mathrm{d}g_{j}^{i}=\pi^{N}(\det B)^{-1}\]</span></p>          </div><p>The last integral is immediate and gives <spanclass="math display">\[    \pi^{N-1}\{\det[H^{-1}(E^{*}-z_{N}^{*}I)H(E-z_{N}I)H^{-1}]\}^{-1}=\pi^{N-1}\prod_{i=1}^{N-1}\lvert z_i-z_{N} \rvert ^{-2}. \tag{2.21}\]</span></p><p>The process can be repeated <span class="math inline">\(N\)</span>times and we finally get <span class="math display">\[    J=C\exp \left( -\sum_{i=1}^{N} \lvert z_i \rvert ^{2} \right)\prod_{1\leqslant i&lt;j\leqslant N}\lvert z_i-z_j \rvert ^{-2},\tag{2.22}\]</span></p><p>where <span class="math inline">\(C\)</span> is a new constant.</p><h3 id="the-joint-probability-density-for-g">The joint probabilitydensity for <span class="math inline">\(G\)</span></h3><p>We have proved the joint probability density for the eigenvalues of<span class="math inline">\(G\)</span> belonging to the Complex GinibreEnsemble is given by <span class="math display">\[    P_c(z_1,\cdots ,z_{N})=\mu(\mathrm{d}G)=C\exp \left( -\sum_{i=1}^{N}\lvert z_i \rvert ^{2} \right) \prod_{1\leqslant i&lt;j\leqslantN}\lvert z_i-z_j \rvert ^{2}, \tag{2.23}\]</span></p><p>where <span class="math inline">\(C\)</span> is the normalizationconstant. We now compute <span class="math inline">\(C\)</span>.</p><p>The probability that all the eigenvalues <spanclass="math inline">\(z_i\)</span> will lie outside a circle of radius<span class="math inline">\(\alpha\)</span> centered at <spanclass="math inline">\(z=0\)</span> is <span class="math display">\[    E_{N_c}(\alpha)=\int_{\lvert z_i \rvert \geqslant \alpha}^{}P_c(z_1,\cdots ,z_{N})\prod_{i}^{} \mathrm{d}x_i\mathrm{d}y_i.\]</span></p><p>By writing (Here we don't require $z_1 z_2 z_{N} $) <spanclass="math display">\[    \begin{aligned}        \prod_{i&lt;j}^{} \lvert z_i-z_j \rvert^{2}&amp;=\prod_{i&lt;j}^{} (z_i-z_j)(z_i^{*}-z_j^{*}) \\        &amp;=\det        \begin{bmatrix}        1 &amp; \cdots &amp; 1 \\        z_1 &amp; \cdots &amp; z_{N} \\        \vdots &amp; &amp; \vdots \\        z_1^{N-1} &amp; \cdots &amp; z_{N}^{N-1} \\        \end{bmatrix}        \det        \begin{bmatrix}        1 &amp; \cdots &amp; 1 \\        z_1^{*} &amp; \cdots &amp; z_{N}^{*} \\        \vdots &amp; &amp; \vdots \\        z_1^{*N-1} &amp; \cdots &amp; z_{N}^{*N-1} \\            \end{bmatrix} \\        &amp;=\det        \begin{bmatrix}        N &amp; \sum_{i}^{} z_i &amp; \cdots &amp; \sum_{i}^{} z_i^{N-1}\\        \sum_{i}^{} z_i^{*} &amp; \sum_{i}^{} z_i^{*}z_i &amp; \cdots&amp; \sum_{i}^{} z_i^{*}z_i^{N-1} \\        \vdots &amp; &amp; &amp; \vdots \\        \sum_{i}^{} z_i^{*N-1} &amp; \sum_{i}^{} z_i^{*N-1}z_i &amp;\cdots &amp; \sum_{i}^{} z_i^{*N-1}z_i^{N-1}        \end{bmatrix}    \end{aligned}\]</span></p><p>The integrand is symmetric in all the <spanclass="math inline">\(z_i\)</span>, we can replace the first row with<span class="math inline">\(1,z_1,z_1^{2},\cdots ,z_1^{N-1}\)</span> andmultiply the result by <span class="math inline">\(N\)</span>; <spanclass="math inline">\(z_1\)</span> can be eliminated from the other rowsby subtracting a suitable multiple of the first row. The resultingdeterminant is symmetric in the <span class="math inline">\(N-1\)</span>variables <span class="math inline">\(z_2,z_3,\cdots ,z_{N}\)</span>;therefore we replace the second row with <spanclass="math inline">\(z_2^{*},z_2^{*}z_2,\cdots,z_2^{*}z_2^{N-1}\)</span> and multiply the result by <spanclass="math inline">\(N-1\)</span>. The process can be repeated and weget <span class="math display">\[    \begin{aligned}        E_{N_c}(\alpha)&amp;=CN!\int_{\lvert z_i \rvert \geqslant\alpha}^{} \left\{ \prod_{i}^{} \mathrm{d}x_i\mathrm{d}y_i\right\}\exp\left( -\sum_{i}^{N} \lvert z_i \rvert ^{2} \right) \\        &amp;\times\det        \begin{bmatrix}        1 &amp; z_1 &amp; \cdots &amp; z_1^{N-1} \\        z_2^{*} &amp; z_2^{*}z_2 &amp; \cdots &amp; z_2^{*}z_2^{N-1} \\        \vdots &amp; &amp; &amp;\vdots \\        z_{N}^{*N-1} &amp; z_{N}^{*N-1}z_{N} &amp; \cdots &amp;z_{N}^{*N-1}z_{N}^{N-1} \\        \end{bmatrix}    \end{aligned}\]</span></p><p>By changing to polar coordinates and performing the angularintegrations first we see that <span class="math display">\[    \int_{\lvert z \rvert \geqslant \alpha}^{} \mathrm{e}^{-\lvert z\rvert ^{2}} z^{*j}z^{k} \mathrm{d}x\mathrm{d}y=\pi \delta_{jk}\Gamma(j+1,\alpha^{2}), \tag{2.24}\]</span></p><p>so that <span class="math display">\[    E_{N_c}(\alpha)=CN!\pi^{N}\prod_{j=1}^{N} \Gamma(j,\alpha^{2}),\tag{2.25}\]</span></p><p>where <span class="math inline">\(\Gamma(j,\alpha^{2})\)</span> isthe incomplete gamma function <span class="math display">\[    \Gamma(j,\alpha^{2})=\int_{\alpha^{2}}^{\infty} \mathrm{e}^{-x}x^{j-1} \mathrm{d}x=\Gamma(j)\mathrm{e}^{-\alpha^{2}} \sum_{l=0}^{j-1}\frac{\alpha^{2l}}{l!}. \tag{2.26}\]</span></p><p>Since <span class="math inline">\(E_{N_c}(0)=1\)</span>, the constant<span class="math inline">\(C\)</span> can be determined from (3.1.23)as <span class="math display">\[    C^{-1}=\pi^{N}\prod_{j=1}^{N} j!, \tag{2.27}\]</span></p><p>and therefore <span class="math display">\[    E_{N_c}(\alpha)=\prod_{j=1}^{N} \left( \mathrm{e}^{-\alpha^{2}}\sum_{l=0}^{j-1} \frac{\alpha^{2l}}{l!} \right). \tag{2.28}   \]</span></p><p>We will use the spectral law with symmetric functions of the form<span class="math display">\[    F(z_1,\cdots ,z_n)=\sum_{i_1,\cdots i_k\ \text{distinct}}^{}f(z_{i_1})\cdots f(z_{i_k}).\]</span></p><h2 id="k-points-correlations"><spanclass="math inline">\(k\)</span>-points correlations</h2><p>Let <span class="math inline">\(z\in \mathbb{C}\mapsto\gamma(z)=\pi^{-1}\mathrm{e}^{-\lvert z \rvert ^{2}}\)</span> be thedensity of the standard Gaussian <spanclass="math inline">\(\mathcal{N}(0,\frac{1}{2}I_2)\)</span> on <spanclass="math inline">\(\mathbb{C}\)</span>. For every <spanclass="math inline">\(1\leqslant k\leqslant n\)</span>, the <spanclass="math inline">\(k\)</span>-point correlation is <spanclass="math display">\[    \varphi_{n,k}(z_1,\cdots ,z_k):=\int_{\mathbb{C}^{n-k}}^{}\varphi_{n}(z_1,\cdots ,z_n)\mathrm{d}z_{k+1}^{r}\mathrm{d}z_{k+1}^{i}\cdots\mathrm{d}z_{n}^{r}\mathrm{d}z_n^{i}.  \tag{2.29}\]</span></p><div class="note note-info">            <p><strong>Theorem (Dyson, 1970)</strong> Let <spanclass="math inline">\(K(x,y)\)</span> be a function with complex values,such that <span class="math display">\[    \bar{K}(x,y)=K(y,x), \tag{2.30}\]</span></p><p>Assume that <span class="math display">\[    \int K(x,y)K(y,z) \mathrm{d}y=K(x,z), \tag{2.31}\]</span></p><p>Let <span class="math inline">\([K(z_i,z_j)]_{N}\)</span> denote the<span class="math inline">\(N\times N\)</span> matrix with its <spanclass="math inline">\((i,j)\)</span> element equal to <spanclass="math inline">\(K(z_i,z_j)\)</span>. Then <spanclass="math display">\[    \int_{}^{} \det[K(z_i,z_j)]_{N}\mathrm{d}z_{N}^{r}\mathrm{d}z_{N}^{i}=(c-N+1)\det[K(z_i,z_j)]_{N-1},\tag{2.32}\]</span></p><p>where <span class="math display">\[    c=\int_{}^{} K(z,z) \mathrm{d}z^{r}\mathrm{d}z^{i} \tag{2.33}\]</span></p><p>Note that (2.30) and (2.31) mean that the linear operator defined bythe kernel <span class="math inline">\(K(x,y)\)</span> is a projector,and the constant <span class="math inline">\(c\)</span> is its trace(hence a nonnegative integer).</p>          </div><p><strong>proof</strong> From the definition of the determinant, <spanclass="math display">\[    \det[K(z_i,z_j)]_{N}=\sum_{P}^{} \sigma(P)\prod_{1}^{l}(K(\xi,\eta)K(\eta,\zeta)\cdots K(\theta,\xi)), \tag{2.34}\]</span></p><p>where the permutation <span class="math inline">\(P\)</span> consistsof <span class="math inline">\(l\)</span> cycles of the form <spanclass="math inline">\((\xi \rightarrow \eta \rightarrow \zeta\rightarrow\cdots \rightarrow \theta\rightarrow \xi)\)</span>. Now theindex <span class="math inline">\(N\)</span> occurs somewhere. - <spanclass="math inline">\(N\)</span> forms a cycle by itself, and <spanclass="math inline">\(K(z_{N},z_{N})\)</span> gives on integration thescalar constant <span class="math inline">\(c\)</span>; the remainingfactor is by definition <spanclass="math inline">\(\det[K(z_i,z_j)]_{N-1}\)</span>; - <spanclass="math inline">\(N\)</span> occurs in a longer cycle andintegration on <span class="math inline">\(x_{N}\)</span> reduces thelength of this cycle by one. This can happen in <spanclass="math inline">\((N-1)\)</span> ways since the index <spanclass="math inline">\(N\)</span> can be inserted between any two indicesof the cyclic sequence <span class="math inline">\(1,2,\cdots,N-1\)</span>. Alos the resulting permutation over <spanclass="math inline">\(N-1\)</span> indices has an opposite sign. Theremaining expression is by definition <spanclass="math inline">\(\det[K(z_i,z_j)]_{N-1}\)</span>.</p><p>Adding the two contributions we get the result.</p><div class="note note-info">            <p><strong>Theorem (<span class="math inline">\(k\)</span>-pointscorrelations)</strong> For every <span class="math inline">\(1\leqslantk\leqslant n\)</span>, <span class="math display">\[    \varphi_{n,k}(z_1,\cdots ,z_k)=\frac{(n-k)!}{n!}\gamma(z_1)\cdots\gamma(z_k)\det [K(z_i,z_j)]_{1\leqslant i,j\leqslant k} \tag{2.35}\]</span></p><p>where <span class="math display">\[    K(z_i,z_j):=\sum_{l=0}^{n-1}\frac{(z_iz_j^{*})^{l}}{l!}=\sum_{l=0}^{n-1} H_l(z_i)H_l(z_j)^{*}\tag{2.36}\]</span></p><p>with <span class="math display">\[    H_{l}(z):=\frac{1}{\sqrt{l!}}z^{l}. \tag{2.37}\]</span></p><p>In particular, by taking <span class="math inline">\(k=n\)</span> weget <span class="math display">\[    \varphi_{n,n}=\varphi_{n}(z_1,\cdots,z_n)=\frac{1}{n!}\gamma(z_1)\cdots\gamma(z_n)\det[K(z_i,z_j)]_{1\leqslant i,j\leqslant n}.       \]</span></p>          </div><p><strong>proof</strong></p><p>It's easy to check the <span class="math inline">\(k=n\)</span>situation <span class="math display">\[    \varphi_{n}(z_1,\cdots ,z_n)=\frac{\pi^{-n}}{n!}\exp \left(-\sum_{i=1}^{n} \lvert z_i \rvert ^{2} \right)\det[K(z_i,z_j)]_{1\leqslant i,j\leqslant n}.\]</span></p><p>By the previous theorem, setting <spanclass="math inline">\(K(x,y)=\sum_{l=0}^{n-1} H_l(x)H_l(y)^{*}\)</span>,we get</p><p><span class="math display">\[    \int_{}^{} \mathrm{e}^{-\lvert y \rvert ^{2}} K(x,y)K(y,z)\mathrm{d}y^{r}\mathrm{d}y^{i}=\pi K(x,z)\]</span></p><p>and <span class="math display">\[    \int_{}^{} \mathrm{e}^{-\lvert z \rvert ^{2}} K(z,z)\mathrm{d}z^{r}\mathrm{d}z^{i}=n\pi\]</span></p><p>So we have <span class="math display">\[    \begin{aligned}        \varphi_{n,n-1}&amp;=\int_{\mathbb{C}}^{}\frac{\pi^{-n}}{n!}\exp \left( -\sum_{i=1}^{n} \lvert z_i \rvert ^{2}\right) \det[K(z_i,z_j)]_{1\leqslant i,j\leqslant n}\mathrm{d}z_n^{r}\mathrm{d}z_n^{i} \\        &amp;=\frac{\pi^{-n+1}}{n!}\exp \left( -\sum_{i=1}^{n-1} \lvertz_i \rvert ^{2} \right) \det[K(z_i,z_j)]_{1\leqslant i,j\leqslant n-1}\\    \end{aligned}\]</span></p><p>By induction, (2.35) holds.</p><h2 id="reference">Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Charles Bordenave and DjalilChafaï, The circular law<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>R. A. Horn and Ch. R.Johnson, Topics in matrix analysis, Cambridge University Press,Cambridge, 1994, Corrected reprint of the 1991 original.<a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Madan Lal Mehta, Randommatrices, third ed. Pure and Applied Mathematics, vol.142, Acad. Press,2004. <a href="#fnref:3" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>概率论</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>几个数理统计相关的问题</title>
    <link href="/2023/01/19/%E5%87%A0%E4%B8%AA%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E7%9B%B8%E5%85%B3%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <url>/2023/01/19/%E5%87%A0%E4%B8%AA%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E7%9B%B8%E5%85%B3%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h1 id="几个数理统计相关的问题">几个数理统计相关的问题</h1><h3 id="为什么样本方差的分母是-n-1">为什么样本方差的分母是 <spanclass="math inline">\(n-1\)</span> ？</h3><p>如果已知随机变量 <span class="math inline">\(X\)</span> 的期望为<span class="math inline">\(\mu\)</span>，方差 <spanclass="math inline">\(\sigma^{2}=E[(X-\mu)^{2}]\)</span>.</p><p>那么采样后用 <span class="math display">\[    S^{2}=\frac{1}{n}\sum_{i=1}^{n} (X_i-\mu)^{2}\]</span></p><p>来近似 <span class="math inline">\(\sigma^{2}\)</span>.</p><p>一般说来 <span class="math inline">\(\mu\)</span>是未知的，只有样本的均值 <span class="math display">\[    \bar{X}=\frac{1}{n}\sum_{i=1}^{n} X_{i}.\]</span></p><p>我们要证明可以用 <span class="math display">\[    S^{2}=\frac{1}{n-1}\sum_{i=1}^{n} (X_{i}-\bar{X})^{2}\]</span></p><p>来近似 <span class="math inline">\(\sigma^{2}\)</span>.</p><p>首先我们知道 <span class="math display">\[    E[\frac{1}{n}\sum_{i=1}^{n} (X_{i}-\mu)^{2}]=\sigma^{2}\]</span></p><p>根据中心极限定理，<span class="math inline">\(S^{2}\)</span>的采样均值会服从 <span class="math inline">\(\mu=\sigma^{2}\)</span>的正态分布. 这就是所谓的无偏估计.</p><p>易知 <span class="math display">\[    \sum_{i=1}^{n} (X_{i}-\bar{X})^{2}\leqslant \sum_{i=1}^{n}(X_{i}-\mu)^{2}\]</span></p><p>如果我们令</p><p><span class="math display">\[    \tilde{S}^{2}=\frac{1}{n}\sum_{i=1}^{n} (X_{i}-\bar{X})^{2}\]</span></p><p>则有 <span class="math display">\[    \begin{aligned}        E[\tilde{S}^{2}]&amp;=E[\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\bar{X})^{2}]=E[\frac{1}{n}\sum_{i=1}^{n}((X_{i}-\mu)-(\bar{X}-\mu))^{2}] \\        &amp;=E[\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\mu)^{2}-\frac{2}{n}(\bar{X}-\mu)\sum_{i=1}^{n}(X_{i}-\mu)+(\bar{X}-\mu)^{2}]\\        &amp;=\sigma^{2}-E[(\bar{X}-\mu)^{2}]    \end{aligned}\]</span></p><p>而 <span class="math display">\[    \begin{aligned}        E[(\bar{X}-\mu)^{2}]=E(\bar{X}-E[\bar{X}])^{2}&amp;=var(\bar{X})\\        &amp;=var\left(\frac{\sum_{i=1}^{n} X_{i}}{n}\right)\\        &amp;=\frac{1}{n^{2}}\sum_{i=1}^{n} var(X_{i}) \\        &amp;=\frac{\sigma^{2}}{n}    \end{aligned}\]</span></p><p>故 <span class="math display">\[    E[\tilde{S}^{2}]=\frac{n-1}{n}\sigma^{2}\]</span></p><p>故 <span class="math display">\[    S^{2}=\frac{1}{n-1}\sum_{i=1}^{n} (X_{i}-\bar{X})^{2}\]</span></p><p>得到的就是无偏估计.</p><h3id="只考虑一阶和二阶统计量的高斯过程">只考虑一阶和二阶统计量的高斯过程</h3><p>设有两个高斯过程 <span class="math inline">\(M\)</span> 和 <spanclass="math inline">\(N\)</span>，如果只考虑一阶和二阶统计量，可以设<span class="math display">\[    M=\mu_{M}+\Sigma_{M}\sqrt{1-\rho}X_1+\Sigma_{M}\sqrt{\rho}Y,\]</span></p><p><span class="math display">\[    N=\mu_{N}+\Sigma_{N}\sqrt{1-\rho}X_2+\Sigma_{M}\sqrt{\rho}Y.\]</span></p><p>其中 <span class="math inline">\(X_1,X_2,Y\)</span> 都是标准高斯过程.这样就有</p><p><span class="math display">\[    E[M]=\mu_{M},E[N]=\mu_{N}.\]</span></p><p><span class="math display">\[    var(M)=\Sigma_{M}, var(N)=\Sigma_{N}.\]</span></p><p><span class="math display">\[    cov(M,N)=\rho\Sigma_{M}\Sigma_{N}.\]</span></p><p>这与二元正态分布是很类似的.</p><h3 id="低通滤波">低通滤波</h3><p>函数的低通滤波（？）对于函数 <spanclass="math inline">\(\kappa(t)\)</span>，定义 <spanclass="math inline">\(\tilde{\kappa}(t)\)</span> 为 <spanclass="math display">\[    (1+\frac{\mathrm{d}}{\mathrm{d}t})\tilde{\kappa}(t)=\kappa(t)\]</span></p><p>可以认为是 <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}\tilde{\kappa}(t)=-\tilde{\kappa}(t)+\kappa(t)\]</span></p><p>的变形.</p><h3 id="统计量的公式">统计量的公式</h3><p>对于一个 Markov chain <span class="math inline">\(Z \rightarrowX_1,X_2\)</span>，有 <span class="math display">\[    \mathbb{E}_{X_1}(X_1)=\mathbb{E}_{Z}(\mathbb{E}_{X_1|Z}(X_1)),  \]</span></p><p><span class="math display">\[    \operatorname{var}_{X_1}(X_1)=\mathbb{E}_{Z}(\operatorname{var}_{X_1|Z}(X_1))+\operatorname{var}_{Z}(\mathbb{E}_{X_1|Z}(X_1)),\]</span></p><p><span class="math display">\[    \operatorname{cov}_{X_1,X_2}(X_1,X_2)=\mathbb{E}_{Z}(\operatorname{cov}_{X_1,X_2|Z}(X_1,X_2))+\operatorname{cov}_{Z}(\mathbb{E}_{X_1|Z}(X_1),\mathbb{E}_{X_2|Z}(X_2)).\]</span></p><h2 id="pca-ica-cca-pls">PCA, ICA, CCA, PLS</h2><h3 id="pca-ica">PCA, ICA</h3><p>PCA 和 ICA 都是降维算法. 不同于 PCA得到的主成分在时间和空间上都不相关（左右奇异向量都是正交的），ICA得到的主成分只在一个部分上具有最大的统计独立性.</p><p>PCA 目标是找一组轴，使得数据在这组轴上的投影方差最大. ICA的想法是观察到的信号是由一些独立的信号混合而成的. By the central limittheorem, any linear mixture of independent variables will be more"Gaussian" than the original variables. Thus, ICA seeks to create a newset of axes. The axes are oriented such that the projection of datapoints onto the axes is maximally non-Gaussian. We can use kurtois,negentropy, or mutual information to measure the non-Gaussianity.</p><p>Independent components can be interpreted as the dominant functionalnetworks or modes of activity that contribute to the observedneuroimagingdata<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Anthony R.McIntosh &amp; Bratislav Misic, 2013. Multivariate Statistical Analyses for Neuroimaging Data. Annual Reviews.">[1]</span></a></sup>.</p><h3 id="canonical-correlation-analysis-cca">Canonical CorrelationAnalysis (CCA)</h3><p>The goal of CCA is to relate two sets of data, <spanclass="math inline">\(\mathbf{X}_{n \times p}\)</span> and <spanclass="math inline">\(\mathbf{Y}_{n \times q}\)</span>, with <spanclass="math inline">\(p\)</span> and <spanclass="math inline">\(q\)</span> variables in their respective columnsand <span class="math inline">\(n\)</span> observations in the row.</p><p>CVA (Friston et al. 1996, Strother et al. 2002), linear discriminantanalysis (LDA), and multivariate analysis of variance are special caseof CCA.</p><p>We want to create pairs of new variable that are linear combinationsof the original variables in <spanclass="math inline">\(\mathbf{X}(\mathbf{XU}(i)_{n \times 1})\)</span>and the variables in <spanclass="math inline">\(\mathbf{Y}(\mathbf{YV}(i)_{n \times 1})\)</span>and that have the maximum correlation with each other. In addition,pairs of canonical variates are mutually orthogonal with all the otherpairs. Factorize the adjusted correlation matrix <spanclass="math display">\[    (\mathbf{X}^{\mathsf{T}}\mathbf{X})^{-1/2}\mathbf{X}^{\mathsf{T}}\mathbf{Y}(\mathbf{Y}^{\mathsf{T}}\mathbf{Y})^{-1/2}=\mathbf{USV}^{\mathsf{T}}.\]</span></p><h3 id="pls">PLS</h3><h2 id="reference">Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Anthony R.McIntosh &amp;Bratislav Misic, 2013. Multivariate Statistical Analyses forNeuroimaging Data. Annual Reviews.<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Statistics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>庞加莱定理及其应用</title>
    <link href="/2022/12/26/%E5%BA%9E%E5%8A%A0%E8%8E%B1%E5%AE%9A%E7%90%86%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/"/>
    <url>/2022/12/26/%E5%BA%9E%E5%8A%A0%E8%8E%B1%E5%AE%9A%E7%90%86%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="庞加莱定理及其应用">庞加莱定理及其应用</h1><p>本文是数分三的期末小论文，时间很赶，我对这篇小论文很不满意.数分三期末有道大题看错了算了一堆不对的东西，最后发现了也没来得及算完，简直依托答辩.这篇文章缝的东西来源也不多，De Rham上同调群纯粹凑字数，不过以后应该是碰不到了，或许也有可能碰到.下学期必修课要冲冲分了，想去听的黎曼几何和随机矩阵也要好好听.</p><h2 id="导言">导言</h2><p>在微分形式的理论中，外微分算子 <spanclass="math inline">\(\mathrm{d}\)</span> 起到相当重要的作用.我们定义过闭微分形式和恰当微分形式的概念. 本文中记号 <spanclass="math inline">\(\Omega^{p}(M)\)</span> 表示光滑流形 <spanclass="math inline">\(M\)</span> 上的所有光滑实值 <spanclass="math inline">\(p\)</span> 次微分形式的空间，而 <spanclass="math inline">\(\Omega(M)\)</span></p><p><strong>定义1.1</strong> 微分形式 <span class="math inline">\(\omega\in \Omega^{p}(M)\)</span> 称为闭微分形式（下简称闭形式），如果 <spanclass="math inline">\(\mathrm{d}\omega=0\)</span>.</p><p><strong>定义1.2</strong> 微分形式 <span class="math inline">\(\omega\in \Omega^{p}(M)(p&gt;0)\)</span>称为恰当微分形式（下简称恰当形式），如果满足 <spanclass="math inline">\(\omega=\mathrm{d}\alpha\)</span> 的微分形式 <spanclass="math inline">\(\alpha \in \Omega^{p-1}(M)\)</span> 存在.</p><p>流形 <span class="math inline">\(M\)</span> 的所有闭 <spanclass="math inline">\(p\)</span> 形式的集合记为 <spanclass="math inline">\(Z^{p}(M)\)</span>，而 <spanclass="math inline">\(M\)</span> 上的所有恰当 <spanclass="math inline">\(p\)</span> 形式的集合记为 <spanclass="math inline">\(B^{p}(M)\)</span>.</p><p>课上在定义外微分算子 <span class="math inline">\(\mathrm{d}\)</span>时，自动有 <span class="math inline">\(\mathrm{d}^{2}=0\)</span>，所以$B<sup>{p}(M)Z</sup>{p}(M) $. 一般而言这个包含关系是严格的.</p><p>这就引出一个问题，当微分形式 <spanclass="math inline">\(\omega\)</span> 满足必要条件 <spanclass="math inline">\(\mathrm{d}\omega=0\)</span> 时，方程 <spanclass="math inline">\(\mathrm{d}\alpha=\omega\)</span> 是否可解. 这与<span class="math inline">\(M\)</span>的拓扑结构有关，本文将介绍最简单版本的庞加莱定理及其应用.</p><h2 id="庞加莱定理">庞加莱定理</h2><p>先给出一个定义</p><p><strong>定义2.1</strong> 流形 <span class="math inline">\(M\)</span>称为可收缩（于点 <span class="math inline">\(x_0\inM\)</span>）的或单点同伦的，如果存在光滑映射 <spanclass="math inline">\(h\colon M\times I\rightarrow M\)</span>，其中<span class="math inline">\(I=[0,1]\)</span>，使 <spanclass="math inline">\(h(x,1)=x\)</span>，<spanclass="math inline">\(h(x,0)=x_0\)</span>.</p><p><strong>定理2.2（庞加莱定理）.</strong> 可收缩于点的流形 <spanclass="math inline">\(M\)</span> 上的任何闭 <spanclass="math inline">\(p+1\)</span> 形式 <spanclass="math inline">\((p\geqslant 0)\)</span> 都是恰当的.</p><p>考虑“柱体” <span class="math inline">\(M\times I\)</span>，即 <spanclass="math inline">\(M\)</span> 与单位区间 <spanclass="math inline">\(I\)</span> 的直积，以及两个映射 <spanclass="math inline">\(j_{i}\colon M\rightarrow M\times I\)</span>，<spanclass="math inline">\(j_{i}(x)=(x,i),i=0,1\)</span>，它们使 <spanclass="math inline">\(M\)</span> 分别等同于柱体 <spanclass="math inline">\(M\times I\)</span> 的两个底面.于是有相应的拉回映射 <span class="math inline">\(j_{i}^{*}\colon\Omega^{p}(M\times I)\rightarrow \Omega^{p}(M)\)</span>，其结果是把<span class="math inline">\(\Omega^{p}(M\times I)\)</span>中的微分形式中的变量 <span class="math inline">\(t\)</span> 改为 <spanclass="math inline">\(i\)</span> 的值. 注意此时若自变量中含 <spanclass="math inline">\(\mathrm{d}t\)</span> 项，则拉回的结果当然是 <spanclass="math inline">\(0\)</span>.</p><h3 id="证明一">证明一</h3><p>构造一个线性算子 <span class="math inline">\(K\colon\Omega^{p+1}(M\times I)\rightarrow\Omega^{p}(M)\)</span>，它在作用于单项式的时候由以下方式确定： <spanclass="math display">\[    K(a(x,t)\mathrm{d}x^{i_1}\wedge \cdots \wedge\mathrm{d}x^{i_{p+1}}):=0,\]</span></p><p><span class="math display">\[    K(a(x,t)\mathrm{d}t \wedge \mathrm{d}x^{i_1}\wedge \cdots\wedge\mathrm{d}x^{i_p}):=\left( \int_{0}^{1} a(x,t) \mathrm{d}r \right)\mathrm{d}x^{i_1}\wedge \cdots \wedge \mathrm{d}x^{i_p}.\]</span></p><p>关于算子 <span class="math inline">\(K\)</span>的我们所需要的基本性质是，对于任何一个微分形式 <spanclass="math inline">\(\omega \in \Omega^{p+1}(M\timesI)\)</span>，以下关系式成立： <span class="math display">\[    K(\mathrm{d}\omega)+\mathrm{d}(K\omega)=j_1^{*}\omega-j_0^{*}\omega.\tag{1}\]</span></p><p>因为算子 <span class="math inline">\(K,d,j_1^{*},j_0^{*}\)</span>都是线性的，故只要对单项式验证这个关系式. 如果 <spanclass="math inline">\(\omega=a(x,t)\mathrm{d}x^{i_1}\wedge\cdots \wedge\mathrm{d}x^{i_{p+1}}\)</span>，则 <spanclass="math inline">\(K\omega=0\)</span>，<spanclass="math inline">\(\mathrm{d}(K\omega)=0\)</span>， <spanclass="math display">\[    \mathrm{d}\omega=\frac{\partial a}{\partialt}\mathrm{d}t\wedge\mathrm{d}x^{i_1}\wedge\cdots\wedge\mathrm{d}x^{i_{p+1}}+\text{不含 $\mathrm{d}t$ 的项},\]</span></p><p><span class="math display">\[\begin{aligned}    K(\mathrm{d}\omega)&amp;=\left( \int_{0}^{1} \frac{\partiala}{\partial t} \mathrm{d}t \right) \mathrm{d}x^{i_1} \wedge\cdots \wedge\mathrm{d}x^{i_{p+1}} \\    &amp;=(a(x,1)-a(x,0))\mathrm{d}x^{i_1}\wedge\cdots\wedge\mathrm{d}x^{i_{p+1}}=j_1^{*}(\omega)-j_0^{*}(\omega),\end{aligned}\]</span></p><p>所以关系式（1）成立.</p><p>如果 <span class="math inline">\(\omega=a(x,t)\mathrm{d}t\wedge\mathrm{d}x^{i_1}\wedge\cdots \wedge \mathrm{d}x^{i_p}\)</span>，则<span class="math inline">\(j_1^{*}\omega=j_0^{*}\omega=0\)</span>. 于是<span class="math display">\[\begin{aligned}    K(\mathrm{d}\omega)&amp;=K\left(-\sum_{i_0}^{} \frac{\partiala}{\partialx^{i_0}}\mathrm{d}t\wedge\mathrm{d}x^{i_0}\wedge\mathrm{d}x^{i_1}\wedge\cdots\wedge \mathrm{d}x^{i_p} \right)\\    &amp;=-\sum_{i_0}^{} \left( \int_{0}^{1} \frac{\partial a}{\partialx^{i_0}}\mathrm{d}x \right) \mathrm{d}x^{i_0}\wedge\mathrm{d}x^{i_1}\wedge\cdots \wedge \mathrm{d}x^{i_p},\end{aligned}\]</span></p><p><span class="math display">\[\begin{aligned}    \mathrm{d}(K\omega)&amp;=\mathrm{d}\left( \left( \int_{0}^{1} a(x,t)\mathrm{d}t \right) \mathrm{d}x^{i_1}\wedge \cdots \wedge\mathrm{d}x^{i_p} \right) \\    &amp;=\sum_{i_0}^{} \frac{\partial }{\partial x^{i_0}}\left(\int_{0}^{1} a(x,t) \mathrm{d}t \right) \mathrm{d}x^{i_0}\wedge\mathrm{d}x^{i_1}\wedge\cdots \wedge \mathrm{d}x^{i_p}\\    &amp;=\sum_{i_0}^{} \left( \int_{0}^{1} \frac{\partial a}{\partialx^{i_0}} \mathrm{d}t \right)\mathrm{d}x^{i_0}\wedge\mathrm{d}x^{i_1}\wedge\cdots \wedge\mathrm{d}x^{i_p}\end{aligned}\]</span></p><p>因此在这种情况下（1）也成立.</p><p>现在，设 <span class="math inline">\(M\)</span> 是可收缩于点 <spanclass="math inline">\(x_0 \in M\)</span> 的流形，<spanclass="math inline">\(h\colon M\times I\rightarrow M\)</span>是定义3中的映射，<span class="math inline">\(\omega\)</span> 是 <spanclass="math inline">\(M\)</span> 上的 <spanclass="math inline">\(p+1\)</span> 形式. 显然 <spanclass="math inline">\(h \circ j_1\colon M\rightarrow M\)</span>是恒等映射，<span class="math inline">\(h\circ j_0\colon M\rightarrowx_0\)</span> 是 <span class="math inline">\(M\)</span> 到点 <spanclass="math inline">\(x_0\)</span> 的映射，所以 <spanclass="math inline">\((j_1^{*}\circ h^{*})\omega=\omega\)</span>，<spanclass="math inline">\((j_0^{*}\circ h^{*})\omega=0\)</span>.因此，这时从（1）推出 <span class="math display">\[    K(\mathrm{d}(h^{*}\omega))+\mathrm{d}(K(h^{*}\omega))=\omega.\tag{2}   \]</span> 又因为 <span class="math inline">\(\omega\)</span> 是 <spanclass="math inline">\(M\)</span> 上的闭形式，而且 <spanclass="math inline">\(\mathrm{d}(h^{*}\omega)=h^{*}(\mathrm{d}\omega)=0\)</span>，所以从（2）得到<span class="math display">\[    \mathrm{d}(K(h^{*}\omega))=\omega.\]</span></p><p>因此，闭形式 <span class="math inline">\(\omega\)</span> 是微分形式<span class="math inline">\(\alpha=K(h^{*}\omega)\in\Omega^{p}(M)\)</span> 的外微分，即 <spanclass="math inline">\(\omega\)</span> 是 <spanclass="math inline">\(M\)</span> 上的恰当形式</p><h3 id="证明二">证明二</h3><p>引入向量场 <span class="math inline">\(X\)</span> 与微分形式 <spanclass="math inline">\(\omega\)</span> 的内积.</p><p><strong>定义4.</strong> 设 <span class="math inline">\(X\)</span>是光滑流形 <span class="math inline">\(M\)</span> 上的向量场，<spanclass="math inline">\(\omega\)</span> 是 <spanclass="math inline">\(M\)</span> 上的 <spanclass="math inline">\(k\)</span> 次微分形式. 由关系式 <spanclass="math inline">\((i_{X}\omega)(X_1,\cdots,X_{k-1}):=\omega(X,X_1,\cdots ,X_{k-1})\)</span>（其中 <spanclass="math inline">\(X_1,\cdots ,X_{k-1}\)</span> 是 <spanclass="math inline">\(M\)</span> 上的向量场）确定的 <spanclass="math inline">\(k-1\)</span> 形式 <spanclass="math inline">\(i_{X}\omega\)</span> 称为场 <spanclass="math inline">\(X\)</span> 与微分形式 <spanclass="math inline">\(\omega\)</span> 的内积. 对于 <spanclass="math inline">\(0\)</span> 形式，即对于 <spanclass="math inline">\(M\)</span> 上的函数，取 <spanclass="math inline">\(i_{X}f=0\)</span>.</p><p>先证明一个引理，</p><p><strong>引理1.</strong> 设 <span class="math inline">\(t\mapsto h_t\in C^{\infty}(M,N)\)</span> 是光滑地依赖于参数 $t I $ 的一族从流形<span class="math inline">\(M\)</span> 到流形 <spanclass="math inline">\(N\)</span> 的映射. 则对于任何微分形式 <spanclass="math inline">\(\omega \in \Omega(N)\)</span>，以下同伦公式成立：<span class="math display">\[    \frac{\partial }{\partialt}(h_t^{*}\omega)(x)=\mathrm{d}h_t^{*}(i_{X}\omega)(x)+h_t^{*}(i_{X}\mathrm{d}\omega)(x), \tag{3}\]</span></p><p>其中 <span class="math inline">\(x \in M\)</span>；<spanclass="math inline">\(X\)</span> 是 <spanclass="math inline">\(N\)</span> 上的向量场，并且 <spanclass="math inline">\(X(x,t)\in TN_{h_t(x)}\)</span>，而对于道路 <spanclass="math inline">\(t&#39;\mapsto h_{t&#39;}(x)\)</span>，<spanclass="math inline">\(X(x,t)\)</span> 是在 <spanclass="math inline">\(t&#39;=t\)</span> 时的速度向量.</p><p><strong>证明：</strong> 首先如果在图 $^{n}U M $ 的局部坐标 <spanclass="math inline">\(x^{1},\cdots ,x^{n}\)</span> 下，微分形式 <spanclass="math inline">\(\omega|_{U}\)</span> 的表达式为 <spanclass="math display">\[    \sum_{1\leqslant i_1&lt;\cdots &lt;i_{k}\leqslant n}^{} a_{i_1\cdotsi_k}(x)\mathrm{d}x^{i_1}\wedge\cdots \wedge\mathrm{d}x^{i_k}=\frac{1}{k!}a_{i_1\cdotsi_k}\mathrm{d}x^{i_1}\wedge\cdots \wedge \mathrm{d}x^{i_k},\]</span></p><p>而 <span class="math inline">\(X=X^{i}\frac{\partial }{\partialx^{i}}\)</span>，则</p><p><span class="math display">\[\begin{aligned}    (i_{X}\omega)\left(\frac{\partial }{\partial x^{\alpha_2}},\cdots,\frac{\partial }{\partial x^{\alpha_{k}}}\right)&amp;=\omega \left(X^{i}\frac{\partial }{\partial x^{i}},\frac{\partial }{\partialx^{\alpha_2}},\cdots ,\frac{\partial }{\partial x^{\alpha_k}}\right)  \\    &amp;=X^{i}\omega \left( \frac{\partial }{\partialx^{i}},\frac{\partial }{\partial x^{\alpha_2}},\cdots ,\frac{\partial}{\partial x^{\alpha_k}} \right) \\    &amp;=\frac{1}{k!}X^{i}a_{i_1\cdotsi_k}\mathrm{d}x^{i_1}\wedge\cdots \wedge \mathrm{d}x^{i_k}\left(\frac{\partial }{\partial x^{i}},\frac{\partial }{\partialx^{\alpha_2}},\cdots ,\frac{\partial }{\partial x^{\alpha_k}} \right) \\    &amp;=\frac{1}{(k-1)!}X^{i}a_{ii_2\cdots i_k}\delta_{\alpha_2\cdots\alpha_k}^{i_2\cdots i_k},\end{aligned}\]</span></p><p>故 <span class="math display">\[    i_{X}\omega=\frac{1}{(k-1)!}X^{i}a_{ii_2\cdotsi_k}\mathrm{d}x^{i_2}\wedge \cdots \wedge \mathrm{d}x^{i_k}.\]</span></p><p>下面只对单项式 <span class="math inline">\(\omega\)</span> 证明（3）.如果 <span class="math inline">\(\omega\)</span> 是 <spanclass="math inline">\(N\)</span> 上的函数 <spanclass="math inline">\(\omega=f\)</span>，由定义知 <spanclass="math inline">\(i_{X}f=0\)</span>，故有 <spanclass="math display">\[    \frac{\partial }{\partial t}(h_t^{*}\omega)=\frac{\partial}{\partial t}(h_t^{*}f)=\frac{\partial f}{\partial y^{j}}\frac{\partialh_t^{j}}{\partial t}=X^{j}\frac{\partial f}{\partialy^{j}}=h_t^{*}(i_{X}\frac{\partial f}{\partialy^{j}}\mathrm{d}y^{j})=h_t^*(i_{X}\mathrm{d}\omega) \tag{4}\]</span></p><p>即对 <span class="math inline">\(\omega=f\)</span>的情况，（3）成立.</p><p>下面设 <spanclass="math inline">\(\omega=a(y)\mathrm{d}y^{j_1}\wedge\cdots \wedge\mathrm{d}y^{j_k}\)</span>，<spanclass="math inline">\(X(x,t)=X^{j}(x,t)\frac{\partial }{\partialy^{j}}=\frac{\partial h_t^{j}}{\partial y^{j}}\frac{\partial }{\partialy^{j}}\)</span>. 可得 <span class="math display">\[    i_{X}\omega=\sum_{p=1}^{k} (-1)^{p-1}X^{j_p}a(y)\mathrm{d}y^{j_1}\wedge\cdots \wedge\widehat{\mathrm{d}y^{j_p}}\wedge\cdots \wedge\mathrm{d}y^{j_k}\]</span></p><p>一方面， <span class="math display">\[    \begin{aligned}        \frac{\partial }{\partial t}(h_t^{*}\omega)&amp;=\frac{\partial}{\partial t}\left( a\circ h_t \frac{\partial y^{j_1}}{\partialx^{i_1}}\cdots \frac{\partial y^{j_k}}{\partialx^{i_k}}\mathrm{d}x^{i_1}\wedge\cdots \wedge\mathrm{d}x^{i_k}\right)  \\        &amp;=\frac{\partial a}{\partial y^{j_0}} \frac{\partialh_t^{j_0}}{\partial t}\frac{\partial y^{j_1}}{\partial x^{i_1}}\cdots\frac{\partial y^{j_k}}{\partial x^{i_k}}\mathrm{d}x^{i_1}\wedge\cdots\wedge \mathrm{d}x^{i_k}        \end{aligned}\]</span> (5)</p><p>另一方面，由于 <spanclass="math inline">\(\mathrm{d}h_t^{*}(i_{X}\omega)=h_t^{*}(\mathrm{d}i_{X}\omega)\)</span>，故右边<spanclass="math inline">\(=h_t^{*}(\mathrm{d}i_{X}\omega+i_{X}\mathrm{d}\omega)\)</span>，又<span class="math display">\[    \begin{aligned}    i_{X}\mathrm{d}\omega&amp;=i_{X}\left( \frac{\partial a}{\partialy^{j_0}}\mathrm{d}y^{j_0}\wedge\mathrm{d}y^{j_1}\wedge\cdots \wedge\mathrm{d}y^{j_k} \right)  \\    &amp;=\sum_{p=0}^{k} (-1)^{p}X^{j_p}\frac{\partial a}{\partialy^{j_0}}\mathrm{d}y^{i_0}\wedge\cdots \wedge\widehat{\mathrm{d}y^{j_p}}\wedge\cdots \wedge \mathrm{d}y^{j_k}\\    &amp;=\sum_{p=0}^{k} (-1)^{p}\frac{\partial h_t^{j_p}}{\partialy^{j_p}}\frac{\partial a}{\partial y^{j_0}}\mathrm{d}y^{i_0}\wedge\cdots\wedge \widehat{\mathrm{d}y^{j_p}}\wedge\cdots \wedge \mathrm{d}y^{j_k}    \end{aligned}\]</span> (6)</p><p><span class="math display">\[    \begin{aligned}        \mathrm{d}i_{X}\omega&amp;=\mathrm{d}\left( \sum_{p=1}^{k}(-1)^{p-1} X^{j_p}a(y)\mathrm{d}y^{j_1}\wedge \cdots \wedge\widehat{\mathrm{d}y^{j_p}}\wedge\cdots \wedge\mathrm{d}y^{j_k}\right)  \\        &amp;=\mathrm{d}\left( \sum_{p=1}^{k} (-1)^{p-1}\frac{\partialh_t^{j_p}}{\partial t}a(y)\mathrm{d}y^{j_1}\wedge \cdots \wedge\widehat{\mathrm{d}y^{j_p}}\wedge\cdots \wedge\mathrm{d}y^{j_k} \right)\\        &amp;=\sum_{p=1}^{k} (-1)^{p-1}\frac{\partialh_t^{j_p}}{\partial t}\frac{\partial a}{\partialy^{j_0}}\mathrm{d}y^{j_0}\wedge\mathrm{d}y^{j_1}\wedge \cdots \wedge\widehat{\mathrm{d}y^{j_p}}\wedge\cdots \wedge\mathrm{d}y^{j_k}    \end{aligned}\]</span> (7)</p><p>从而有 <span class="math display">\[    \begin{aligned}        h_t^{*}(\mathrm{d}i_{X}\omega+i_{X}\mathrm{d}\omega)&amp;=h_t^{*}\left( \frac{\partial h_t^{j_0}}{\partial t}\frac{\partial a}{\partialy^{j_0}}\mathrm{d}y^{j_1}\wedge\cdots \wedge \mathrm{d}y^{j_k}\right) \\        &amp;=\frac{\partial a}{\partial y^{j_0}} \frac{\partialh_t^{j_0}}{\partial t}\frac{\partial y^{j_1}}{\partial x^{i_1}}\cdots\frac{\partial y^{j_k}}{\partial x^{i_k}}\mathrm{d}x^{i_1}\wedge\cdots\wedge \mathrm{d}x^{i_k}    \end{aligned}\]</span> (8)</p><p>对比（5）和（8）知（3）成立. 由各算子的线性性知对一般的 <spanclass="math inline">\(\omega\)</span>，（3）都成立.</p><p>回到庞加莱定理，在引理中取 <span class="math inline">\(M=N\)</span>.设 <span class="math inline">\(M\)</span> 上有一闭形式 <spanclass="math inline">\(\omega\)</span>，则 <span class="math display">\[    \frac{\partial }{\partialt}(h_t^{*}\omega)(x)=\mathrm{d}h_t^{*}(i_{X}\omega)(x). \tag{9}\]</span></p><p>因为 <span class="math inline">\(M\)</span> 可缩于点 <spanclass="math inline">\(x_0\)</span>，由定义知存在对参数 <spanclass="math inline">\(t \in [0,1]\)</span> 光滑依赖的 <spanclass="math inline">\(M\)</span> 到 <spanclass="math inline">\(M\)</span> 的映射，且对任意 <spanclass="math inline">\(x \in M\)</span>， <spanclass="math inline">\(h_1(x)=x\)</span>，<spanclass="math inline">\(h_0(x)=x_0\)</span>. 故在（9）中对 <spanclass="math inline">\(t\)</span> 从<spanclass="math inline">\(0\)</span>到<spanclass="math inline">\(1\)</span>积分，得 <span class="math display">\[    \omega=h_1^{*}(\omega)-h_0^{*}(\omega)=\int_{0}^{1} \frac{\partial}{\partial t}(h_t^{*}\omega) \mathrm{d}t=\int_{0}^{1}\mathrm{d}h_t^{*}(i_{X}\omega) \mathrm{d}t=\mathrm{d} \int_{0}^{1}h_t^{*}(i_{X}\omega) \mathrm{d}t. \tag{10}\]</span></p><p>即 <span class="math inline">\(\omega\)</span> 是 <spanclass="math inline">\(M\)</span> 上的恰当形式.</p><p>由于光滑流形局部同胚于欧氏空间，而欧氏空间是可缩的，所以流形上的任何闭微分形式在局部都是恰当的.但是，并非总是能够把这些局部原像粘合为整个流形上的一个微分形式，这与流形的拓扑结构有关.参考文献中提供了 <span class="math inline">\(\mathbb{R}^{2}\backslash0\)</span> 上的微分形式 <span class="math inline">\(\omega=\displaystyle\frac{-y\mathrm{d}x+x\mathrm{d}y}{x^{2}+y^{2}}\)</span>这样一个例子.</p><h2 id="de-rham-上同调群">De Rham 上同调群</h2><p><strong>定义5.</strong> 商空间 <span class="math display">\[    H^{p}(M):=Z^{p}(M)/ B^{p}(M) \tag{11}\]</span></p><p>称为流形 <span class="math inline">\(M\)</span> 的（实系数） <spanclass="math inline">\(p\)</span> 维上同调群（关于加法）.</p><p>该定义由De Rham提出，不难验证良定义性. 因此，如果两个闭形式 <spanclass="math inline">\(\omega_1,\omega_2 \in Z^{p}(M)\)</span> 满足 <spanclass="math inline">\(\omega_1-\omega_2 \inB^{p}(M)\)</span>，则这两个闭形式属于同一个上同调类，即它们是上同调的.用记号 <span class="math inline">\([\omega]\)</span> 表示微分形式 <spanclass="math inline">\(\omega \in Z^{p}(M)\)</span> 所属的上同调类. 因为<span class="math inline">\(Z^{p}(M)\)</span> 是算子 <spanclass="math inline">\(\mathrm{d}^{p}\colon \Omega^{p}(M)\rightarrow\Omega^{p+1}(M)\)</span> 的核，而 <spanclass="math inline">\(B^{p}(M)\)</span> 是算子 <spanclass="math inline">\(\mathrm{d}^{p-1}\colon \Omega^{p-1}(M)\rightarrow\Omega^{p}(M)\)</span> 的像，所以可以把（11）改写为 <spanclass="math display">\[    H^{p}(M)=\text{Ker}\ \mathrm{d}^{p}/\text{Im}\ \mathrm{d}^{p-1}.\]</span></p><p>设 <span class="math inline">\(M\)</span> 是一个光滑流形，<spanclass="math inline">\(H^{p}(M)\)</span> 是 <spanclass="math inline">\(M\)</span> 的 <spanclass="math inline">\(p\)</span> 维上同调群.</p><p><strong>证明：</strong> 1是定义3.1的直接结果. 2是庞加莱定理的推论.3是由于如果连通流形上的函数 <span class="math inline">\(f\colonM\rightarrow \mathbb{R}\)</span> 满足 <spanclass="math inline">\(\mathrm{d}f=0\)</span>，则 <spanclass="math inline">\(f\)</span> 为常数.</p><p>通过庞加莱定理的证明二，可以发现：如果把光滑映射 <spanclass="math inline">\(h\colon M\times I\rightarrow M\)</span>看作一族依赖于参数 <span class="math inline">\(t \in I\)</span> 的映射<span class="math inline">\(h_t\colon M\rightarrow M\)</span>，则对于<span class="math inline">\(M\)</span> 上的任何一个闭形式 <spanclass="math inline">\(\omega\)</span>，有 <span class="math display">\[    h_{t}^{*}(\omega)=\mathrm{d}\int_{0}^{t} h_t^{*}(i_{X}\omega)\mathrm{d}t\]</span> 这说明所有微分形式 <span class="math inline">\(h_t^{*}\omega(t\in I)\)</span> 都属于同一个上同调类.通过引理，我们可以证明一个关于同伦与上同调群的初步结论.</p><p>如果 <span class="math inline">\(K\)</span>是可收缩于一个点的流形，则对于任何一个流形 <spanclass="math inline">\(M\)</span> 和任何一个整数 <spanclass="math inline">\(p\)</span>，等式 <spanclass="math inline">\(H^{p}(K\times M)=H^{p}(M)\)</span> 成立.</p><p><strong>证明：</strong> 由 <span class="math inline">\(K\)</span>可缩于一点知，对于任何一个流形 <spanclass="math inline">\(M\)</span>，存在一族光滑地依赖于参数 <spanclass="math inline">\(t \in I=[0,1]\)</span> 的从 <spanclass="math inline">\(M\times K\)</span> 到自身的映射，满足任意 <spanclass="math inline">\(x \in M,p \in K\)</span>， <spanclass="math display">\[    h_1(x,p)=(x,p),\ h_0(x,p)=(x,p_0)\]</span></p><p>所以对于 <span class="math inline">\(M\times K\)</span>上的任何一个闭形式 <span class="math inline">\(\omega\)</span>， <spanclass="math inline">\(h_1^{*}\omega\)</span> 和 <spanclass="math inline">\(h_0^{*}\omega\)</span> 属于同一个上同调类，故<span class="math inline">\(H^{p}(K\times M)=H^{p}(M\times{p_0})\)</span>. 显然 <spanclass="math inline">\(H^{p}(M\times{p_0})=H^{p}(M)\)</span>，从而有<span class="math inline">\(H^{p}(K\times M)=H^{p}(M)\)</span> 成立.</p><p>事实上，<span class="math inline">\(M\times K\)</span> 和 <spanclass="math inline">\(M\)</span> 是同伦的. 更一般的，De Rham上同调群是一个同伦不变量，但证明涉及上链复形等高级内容，超出了本文范围.限于篇幅所限，不介绍同伦的概念和上同调群同伦不变量的性质.</p><p>有一个性质是可以在此证明的：De Rham 上同调群是一个拓扑不变量，即DeRham 上同调群关于微分同胚在同构意义下不变. 因此，Re Rham上同调群能够刻画一部分的流形拓扑性质，由于同伦的拓扑空间不一定同胚，ReRham 上同调群不能完全刻画流形的拓扑性质.</p><p>如果 <span class="math inline">\(\varphi\colon M\rightarrowN\)</span> 是一个微分同胚，那么 <span class="math display">\[    \varphi^{*}\colon H^{p}(N)\rightarrow H^{p}(M)\]</span> 是一个线性同构.</p><p><strong>定义6.</strong>定义 <span class="math inline">\([\omega]\inH^{k}(M)\)</span> 和 <span class="math inline">\([\eta]\inH^{l}(M)\)</span> 之间的杯积 <span class="math display">\[    [\omega]\cup [\eta]:=[\omega \wedge \eta] \in H^{k+l}(M).    \]</span></p><p>我们需要验证良定义性. 设 <span class="math inline">\(\omega \inZ^{k}(M)\)</span>，<span class="math inline">\(\eta \inZ^{l}(M)\)</span>，那么 <span class="math display">\[    \mathrm{d}(\omega\wedge \eta)=\mathrm{d}\omega\wedge\eta+(-1)^{k}\omega \wedge \mathrm{d}\eta=0,\]</span> 即 <span class="math inline">\(\omega\wedge \eta \inZ^{k+l}(M)\)</span>. 另外，对于任意 <span class="math inline">\(\xi_1\in \Omega^{k-1}(M)\)</span> 和 <span class="math inline">\(\xi_2 \in\Omega^{l-1}(M)\)</span>， <span class="math display">\[    (\omega+\mathrm{d}\xi_1)\wedge (\eta+\mathrm{d}\xi_2)=\omega\wedge\eta +\mathrm{d}[(-1)^{k}\omega\wedge\xi_2+(-1)^{k-1}\xi_1 \wedge\eta+(-1)^{k-1}\xi_1\wedge\xi_2].\]</span> 即该定义是良定的.</p><p>类似地假设 <span class="math inline">\(\varphi \colon M\rightarrowN\)</span> 光滑. 那么由 <spanclass="math inline">\(\mathrm{d}\varphi^{*}=\varphi^{*}\mathrm{d}\)</span>得 <span class="math display">\[    \varphi^{*}(Z^{k}(N))\subset Z^{k}(M), \quad\varphi^{*}(B^{k}(N))\subset B^{k}(M).\]</span> 从而 <span class="math inline">\(\varphi^{*}\colon\Omega^{k}(N)\rightarrow \Omega^{k}(M)\)</span>诱导出上同调群上的拉回映射 <span class="math inline">\(\varphi^{*}\colonH^{k}(N)\rightarrow H^{k}(M)\)</span>： <span class="math display">\[    \varphi^{*}([\omega]):=[\varphi^{*}\omega].\]</span> 显然 <span class="math inline">\(\varphi^{*}\)</span>是一个群同态. 容易验证有 - <span class="math inline">\((\psi\circ\varphi)^{*}=\varphi^{*}\circ \psi^{*}\)</span>. - <spanclass="math inline">\(\text{Id}^{*}=\text{Id}\)</span>.</p><p>取 <spanclass="math inline">\(\psi=\varphi^{-1}\)</span>，就知道命题3.4成立.</p><h2 id="结语">结语</h2><p>本文给出了庞加莱定理的两个证明，后一种尤其具有启发意义，因为其与同伦有着密切关系，给出的引理还可以推出Cartan同伦公式.庞加莱引理的本质是局部的，大范围的结果是本文未介绍的德拉姆定理.本文还介绍了De Rham上同调群，这是代数拓扑中极为重要和基础的概念，与之相关的同调群可以配合着导出Stokes 公式，从中可以一窥数学的对称与美妙.</p>]]></content>
    
    
    <categories>
      
      <category>拓扑学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>浅谈Hartman-Grobman定理</title>
    <link href="/2022/12/12/%E6%B5%85%E8%B0%88Hartman-Grobman%E5%AE%9A%E7%90%86/"/>
    <url>/2022/12/12/%E6%B5%85%E8%B0%88Hartman-Grobman%E5%AE%9A%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="浅谈hartman-grobman定理">浅谈Hartman-Grobman定理</h1><p>这是ODE课的期末大作业.期中考考得我心态崩了，期末考总体尽力了，但是有一个很弱智的计算错误令我无比痛心.这篇文章不能够令我满意的地方有三处：限于篇幅原因关于双曲同构的一些结果没有给出证明，关于非线性微分方程组线性化的Hartman-Grobman定理说得不是很清楚，最后的例子只给出了思路而没有给出证明.总的来说感觉学到了很多.</p><h2 id="banach空间上的双曲同构">Banach空间上的双曲同构</h2><p>首先我们要引入双曲同构的概念.我们需要将有限维线性空间同构的一些结果推广到无限维空间中去.</p><p>我们知道，对于 <span class="math inline">\(\mathbb{R}\)</span> 或者<span class="math inline">\(\mathbb{C}\)</span> 上有限维线性空间 <spanclass="math inline">\(E\)</span> 的自同构 <spanclass="math inline">\(A:E \to E\)</span>，如果 <spanclass="math inline">\(A\)</span> 没有模为1的特征值，那么存在 <spanclass="math inline">\(E\)</span> 的子空间 <spanclass="math inline">\(E^{s}\)</span> 和 <spanclass="math inline">\(E^{u}\)</span> 满足： - <spanclass="math inline">\(E=E^{s}\oplus E^{u}\)</span>; - <spanclass="math inline">\(A^{n}x\to 0\)</span> as <spanclass="math inline">\(n\to +\infty\)</span> for all <spanclass="math inline">\(x \in E^{s}\)</span>; - <spanclass="math inline">\(A^{-n}x \to 0\)</span> as <spanclass="math inline">\(n\to +\infty\)</span> for all <spanclass="math inline">\(x \in E^{u}\)</span>.</p><p>我们将 <span class="math inline">\(E^{s}\)</span> 称为 <spanclass="math inline">\(A\)</span> 的稳定空间，将 <spanclass="math inline">\(E^{u}\)</span> 称为 <spanclass="math inline">\(A\)</span> 的不稳定空间.无穷维Banach空间上的线性同构可能没有通常意义下的特征值，解决方法是考虑算子的谱.### 有界线性算子的谱和预解</p><p>本节我们介绍复Banach空间上有界线性算子的谱，并证明相关的一些性质.</p><p><strong>定义1.1.1</strong> 设 <span class="math inline">\(E\)</span>是一个复Banach空间. 定义 <span class="math inline">\(T \in\mathscr{L}(E)\)</span> 的谱为 <span class="math display">\[    \sigma(T)=\{ \lambda \in \mathbb{C}\colon \lambda I-T\text{不可逆}\}.\]</span></p><p>定义 <span class="math inline">\(T\)</span> 的预解集为 <spanclass="math display">\[    \rho(T)=\mathbb{C} \backslash \sigma (T).\]</span></p><p>给定 <span class="math inline">\(\lambda \in \rho(T)\)</span>，定义<span class="math inline">\(T\)</span> 在 <spanclass="math inline">\(\lambda\)</span> 处的预解算子为 <spanclass="math display">\[    R_{\lambda}(T)=(\lambda I-T)^{-1}.\]</span></p><p><strong>引理1.1.2.</strong> 设 <span class="math inline">\(E\)</span>是一个复Banach空间，<span class="math inline">\(A,B\colon E \toE\)</span> 是有界线性同构，则 - <span class="math inline">\(\sigma(A^{-1})=\{1/\lambda\colon \lambda \in \sigma(A)\}\)</span>; - <spanclass="math inline">\(\sigma(BAB ^{-1})=\sigma(A)\)</span>.</p><p><strong>证明</strong> 注意到 <span class="math inline">\(A\)</span>可逆，</p><h3 id="复化">复化</h3><p>给定实线性空间 <span class="math inline">\(E\)</span>，定义 <spanclass="math display">\[    E_{\mathbb{C}}=\{x+iy\colon x,y \in E\}.\]</span></p><p>定义 <span class="math inline">\(E_{\mathbb{C}}\)</span>上的复结构如下：对于 <span class="math inline">\(x+iy,u+iv \inE_{\mathbb{C}}\)</span>，定义 <span class="math display">\[    (x+iy)+(u+iv)=(x+u)+i(y+v).\]</span></p><p>对于 <span class="math inline">\(\alpha +i \beta \in\mathbb{C}\)</span> 和 <span class="math inline">\(x+iy \inE_{\mathbb{C}}\)</span>，定义 <span class="math display">\[    (\alpha+i\beta)(x+iy)=(\alpha x-\beta y)+i(\alpha y+\beta x).\]</span></p><p>则 <span class="math inline">\(E_{\mathbb{C}}\)</span>成为一个复线性空间，将其称为 <span class="math inline">\(E\)</span>的复化.</p><p>我们希望在 <span class="math inline">\(E_{\mathbb{C}}\)</span>上赋予范数，使得在 <span class="math inline">\(E\)</span>是Banach空间时，<span class="math inline">\(E_{\mathbb{C}}\)</span>也是Banach空间. 寻找这样的范数并不平凡，一个选择由A.E.Taylor提出，对<span class="math inline">\(x+iy \in E_{\mathbb{C}}\)</span>， <spanclass="math display">\[    \left\| x+iy \right\|_{\mathbb{C}}=\sup_{\theta \in [0,2\pi]}\left\|x\cos \theta- y\sin \theta \right\|_{}\]</span></p><p>不难验证，<span class="math inline">\(\left\| \cdot\right\|_{\mathbb{C}}\)</span> 给出了 <spanclass="math inline">\(E_{\mathbb{C}}\)</span> 上的范数，且满足对于任何<span class="math inline">\(x,y \in E\)</span>， <spanclass="math display">\[    \max \{\left\| x \right\|_{},\left\| y \right\|_{}\}\leqslant\left\| x+iy \right\|_{\mathbb{C}}\leqslant \left\| x\right\|_{}+\left\| y \right\|_{}. \tag{2}\]</span></p><p>如此定义的好处是，我们不会丢失 <span class="math inline">\(E\)</span>的完备性.</p><p><strong>命题1.2.1.</strong> 如果 <spanclass="math inline">\(E\)</span> 是一个配有范数 <spanclass="math inline">\(\left\| \cdot \right\|_{}\)</span>的Banach空间，则 <span class="math inline">\(E_{\mathbb{C}}\)</span>是一个配有范数 <span class="math inline">\(\left\| \cdot\right\|_{\mathbb{C}}\)</span> 的Banach空间.</p><p><strong>证明</strong> 任取 <spanclass="math inline">\(E_{\mathbb{C}}\)</span> 中的Cauchy列 <spanclass="math inline">\(\{x_{n}+iy_{n}\}\)</span>，由（2）知 <spanclass="math inline">\(\{x_n\}\)</span> 和 <spanclass="math inline">\(\{y_n\}\)</span> 都是 <spanclass="math inline">\(E\)</span> 中的Cauchy列. 由 <spanclass="math inline">\(E\)</span> 的完备性知，存在 <spanclass="math inline">\(x \in E\)</span>，<span class="math inline">\(y\in E\)</span>，使得 <span class="math inline">\(x_n \tox\)</span>，<span class="math inline">\(y_n\to y\)</span>. 故 <spanclass="math inline">\(x_n+iy_n \to x+iy\)</span>，从而 <spanclass="math inline">\(E_{\mathbb{C}}\)</span> 完备.</p><p>另一方面，我们可以定义算子的复化. 给定实线性空间 <spanclass="math inline">\(E\)</span> 上的线性算子 <spanclass="math inline">\(T\colon E \to E\)</span>，定义 <spanclass="math inline">\(T\)</span> 的复化为 <span class="math display">\[    \begin{aligned}        T_{\mathbb{C}}\colon E_{\mathbb{C}} &amp;\to E_{\mathbb{C}}\\        x+iy&amp;\mapsto Tx+iTy.    \end{aligned}\]</span> 不难验证 <span class="math inline">\(T_{\mathbb{C}}\)</span>是 <span class="math inline">\(E_{\mathbb{C}}\)</span> 上的线性算子.一个不平凡的结果是，复化保持算子范数.</p><p><strong>引理1.2.2.</strong> 设 <span class="math inline">\(E\)</span>是一个实Banach空间. 如果 <span class="math inline">\(T\colon E \toE\)</span> 是一个有界算子，则 <spanclass="math inline">\(T_{\mathbb{C}}\colon E_{\mathbb{C}} \toE_{\mathbb{C}}\)</span> 是一个有界算子且 <spanclass="math inline">\(\left\| T_{\mathbb{C}}\right\|_{\mathbb{C}}=\left\| T \right\|_{}\)</span>.</p><p><strong>证明</strong> 对任意 <span class="math inline">\(x+iy \inE_{\mathbb{C}}\)</span>，我们有 <span class="math display">\[    \begin{aligned}        \left\| T_{\mathbb{C}}(x+iy) \right\|_{\mathbb{C}} &amp;=\left\| Tx+iTy \right\|_{\mathbb{C}}\\        &amp;=\sup_{\theta \in [0,2\pi]} \left\| T_x \cos \theta-Ty \sin\theta \right\|_{} \\        &amp;=\sup_{\theta \in [0,2\pi]} \left\| T(x \cos \theta-y\sint\eta) \right\|_{} \\        &amp; \leqslant  \left\| T \right\|_{} \sup_{\theta \in[0,2\pi]} \left\| x\cos \theta-y\sin \theta \right\|_{} \\        &amp;= \left\| T \right\|_{} \left\| x+iy \right\|_{\mathbb{C}},    \end{aligned}\]</span></p><p>故我们证明了 <span class="math inline">\(\left\| T_{\mathbb{C}}\right\|_{\mathbb{C}}\leqslant \left\| T \right\|_{}\)</span>.进一步，注意到对于任意 <span class="math inline">\(x \in E\)</span>我们有 <span class="math inline">\(\left\| x\right\|_{\mathbb{C}}=\left\| x \right\|_{}\)</span>，于是有 <spanclass="math display">\[    \left\| T_{\mathbb{C}} \right\|_{\mathbb{C}}\geqslant \sup _{x\neq0} \frac{\left\| T_{\mathbb{C}}(x+i 0) \right\|_{\mathbb{C}}}{\left\|x+i0 \right\|_{\mathbb{C}}}=\sup_{x\neq 0} \frac{\left\| Tx\right\|_{\mathbb{C}}}{\left\| x \right\|_{\mathbb{C}}}=\sup_{x\neq0}\frac{\left\| Tx \right\|_{}}{\left\| x \right\|_{}}=\left\| T\right\|_{},\]</span></p><p>故引理成立.</p><p><strong>推论1.2.3.</strong> 设 <span class="math inline">\(E\)</span>是一个实Banach空间. 映射 <span class="math inline">\(\mathscr{L}(E)\ni T\mapsto T_{\mathbb{C}} \in \mathscr{L}(E_{\mathbb{C}})\)</span>是连续的.</p><p>最后，对于给定的实线性空间 <span class="math inline">\(E\)</span>上的线性算子 <span class="math inline">\(T\colon E\toE\)</span>，定义它的谱 <span class="math inline">\(\sigma(T)\)</span>为它的复化的谱，即 <span class="math display">\[    \sigma(T)=\sigma(T_{\mathbb{C}}).\]</span></p><h3 id="谱分解">谱分解</h3><p>本节我们说明如果对复Banach空间上某算子的谱作分划，那么存在空间本身的分划与谱的分划对应.具体的，我们有</p><p><strong>定理1.3.1.</strong>（谱分解）设 <spanclass="math inline">\(E\)</span> 是一个复Banach空间，<spanclass="math inline">\(T \in \mathscr{L}(E)\)</span>. 假设存在分划 <spanclass="math inline">\(\sigma(T)=\sigma_1 \cup \sigma_2\)</span>，其中<span class="math inline">\(\sigma_1,\sigma_2\)</span>是不相交的紧集，那么存在闭子空间 <spanclass="math inline">\(E_1,E_2\)</span> 使得： - <spanclass="math inline">\(E=E_1 \oplus E_2\)</span>； - <spanclass="math inline">\(T(E_1) \subset E_1\)</span> 且 $T(E_2) E_2 $； -<span class="math inline">\(\sigma(T|_{E_1})=\sigma_1\)</span> 且 <spanclass="math inline">\(\sigma(T|_{E_2})=\sigma_2\)</span>.</p><p><strong>命题1.3.2</strong> 设 <span class="math inline">\(E\)</span>是一个复Banach空间，配有范数 <span class="math inline">\(\left\| \cdot\right\|_{}\)</span>. 对于 <span class="math inline">\(T \in\mathscr{L}(E)\)</span>，下述条件等价： - <spanclass="math inline">\(T\)</span> 的谱半径严格小于1； - 存在与 <spanclass="math inline">\(\left\| \cdot \right\|_{}\)</span> 等价的范数 $$使得 <span class="math inline">\(\lvert T \rvert &lt;1\)</span>； - 存在<span class="math inline">\(C&gt;0\)</span> 和 <spanclass="math inline">\(0&lt;\lambda&lt;1\)</span> 使得对于任意 <spanclass="math inline">\(x \in E\)</span>，<span class="math inline">\(n\in \mathbb{N}\)</span>，有 <span class="math inline">\(\left\| T^{n}x\right\|_{}\leqslant C \lambda^{n}\left\| x \right\|_{}\)</span>.</p><h3 id="双曲同构">双曲同构</h3><p>给定线性空间 <span class="math inline">\(E\)</span>，令 <spanclass="math inline">\(GL(E)\)</span> 是 <spanclass="math inline">\(\mathscr{L}(E)\)</span> 中所有可逆元所组成的集合.注意如果 <span class="math inline">\(E\)</span>是Banach空间，那么由开映射定理，若 <span class="math inline">\(A \inGL(E)\)</span>，则 <span class="math inline">\(A ^{-1} \inGL(E)\)</span>.</p><p><strong>定义1.4.1.</strong> 如果算子 <span class="math inline">\(A\in GL(E)\)</span> 的谱都不在单位圆 $S^{1} $上，则称 <spanclass="math inline">\(A\)</span> 是双曲的. 记 <spanclass="math inline">\(H(E)=\{A \in GL(E)\colon A\text{是双曲的}\}\)</span>.</p><p><strong>命题1.4.2.</strong> 设 <span class="math inline">\(E\)</span>是一个Banach空间，则 <span class="math inline">\(H(E)\)</span> 在 <spanclass="math inline">\(GL(E)\)</span> 中是开的.</p><p><strong>证明</strong> 先假设 <span class="math inline">\(E\)</span>是一个复Banach空间. 给定 <span class="math inline">\(A \inH(E)\)</span>，我们有 $S^{1} (A) $. 考虑 <span class="math display">\[    M=\max_{\lambda \in  S^{1}} \left\| (\lambda I-A)^{-1} \right\|_{}\]</span></p><p>则当 <span class="math inline">\(B \in GL(E)\)</span>，<spanclass="math inline">\(\left\| B-A \right\|_{}&lt;M ^{-1}\)</span>时，对任意 <span class="math inline">\(\lambda \in S^{1}\)</span>，<span class="math display">\[    \lambda I-B=(\lambda I-A)[I+(\lambda I-A)^{-1}(A-B)].\]</span></p><p>由于 <span class="math inline">\(\left\| (\lambda I-A) ^{-1}(A-B)\right\|_{}&lt;1\)</span>，故 <span class="math inline">\(\lambdaI-B\)</span> 可逆. 故 <span class="math inline">\(B \inH(E)\)</span>，从而 <span class="math inline">\(H(E)\)</span> 在 <spanclass="math inline">\(GL(E)\)</span> 中开.</p><p>再考虑 <span class="math inline">\(E\)</span> 是实空间的情况.由定义，<spanclass="math inline">\(\sigma(A)=\sigma(A_{\mathbb{C}})\)</span>，故<span class="math inline">\(A \in GL(E)\)</span> 是双曲的当且仅当 <spanclass="math inline">\(A_{\mathbb{C}}\in GL(E_{\mathbb{C}})\)</span>是双曲的. 由 <span class="math inline">\(A \mapstoA_{\mathbb{C}}\)</span> 连续和 <spanclass="math inline">\(H(E_{\mathbb{C}})\)</span> 在 <spanclass="math inline">\(GL(E_{\mathbb{C}})\)</span> 中开知 <spanclass="math inline">\(H(E)\)</span> 在 <spanclass="math inline">\(GL(E)\)</span> 中开.</p><p><strong>定义1.4.3.</strong> 定义 <span class="math inline">\(A \inGL(E)\)</span> 的稳定子空间为 <span class="math display">\[    E^{s}=\{x \in E \colon \lim_{n \to +\infty}A^{n}x=0\}\]</span></p><p><span class="math inline">\(A\)</span> 的不稳定子空间为 <spanclass="math display">\[    E^{u}=\{x \in E \colon \lim_{n \to +\infty} A^{-n}x=0\}.\]</span></p><p>容易验证 <span class="math inline">\(E^{s}\)</span> 和 <spanclass="math inline">\(E^{u}\)</span> 都是 <spanclass="math inline">\(E\)</span> 的线性子空间且具有不变性质，即 <spanclass="math display">\[    A(E^{s})=E^{s}\ \text{且} \ A(E^{u})=E^{u}.\]</span></p><p>并且存在两个常数 <span class="math inline">\(C\geqslant 1\)</span> 和<span class="math inline">\(0&lt;\lambda&lt;1\)</span>，使得 <spanclass="math display">\[    \left\| A^{n}x \right\|_{}\leqslant C \lambda^{n}\left\| x\right\|_{}, \quad \forall x \in E^{s}, n\geqslant 0,\]</span></p><p><span class="math display">\[    \left\| A^{-n}x \right\|_{}\leqslant C \lambda^{n}\left\| x\right\|_{}, \quad \forall x \in E^{u}, n\geqslant 0.\]</span></p><p>这时称 <span class="math inline">\(E^{s}\)</span> 为 <spanclass="math inline">\(A\)</span> （关于范数 <spanclass="math inline">\(\left\| \cdot\right\|_{}\)</span>）的压缩子空间，<spanclass="math inline">\(E^{u}\)</span> 为 <spanclass="math inline">\(A\)</span> （关于范数 <spanclass="math inline">\(\left\| \cdot \right\|_{}\)</span>）的扩张子空间.称 <span class="math inline">\(\dim E^{s}\)</span> 为 <spanclass="math inline">\(A\)</span> 的指标. 当 <spanclass="math inline">\(E\)</span> 为有限维欧氏空间时，由范数等价性，<spanclass="math inline">\(A\)</span> 的双曲性不依赖于范数的选择. 但当 <spanclass="math inline">\(E\)</span>是一般的Banach空间时，范数的选取就至关重要.</p><p>下面这个结果说明当同构为双曲时，稳定空间和不稳定空间是互补的.双曲的条件是至关重要的，否则即使是有限维空间，该结果也不一定对.</p><p><strong>定理1.4.4.</strong> 设 <span class="math inline">\(E\)</span>是一个Banach空间，<span class="math inline">\(A \in H(E)\)</span>，则<span class="math inline">\(E=E^{s} \oplus E^{u}\)</span>.</p><p><strong>证明</strong> 先假设 <span class="math inline">\(E\)</span>是一个复Banach空间. 谱分解定理告诉我们，存在 <spanclass="math inline">\(E\)</span> 的分划 <spanclass="math inline">\(E=E_1 \oplus E_2\)</span>，其中 <spanclass="math inline">\(E_1\)</span> 和 <spanclass="math inline">\(E_2\)</span> 是 <spanclass="math inline">\(E\)</span> 的闭子空间，在 <spanclass="math inline">\(A\)</span> 作用下不变，满足 <spanclass="math display">\[    \sigma(A|_{E_1})=\sigma(A) \cap \{\lvert z \rvert &lt;1\}\ \text{且}\ \sigma(A|_{E_2})=\sigma(A) \cap \{\lvert z \rvert &gt;1\}. \tag{3}\]</span></p><p>我们将证明 <span class="math inline">\(E^{s}=E_1\)</span> 且 <spanclass="math inline">\(E^{u}=E_2\)</span>. 注意（3）表明 <spanclass="math inline">\(A|_{E_1}\)</span> 的谱半径 <spanclass="math inline">\(r(A|_{E_1})\)</span> 满足 <spanclass="math display">\[    r(A|_{E_1})&lt;1, \tag{4}\]</span></p><p>由命题1.3.2知</p><h2 id="hartman-grobman-定理的证明">Hartman-Grobman 定理的证明</h2><p>设 <span class="math inline">\(E\)</span> 是一个Banach空间，<spanclass="math inline">\(L\colon E\to E\)</span> 是一个给定的双曲同构.那么我们有 <span class="math inline">\(E=E^{u}\oplusE^{s}\)</span>，<span class="math inline">\(E^{u}\)</span> 和 <spanclass="math inline">\(E^{s}\)</span> 是 <spanclass="math inline">\(E\)</span> 的关于 <spanclass="math inline">\(L\)</span> 的不变子空间. <spanclass="math inline">\(L_{u}=L|_{E^{u}}\)</span> 是一个扩张，<spanclass="math inline">\(L_{s}=L|_{E^{s}}\)</span> 是一个收缩：<spanclass="math inline">\(\left\| L_{u}^{-1} \right\|_{}&lt;1\)</span> 且<span class="math inline">\(\left\| L_{s} \right\|_{}&lt;1\)</span>.如果一个 <span class="math inline">\(E\)</span>上线性同构的谱与单位圆不交，不难得到该线性同构对于某个 <spanclass="math inline">\(E\)</span> 上的范数是双曲的.</p><p>在后续部分中，我们令 <span class="math inline">\(a=\max (\left\|L_u^{-1} \right\|_{},\left\| L_{s} \right\|_{})&lt;1\)</span>. <spanclass="math inline">\(a\)</span> 称为 <spanclass="math inline">\(L\)</span> 关于范数 <spanclass="math inline">\(\left\| \cdot \right\|_{}\)</span> 的双曲度. <spanclass="math inline">\(E\)</span> 配有范数 <spanclass="math inline">\(\lvert x+y \rvert =\max(\lvert x \rvert ,\lvert y\rvert )\)</span> （<span class="math inline">\(x\in E^{u}, y\inE^{s}\)</span>）. 对任意 <span class="math inline">\(\mu&gt;0\)</span>，我们定义 <span class="math display">\[    C_{*}^{0}(E,E)=\{\text{ $E$到 $E$ 一致有界，一致连续的映射}\}\]</span></p><p><span class="math display">\[    \mathscr{L}_{\mu}(L)=\{\Lambda=L+\lambda\colon \lambda \inC_{*}^{0}(E,E) \text{是Lipschitz的，有界 $\mu$，有$\leqslant\mu$的Lipschitz常数}\}\]</span></p><p><span class="math display">\[    \mathscr{H}=\{h=\mathbf{1}+g\colon g \in C_{*}^{0}(E,E)\}\]</span></p><p>这里 <span class="math inline">\(\mathbf{1}\)</span> 是 <spanclass="math inline">\(E\)</span> 上的恒同映射. 在 <spanclass="math inline">\(C_{*}^{0}(E,E)\)</span> 上赋 <spanclass="math inline">\(C^{0}\)</span> 范数 <span class="math display">\[    \left\| \phi \right\|_{}=\sup_{x \in E} \lvert \phi(x) \rvert .\]</span></p><p>使其成为一个Banach空间，也使得 <spanclass="math inline">\(\mathscr{L}_{\mu}(L)\)</span>，<spanclass="math inline">\(\mathscr{H}\)</span> 成为完备的度量空间.</p><h3 id="映射的-hartman-grobman-定理">映射的 Hartman-Grobman 定理</h3><p>如果 <span class="math inline">\(\mu\)</span> 足够小，那么对于每个<span class="math inline">\(\Lambda \in\mathscr{L}_{\mu}(L)\)</span>，存在唯一的 <spanclass="math inline">\(h=h_{\Lambda} \in \mathscr{H}\)</span> 使得 <spanclass="math inline">\(h\Lambda=Lh\)</span>. 进一步的，<spanclass="math inline">\(h_{\Lambda}\)</span> 是一个关于 <spanclass="math inline">\(\Lambda \in \mathscr{L}_{\mu}(L)\)</span>连续依赖的同胚.</p><p>特别的，这说明所有 <span class="math inline">\(\Lambda \in\mathscr{L}_{\mu}(L)\)</span> 都是同胚.</p><p><strong>引理2.1.1.</strong> 如果 <spanclass="math inline">\(\mu\)</span> 足够小，那么每个 <spanclass="math inline">\(\Lambda \in \mathscr{L}_{\mu}(L)\)</span>是一个Lipschitz连续的同胚. 其逆也是Lipschitz连续的.</p><p><strong>引理2.1.2</strong> 设 <span class="math inline">\(P\)</span>是一个拓扑空间，<span class="math inline">\(Y\)</span>是一个完备度量空间，<span class="math inline">\(F\colon P \times Y \toY\)</span> 是连续的. 假设每个 <span class="math inline">\(F_p=F(p,\cdot)\colon Y\to Y\)</span> 是一个具有收缩系数 <spanclass="math inline">\(k_{p}&lt;1\)</span> 的收缩. 如果 <spanclass="math inline">\(k_p\)</span> 有一个严格小于1的上界，那么 <spanclass="math inline">\(F_p\)</span> 的唯一不动点关于 <spanclass="math inline">\(p\)</span> 连续依赖.</p><p><strong>映射的 Hartman-Grobman 定理的证明</strong>我们证明一个加强的命题： 对每一对 <spanclass="math inline">\(\Lambda,\Lambda&#39;\in\mathscr{L}_{\mu}(L)\)</span>，存在一个唯一的 <spanclass="math inline">\(h \in \mathscr{H}\)</span> 使得 <spanclass="math inline">\(h\Lambda=\Lambda&#39;h\)</span>. <spanclass="math inline">\(h\)</span> 是一个关于 <spanclass="math inline">\(\Lambda,\Lambda&#39;\)</span> 连续依赖的同胚.</p><p>解方程 <span class="math inline">\(h\Lambda=\Lambda&#39;h\)</span>等价于求一个唯一的 <span class="math inline">\(g \inC_{*}^{0}(E,E)\)</span>，使得 <span class="math display">\[    (\mathbf{1}+g)(L+\lambda)=(L+\lambda&#39;)(\mathbf{1}+g),\]</span></p><p>这等价于 <span class="math display">\[    \lambda+g\Lambda=Lg+\lambda&#39;(\mathbf{1}+g), \tag{5}\]</span></p><p>或由引理2.1.1保证 <span class="math inline">\(\Lambda ^{-1}\)</span>存在的情况下，<spanclass="math inline">\(g=[Lg+\lambda&#39;(\mathbf{1}+g)-\lambda]\Lambda^{-1}\)</span>. 对于 <span class="math inline">\(\forall \phi \inC_{*}^{0}(E,E)\)</span>，定义 <span class="math inline">\(\phi_s=\pi_s\circ\phi\)</span>，<span class="math inline">\(\phi_u=\pi_u \circ\phi\)</span>，其中 <span class="math inline">\(\pi_s\)</span> 和 <spanclass="math inline">\(\pi_u\)</span> 是投影算子. 那么就有 <spanclass="math display">\[    g_{u}=[L_u g_u+\lambda_u&#39;(\mathbf{1}+g)-\lambda_u]\Lambda^{-1}\tag{6a}\]</span></p><p><span class="math display">\[    g_{s}=[L_s g_s+\lambda_s&#39;(\mathbf{1}+g)-\lambda_s]\Lambda^{-1}.\tag{6b}\]</span> 另一方面，（5）也等价于 <span class="math inline">\(g=L^{-1}[g\Lambda+\lambda-\lambda&#39;(\mathbf{1}+g)]\)</span>，故 <spanclass="math display">\[    g_u=L_u^{-1}[g_u\Lambda+\lambda_u-\lambda_u&#39;(\mathbf{1}+g)]\tag{7a}\]</span></p><p><span class="math display">\[    g_s=L_s^{-1}[g_s\Lambda+\lambda_s-\lambda_s&#39;(\mathbf{1}+g)].\tag{7b}\]</span> （6a,b）和（7a,b）中任意两个都独立. 我们考虑（6b,7a）.对于充分小的 <spanclass="math inline">\(\mu&gt;0\)</span>，（6b,7a）定义了一个映射 <spanclass="math inline">\(K\colon C_{*}^{0}(E,E)\toC_{*}^{0}(E,E)\)</span>， <span class="math display">\[    g=(g_u,g_s)\mapsto(L_u^{-1}[g_u\Lambda+\lambda_u-\lambda_u&#39;(\mathbf{1}+g)],[L_sg_s+\lambda_s&#39;(\mathbf{1}+g)-\lambda_s]\Lambda ^{-1})\]</span></p><p>由于 <span class="math inline">\(\lambda,\lambda&#39;,g\)</span>都属于 <span class="math inline">\(C_{*}^{0}(E,E)\)</span>，易见 <spanclass="math inline">\(K(g)\)</span> 确实属于 <spanclass="math inline">\(C_{*}^{0}(E,E)\)</span>. 下面验证 <spanclass="math inline">\(K\)</span> 是一个压缩映射：对所有 <spanclass="math inline">\(g,g&#39; \in C_{*}^{0}(E,E)\)</span>，我们有 <spanclass="math display">\[    \begin{aligned}        &amp;\left\| K_s(g)-K_s(g&#39;) \right\|_{} \\        &amp;=\left\|[L_s(g_s-g_s&#39;)+\lambda_s&#39;(g-g&#39;)]\Lambda ^{-1} \right\|_{} \\        &amp;=\sup _{x \in E} \lvert[L_s(g_s-g_s&#39;)+\lambda_s&#39;(g-g&#39;)]\Lambda ^{-1}(x) \rvert \\        &amp;=\sup _{y \in E} \lvert(L_s(g_s-g_s&#39;)+\lambda_s&#39;(g-g&#39;))(y) \rvert \\        &amp;\leqslant \sup_{y \in E}(a\cdot \lvert g_s(y)-g_s&#39;(y)\rvert +\mu\cdot \lvert g(y)-g&#39;(y) \rvert ) \\        &amp;\leqslant (a+\mu) \left\| g-g&#39; \right\|_{}.    \end{aligned}\]</span></p><p>类似地， <span class="math display">\[    \begin{aligned}        \left\| K_u(g)-K_u(g&#39;) \right\|_{} &amp;\leqslant(a+a\mu)\left\| g-g&#39; \right\|_{} \\        &amp;\leqslant (a+\mu)\left\| g-g&#39; \right\|_{}.    \end{aligned}\]</span></p><p>故 <span class="math inline">\(K\)</span>确实为压缩映射，从而有唯一不动点 <spanclass="math inline">\(g=g_{\Lambda,\Lambda&#39;} \inC_{*}^{0}(E,E)\)</span>，且关于 <spanclass="math inline">\(\Lambda,\Lambda&#39; \in\mathscr{L}_{\mu}(L)\)</span> 连续依赖，从而 <spanclass="math inline">\(h_{\Lambda,\Lambda&#39;}=\mathbf{1}+g_{\Lambda,\Lambda&#39;}\)</span>也是如此.</p><p>注意原命题为 <span class="math display">\[    h_{\Lambda,\Lambda&#39;}\Lambda=\Lambda&#39;h_{\Lambda,\Lambda&#39;}.\tag{8}\]</span></p><p>将原命题中的 <span class="math inline">\(\Lambda\)</span> 和 <spanclass="math inline">\(\Lambda&#39;\)</span> 对换，得 <spanclass="math display">\[    h_{\Lambda&#39;,\Lambda}\Lambda&#39;=\Lambdah_{\Lambda&#39;,\Lambda}. \tag{9}\]</span></p><p>将（8）和（9）合并，得 <span class="math display">\[    h_{\Lambda&#39;,\Lambda}h_{\Lambda,\Lambda&#39;}\Lambda=\Lambdah_{\Lambda&#39;,\Lambda}h_{\Lambda,\Lambda&#39;}.\]</span></p><p>由于 <span class="math inline">\(\mathbf{1}\)</span> 是 <spanclass="math inline">\(\Lambda\)</span> 在 <spanclass="math inline">\(\mathbf{1}+C_{*}^{0}(E,E)\)</span>中的一个自共轭，由自共轭的唯一性 <span class="math display">\[    h_{\Lambda,\Lambda&#39;}h_{\Lambda&#39;,\Lambda}=h_{\Lambda&#39;,\Lambda}h_{\Lambda,\Lambda&#39;}=\mathbf{1}.\]</span></p><p>这说明 <span class="math inline">\(h_{\Lambda,\Lambda&#39;}\)</span>是同胚，证毕.</p><h3 id="流的-hartman-定理">流的 Hartman 定理</h3><p>设 <span class="math inline">\(\Lambda=L+\lambda\)</span> 是 <spanclass="math inline">\(E\)</span> 上的一个向量场，$^{L} $ 是关于直和分解<span class="math inline">\(E=E^{u}\oplus E^{s}\)</span>的双曲同构，<span class="math inline">\(\nu\)</span> 足够小，那么对每个<span class="math inline">\(\Lambda \in \mathscr{L}_{\mu}(L)\)</span>存在唯一的 <span class="math inline">\(H=H_{\Lambda}\in\mathscr{H}\)</span> 使得对于 <span class="math inline">\(x \in E, t\in\mathbb{R}\)</span>，有 <span class="math inline">\(H\phi_{\Lambda}(t,x)\equiv \phi_{L}(t,Hx)\)</span> 成立，其中 <spanclass="math inline">\(\phi_{L}, \phi_{\Lambda}\)</span> 是 <spanclass="math inline">\(L\)</span>- 和 <spanclass="math inline">\(\Lambda\)</span>- 流. <spanclass="math inline">\(H_{\Lambda}\)</span> 是一个关于 <spanclass="math inline">\(\Lambda \in \mathscr{L}_{\nu}(L)\)</span>连续依赖的同胚.</p><p>注意：这里 $^{L} $ 是双曲同构，而不是 <spanclass="math inline">\(L\)</span> 是双曲同构.</p><p><strong>证明</strong> 考虑映射 <spanclass="math inline">\(\tilde{\phi}_{L}=\phi_{L}(1,\cdot)=\mathrm{e}^{L}\)</span> 和 <spanclass="math inline">\(\tilde{\phi}_{\Lambda}=\phi_{\Lambda}(1,\cdot)\)</span>. 容易验证映射 <span class="math inline">\(\Lambda \mapsto\tilde{\phi}_{\Lambda}\)</span> 是连续的，因此对于足够小的 <spanclass="math inline">\(\nu&gt;0\)</span>，由映射的 Hartman定理知，存在足够小的 <span class="math inline">\(\mu\)</span>，使得<span class="math inline">\(\Lambda \in \mathscr{L}_{\nu}(L) \Rightarrow\tilde{\phi}_{\Lambda}\in \mathscr{L}_{\mu}(\mathrm{e}^{L} )\)</span>.相应地存在唯一的 <span class="math inline">\(h \in \mathscr{H}\)</span>使得 <span class="math inline">\(\tilde{\phi}_{L}h=h\tilde{\phi}_{\Lambda}\)</span>.</p><p>我们断言 <span class="math inline">\(h\)</span> 满足 <spanclass="math inline">\(h \phi_{\Lambda}(t,x)\equiv\phi_{L}(t,hx)\)</span> 从而 <span class="math inline">\(h=H\)</span>满足定理. 这等价于证明 <span class="math display">\[\phi_{L}(t,h\phi_{\Lambda}(-t,\cdot ))\equiv h. \tag{10}\]</span></p><p>观察到 <spanclass="math inline">\(\phi_{L}(t,h\phi_{\Lambda}(-t,\cdot ))\in\mathscr{H}\)</span>，对 <spanclass="math inline">\(\tilde{\phi}_{L}()=()\tilde{\phi}_{\Lambda}\)</span>应用映射的 Hartman 定理的唯一性部分即得（10）成立. 任何 <spanclass="math inline">\(\phi_{L}\)</span> 与 <spanclass="math inline">\(\phi_{\Lambda}\)</span> 之间的等价关系当然也是<span class="math inline">\(\tilde{\phi}_{L}\)</span> 与 <spanclass="math inline">\(\tilde{\phi}_{\Lambda}\)</span> 之间的等价关系.因此 <span class="math inline">\(H=h\)</span> 唯一，是一个同胚，并且关于<span class="math inline">\(\Lambda \in \mathscr{L}_{\nu}(L)\)</span>连续依赖.</p><p>动力系统就是抽象的“流”，点的流动就形成了“轨道”. 流的 Hartman-Grobman定理就是判断非线性系统零解稳定性中用到的定理. 仍考虑系统（4），<spanclass="math inline">\(Df(\mathbf{0})\)</span> 满足 <spanclass="math inline">\(\mathrm{e}^{Df(\mathbf{0})}\)</span>是一个双曲线性同构. 固定 <spanclass="math inline">\(Df(\mathbf{0})\)</span>，在流的 Hartman-Grobman定理中确定相应的 <span class="math inline">\(\nu\)</span>，再取定理中的<span class="math inline">\(E\)</span> 为原点的一个充分小的领域 <spanclass="math inline">\(U\)</span>，使得在原点的邻域内，<spanclass="math inline">\(f-Df(\mathbf{0}) \in\mathscr{L}_{\nu}(L)\)</span>. Hartman-Grobman定理告诉我们存在唯一的同胚 <span class="math inline">\(H \in\mathscr{H}\)</span>，使得 <span class="math inline">\(H\phi_{\Lambda}(t,x)\equiv \phi_{L}(t,Hx)\)</span>成立，即系统（3）和其在原点的线性化系统（5）局部拓扑共轭.</p><p>最后，我们来说明 Hartman-Grobman 定理给出的拓扑共轭 <spanclass="math inline">\(h\)</span> 一般不能是 Lipschitz 同胚.</p><h4 id="一个例子">一个例子</h4><p>设 <span class="math inline">\(A_{\alpha}\colon \mathbb{R}\to\mathbb{R}\)</span> 表示线性映射 <span class="math display">\[    A_{\alpha}(x)=\alpha x.\]</span></p><p>若 <span class="math inline">\(0&lt;\alpha&lt;1,0&lt;\beta&lt;1\)</span>，令 <span class="math inline">\(h(x)=x^{\ln\beta/\ln \alpha},x\geqslant 0\)</span>，再对 <spanclass="math inline">\(h\)</span> 作奇延拓. 则 <spanclass="math inline">\(h(x)\)</span> 是一个 <spanclass="math inline">\(\mathbb{R}\)</span> 上的同胚，且 <spanclass="math inline">\(hA_{\alpha}=A_{\beta}h\)</span>. 然而，若 <spanclass="math inline">\(\alpha \neq \beta\)</span>，则不存在 Lipschitz同胚 <span class="math inline">\(h\colon \mathbb{R}\to\mathbb{R}\)</span> 满足 <span class="math inline">\(hA_{\alpha}=A_{\beta} h\)</span>.</p><p>事实上，假设存在这样的同胚 <span class="math inline">\(h\colon\mathbb{R}\to \mathbb{R}\)</span>. 如果 <spanclass="math inline">\(h(0)\neq 0\)</span>，则由 <spanclass="math inline">\(h(\alpha x)=\beta h(x)\)</span> 知 <spanclass="math inline">\(\beta=1\)</span>，再由 <spanclass="math inline">\(h\)</span> 是单射知 <spanclass="math inline">\(\alpha=1\)</span>. 这与 <spanclass="math inline">\(\alpha\neq \beta\)</span> 矛盾.</p><p>如果 <span class="math inline">\(h(0)=0\)</span>，显然 <spanclass="math inline">\(\alpha \beta\neq 0\)</span>. 有如下几式成立 -<spanclass="math inline">\(h(\alpha^{n}x)/(\alpha^{n}x)=(\beta^{n}h(x))/(\alpha^{n}x)\)</span>；- <span class="math inline">\(h ^{-1}(\beta^{n}y)/(\beta^{n}y)=(\alpha^{n}h ^{-1}(y))/(\beta^{n}y)\)</span>； -$h(<sup>{-n}x)/(</sup>{-n}x)=(<sup>{n}h</sup>{-1}(x))/(^{n}x) $； -<span class="math inline">\(h^{-1}(\beta^{-n}y)/(\beta^{-n}y)=(\beta^{n} h^{-1}(y))/(\alpha^{n}y)\)</span>.</p><p>分析原点附近性态可得 <span class="math inline">\(h\)</span> 和 <spanclass="math inline">\(h ^{-1}\)</span> 不能同时为 Lipschitz 的，即 <spanclass="math inline">\(h\)</span> 不能是Lipschitz 同胚.</p>]]></content>
    
    
    <categories>
      
      <category>analysis</category>
      
      <category>differential equation</category>
      
      <category>dynamical system</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (14)</title>
    <link href="/2022/12/11/Neuronal-Dynamics-14/"/>
    <url>/2022/12/11/Neuronal-Dynamics-14/</url>
    
    <content type="html"><![CDATA[<h1id="quasi-renewal-theory-and-the-integral-equation-approach">Quasi-RenewalTheory and the Integral-equation Approach</h1><p>The online version of this chapter:</p><hr /><p>Chapter 14 Quasi-Renewal Theory and the Integral-equation Approachhttps://neuronaldynamics.epfl.ch/online/Ch14.html</p><hr /><p>For neuron models that include biophysical phenomena such asrefractoriness and adaptation on multiple time-scales, the resultingPDEs are situated in more than 2D and therefore difficult to solveanalytically. We indicate an alternative to describing the populationactivity in networks of model neurons.</p><h2 id="population-activity-equations">Population activityequations</h2><h3 id="assumptions-of-time-dependent-renewal-theory">Assumptions oftime-dependent renewal theory</h3><p>The state of a neuron <span class="math inline">\(i\)</span> at time<span class="math inline">\(t\)</span> is described by - its last firingtime <span class="math inline">\(\hat{t}_i\)</span>; - the input <spanclass="math inline">\(I(t&#39;)\)</span> it received for times <spanclass="math inline">\(t&#39;&lt;t\)</span>; - the characteristics ofpotential noise sources, be it noise in the input, noise in the neuronalparameters, or noise in the output.</p><h3 id="integral-equations-for-non-adaptive-neurons">Integral equationsfor non-adaptive neurons</h3><p>The integral equation for activity dynamics with time-dependentrenewal theory states that <span class="math display">\[    A(t)=\int_{-\infty}^{t} P_{I}(t|\hat{t})A(\hat{t})\mathrm{d}\hat{t}. \tag{14.5}\]</span></p><p><span class="math inline">\(A(t)\)</span> on the LHS is the expectedactivity at time <span class="math inline">\(t\)</span> while <spanclass="math inline">\(A(\hat{t})\)</span> on the RHS is the observedactivity in the past. (14.5) becomes exact in the limit of <spanclass="math inline">\(N \to \infty\)</span>. Finite-size effects arediscussed later.</p><ul><li>(14.5) is linear in the variable <spanclass="math inline">\(A\)</span>. Instead of defining the activity by aspike count divided by <span class="math inline">\(N\)</span>, we couldhave chosen to work directly with the spike count per unit of time orany other normalization.</li><li>(14.5) is a highly nonlinear equation in the drive because thekernel <span class="math inline">\(P_{I}(t|\hat{t})\)</span> dependsnonlinearly on the input <span class="math inline">\(I(t)\)</span>.</li></ul><h3id="normalization-and-derivation-of-the-integral-equation">Normalizationand derivation of the integral equation</h3><p>Recap <span class="math display">\[    S_{I}(t|\hat{t})=1-\int_{\hat{t}}^{t} P_{I}(s|\hat{t}) \mathrm{d}s,\tag{14.6}\]</span></p><p>We now return to the homogeneous population of neurons in the limitof <span class="math inline">\(N \to \infty\)</span> and assume that thefiring of different neurons at time <spanclass="math inline">\(t\)</span> is independent, given that we know thehistory of each neuron. ('conditional independence')</p><p>Define the proportion of neurons at time <spanclass="math inline">\(t\)</span> which have fired their last spikebetween <span class="math inline">\(t_0\)</span> and <spanclass="math inline">\(t_1&lt;t\)</span> (and have not firied since) as<span class="math display">\[    \left\langle \frac{\text{number of neurons at $t$ with last spikein}\ [t_0,t_1]}{\text{total number of neurons}} \right\rangle=\int_{t_0}^{t_1} S_{I}(t|\hat{t})A(\hat{t}) \mathrm{d}\hat{t}.\tag{14.7}  \]</span></p><p>We use the fact that the total number of neurons remains constant.<span class="math display">\[    1=\int_{-\infty}^{t} S_{I}(t|\hat{t})A(\hat{t}) \mathrm{d}\hat{t},\tag{14.8}\]</span> The normalization of (14.8) must hold at arbitrary times <spanclass="math inline">\(t\)</span>.</p><p>Take the derivative of (14.8) w.r.t <spanclass="math inline">\(t\)</span>, <span class="math display">\[    0=S_{I}(t|t)A(t)+\int_{-\infty}^{t} \frac{\partialS_{I}(t|\hat{t})}{\partial t}A(\hat{t}) \mathrm{d}\hat{t}. \tag{14.9}\]</span></p><p>Use <span class="math inline">\(P_{I}(t|\hat{t})=-\frac{\partial}{\partial t}S_{I}(t|\hat{t})\)</span> and <spanclass="math inline">\(S_{I}(t|t)=1\)</span> and we yield (14.5).</p><h4id="example-absolute-refractoriness-and-the-wilson-cowan-integral-equation">Example:Absolute refractoriness and the Wilson-Cowan integral equation</h4><p>Consider a population of Poisson neurons with an absolute refractoryperiod <span class="math inline">\(\Delta^{abs}\)</span>.</p><p>The population activity of a homogeneous group of Poisson neuronswith absolute refractoriness is <span class="math display">\[    A(t)=f[h(t)] \left\{ 1-\int_{t-\Delta^{abs}}^{t} A(t&#39;)\mathrm{d}t&#39; \right\}. \tag{14.10}\]</span> where <span class="math inline">\(h(t)=\int_{0}^{\infty}\kappa(s)I(t-s) \mathrm{d}s\)</span>. <spanclass="math inline">\(f\)</span> is the stochastic intensity of aninhomogeneous Poisson process describing neurons in a homogeneouspopulation.</p><p>For constant input current, <span class="math inline">\(h(t)=h_0=I_0\int_{0}^{\infty} \kappa(s) \mathrm{d}s\)</span>. The populationactivity has a stationary solution <span class="math display">\[    A_0=\frac{f(h_0)}{1+\Delta^{abs}f(h_0)}=g(h_0). \tag{14.11}\]</span></p><h3 id="integral-equation-for-adaptive-neurons">Integral equation foradaptive neurons</h3><p>For an isolated adaptive neuron in the presence of noise, theprobability density of firing around time <spanclass="math inline">\(t\)</span> will depend on its past firing times<span class="math inline">\(\hat{t}_{n}&lt;\cdots&lt;\hat{t}_2&lt;\hat{t}_1=\hat{t}&lt;t\)</span> where <spanclass="math inline">\(\hat{t}=\hat{t}_1\)</span> denotes the most recentspike time.</p><p>In a population of neurons, we can approximate the past firing timesby the population activity <span class="math inline">\(A(t)\)</span>.Let <span class="math inline">\(P_{I,A}(t|\hat{t})\)</span> be theprobability of observing a spike at time <spanclass="math inline">\(t\)</span> given the last spike at time <spanclass="math inline">\(\hat{t}\)</span>, the input-current and theactivity history <span class="math inline">\(A(t)\)</span> until time<span class="math inline">\(t\)</span>, then <spanclass="math display">\[    A(t)=\int_{-\infty}^{t} P_{I,A}(t|\hat{t})A(\hat{t})\mathrm{d}\hat{t}. \tag{14.12}\]</span></p><h3 id="numerical-methods-for-integral-equations">Numerical methods forintegral equations</h3><p>We take as an example the quasi-renewal equivalent of (14.8) <spanclass="math display">\[    1=\int_{-\infty}^{t} S_{I,A}(t|\hat{t})A(\hat{t}) \mathrm{d}\hat{t}.\tag{14.13}\]</span></p><p>Let <span class="math inline">\(\tau_c\)</span> be a period time suchthat the survivor <spanclass="math inline">\(S_{I,A}(t|\hat{t-\tau_c})\)</span> is very small.Then <span class="math display">\[    1=\int_{t-\tau_c}^{t} S_{I,A}(t|\hat{t})A(\hat{t})\mathrm{d}\hat{t}. \tag{14.14}\]</span></p><p>Discretize the integral on small bins of size <spanclass="math inline">\(\Delta t\)</span>. Let <spanclass="math inline">\(\mathbf{m}^{(t)}\)</span> be the vector made ofthe fraction of neurons at <span class="math inline">\(t\)</span> withlast spike within <span class="math inline">\(\hat{t}\)</span> and <spanclass="math inline">\(\hat{t}+\Delta t\)</span>, which means that the<span class="math inline">\(k\)</span>th element is <spanclass="math inline">\(m_k^{(t)}=S(t|t-k\Delta t)A(t-k\Delta t)\Deltat\)</span>, <span class="math inline">\(m_0^{(t)}=A_t\Delta t\)</span>since <span class="math inline">\(S(t|t)=1\)</span>. Therefore <spanclass="math display">\[    A_t \Delta t=1-\sum_{k=1}^{K} m_k^{(t)}. \tag{14.15}\]</span></p><p>Because of <span class="math inline">\(S(t|\hat{t})=\exp[-\int_{\hat{t}}^{t} \rho(t&#39;|\hat{t}) \mathrm{d}t&#39;]=\exp[-\int_{t-\Delta t}^{t} \rho(t&#39;|\hat{t}) \mathrm{d}t&#39;]S(t-\Deltat|\hat{t})\)</span>, we find for sufficiently small <spanclass="math inline">\(\Delta t\)</span>, <span class="math display">\[    m_k^{(t)}=m_{k-1}^{(t-\Delta t)}\exp [-\rho(t|t-k\Delta t)\Deltat]  \quad\text{for}\ k\geqslant 1 \tag{14.16}\]</span></p><p>Note that <span class="math inline">\(m_k^{(t)}\)</span> and <spanclass="math inline">\(m_{k-1}^{(t-\Delta t)}\)</span> refer to the samegroup of neurons, i.e., those that have fired their last spike aroundtime <span class="math inline">\(\hat{t}=t-k\Delta t\)</span>. Together,(14.15) and (14.16) can be used to solve <spanclass="math inline">\(A_t\)</span> iteratively.</p><h2 id="recurrent-networks-and-interacting-populations">RecurrentNetworks and Interacting Populations</h2>]]></content>
    
    
    <categories>
      
      <category>神经动力学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>神经科学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>永久记录（七）</title>
    <link href="/2022/11/30/%E6%B0%B8%E4%B9%85%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%83%EF%BC%89/"/>
    <url>/2022/11/30/%E6%B0%B8%E4%B9%85%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%83%EF%BC%89/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>杂项</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随想</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (13)</title>
    <link href="/2022/10/24/Neuronal-Dynamics-13/"/>
    <url>/2022/10/24/Neuronal-Dynamics-13/</url>
    
    <content type="html"><![CDATA[<h1 id="continuity-equation-and-the-fokker-planck-approach">ContinuityEquation and the Fokker-Planck Approach</h1><p>The online version of this chapter:</p><hr /><p>Chapter 13 Continuity Equation and the Fokker-Planck Approachhttps://neuronaldynamics.epfl.ch/online/Ch13.html</p><hr /><p>In this chapter we present a formulation of population activityequations that can account for the temporal aspects of populationdynamics.</p><h2 id="continuity-equation">Continuity equation</h2><p>In this chapter and the next, the population activity is the expectedpopulation activity <span class="math inline">\(A(t)\equiv \langleA(t)\rangle\)</span>.</p><h3 id="distribution-of-membrane-potentials">Distribution of membranepotentials</h3><p>The aim of this section is to describe the evolution of the density<span class="math inline">\(p(u,t)\)</span> as a function of time. <spanclass="math display">\[    \lim_{N \to \infty}\left\{\frac{\text{neurons with }u_0&lt;u_i(t)\leqslant u_0+\Delta u}{N}\right\}=\int_{u_0}^{u_0+\Deltau} p(u,t) \mathrm{d}u, \tag{13.2}\]</span> <span class="math display">\[    \int_{-\infty}^{\theta_{reset}} p(u,t) \mathrm{d}u=1. \tag{13.3}\]</span></p><h3 id="flux-and-continuity-equation">Flux and continuity equation</h3><p>Consider the portion of neurons with a membrane potential between<span class="math inline">\(u_0\)</span> and <spanclass="math inline">\(u_1\)</span>. The <strong>flux</strong> <spanclass="math inline">\(J(u,t)\)</span> is the net fraction oftrajectories per unit time that crosses the value <spanclass="math inline">\(u\)</span>. A positive flux <spanclass="math inline">\(J(u,t)&gt;0\)</span> is defined as a flux towardincreasing values of <span class="math inline">\(u\)</span>.</p><p>We have the conservation law <span class="math display">\[    \frac{\partial }{\partial t}\int_{u_0}^{u_1} p(u&#39;,t)\mathrm{d}u&#39;=J(u_0,t)-J(u_1,t). \tag{13.5}\]</span> The continuity equation, <span class="math display">\[    \frac{\partial }{\partial t}p(u,t)=-\frac{\partial }{\partialu}J(u,t) \quad u \neq u_r, u\neq \theta_{reset}, \tag{13.6}\]</span></p><p>Since neurons that have fired start a new trajectory at <spanclass="math inline">\(u_r\)</span>, we have a 'source of newtrajectories' at <span class="math inline">\(u=u_r\)</span>, i.e., newtrajectories appear in the interval <spanclass="math inline">\([u_r-\varepsilon, u_r+\varepsilon]\)</span> thathave not entered the interval through one of the borders. Adding a term<span class="math inline">\(A(t)\delta(u-u_r)\)</span> on the RHS of(13.6) accounts for this source of trajectories. <spanclass="math display">\[    \frac{\partial }{\partial t}p(u,t)=-\frac{\partial }{\partialu}J(u,t)+A(t)\delta(u-u_r)-A(t)\delta(u-\theta_{reset}) \tag{13.7}\]</span> The density <span class="math inline">\(p(u,t)\)</span>vanishes for all values <spanclass="math inline">\(u&gt;\theta_{reset}\)</span>.</p><p>The population activity <span class="math inline">\(A(t)\)</span> isthe fraction of neurons that fire, <span class="math display">\[    A(t)=J(\theta_{reset},t). \tag{13.8}\]</span></p><h2 id="stochastic-spike-arrival">Stochastic spike arrival</h2><p>We consider the flux <span class="math inline">\(J(u,t)\)</span> in ahomogeneous population of integrate-and-fire neurons with voltageequation (13.1). An input spike at a synapse of type <spanclass="math inline">\(k\)</span> causes a jump of the membrane potentialby an amount <span class="math inline">\(w_k\)</span>. The effectivespike arrival rate (summed over all synapses of the same type <spanclass="math inline">\(k\)</span>) is denoted as <spanclass="math inline">\(\nu_k\)</span>.</p><p>A finite input current <spanclass="math inline">\(I^{ext}(t)\)</span> generates a smooth drift ofthe membrane potential trajectories. The flux <spanclass="math inline">\(J(u,t)\)</span> can be generated through a 'jump'or a 'drift' of trajectories. <span class="math display">\[    J(u_0,t)=J_{drift}(u_0,t)+J_{jump}(u_0,t), \tag{13.9}\]</span></p><h3id="jumps-of-membrane-potential-due-to-stochastic-spike-arrival">Jumpsof membrane potential due to stochastic spike arrival</h3><p><span class="math display">\[    J_{jump}(u_0,t)=\sum_{k}^{} \nu_k(t) \int_{u_0-w_k}^{u_0} p(u,t)\mathrm{d}u. \tag{13.10}\]</span></p><h3 id="drift-of-membrane-potential">Drift of membrane potential</h3><p><span class="math display">\[    J_{drift}(u_0,t)=\frac{\mathrm{d}u}{\mathrm{d}t}\bigg|_{u_0}p(u_0,t)=\frac{1}{\tau_m}[f(u_0)+RI^{ext}(t)]p(u_0,t),\tag{13.11}\]</span></p><p>Synaptic <span class="math inline">\(\delta\)</span>-curent pulsescause a jump of the membrane potential and therefore contribute only to<span class="math inline">\(J_{jump}\)</span></p><div class="note note-info">            <p>(13.7) 和 (13.11) 好像都表明 <spanclass="math inline">\(p(u,t)\)</span> 关于 <spanclass="math inline">\(t\)</span> 都是可导的，从而必须是连续的，但是按<span class="math inline">\(A(t)\)</span> 的比例出现spike时，postneuron的电位分布密度应该是会突变的？但是 <spanclass="math inline">\(p(u,t)\)</span> 关于 <spanclass="math inline">\(t\)</span> 不能突变也与 (13.11) 中 <spanclass="math inline">\(J_{drift}\)</span> 是平滑的相洽。</p>          </div><h3 id="population-activity">Population activity</h3><p>A positive flux through the threshold <spanclass="math inline">\(\theta_{reset}\)</span> yields the populationactivity <span class="math inline">\(A(t)\)</span>. The total flux atthe threshold is <span class="math display">\[    A(t)=\frac{1}{\tau_m}[f(\theta_{reset}+RI^{ext}(t)]p(\theta_{reset},t)+\sum_{k}^{}\nu_k \int_{\theta_{reset}-w_k}^{\theta_{reset}} p(u,t) \mathrm{d}u.\tag{13.13}\]</span> Since the probability density vanishes for <spanclass="math inline">\(u&gt;\theta_{reset}\)</span>, the sum over thesynapses <span class="math inline">\(k\)</span> can be restricted to allexcitatory synapses.</p><p><span class="math display">\[\begin{aligned}    \frac{\partial }{\partialt}p(u,t)&amp;=-\frac{1}{\tau_m}\frac{\partial }{\partialu}{[f(u)+RI^{ext}(t)]p(u,t)}\\    &amp;+\sum_{k}^{} \nu_k(t)[p(u-w_k,t)-p(u,t)] \\    &amp;+A(t)\delta(u-u_r)-A(t)\delta(u-\theta_{reset}).\end{aligned}\]</span> (13.14)</p><p>we have <span class="math inline">\(p(u,t)=0\)</span> for <spanclass="math inline">\(u&gt;\theta_{reset}\)</span>.</p><h2 id="fokker-planck-equation">Fokker-Planck equation</h2><p>We expand the RHS of (13.14) into a Taylor series up to second orderin <span class="math inline">\(w_k\)</span>. <spanclass="math display">\[    \begin{aligned}        \tau_m \frac{\partial }{\partial t}p(u,t)=&amp;-\frac{\partial}{\partial u} \left\{ \left[ f(u)+RI^{ext}(t)+\tau_m\sum_{k}^{}\nu_k(t)w_k\right]p(u,t) \right\} \\        &amp;+\frac{1}{2}\left[ \tau_m \sum_{k}^{}\nu_k(t)w_k^{2}\right] \frac{\partial ^{2}}{\partial u^{2}}p(u,t) \\        &amp;+ \tau_m A(t)\delta(u-u_r)-\tau_mA(t)\delta(u-\theta_{reset})+o(w_k^{3})    \end{aligned}\]</span> (13.16)</p><p>The term with the second derivative describes a 'diffusion' in termsof the membrane potential.</p><p>We define the total 'drive' in voltage units as <spanclass="math display">\[    \mu(t)=RI^{ext}(t)+\tau_m \sum_{k}^{} \nu_k(t)w_k \tag{13.17}\]</span> and the amount of diffusive noise (again in voltage units) as<span class="math display">\[    \sigma^{2}(t)=\tau_m \sum_{k}^{} \nu_k(t)w_k^{2}. \tag{13.18}\]</span></p><p>The firing threshold acts as an absorbing boundary so that thedensity at threshold vanishes <span class="math display">\[    p(\theta_{reset},t)=0. \tag{13.19}\]</span></p><p>We expand (13.13) in <span class="math inline">\(w_k\)</span> about<span class="math inline">\(u=\theta_{reset}\)</span> and obtain <spanclass="math display">\[    A(t)=-\frac{\sigma^{2}(t)}{2\tau_m}\frac{\partial p(u,t)}{\partialu} \bigg |_{u=\theta_{reset}}, \tag{13.20}\]</span></p><h4 id="example-flux-in-the-diffusion-limit">Example: Flux in thediffusion limit</h4><p>Compare (13.7) with (13.16), we can identify the flux caused bystochastic spike arrival and external current in the diffusion limit<span class="math display">\[    J^{diff}(u,t)=\frac{f(u)+\mu(t)}{\tau_m}p(u,t)-\frac{1}{2}\frac{\sigma^{2}(t)}{\tau_m}\frac{\partial}{\partial u}p(u,t). \tag{13.21}\]</span></p><p>Stochastic spike arrival contributes to the mean drive <spanclass="math inline">\(\mu(t)=RI^{ext}(t)+\tau_m\sum_{k}^{}\nu_k(t)w_k\)</span> as well as to the diffusive noise <spanclass="math inline">\(\sigma^{2}(t)=\tau_m \sum_{k}^{}\nu_k(t)w_k^{2}\)</span>.</p><h3id="stationary-solution-for-leaky-integrate-and-fire-neurons">Stationarysolution for leaky integrate-and-fire neurons</h3><p>We now derive the stationary solution <spanclass="math inline">\(p(u,t)\equiv p(u)\)</span> of (13.16) for apopulation of leaky integrate-and-fire neurons (<spanclass="math inline">\(f(u)=-u\)</span>, the voltage scale is shifted sothat theh equilibrium potential is at zero). The reset threshold is thesame as the rheobase firing threshold and will be denoted by <spanclass="math inline">\(\theta=\theta_{reset}=\theta_{rh}\)</span>.</p><p>We assume that the total input <spanclass="math inline">\(h_0=RI^{ext}+\tau_m \sum_{k}^{} \nu_k w_k\)</span>is constant. For <span class="math inline">\(u&lt;\theta\)</span>, <spanclass="math display">\[    0=-\frac{\partial }{\partial u}J(u)+A_0 \delta(u-u_r), \tag{13.22}\]</span> where <span class="math inline">\(A_0\)</span> is thepopulation activity (or mean firing rate) in the stationary state and<span class="math display">\[    J(u)=\frac{-u+h_0}{\tau_m}p(u)-\frac{1}{2}\frac{\sigma^{2}}{\tau_m}\frac{\partial}{\partial u}p(u) \tag{13.23}\]</span></p><p>is the total flux. The meaning of (13.22) is that the flux isconstant except at <span class="math inline">\(u=u_r\)</span> where itjumps by an amount <span class="math inline">\(A_0\)</span>.</p><p>The boundary condition <spanclass="math inline">\(p(\theta,t)=0\)</span> implies a seconddiscontinuity of the flux at <spanclass="math inline">\(u=\theta\)</span>.</p><p>For any constant <span class="math inline">\(c_1\)</span>, <spanclass="math display">\[    p(u)=\frac{c_1}{\sigma}\exp \left[-\frac{(u-h_0)^{2}}{\sigma^{2}}\right]\]</span> for <span class="math inline">\(u\leqslant u_r\)</span> is asolution of (13.22) with <spanclass="math inline">\(J(u)=0\)</span>.</p><p>The solution to (13.22) with <spanclass="math inline">\(p(\theta)=0\)</span> is a modified Gaussian, <spanclass="math display">\[    p(u)=\frac{c_2}{\sigma^{2}}\exp\left[-\frac{(u-h_0)^{2}}{\sigma^{2}}\right] \cdot \int_{u}^{\theta}\exp \left[\frac{(x-h_0)^{2}}{\sigma^{2}} \right] \mathrm{d}x\tag{13.25}\]</span> for <span class="math inline">\(u_r&lt;u\leqslant\theta\)</span> with some constant <spanclass="math inline">\(c_2\)</span>. The constant <spanclass="math inline">\(c_2\)</span> is proportional to the flux, <spanclass="math display">\[    c_2=2\tau_m J(u) \tag{13.26}        \]</span> for <span class="math inline">\(u_r&lt;u\leqslant\theta\)</span>.</p><p>The solution defined by (13.24) and (13.25) must be continuous at<span class="math inline">\(u=u_r\)</span>. Hence <spanclass="math display">\[    c_1=\frac{c_2}{\sigma}\int_{u_r}^{\theta} \exp\left[\frac{(x-h_0)^{2}}{\sigma^{2}}\right] \mathrm{d}x. \tag{13.27}\]</span></p><p>The constant <span class="math inline">\(c_2\)</span> is determinedby (13.3). <span class="math display">\[    \frac{1}{c_2}=\int_{u_r}^{\theta} \int_{-\infty}^{x} f(x,u)\mathrm{d}u \mathrm{d}x, \tag{13.28}\]</span> with <span class="math display">\[    f(x,u)=\frac{1}{\sigma^{2}}\exp\left[-\frac{(u-h_0)^{2}}{\sigma^{2}}\right]\exp\left[\frac{(x-h_0)^{2}}{\sigma^{2}}\right]. \tag{13.29}\]</span> <span class="math display">\[    A_0^{-1}=\tau_m \sqrt{\pi}\int_{\frac{u_r-h_0}{\sigma}}^{\frac{\theta-h_0}{\sigma}} \exp(x^{2})[1+\text{erf}(x)] \mathrm{d}x, \tag{13.30}\]</span></p><h2 id="networks-of-leaky-integrate-and-fire-neurons">Networks of leakyintegrate-and-fire neurons</h2><h3 id="multiple-populations">Multiple populations</h3><p><span class="math display">\[    \begin{aligned}        \tau_n \frac{\partial }{\partial t}p_n(u,t)=&amp;-\frac{\partial}{\partial u} \left\{ \left[ -u+R_nI_n^{ext}(t)+\tau_n\sum_{k}^{}C_{nk}A_k(t)w_{nk}\right]p_n(u,t) \right\} \\        &amp;+\frac{1}{2}\left[ \tau_n \sum_{k}^{}C_{nk}A_k(t)w_{nk}^{2}\right] \frac{\partial ^{2}}{\partialu^{2}}p_n(u,t) \\        &amp;+ \tau_n A_n(t)\delta(u-u_r^{n})-\tau_nA_n(t)\delta(u-\theta_{n})    \end{aligned}\]</span> (13.31)</p><p><span class="math display">\[    A_n(t)=-\frac{1}{2}\left[ \sum_{k}^{} C_{nk}A_k(t)w_{nk}^{2}\right]\left( \frac{\partial p_n(u,t)}{\partial u}\right)_{u=\theta_n}.\tag{13.32}\]</span></p><p>Thus populations interact with each other via the variable <spanclass="math inline">\(A_k(t)\)</span>.</p><h3 id="synchrony-oscillations-and-irregularity">Synchrony,oscillations, and irregularity</h3><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x334.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x335.png" /></div></div></div><ul><li>Asynchronous irregular (AI):</li><li>Synchronous regular (SR):</li><li>Synchronous irregular (SI):</li></ul><h2 id="networks-of-nonlinear-integrate-and-fire-neurons">Networks ofNonlinear Integrate-and-Fire Neurons</h2><p>Now we determine, for arbitrary nonlinear integrate-and-fire modelsdriven by a diffusive input with constant mean <spanclass="math inline">\(\mu=RI_0\)</span> and noise <spanclass="math inline">\(\sigma^{2}\)</span>, the distribution of membranepotentials <span class="math inline">\(p_0(u)\)</span> as well as thelinear response of the population activity <span class="math display">\[    A(t)=A_0+A_1(t)=A_0+\int_{0}^{\infty} G(s)I_1(t-s) \mathrm{d}s,\tag{13.37}\]</span> to a drive <spanclass="math inline">\(\mu(t)=R[I_0+I_1(t)]\)</span>.</p><h3 id="steady-state-population-activity">Steady state populationactivity</h3><p>We start with the continuity equation (13.7) <spanclass="math display">\[    \frac{\partial }{\partial t}p(u,t)=-\frac{\partial }{\partialu}J(u,t)+A(t)\delta(u-u_r)-A(t)\delta(u-\theta_{reset}). \tag{13.38}\]</span> In the stationary state, <span class="math display">\[    \frac{\partial }{\partialu}J(u,t)=A(t)\delta(u-u_r)-A(t)\delta(u-\theta_{reset}). \tag{13.39}\]</span></p><p>The flux takes a constant value except at <spanclass="math inline">\(\theta_{reset}\)</span> and <spanclass="math inline">\(u_r\)</span>. For <spanclass="math inline">\(u_r&lt;u&lt;\theta_{reset}\)</span> the constantvalue <span class="math inline">\(J(u,t)=c&gt;0\)</span>.</p><p>In the diffusion limit the flux according to (13.21) is <spanclass="math display">\[    J(u,t)=\frac{1}{\tau_m}\left[f(u)+\mu(t)-\frac{1}{2}\sigma^{2}(t)\frac{\partial }{\partialu}\right]p(u,t). \tag{13.40}    \]</span></p><p>In the stationary state, <spanclass="math inline">\(p(u,t)=p_0(u)\)</span> and <spanclass="math inline">\(J(u,t)=c\)</span> for <spanclass="math inline">\(u_r&lt;u&lt;\theta_{reset}\)</span>. Hence <spanclass="math display">\[    \frac{\mathrm{d}p_0(u)}{\mathrm{d}u}=\frac{2\tau_m}{\sigma^{2}}\left[\frac{f(u)+\mu}{\tau_m}p_0(u)-c\right]. \tag{13.41}   \]</span></p><p>With initial condition <spanclass="math inline">\(p_0(\theta_{reset})=0\)</span> and <spanclass="math inline">\(\mathrm{d} p_0/\mathrm{d}u|_{\theta_{reset}}=-2c\tau_m/\sigma^{2}\)</span>, one can integrate(13.41) from <span class="math inline">\(u=\theta_{reset}\)</span>. Whenthe integration passes <span class="math inline">\(u=u_r\)</span> theconstant switches from <span class="math inline">\(c\)</span> to <spanclass="math inline">\(0\)</span>. The integration is stopped at a lowerbound <span class="math inline">\(u_{low}\)</span> when <spanclass="math inline">\(p_0\)</span> has approached <spanclass="math inline">\(0\)</span>.</p><h3 id="response-to-modulated-input">Response to modulated input</h3><p>Recall the exponential integrate-and-fire model <spanclass="math display">\[    \tau \frac{\mathrm{d}}{\mathrm{d}t}u=-(u-u_{r})+\Delta_{T}\exp\left(\frac{u-\theta_{rh}}{\Delta_{T}}\right)+\mu, \tag{13.42}\]</span></p><p>We now add a small periodic perturbation to the input <spanclass="math display">\[    I(t)=I_0+\varepsilon \cos (\omega t), \tag{13.43}\]</span> where <span class="math inline">\(\omega=2\pi/T\)</span>. Weexpect the periodic drive to lead to a small periodic change in thepopulation activity <span class="math display">\[    A(t)=A_0+A_1(\omega)\cos (\omega t+\phi_{A}(\omega)). \tag{13.44}\]</span></p><p>We are to calculate <span class="math inline">\(\hat{G}\)</span> thatcharacterize the linear response or 'gain' at frequency <spanclass="math inline">\(\omega\)</span> <span class="math display">\[    \hat{G}(\omega)=\frac{A_1(\omega)}{\varepsilon}\mathrm{e}^{i\phi_{A}(\omega)} . \tag{13.45}\]</span></p><p>The small periodic drive at frequency <spanclass="math inline">\(\omega\)</span> leads to a small periodic changein <span class="math inline">\(p(u,t)\)</span> <spanclass="math display">\[    p(u,t)=p_0(u)+p_1(u)\cos (\omega t +\phi_{p}(u)) \tag{13.48}\]</span> We assume that <spanclass="math inline">\(\varepsilon\)</span> is small so that <spanclass="math inline">\(p_1(u)\ll p_0(u)\)</span> for most values of <spanclass="math inline">\(u\)</span>. We say that the change is at most of'order <span class="math inline">\(\varepsilon\)</span>'.</p><p><span class="math inline">\(J(u,t)\)</span> will also exhibit a smallperturbation of order <span class="math inline">\(\varepsilon\)</span>.For (13.42) with <span class="math inline">\(u_{r}=0\)</span> the fluxis <span class="math display">\[    J(u,t)=\left[ \frac{-u+RI(t)}{\tau_m}+\frac{\Delta_{T}}{\tau_m}\exp\left(\frac{u-\theta_{rh}}{\Delta_{T}}\right)-\frac{\sigma^{2}(t)}{2\tau_m}\frac{\partial}{\partial u}\right]p(u,t)=Q(u,t)p(u,t),\]</span> (13.49)</p><p>The stationary state under the assumption of constant input <spanclass="math inline">\(I(t)=I_0\)</span> has a flux <spanclass="math inline">\(J_0(u)=Q_0(u)p_0(u)\)</span>. In the presence ofthe periodic perturbation, the flux is <span class="math display">\[    J(u,t)=J_0(u)+J_1(u)\cos (\omega t+\phi_{J}(u)) \tag{13.50}\]</span></p><p>with <span class="math display">\[    J_1(u)\cos (\omega t+\phi_{J}(u))=Q_0(u)p_1(u)\cos (\omegat+\phi_{p}(u))+Q_1(u,t)p_0(u)+O(\varepsilon^{2}),\]</span> (13.51)</p><p>where <span class="math inline">\(Q_1(u,t)=\varepsilon R\cos (\omegat)/\tau_m\)</span> is the change of the operator <spanclass="math inline">\(Q\)</span> to order <spanclass="math inline">\(\varepsilon\)</span>.</p><p>Note that the flux through the threshold <spanclass="math inline">\(\theta_{reset}\)</span> gives the periodicmodulation of the population activity.</p><p>We insert <span class="math inline">\(A,p,J\)</span> into (13.38),include the phase into the definition of <spanclass="math inline">\(A_1,p_1(u),J_1(u)\)</span>, e.g. <spanclass="math inline">\(\hat{A}_1=A_1 \exp (i\phi_{A})\)</span>; the hatindicates the complex number. If we take the Fourier transform overtime, (13.38) becomes <span class="math display">\[    -\frac{\partial }{\partial u}\hat{J_1}(u)=i \omega\hat{p_1}(u)+\hat{A_1}[\delta(u-\theta_{reset})-\delta(u-u_r)].\tag{13.52}\]</span></p><ul><li>We have quite arbitrarily normalized the membrane potential densityto an integral of unity.</li><li>The flux <span class="math inline">\(J_1\)</span> in (13.51) can bequite naturally separated into two components. The first contribution tothe flux is proportional to the perturbation <spanclass="math inline">\(p_1\)</span> of the membrane potential density.The second component is caused by the direct action of the externalcurrent <span class="math inline">\(Q_1(u,t)=\varepsilon R\cos (\omegat)/\tau_m\)</span>.</li><li>The explicit expression for <span class="math inline">\(Q_0\)</span>and <span class="math inline">\(Q_1\)</span> can be inserted into(13.51) <span class="math display">\[  \frac{\partial }{\partial u} \hat{p_1}(u)=\frac{2}{\sigma^{2}}\left[-u+RI_0+\Delta_{T}\exp\left(\frac{u-\theta_{rh}}{\Delta_{T}}\right)\right]\hat{p_1}(u)+\frac{2R\varepsilon}{\sigma^{2}}p_0(u)-\frac{2\tau}{\sigma^{2}}\hat{J_1}(u).  \]</span> (13.53)</li></ul><p>We therefore have two first-order differential equations (13.52) and(13.53) which are coupled to each other.</p><p>(13.52) and (13.53) are two first-order differential equations whichare coupled to each other. We drop the hats on <spanclass="math inline">\(J_1\)</span>, <spanclass="math inline">\(p_1\)</span>, <spanclass="math inline">\(A_1\)</span> to lighten the notation. <spanclass="math inline">\(\varepsilon\)</span> and <spanclass="math inline">\(A_1\)</span> are parameters.</p><ul><li>We find the 'free' component by integrating (13.53) with <spanclass="math inline">\(\varepsilon=0\)</span> in parallel with (13.52)with <span class="math inline">\(A_1=1\)</span>. The integration startsat the initial condition <spanclass="math inline">\(p_1(\theta_{reset})=0\)</span> and <spanclass="math inline">\(J_1^{free}(\theta_{reset})=A_1=1\)</span> andcontinues toward decreasing voltage values. Integration stops ar a lowerbound <span class="math inline">\(u_{low}\)</span> which we place at anarbitrary large negative value.</li><li>We find the 'driven' component by integrating (13.53) with <spanclass="math inline">\(\varepsilon&gt;0\)</span> in parallel with (13.52)with parameter <span class="math inline">\(A_1=0\)</span> starting atthe initial condition <spanclass="math inline">\(p_1(\theta_{reset})=0\)</span> and <spanclass="math inline">\(J_1^{\varepsilon}(\theta_{reset})=A_1=0\)</span>and continue toward decreasing voltage values. Integration stops ar alower bound <span class="math inline">\(u_{low}\)</span></li></ul><p>Then we combinate them <span class="math display">\[    J_1(u)=a_1 J_1^{free}(u)+a_2 J_1^{\varepsilon}(u)\]</span> and <span class="math display">\[    p_1(u)=a_1 p_1^{free}(u)+a_2 p_1^{\varepsilon}(u)\]</span> We require a boundary condition <span class="math display">\[    0=J(u_{low})=a_1J_1^{free}(u_{low})+a_2 J_1^{\varepsilon}(u_{low}).\tag{13.54}\]</span></p><p>Recall that <span class="math inline">\(J_1^{\varepsilon}\)</span> isproportional to the drive <spanclass="math inline">\(\varepsilon\)</span>. <strong>The factor <spanclass="math inline">\(a_1\)</span> is the populationresponse.</strong></p><p>The gain factor is <span class="math display">\[    \hat{G}(\omega)=\frac{A_1}{\varepsilon}=\frac{a_1}{\varepsilon}=-\frac{a_2}{\varepsilon}\frac{J_1^{\varepsilon}(u_{low})}{J_1^{free}(u_{low})}.\tag{13.55}\]</span> which has an amplitude $() $ and a phase <spanclass="math inline">\(\phi_{G}\)</span>.</p><h2 id="neuronal-adaptation-and-synaptic-conductance">Neuronaladaptation and synaptic conductance</h2><h3 id="adaptation-currents">Adaptation currents</h3><p>Suppose that the stochastic spike arrival can be modeled by a mean<span class="math inline">\(\mu(t)=R\langle I(t)\rangle\)</span> plus awhite-noise term <span class="math inline">\(\xi\)</span>. <spanclass="math display">\[    \tau_m \frac{\mathrm{d}u}{\mathrm{d}t}=-(u-u_{rest})+\Delta_{T}\exp\left(\frac{u-\theta_{rh}}{\Delta _{T}}\right)-Rw+\mu(t)+\xi(t)\tag{13.59}\]</span> <span class="math display">\[    \tau_w \frac{\mathrm{d}w}{\mathrm{d}t}=a(u-u_{rest})-w+b\tau_w\sum_{t^{(f)}}^{} \delta(t-t^{(f)}). \tag{13.60}     \]</span></p><p>Suppose furthermore that the time constant <spanclass="math inline">\(\tau_w\)</span> of the adaptation variable <spanclass="math inline">\(w\)</span> is larger than <spanclass="math inline">\(\tau_m\)</span> and <spanclass="math inline">\(b\ll 1\)</span> so the fluctuations of theadaptation variable <span class="math inline">\(w\)</span> around $w $are small. Therefore for the solution of the membrane potential densityequations <span class="math inline">\(p_0(u)\)</span> the adaptationvariable can be approximated by a constant $w_0=w $.</p><h3 id="embedding-in-a-network">Embedding in a network</h3><p>The mean input <span class="math inline">\(\mu(t)\)</span> to neuron<span class="math inline">\(i\)</span> arriving at time <spanclass="math inline">\(t\)</span> from population <spanclass="math inline">\(k\)</span> is proportional to its activity <spanclass="math inline">\(A_k(t)\)</span>. The contribution of population<span class="math inline">\(k\)</span> to the variance <spanclass="math inline">\(\sigma^{2}\)</span> of <spanclass="math inline">\(\mu(t)\)</span> is also proportional to <spanclass="math inline">\(A_k(t)\)</span>; cd. Eq.(13.31).</p><p>We can analyze the stationary states and their stability in a networkof adaptive model neurons as</p><hr /><p>13.6 Neuronal adaptation and synaptic conductancehttps://neuronaldynamics.epfl.ch/online/Ch13.S6.html#Ch13.E61</p><hr /><h3 id="conductance-input-vs.-current-input">Conductance input vs.current input</h3><p>Synaptic input is more accurately described as a change inconductance <span class="math inline">\(g(t-t_j^{(f)})\)</span>, ratherthan as current injection. A time dependent synaptic conductance leadsto a total synaptic current into neuron <spanclass="math inline">\(i\)</span><br /><span class="math display">\[    I_i(t)=\sum_{j}^{} \sum_{f}^{}w_{ij}g_{ij}(t-t_j^{(f)})(u_i(t)-E_{syn}), \tag{13.63}   \]</span></p><p>We will now show that, in the state of stationary asynchronousactivity, conductance-based input can be approximated by an effectivecurrent input. The main effect of conductance-based input is that themembrane time constant of the stochastically driven neuron is shorterthan the 'raw' passive membrane time constant.</p><p>We consider <span class="math inline">\(N_{E}\)</span> excitatory and<span class="math inline">\(N_{I}\)</span> inhibitory LIF neurons in thesubthreshold regime <span class="math display">\[    C\frac{\mathrm{d}u}{\mathrm{d}t}=-g_{L}(u-E_{L})-g_{E}(t)(u-E_{E})-g_{I}(t)(u-E_{I}),\tag{13.64}\]</span> where <span class="math inline">\(C\)</span> is the membranecapacity, <span class="math inline">\(g_{L}\)</span> the leakconductance and <span class="math inline">\(E_{L}\)</span>, <spanclass="math inline">\(E_{E}\)</span>,<spanclass="math inline">\(E_{I}\)</span> are the reversal potentials forleak, excitation, and inhibition, respectively. Input spikes atexcitatory synapse lead to an increased conductance <spanclass="math display">\[    g_{E}(t)=\Delta g_{E}\sum_{j}^{} \sum_{f}^{} \exp[-(t-t_j^{(f)})/\tau_{E}] \mathscr{H}(t-t_j^{(f)}). \tag{13.65}   \]</span></p><p>The sum over <span class="math inline">\(j\)</span> runs over allexcitatory synapses. We assume that excitatory and inhibitory inputspikes arrive with a total rate <spanclass="math inline">\(\nu_{E}\)</span> and <spanclass="math inline">\(\nu_{I}\)</span>.</p><p>Using the methods from Chapter 8, we can calculate the meanexcitatory conductance <span class="math display">\[    g_{E,0}=\Delta g_{E}\nu_{E}\tau_{E}, \tag{13.66}\]</span> where <span class="math inline">\(\nu_{E}\)</span> is thetotal spike arrival rate at excitatory synapses.</p><p>The variance of the conductance is <span class="math display">\[    \sigma_{E}^{2}=\frac{1}{2}(\Delta g_{E})^{2}\nu_{E} \tau_{E}.\tag{13.67}\]</span></p><p>Write the conductance as the mean plus a fluctuating component <spanclass="math display">\[    g_{E,f}(t)=g_{E}(t)-g_{E,0}.\]</span> This turns (13.64) into <span class="math display">\[    C\frac{\mathrm{d}u}{\mathrm{d}t}=-g_0(u-\mu)-g_{E,f}(t)(u-E_{E})-g_{I,f}(t)(u-E_{I}),\tag{13.68}   \]</span> with a total conductance <spanclass="math inline">\(g_0=g_{L}+g_{E,0}+g_{I,0}\)</span> and aninput-dependent equilibrium potential <span class="math display">\[    \mu=\frac{g_{L}E_{L}+g_{E,0}E_{E}+g_{I,0}E_{I}}{g_0}. \tag{13.69}\]</span></p><p>In (13.64) the dynamics is characterized by a raw membrane timeconstant <span class="math inline">\(C/g_{L}\)</span> whereas (13.68) iscontrolled by an effective membrane time constant <spanclass="math display">\[    \tau_{eff}=\frac{C}{g_0}= \frac{C}{g_{L}+g_{E,0}+g_{I,0}},\tag{13.70}  \]</span></p><p>and a mean depolarization <span class="math inline">\(\mu\)</span>which acts as an effective equilibrium potential.</p><p>Then, we compare the momentary voltage <spanclass="math inline">\(u(t)\)</span> with the effective equilibriumpotential <span class="math inline">\(\mu\)</span>. The fluctuating partof the conductance in (13.68) can be written as <spanclass="math display">\[    g_{E,f}(t)(u-E_{E})=g_{E,f}(t)(\mu-E_{E})+g_{E,f}(t)(u-\mu),\tag{13.71}\]</span></p><p>The second term on the RHS of (13.71) is small compared to the firstterm and needs to be dropped to arrive at a consistent diffusionapproximation. The first term on the RHS of (13.71) can be interpretedas the summed effects of postsynaptic current pulses.</p><h3 id="colored-noise">Colored Noise</h3><p>Some synapse types, such as the NMDA component of excitatorysynapses, are rather slow. A spike that has arrived at an NMDA synapseat time <span class="math inline">\(t_0\)</span> generates afluctuation, which cause the input to exhibit temporal correlations.</p><p>We have two approaches to colored noise in the membrane potentialdensity equations.</p><p>The first approach is to approximate colored noise by white noise andreplace the temporal smoothing by a broad distribution of delays.Problem set: current-based. The mean input to a neuron <spanclass="math inline">\(i\)</span> in population <spanclass="math inline">\(n\)</span> arising from other populations <spanclass="math inline">\(k\)</span> is <span class="math display">\[    I_i(t)=\sum_{k}^{} C_{nk}w_{nk}\int_{0}^{\infty}\alpha_{nk}(s)A(t-s) \mathrm{d}s, \tag{13.72}      \]</span></p><p>Suppose <span class="math inline">\(C_{nk}\)</span> is a largenumber, but the population <span class="math inline">\(k\)</span> itselfis larger so that the connectivity <spanclass="math inline">\(C_{nk}/N_k \ll 1\)</span>. We now replace <spanclass="math inline">\(\alpha(s)\)</span> by <spanclass="math inline">\(q \delta(s-\Delta)\)</span> such that <spanclass="math inline">\(q=\int_{0}^{\infty} \alpha(s)\mathrm{d}s\)</span>. For each of the <spanclass="math inline">\(C_{nk}\)</span> connections we randomly draw thetransmission delay <span class="math inline">\(\Delta\)</span> from adistribution <spanclass="math inline">\(p(\Delta)=\alpha(\Delta)/q\)</span>. Because ofthe low connectivity, we may assume that the firing of different neuronsis uncorrelated. The broad distribution of delays stabilizes thestationary state of asynchronous firing.</p><p>The second approach consists in an explicit model of the synapticcurrent variables. We focus on a single population coupled to itself andsuppose that the synaptic current pulses are exponential <spanclass="math inline">\(\alpha(s)=(q/\tau_q)\exp (-s/\tau_q)\)</span>. Thedriving current of a neuron <span class="math inline">\(i\)</span> in apopulation <span class="math inline">\(n\)</span> is then <spanclass="math display">\[    \frac{\mathrm{d}I_i}{\mathrm{d}t}=-\frac{I_i}{\tau_q}+C_{nn}w_{nn}\frac{q}{\tau_q}A_n(t)\tag{13.73}\]</span> which we can verify by taking the temporal derivative of(13.72). <span class="math inline">\(A(t)\)</span> has a mean <spanclass="math inline">\(\mu(t)\)</span> (which is the same for allneurons) and a fluctuating part <spanclass="math inline">\(\xi_{i}(t)\)</span> with white-noisecharacteristics: <span class="math display">\[    \frac{\mathrm{d}I_i}{\mathrm{d}t}=-\frac{I_i}{\tau_q}+\mu(t)+\xi_{i}(t).\tag{13.74}\]</span> (13.74) needs to be combined with the differential equationfor the voltage <span class="math display">\[    \tau_m \frac{\mathrm{d}u_i}{\mathrm{d}t}=f(u_i)+RI_{i}(t),\tag{13.75}\]</span> We now have two coupled differential equations, the momentarystate of a population of <span class="math inline">\(N\)</span> neuronsis described by a two-dimensional density <spanclass="math inline">\(p(u,I)\)</span>.</p><p>We recall that, in the case of white noise, the membrane potentialdensity at threshold vanishes. The main insight for the mathematicaltreatment of the membrane potential density equations in two dimensionsis that the density at threshold <spanclass="math inline">\(p(\theta_{reset},I(t))\)</span> is finite,whenever the momentary slope of the voltage <spanclass="math inline">\(\mathrm{d}u/\mathrm{d}t \proptoRI(t)+f(\theta_{reset})\)</span> is positive.</p><h2 id="summary">Summary</h2><p>The momentary state of a population of one-dimensionalintegrate-and-and fire neurons can be characterized by the membranepotential density <span class="math inline">\(p(u,t)\)</span>. Thecontinuity equation describes the evolution of <spanclass="math inline">\(p(u,t)\)</span> over time. In the special casethat neurons in the population receive many inputs that each cause asmall change of the membrane potential, the continuity equation has theform of a Fokker-Planck equation. Several populations ofintegrate-and-fire neurons interact via the population activity <spanclass="math inline">\(A(t)\)</span> which is identified with the fluxacross the threshold.</p><p>The stationary state of the Fokker-Planck equation and the stabilityof the stationary solution can be calculated by a mix of analytical andnumerical methods, be it for a population of independent orinterconnected neurons. The mathematical and numerical methods developedfor membrane potential density equations apply to leaky as well as toarbitrary nonlinear one-dimensional integrate-and-fire model. A slowadaptation variable such as in the adaptive exponentialintegrate-and-fire model can be treated as quasi-stationary in theproximity of the stationary solution. Conductance input can beapproximated by an equivalent current-based model.</p>]]></content>
    
    
    <categories>
      
      <category>神经动力学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>神经科学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (12)</title>
    <link href="/2022/10/05/Neuronal-Dynamics-12/"/>
    <url>/2022/10/05/Neuronal-Dynamics-12/</url>
    
    <content type="html"><![CDATA[<h1 id="neuronal-populations">Neuronal Populations</h1><p>The online version of this chapter:</p><hr /><p>Chapter 12 Neuronal Populationshttps://neuronaldynamics.epfl.ch/online/Ch12.html</p><hr /><p>The aim of this chapter is to provide the foundation of the notionsof 'neuronal population' and 'population activity'.</p><h2 id="columnar-organization">Columnar organization</h2><p>We present in this section a short introduction into the structuralorganization and functional characterization of cortex.</p><h3 id="receptive-fields">Receptive fields</h3><p>Simple cells in visual cortex are sensitive to the orientationn of alight bar.</p><p>In this and the following chapters, we exploit the fact thatneighboring neurons in visual cortex have similar receptive fields.</p><h4 id="example-cortical-maps">Example: Cortical Maps</h4><p>Neighboring neurons have similar receptive fields, but the exactcharacteristics of the receptive fields change slightly as one movesparallel to the cortical surface.</p><h3 id="how-many-populations">How many populations?</h3><p>Inside a column neurons are organized in different layers. Each layercontains one or several types of neurons. From shallow to deep, neuronsbecomes indistinguishable. Besides, the number of populations that atheoretician takes into account depends on the level of'coarse-graining' that he is ready to accept, as well as on the amountof information that is available from experiments.</p><h3 id="distributed-assemblies">Distributed assemblies</h3><p>The mathematical notion of population does not require that neuronsneed to form local groups to qualify as a homogeneous populaton.</p><p>Donald Hebb introduced the notion of neuronal assemblies, i.e.,groups of cells which get activated together so as to represent a mentalconcept. An assembly can be a group of neurons which are distributedacross one or several areas. However, such an assignment of a neuron toa population is not fixed, but can depend on the stimulus.</p><h2 id="identical-neurons-a-mathematical-abstraction">Identical Neurons:A Mathematical Abstraction</h2><p>In a population of <span class="math inline">\(N\)</span> neurons,the population activity is <span class="math display">\[    A(t)=\lim_{\Delta t \to 0}\frac{1}{\Delta t}\frac{n_{act}(t;t+\Deltat)}{N}=\frac{1}{N}\sum_{j=1}^{N} \sum_{f}^{} \delta(t-t_j^{(f)}),\tag{12.1}\]</span></p><p>For the sake of notational simplicity, we do not distinguish theobserved activity from its expectation value and denote in the followingthe expected activity by <span class="math inline">\(A(t)\)</span>.</p><h3 id="homogeneous-networks">Homogeneous networks</h3><p>By homogeneous we mean that - all neurons <spanclass="math inline">\(1\leqslant i\leqslant N\)</span> are identical; -all neurons receive the same external input <spanclass="math inline">\(I^{ext}_i(t)=I^{ext}(t)\)</span>; - theinteraction strength <span class="math inline">\(w_{ij}\)</span> for theconnection between any pair <span class="math inline">\(j,i\)</span> ofpre- and postsynaptic neurons is 'statistically uniform'.</p><h4id="homogeneous-population-of-integrate-and-fire-neurons">Homogeneouspopulation of integrate-and-fire neurons</h4><p>We assume that a neuron is coupled to all others as well as to itselfwith coupling strength <span class="math inline">\(w_{ij}=w_0\)</span>.The input current <span class="math inline">\(I_i\)</span> takes care ofboth the external drive and synaptic coupling <spanclass="math display">\[    I_i=\sum_{j=1}^{N} \sum_{f}^{} w_{ij}\alpha(t-t_j^{(f)})+I^{ext}(t). \tag{12.3}\]</span> Here we have assumed that each input spike generates apostsynaptic current with some generic time course <spanclass="math inline">\(\alpha(t-t_j^{(f)})\)</span>.</p><p>Using (12.1), we find a total input current, <spanclass="math display">\[    I(t)=w_0 N \int_{0}^{\infty} \alpha(s)A(t-s) \mathrm{d}s+I^{ext}(t),\tag{12.4}\]</span> which is independent of the neuronal index <spanclass="math inline">\(i\)</span>. Thus, the input current at time <spanclass="math inline">\(t\)</span> depends on the past population activityand is the same for all neurons.</p><h2 id="connectivity-schemes">Connectivity Schemes</h2><p>In the following we discuss some schemes with a special focus on thescaling behavior induced by each choice of coupling scheme. Here,scaling behavior refers to a change in the number <spanclass="math inline">\(N\)</span> of neurons that participate in thepopulation.</p><h3 id="full-connectivity">Full connectivity</h3><p>All-to-all connectivity, all connections have the same strength. Anappropriate scaling law is <span class="math display">\[    w_{ij}=\frac{J_0}{N}. \tag{12.6}\]</span></p><p>A slightly more intricate all-to-all coupling scheme is thefollowing: weights <span class="math inline">\(w_{ij}\)</span> are drawnfrom a Gaussian distribution with mean <spanclass="math inline">\(J_0/N\)</span> and standard deviation <spanclass="math inline">\(\sigma/\sqrt{N}\)</span>. The fluctuations of themembrane potential are of the order <spanclass="math inline">\(\sigma_0\)</span> even in the limit of large <spanclass="math inline">\(N\)</span>.</p><h3 id="random-coupling-fixed-coupling-probability">Random coupling:Fixed coupling probability</h3><p>Experimentally the probability <span class="math inline">\(p\)</span>that a neuron inside a cortical column makes a functional connection toanother neuron in the same column is in the range of 10%, but variesacross layers.</p><p>The number of presynaptic input links <spanclass="math inline">\(C_j\)</span> to a postsynaptic neuron <spanclass="math inline">\(j\)</span> has a mean value of <spanclass="math inline">\(\langle C_j \rangle=pN\)</span>, but fluctuatesbetween one neuron and the next with variance <spanclass="math inline">\(p(1-p)N\)</span>.</p><p>Alternatively, we can take one model neuron <spanclass="math inline">\(j=1,2,3,\cdots N\)</span> after the other andchoose randomly <span class="math inline">\(C=pN\)</span> presynapticpartners for it.</p><p>It is useful to scale the strengh of the connections as <spanclass="math display">\[    w_{ij}=\frac{J_0}{C}=\frac{J_0}{pN}, \tag{12.7}\]</span></p><h3 id="random-coupling-fixed-number-of-presynaptic-partners">Randomcoupling: Fixed number of presynaptic partners</h3><p>We pick one model neuron <span class="math inline">\(j=1,2,3,\cdotsN\)</span> after the other and choose randomly its <spanclass="math inline">\(C\)</span> presynaptic partners. Whenever thenetwork size <span class="math inline">\(N\)</span> is much bigger than<span class="math inline">\(C\)</span>, the inputs to a given neuron canbe thought of as random samples from the current network activity. Noscaling of the connections with the population size <spanclass="math inline">\(N\)</span> is necessary.</p><h3 id="balanced-excitation-and-inhibition">Balanced excitation andinhibition</h3><p>If the total amount of excitation and inhibition cancel each other,so that excitation and inhibition are 'balanced'. The resulting networkis called a balanced network or a population with balanced excitationand inhibition.</p><p>We can scale synaptic weights so as to control specifically theamount of fluctuations of the input current around zero. An appropriatechoice is <span class="math display">\[    w_{ij}=\frac{J_0}{\sqrt{C}}=\frac{J_0}{\sqrt{pN}}. \tag{12.8}\]</span></p><h3 id="interacting-populations">Interacting Populations</h3><p>We assume that neurons are homogeneous within each pool. The activityof neurons in pool <span class="math inline">\(n\)</span> is <spanclass="math display">\[    A_n(t)=\frac{1}{N_n}\sum_{j \in \Gamma_n}^{} \sum_{f}^{}\delta(t-t_j^{(f)}), \tag{12.9}\]</span> where <span class="math inline">\(N_n\)</span> is the numberof neurons in pool <span class="math inline">\(n\)</span> and <spanclass="math inline">\(\Gamma_n\)</span> denotes the set of neurons thatbelong to pool <span class="math inline">\(n\)</span>. Each neuron <spanclass="math inline">\(i\)</span> in pool <spanclass="math inline">\(n\)</span> receives input from all neuons <spanclass="math inline">\(j\)</span> in pool <spanclass="math inline">\(m\)</span> with strength <spanclass="math inline">\(w_{ij}=J_{nm}/N_m\)</span>. The time course <spanclass="math inline">\(\alpha_{ij}(s)\)</span> caused by a spike of apresynaptic neuron <span class="math inline">\(j\)</span> may depend onthe synapse type. The input current to a neuron <spanclass="math inline">\(i\)</span> in group <spanclass="math inline">\(\Gamma_n\)</span> is generated by the spikes ofall neurons in the network, <span class="math display">\[    I_{i,n}=\sum_{j}^{} \sum_{f}^{}w_{ij}\alpha_{ij}(t-t_j^{(f)})=\sum_{m}^{} J_{nm}\int_{0}^{\infty}\alpha_{nm}\sum_{j\in \Gamma_m}^{} \sum_{f}^{}\frac{\delta(t-t_j^{(f)}-s)}{N_m} \mathrm{d}s,\]</span> (12.10)</p><p>where <span class="math inline">\(\alpha_{nm}(t-t_j^{(f)})\)</span>denotes thhe time course of a postsynaptic current caused by spikefiring at time <span class="math inline">\(t_j^{(f)}\)</span> of thepresynaptic neuron <span class="math inline">\(j\)</span> which is partof population <span class="math inline">\(m\)</span>. So <spanclass="math display">\[    I_n=\sum_{m}^{} J_{nm}\int_{0}^{\infty} \alpha(s)A_m(t-s)\mathrm{d}s. \tag{12.11}      \]</span> We have dropped the index <spanclass="math inline">\(i\)</span> since the input current is the same forall neurons in pool <span class="math inline">\(n\)</span>.</p><h3 id="distance-dependent-connectivity">Distance dependentconnectivity</h3><p>For models of distance-dependent connectivity it is necessary toassign to each model neuron <span class="math inline">\(i\)</span> alocation <span class="math inline">\(x(i)\)</span> on thetwo-dimensional cortical sheet.</p><p>Two different algorithmic procedures can be used to assigndistance-dependent connectivity. The first one assumes full connectivitywith a strength <span class="math inline">\(w_{ij}\)</span> which fallsoff with distance</p><p><span class="math display">\[    w_{ij}=w(\lvert x(i)-x(j) \rvert ), \tag{12.12}\]</span></p><p>One may assume finite support so that <spanclass="math inline">\(w\)</span> vanishes for distances <spanclass="math inline">\(\lvert x(i)-x(j) \rvert &gt;d\)</span>.</p><p>The second alternative is to give all connections the same weight,but to assume that the probability <spanclass="math inline">\(P\)</span> of forming a connection depends on thedistance <span class="math display">\[    \operatorname{Pr}(w_{ij}=1)=P(\lvert x(i)-x(j) \rvert ), \tag{12.13}\]</span></p><h3 id="spatial-continuum-limite">Spatial Continuum Limite</h3><p>For neurons organized in a spatially extended multidimensionalnetwork, a description by discrete pool does not seem appropriate.However, a transition from discrete pools to a continuous population ispossible.</p><p>We consider a population of neurons that extends along aone-dimensional axis and discretize space in segments of size <spanclass="math inline">\(d\)</span>. The number of neurons in the interval<span class="math inline">\([nd,(n+1)d]\)</span> is <spanclass="math inline">\(N_n=\rho d\)</span> where <spanclass="math inline">\(\rho\)</span> is the spatial density. Neurons inthat interval form the group <spanclass="math inline">\(\Gamma_n\)</span>.</p><p>We replace our notation <span class="math display">\[    A_m(t) \longrightarrow A(md,t) =A(y,t). \tag{12.14}\]</span></p><p>We have <span class="math inline">\(J_{nm}=\rho d w(nd,md)\)</span>.Use (12.11) and find <span class="math display">\[    I(nd,t)=\rho \sum_{m}^{} d w(nd,md) \int_{0}^{\infty}\alpha(s)A(md,t-s) \mathrm{d}s, \tag{12.15}\]</span> where <span class="math inline">\(\alpha(s)\)</span> describesthe time course of the postsynaptic current caused by spike firing inone of the presynaptic neurons. For <span class="math inline">\(d \to0\)</span>, we arrive at <span class="math display">\[    I(x,t)=\rho \int_{}^{} w(x,y)\int_{0}^{\infty} \alpha(s)A(y,t-s)\mathrm{d}s \mathrm{d}y, \tag{12.16}\]</span></p><p>To rephrase (12.16) in words, the input to neurons at location <spanclass="math inline">\(x\)</span> depends on the spatial distribution ofthe population activity convolved with the spatial coupling filter <spanclass="math inline">\(w(x,y)\)</span> and the temporal filter <spanclass="math inline">\(\alpha(s)\)</span>. The population activity <spanclass="math inline">\(A(y,t-s)\Delta s\)</span> is the number of spikesin a short interval <span class="math inline">\(\Delta s\)</span> summedacross neurons in the neighborhood around <spanclass="math inline">\(y\)</span> normalized by the number of neurons inthat neighborhood.</p><h2 id="from-microscopic-to-macroscopic">From Microscopic toMacroscopic</h2><p>We now make the transition from the properties of single spikingneurons to the population activity in a homogeneous group ofneurons.</p><h3 id="stationary-activity-and-asynchronous-firing">Stationary activityand asynchronous firing</h3><p>We define asynchronous firing of a neuronal population as amacroscopic firing state with constant activity <spanclass="math inline">\(A(t)=A_0\)</span>. We will see that the onlyrelevant single-neuron property is its gain function, i.e. its meanfiring rate as a function of input.</p><p>If the filter is kept fixed, while the population size is increased,the population activity in the stationary state of asynchronous firingapproaches the constant value <spanclass="math inline">\(A_0\)</span>.</p><h3 id="stationary-activity-as-single-neuron-firing-rate">StationaryActivity as Single-Neuron Firing Rate</h3><p>In a finite population, the empirical activity fluctuates and we canpredict the expectation value <span class="math display">\[    \langle A_0\rangle =\nu_i. \tag{12.18}\]</span> The mean firing rate is given by the gain function <spanclass="math display">\[    \nu_i=g_{\sigma}(I_0), \tag{12.19}\]</span> where the subscript <spanclass="math inline">\(\sigma\)</span> is intended to remind the readerthat the shape of the gain function depends on the level of noise.</p><h3 id="activity-of-a-fully-connected-network">Activity of a fullyconnected network</h3><p>We know <span class="math display">\[    \langle A_0\rangle =g_{\sigma}(I). \tag{12.21}\]</span> The gain function in the absence of any noise (fluctuationamplitude <span class="math inline">\(\sigma=0\)</span>) will be denotedby <span class="math inline">\(g_0\)</span>.</p><p>We can impose a normalization <spanclass="math inline">\(\int_{0}^{\infty} \alpha(s) \mathrm{d}s=1\)</span>and set <span class="math inline">\(\int_{0}^{\infty} \alpha(s)A(t-s)\mathrm{d}s=A_0\)</span>.</p><p>Therefore, the assumption of stationarity activity <spanclass="math inline">\(A_0\)</span> combined with the assumption ofconstant external input <spanclass="math inline">\(I^{ext}(t)=I_0^{ext}\)</span> yields a constanttotal driving current <span class="math display">\[    I_0=w_0 NA_0+I_0^{ext}. \tag{12.23}\]</span></p><p>Together with (12.21) we arrive at an implicit equation for thepopulation activity <span class="math inline">\(A_0\)</span>, <spanclass="math display">\[    A_0=g_0(J_0A_0+I_0^{ext}). \tag{12.24}\]</span> where <span class="math inline">\(g_0\)</span> is thenoise-free gain function of single neurons and <spanclass="math inline">\(J_0=w_0N\)</span>.</p><h4id="example-leaky-integrate-and-fire-model-with-diffusive-noise">Example:Leaky integrate-and-fire model with diffusive noise</h4><p>We consider a large and fully connected network of identical leakyintegrate-and-fire neurons with homogeneous coupling <spanclass="math inline">\(w_{ij}=J_0/N\)</span> and normalized postsynapticcurrents <span class="math inline">\(\int_{0}^{\infty} \alpha(s)\mathrm{d}s=1\)</span>. In the state of asynchronous firing, the totalinput current driving a typical neuron of the network is then</p><p><span class="math display">\[    I_0=I^{ext}+J_0A_0. \tag{12.25}\]</span> In addition, each neuron receives individual diffusive noiseof variance <span class="math inline">\(\sigma^{2}\)</span> that couldrepresent spike arrival from other populations. The single-neuron gainfunction in the presence of diffusive noise has been stated in (8.54).We use the formula of the gain function to calculate the populationacitvity <span class="math display">\[    A_0=g_{\sigma}(I_0)=\left\{ \tau_m\sqrt{\pi}\int_{\frac{u_r-RI_0}{\sigma}}^{\frac{\theta-RI_0}{\sigma}}\exp (u^{2})[1+\text{erf}(u)]  \mathrm{d}u\right\}^{-1}, \tag{12.26}\]</span></p><p><strong>(Siegert-formula)</strong></p><h3 id="activity-of-a-randomly-connected-network">Activity of a randomlyconnected network</h3><p>In this subsection, we discuss how to mathematically treat theadditional noise arising from the network.</p><p>If all neurons fire at a rate <spanclass="math inline">\(\nu\)</span> then the mean input current to neuron<span class="math inline">\(i\)</span> generated by its <spanclass="math inline">\(C_{pre}\)</span> presynaptic partners is <spanclass="math display">\[    \langle I_0\rangle =C_{pre}qw\nu+I_0^{ext}, \tag{12.27}\]</span> where <span class="math inline">\(q=\int_{0}^{\infty}\alpha(s) \mathrm{d}s\)</span> denotes the integral over thepostsynaptic current and can be interpreted as the total electric chargedelivered by a single input spike.</p><p>The input current is not constant but fluctuates with a variance<span class="math inline">\(\sigma_{I}^{2}\)</span> given by <spanclass="math display">\[    \sigma_{I}^{2}=C_{pre} w^{2} q_2 \nu, \tag{12.28}\]</span> where <span class="math inline">\(q_2=\int_{0}^{\infty}\alpha^{2}(s) \mathrm{d}s\)</span>.</p><h4 id="brunel-network-excitatory-and-inhibitory-populations">Brunelnetwork: excitatory and inhibitory populations</h4><p>We assume that excitatory and inhibitory neurons have the sameparameters <span class="math inline">\(\theta, \tau_m, R\)</span> and<span class="math inline">\(u_r\)</span>. All neurons are driven by acommon external current <span class="math inline">\(I^{ext}\)</span>.Each neuron in the population receives <spanclass="math inline">\(C_{E}\)</span> synapses from excitatory neuronswith weight <span class="math inline">\(w_{E}&gt;0\)</span> and <spanclass="math inline">\(C_{I}\)</span> synapses from inhibitory neuronswith weight <span class="math inline">\(w_{I}&lt;0\)</span>.</p><p>If an input spike arrives at the synapses of neuron <spanclass="math inline">\(i\)</span> from a presynaptic neuron <spanclass="math inline">\(j\)</span>, its membrane potential changes by anamount <span class="math inline">\(\Delta u_{E}=w_{E}qR/\tau_m\)</span>if <span class="math inline">\(j\)</span> is excitatory and <spanclass="math inline">\(\Delta u_{I}=\Delta u_{E} w_{I}/w_{E}\)</span> if<span class="math inline">\(j\)</span> is inhibitory. We set <spanclass="math display">\[    \gamma=\frac{C_{I}}{C_{E}}, \quadg=-\frac{w_{I}}{w_{E}}=-\frac{\Delta u_{I}}{\Delta u_{E}}.\tag{12.30}    \]</span></p><p>Since excitatory and inhibitory neurons receive the same number ofinput connections in our model, we assume that they fire with a commonfiring rate <span class="math inline">\(\nu\)</span>. The total inputcurrent generated by the external current and by the lateral couplingsis <span class="math display">\[    I_0=I^{ext}+q\sum_{j}^{} \nu_j w_j=I_0^{ext}+q\nuw_{E}C_{E}[1-\gamma g]. \tag{12.31}\]</span></p><p>We measure the noise strength by the variance <spanclass="math inline">\(\sigma_{u}^{2}\)</span> of the membrane potential(as opposed to the variance <spanclass="math inline">\(\sigma_{I}^{2}\)</span> of the input). FromChapter 8, we set <spanclass="math inline">\(\sigma_{u}^{2}=\frac{1}{2}\sigma^{2}\)</span>where <span class="math display">\[    \sigma^{2}=\sum_{j}^{} \nu_j \tau(\Delta u_j^{2})=\nu(\Deltau_{E})^{2} C_{E}[1+\gamma g^{2}]. \tag{12.32}\]</span> The stationary firing rate <spanclass="math inline">\(A_0\)</span> of the population with mean input<span class="math inline">\(I_0\)</span> and variance <spanclass="math inline">\(\sigma\)</span> is copied from (12.26) andrepeated here for convenience</p><p><span class="math display">\[    A_0=\nu=g_{\sigma}(I_0)=\frac{1}{\tau_m}\left\{\sqrt{\pi}\int_{\frac{u_r-RI_0}{\sigma}}^{\frac{\theta-RI_0}{\sigma}}\exp (u^{2})[1+\text{erf}(u)]  \mathrm{d}u\right\}^{-1}, \tag{12.33}\]</span></p><p>Numerical solutions of (12.31)-(12.33) have been obtained by Amit andBrunel.</p><h4 id="example-inhibition-dominated-network">Example: Inhibitiondominated network</h4><p>Suppose the mean feedback is dominated by inhibition. The effectivecoupling <span class="math inline">\(J^{eff}=\tau C_{E}\Deltau_{E}(1-\gamma g)\)</span>. In this case (12.31) is to be replaced by<span class="math display">\[    h_0=\tau_m \nu \Delta u_{E}C_{E}[1-\gamma g]+\tau_m \nu_{ext}\Deltau_{ext} C_{ext}, \tag{12.34}\]</span> with <span class="math inline">\(C_{ext}\)</span> the numberof connections that a neuron receives from neurons outside thepopulation, <span class="math inline">\(\Delta u_{ext}\)</span> theirtypical coupling strength characterized by the amplitude of the voltagejump, and <span class="math inline">\(\nu_{ext}\)</span> their spikearrival rate. Due to the extra stochasticity in the input, the variance<span class="math inline">\(\sigma_u^{2}\)</span> of the membranevoltage is larger <span class="math display">\[    \sigma_u^{2}=\frac{1}{2}\sigma^{2}=\frac{1}{2}\tau_m \nu(\Deltau_{E})^{2}C_{E}[1+\gamma g^{2}]+\frac{1}{2}\tau_m \nu_{ext}(\Deltau_{ext})^{2} C_{ext} \tag{12.35}\]</span></p><p>(12.33)-(12.35) can be solved numerically.</p><h4 id="example-vogels-abbott-network">Example: Vogels-Abbottnetwork</h4><p>Excitatory and inhibitory model neurons have the same parameters andare connected with the same probability <spanclass="math inline">\(p\)</span> within and across the twosub-populations. The two difference to the Brunel network are - thechoice of random connectivity in the Vogels-Abbott network does notpreserve the number of presynaptic partners per neuron so that someneurons receive more and others less than <spanclass="math inline">\(pN\)</span> connections - neurons in theVogels-Abbott network communicates with each other by conductance-basedsynapses. A spike fired at time <spanclass="math inline">\(t_j^{(f)}\)</span> causes a change in conductance<span class="math display">\[    \tau_g \frac{\mathrm{d}g}{\mathrm{d}t}=-g+\tau_g \Delta g\sum_{f}^{} \delta(t-t_j^{(f)}). \tag{12.36}\]</span> Thus, a synaptic input causes for <spanclass="math inline">\(t&gt;t_j^{(f)}\)</span> a contribution to theconductance <span class="math inline">\(g(t)=\Delta g \exp[-(t-t_j^{(f)})/\tau_g]\)</span>.</p><p>The dominant effect of conductance based input is a decrease of theeffective membrane time constant. The mean input current <spanclass="math inline">\(I_0\)</span> and the fluctuations <spanclass="math inline">\(\sigma\)</span> of the membrane voltage also enterinto the time constant <spanclass="math inline">\(\tau_{eff}\)</span>.</p><p>The Siegert formula holds only for short time constants for theconductances (<span class="math inline">\(\tau_{E}\to 0\)</span> and<span class="math inline">\(\tau_{I}\to 0\)</span>).</p>]]></content>
    
    
    <categories>
      
      <category>神经动力学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>神经科学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (11)</title>
    <link href="/2022/10/04/Neuronal-Dynamics-11/"/>
    <url>/2022/10/04/Neuronal-Dynamics-11/</url>
    
    <content type="html"><![CDATA[<h1 id="encoding-and-decoding-with-stochastic-neuron-models">Encodingand Decoding with Stochastic Neuron models</h1><p>The online version of this chapter:</p><hr /><p>Chapter 11 Encoding and Decoding with Stochastic Neuron modelshttps://neuronaldynamics.epfl.ch/online/Ch11.html</p><hr /><h2 id="encoding-models-for-intracellular-recordings">Encoding Modelsfor Intracellular Recordings</h2><p>We focus on GLMs with escape noise, also called soft-thresholdintegrate-and-fire models.</p><h3 id="predicting-membrane-potential">Predicting MembranePotential</h3><p>The SRM model <span class="math display">\[    u(t)=\sum_{f}^{} \eta(t-t^{(f)})+\int_{0}^{\infty}\kappa(s)I^{ext}(t-s) \mathrm{d}s +u_{rest}. \tag{11.1}\]</span></p><p>For both the main type of excitatory neurons and the main type oninhibitory neurons, the membrane filter <spanclass="math inline">\(\kappa(t)\)</span> is well described by a singleexponential. Different cell types have different amplitudes and timeconstants. The inhibitory neurons are typically faster, with a smallertime constant than the excitatory neurons, suggesting we coulddiscriminate between excitatory and inhibitory neurons in terms of theshape of <span class="math inline">\(\kappa(t)\)</span>. When we takeinto account the spike-afterpotential, discrimination of cell types ismuch improved. The shape of <span class="math inline">\(\eta(t)\)</span>in inhibitory cells is very different than that in excitatory ones.</p><p>While the spike-afterpotential is a monotonically decreasing functionin the excitatory cells, in the inhibitory cells the function <spanclass="math inline">\(\eta(t)\)</span> is better fitted by twoexponentials of opposite polarity. Spike afterpotential of inhibitoryneurons has an oscillatory component.</p><p>If we set <span class="math inline">\(\kappa(s)=(1/C)\exp(-s/\tau_m)\)</span>, we can take the derivative of (11.1) and write itin the form of <span class="math display">\[    C\frac{\mathrm{d}u(t)}{\mathrm{d}t}=-\frac{1}{R}(u-u_{rest})+\sum_{f}^{}\tilde{\eta}(t-t^{(f)})+I^{ext}(t) \tag{11.2}\]</span> where <span class="math inline">\(\tilde{\eta}\)</span> is thetime course of the net current triggered after a spike.</p><p>After every spike the total current that can charge the membranecapacitance is <span class="math display">\[    C\frac{\mathrm{d}u}{\mathrm{d}t} \propto\eta_{C}(t-\hat{t})(u-E_{rev}) \tag{11.3}\]</span> where <span class="math inline">\(\eta_{C}\)</span> is thespike triggered change in conductance and <spanclass="math inline">\(E_{rev}\)</span> its reversal potential. Theprediction performance based on conductance change instead of spikeafter-effects in terms of potential is not significantly improved.</p><h3 id="predicting-spikes">Predicting Spikes</h3><p>Assuming a moving threshold that can undergo a stereotypical changeat every spike <spanclass="math inline">\(\theta(t)=\theta_0+\sum_{f}^{}\theta_1(t-t^{(f)})\)</span> we can model the conditional firingintensity as (c.f., 9.27) <span class="math display">\[    p(t|S)=\frac{1}{\tau_0}\exp \left[\beta\left(u(t)-\theta_0-\sum_{t^{(f)}\in S}^{} \theta_1(t-t^{(f)})\right)\right] \tag{11.4}\]</span></p><p>Since the parameters regulating <spanclass="math inline">\(u(t)\)</span> were optimized using thesubthreshold membrane potential in 11.1.1, the only free parameters leftare those of the threshold, that is, <spanclass="math inline">\(\theta_0\)</span>,<spanclass="math inline">\(\beta\)</span> and the function <spanclass="math inline">\(\theta_1(t)\)</span>. Once the function <spanclass="math inline">\(\theta_1\)</span> is expanded in a linearcombination of basis functions, maximizing the likelihood (10.40) can bedone through a convex gradient descent.</p><p>Dynamic threshold is not necessary for some neurons. The threshold inthose cells is constant in time. However, the excitatory cells have astrongly moving threshold which is characterized by at least two decaytime constants. Inactivation of sodium channels is a likely candidatefor the biophysical causes of a moving threshold.</p><p>GLMs predict more than 80 percent of the 'predictable' spikes. Somecells were predicted better than others such that the <spanclass="math inline">\(M\)</span> reached up to 95%. Other optimizationmethods but with similar models could improve the spike timingprediction of inhibitory neurons, reaching up to <spanclass="math inline">\(M\)</span>=100% for some cells. However,optimizing a GLM model with refractory effects but no adaptation reducesthe prediction performance by 20-30%, for both the excitatory andinhibitory cortical cells. A single spike has a measurable effect morethan 10 seconds after the action potential has occurred. Thus,adaptation is not characterized by a single time scale and shows up as apower-law decay in both spike-triggered current and threshold.</p><h2 id="encoding-models-in-systems-neuronscience">Encoding Models inSystems Neuronscience</h2><h3 id="receptive-fields-and-linear-nonlinear-poisson-model">Receptivefields and Linear-Nonlinear Poisson Model</h3><p>For a two-dimensional image, we label all pixels with a single index<span class="math inline">\(k\)</span>. A full image corresponds to avector <span class="math inline">\(\mathbf{x}=(x_1,\cdots,x_{K})\)</span> while a single spot of light corresponds to a vectorwith all components equal to zero except one.</p><p>The spatial receptive field of a neuron is a vector <spanclass="math inline">\(\mathbf{k}\)</span> of the same dimensionality as<span class="math inline">\(\mathbf{x}\)</span>. The response of theneuron to an arbitrary spatial stimulus <spanclass="math inline">\(\mathbf{x}\)</span> depends on the total drive<span class="math inline">\(\mathbf{k}\cdot \mathbf{x}_t\)</span>, i.e.,the similarity between the stimulus and the spatial filter.</p><p>More generally, the receptive field filter <spanclass="math inline">\(\mathbf{k}\)</span> can be described not only by aspatial component, but also by a temporal component. The scalar product<span class="math inline">\(\mathbf{k}\cdot \mathbf{x}_t\)</span> is ashorthand notation for integration over space as well as over time. Sucha filter <span class="math inline">\(\mathbf{k}\)</span> is called aspatio-temporal receptive field.</p><p>In the linear-nonlinear-Poisson (LNP) model, one assumes that spiketrains are produced by an inhomogeneous Poisson process with rate <spanclass="math display">\[    \rho(t)=f(\mathbf{k}\cdot \mathbf{x}_t) \tag{11.5}\]</span> Note that the LNP model neglects the spike history effectsthat are the hallmark of the SRM and the GLM - otherwise the two modelsare suprisingly similar.</p><h4id="example-detour-on-reverse-correlation-for-receptive-field-estimation">Example:Detour on reverse correlation for receptive field estimation</h4><p>Reverse correlation measurements are an experimental procedure basedon spike-triggered averaging. Stimuli <spanclass="math inline">\(\mathbf{x}\)</span> are drawn from somestatistical ensemble and presented on after the other. Each time theneuron elicits a spike, the stimulus <spanclass="math inline">\(\mathbf{x}\)</span> presented just before thefiring is recorded. The reverse correlation filter is the mean of allinputs that have triggered a spike <span class="math display">\[    \mathbf{x}_{RevCorr}=\langle \mathbf{x}\rangle_{spike}=\frac{\sum_{t}^{} n_t \mathbf{x}_t}{\sum_{t}^{} n_t},\tag{11.6}\]</span> where <span class="math inline">\(n_t\)</span> is the spikecount in trial <span class="math inline">\(t\)</span>. The reversecorrelation technique finds the typical stimulus that causes aspike.</p><p>Consider an ensemble <spanclass="math inline">\(p(\mathbf{x})\)</span> of stimuli <spanclass="math inline">\(\mathbf{x}\)</span> with a 'power' constraint<span class="math inline">\(\lvert \mathbf{x} \rvert ^{2}&lt;c\)</span>.In this case, the stimulus that is most likely to generate a spike underthe linear receptive field model is the one which is aligned with thereceptive field <span class="math display">\[    \mathbf{x}_{opt}\propto \mathbf{k}  \tag{11.7}  \]</span> The receptive field vector <spanclass="math inline">\(\mathbf{k}\)</span> can be interpreted as theoptimal stimulus to cause a spike.</p><p>Then, consider an ensemble of stimuli <spanclass="math inline">\(\mathbf{x}\)</span> with a radially-symmetricdistribution, where the probability of a possibly multidimensional <spanclass="math inline">\(\mathbf{x}\)</span> is equal to the probability ofobserving its norm <span class="math inline">\(\lvert \mathbf{x} \rvert\colon p(\mathbf{x})=p_c(\lvert \mathbf{x} \rvert )\)</span>. Animportant result is that the experimental reverse correlation techniqueyields an unbiased estimator of the filter <spanclass="math inline">\(\mathbf{k}\)</span>, i.e., <spanclass="math display">\[    \langle \mathbf{x}_{RevCorr}\rangle =\mathbf{k}. \tag{11.8}\]</span></p><h3 id="multiple-neurons">Multiple Neurons</h3><p>An SRM-like model of the membrane potential of a neuron <spanclass="math inline">\(i\)</span> surrounded by <spanclass="math inline">\(n\)</span> other neurons is <spanclass="math display">\[    u_i(t)=\sum_{f}^{} \eta_i(t-t_i^{(f)})+\mathbf{k}_i \cdot\mathbf{x}(t)+\sum_{j \neq i}^{} \sum_{f}^{}\varepsilon_{ij}(t-t_j^{(f)})+u_{rest}. \tag{11.12}\]</span></p><h2 id="decoding">Decoding</h2><p>In this section, we apply 'Bayes' rule to obtain the posteriorprobability of the stimulus, conditional on the observed response: <spanclass="math display">\[    p(\mathbf{x}|D)\propto p(D|\mathbf{x})p(\mathbf{x}), \tag{11.13}\]</span></p><h3 id="maximum-a-posteriori-decoding">Maximum a posterioridecoding</h3><p>The <strong>Maximum A Posteriori</strong> (MAP) estimate is thestimulus <span class="math inline">\(\mathbf{x}\)</span> that is mostprobable given the observed spike response <spanclass="math inline">\(D\)</span>, i.e., the <spanclass="math inline">\(\mathbf{x}\)</span> that maximizes <spanclass="math inline">\(p(\mathbf{x}|D)\)</span>.</p><p>The log-posterior, <span class="math display">\[    \log p(\mathbf{x}|D)=\log p(D|\mathbf{x})+\log p(\mathbf{x})+c\tag{11.14}\]</span> is concave as long as the stimulus log-prior <spanclass="math inline">\(\log p(\mathbf{x})\)</span> is itself a concavefunction of <span class="math inline">\(\mathbf{x}\)</span> (e.g. <spanclass="math inline">\(p\)</span> is Gaussian). In this case, again, wemay easily compute <span class="math inline">\(\hat{x}_{MAP}\)</span> bynumerically ascending the function <span class="math inline">\(\logp(\mathbf{x}|D)\)</span>.</p><p>The MAP estimate of the stimulus is, in general, a nonlinear functionof the observed spiking data <span class="math inline">\(D\)</span>.</p><h4 id="example-linear-stimulus-reconstruction">Example: Linear stimulusReconstruction</h4><p>We predict the stimulus <spanclass="math inline">\(\mathbf{x}_t\)</span> by linear filtering of theobserved spike times <span class="math inline">\(t^{1},t^{2},\cdots,t^{F}&lt;t\)</span>, <span class="math display">\[    x(t)=x_0+\sum_{f}^{} k(t-t^{f}) \tag{11.15}\]</span></p><p>The aim is to find the shape of the filter <spanclass="math inline">\(k\)</span>, i.e., the optimal linear estimator(OLE) of the stimulus. It can be obtained using standard least-squaresregression of the spiking data onto the stimulus <spanclass="math inline">\(\mathbf{x}\)</span>.</p><h3 id="assessing-decoding-uncertainty">Assessing decodinguncertainty</h3><p>In addition to providing a reliable estimate of the stimulusunderlying a set of spike responses, computing the MAP estimate <spanclass="math inline">\(\hat{x}_{MAP}\)</span> gives us easy access to thevariance of the posterior distribution around <spanclass="math inline">\(\hat{x}_{MAP}\)</span>. It tells us somethingabout which stimulus features are best encoded by the response <spanclass="math inline">\(D\)</span>.</p><p>For example, along stimulus axes where the posterior has smallvariance (i.e. the posterior declines rapidly as we move away from <spanclass="math inline">\(\hat{x}_{MAP}\)</span>), we have relatively highcertainty that the true <span class="math inline">\(\mathbf{x}\)</span>is close to <span class="math inline">\(\hat{x}_{MAP}\)</span>.Conversely, we have relatively low certainty about any feature axisalong which the posterior variance is large.</p><p>We measure the curvature at <spanclass="math inline">\(\hat{x}_{MAP}\)</span> by computing the 'Hessian'matrix <span class="math inline">\(A\)</span> of second-derivatives ofthe log-posterior, <span class="math display">\[    A_{ij}=-\frac{\partial ^{2}}{\partial x_i \partial x_j}\logp(\mathbf{x}|D). \tag{11.16}\]</span></p><p>Moreover, the eigendecomposition of this matrix <spanclass="math inline">\(A\)</span> tells us exactly which axes of stimulusspace correspond to the 'best' and 'worst' encoded feature of the neuralresponse: small eigenvalues of <span class="math inline">\(A\)</span>correspond to directions of small curvature, where the observed data<span class="math inline">\(D\)</span> poorly constrains the posteriordistribution <span class="math inline">\(p(\mathbf{x}|D)\)</span> (andtherefore the posterior variance will be relatively large in thisdirection), while conversely large eigenvalues in <spanclass="math inline">\(A\)</span> imply relatively precise knowledge of<span class="math inline">\(\mathbf{x}\)</span>, i.e., small posteriorvariance (for this reason the Hessian of the log-likelihood <spanclass="math inline">\(p(D|x)\)</span> is referred to as the 'observedFisher information matrix' in the statistic literature).</p><p>We can use this Hessian to construct a useful approximation to theposterior <span class="math inline">\(p(\mathbf{x}|D)\)</span>. The ideais simply to approximate this log-concave bump with a Gaussian function,where the parameters of the Gaussian are chosen to exactly match thepeak and curvature of the true posterior. <span class="math display">\[    p(\mathbf{x}|D)\thickapprox (2\pi)^{-d/2}\lvert A \rvert^{1/2}\mathrm{e}^{-(\mathbf{x}-\hat{x}_{MAP})^{\mathsf{T}}A(\mathbf{x}-\hat{x}_{MAP})^{2}}, \tag{11.17}\]</span> with <spanclass="math inline">\(d=\text{dim}(\mathbf{x})\)</span>. The approximateposterior entropy of variance of <spanclass="math inline">\(x_i\)</span> is <spanclass="math inline">\(var(x_i|D)\thickapprox [A^{-1}]_{ii}\)</span>.</p>]]></content>
    
    
    <categories>
      
      <category>神经动力学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>神经科学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>永久记录 （五）</title>
    <link href="/2022/09/18/%E6%B0%B8%E4%B9%85%E8%AE%B0%E5%BD%95%EF%BC%88%E4%BA%94%EF%BC%89/"/>
    <url>/2022/09/18/%E6%B0%B8%E4%B9%85%E8%AE%B0%E5%BD%95%EF%BC%88%E4%BA%94%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>今天是918事变爆发91周年，也是我在环庆南路820号隔离的第一天。</p><p>2022年9月12日，开学的前一天，校内的自习教室因为第二天要开学而统一关闭整理。图书馆的预约制度很繁琐，但相比于在寝室面对不确定的空气质量还是图书馆比较好。我在早上八点后预约到了上午到12：30的主馆三楼A300门外角落靠窗的座位。中午应同学之邀在七餐吃了一份西餐。下午我在A300进门右手边靠窗的座位自习，直至4:30。</p><p>2022年9月13日中午，仅上了半天的线上课程后，传出交大新零号病人的信息。从该天下午起便开始上网课。</p><p>9月16日晚22:55，我收到防控办学生组的短信称我被判定为密接，不能参加第二天的四级考试。短信内容显示“预计在明天上午由疾控部门进行转运”。第二天一早我查看了我的随申码，已经成为了红码。</p><p>收拾完行李后，我等待了超过24小时，并于9月17日23:38收到半小时后转运的通知。</p><p>9月18日1:11，我得知自己将去浦东的47号隔离点。1:22，我穿上防护服登上了转运的大巴。防护服很闷，质量缩水的N95口罩带着还是很勒耳朵，大巴前后座位间隙很小，从闵行到浦东的路途很长。</p><p>2:38，大巴到达并停在隔离点园区门口。手机的电量从接近80%到了26%，我从包里拿出了充电宝。4:00，经过将近一个半小时的等待，来得晚的三辆大巴都先于我们进入隔离点，不禁让人感恩登记工作人员的低能和政策制定者的脑残。4:08，疾控的工作人员，一男一女登上大巴，给每人发了两张纸，微信扫码填表。工作人员很耐心且温柔，提供了极大的情绪价值。疲乏的车里没有宣泄，也不应该有向这两位工作人员的宣泄。</p><p>4:28，我进入隔离房间。我把防护服扔到门外，洗澡后在4:41睡去。</p><p>7:33，我被强而有力的敲门声吵醒。门外放着一桶康师傅红烧牛肉面。伴着《契约之吻》和《LycorisRecoil》的最新一集吃完泡面后，我重新睡去。酒店的隔音一般，窗也因栓有铁链而只能打开约20度，但是窗帘的遮光性很好，关灯后好似黑夜。</p><p>11:30，敲门声送来中饭：糖醋里脊、青椒茭白炒肉丝、海带根、一块鱼排，我只吃了前两者。</p><p>我感谢我的舍友，他给了我5瓶冰露，而隔离点的16瓶冰露将是接下去8天的全部水源，隔离点售价3元一瓶。我感谢寝室楼的楼长也是学长，他分发给我们水果，为我们取来穿着并不舒服的防护服。我感谢但不完全感谢学院的思政老师们，为她们连续几十个小时连轴转的苦劳而感谢，为她们在整个流程中起到的聊胜于无的组织作用而惋惜。我感谢登上大巴引导我们登记的姐姐和哥哥，带来离开穿着防护服与口罩在狭窄的大巴内憋屈的希望。我感谢B站UP主@V在燃烧，他的歌曲《Igot smoke》成为我这几天最喜欢一首。</p><p>虽然我一生中做了无数回过头看错误的选择，但我从来不后悔。青年时期，我在一段朦胧的亲密关系中寻找生命的意义，在一段结果差强人意的转学与数学竞赛中寻找生命的意义，在一所充满争议的大学中寻找生命的意义，在荒谬的世界中寻找生命的意义。但是，我遇到无数可爱的人、有趣的人、温暖的人，我三生有幸。</p><p>祝看到这篇记录的每个人都能安稳、健康、不碌碌无为地度过一生。</p><p>本文的后几段参考了水源社区的一个帖子。并同样引用非常棒的一段话，放在这里：</p><p>夫专诸之刺王僚也，彗星袭月；聂政之刺韩傀也，白虹贯日；要离之刺庆忌也，仓鹰击于殿上。此三子者，皆布衣之士也，怀怒未发，休祲降于天，与臣而将四矣。若士必怒，伏尸二人，流血五步，天下缟素，今日是也。——《唐雎不辱使命》</p><p>活下去，让每个人获得与之德行相匹配的下场。</p>]]></content>
    
    
    <categories>
      
      <category>杂项</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随想</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (10)</title>
    <link href="/2022/09/16/Neuronal-Dynamics-10/"/>
    <url>/2022/09/16/Neuronal-Dynamics-10/</url>
    
    <content type="html"><![CDATA[<h1 id="estimating-parameters-of-probabilistic-neuron-models">EstimatingParameters of Probabilistic Neuron Models</h1><p>The online version of this chapter:</p><hr /><p>Chapter 10 Estimating Parameters of Probabilistic Neuron Modelshttps://neuronaldynamics.epfl.ch/online/Ch10.html</p><hr /><h2 id="parameter-optimization-in-linear-and-nonlinear-models">Parameteroptimization in linear and nonlinear models</h2><h3 id="linear-models">Linear Models</h3><p>Suppose the maximal amplitude of the input current has been chosensmall enough for the neuron to stay in the subthreshold regime. <spanclass="math display">\[    u(t)=\int_{0}^{\infty} \kappa(s)I(t-s) \mathrm{d}s+u_{rest}\tag{10.1}\]</span> vector <span class="math display">\[    \mathbf{k}=(\kappa(\mathrm{d}t),\kappa(2 \mathrm{d}t),\cdots,\kappa(K \mathrm{d}t)). \tag{10.2}        \]</span> which describes the time course <spanclass="math inline">\(\kappa\)</span> in discrete time. The inputcurrent <span class="math inline">\(I\)</span> during the last <spanclass="math inline">\(k\)</span> time steps is given by <spanclass="math display">\[    \mathbf{x}_{t}=(I_{t-1},\cdots ,I_{t-K})\mathrm{d}t \tag{10.3}\]</span></p><p>The discrete-time version of (10.1) is <span class="math display">\[    u_t=\sum_{l=1}^{K} k_lI_{t-l}\mathrm{d}t+u_{rest}=\mathbf{k}\cdot\mathbf{x}_t+u_{rest}. \tag{10.4}\]</span></p><p>Prediction <span class="math inline">\(u_t\)</span> of the modelequation (10.4) with the experimental measurement <spanclass="math inline">\(u_t^{exp}\)</span>. The components of the vector<span class="math inline">\(\mathbf{k}\)</span> will be chosen such that<span class="math display">\[    E(\mathbf{k})=\sum_{t=K+1}^{T} [u_t^{exp}-u_t]^{2} \tag{10.5}\]</span> is minimal.</p><h4 id="example-analytical-solution">Example: Analytical solution</h4><p>Let</p><p><span class="math display">\[    X=    \begin{pmatrix}    I_{K} &amp; I_{K-1} &amp; \cdots &amp; I_1 \\    I_{K+1} &amp; I_{K} &amp; \cdots &amp; I_2 \\    \vdots &amp; \vdots &amp; &amp; \vdots \\    I_{T} &amp; I_{T-1} &amp; \cdots &amp; I_{T-K+1} \\    \end{pmatrix} \mathrm{d}t\]</span></p><p><span class="math inline">\(\mathbf{u}^{exp}=(u_{K+1}^{exp},\cdots,u_{T}^{exp})^{\mathsf{T}}\)</span>,<spanclass="math inline">\(\mathbf{u}_{rest}\)</span> is a vector with allcomponents equal to <span class="math inline">\(u_{rest}\)</span>.</p><p>(10.4) can be written as <span class="math display">\[    \mathbf{u}=X \mathbf{k}^{\mathsf{T}}+\mathbf{u}_{rest}\tag{10.6}       \]</span> (10.5) indicates <span class="math display">\[    \nabla _{\mathbf{k}} E=0\]</span></p><p>Since <span class="math display">\[    E(\mathbf{k})=[u^{exp}-X\mathbf{k}-\mathbf{u}_{rest}]^{\mathsf{T}}\cdot [u^{exp}-X\mathbf{k}-\mathbf{u}_{rest}] \tag{10.7}\]</span></p><p>The solution is the parameter vector <span class="math display">\[    \hat{\mathbf{k}}_{LS}=(X^{\mathsf{T}}X)^{-1}X^{\mathsf{T}}(\mathbf{u}^{exp}-\mathbf{u}_{rest})\tag{10.8}\]</span></p><p>assuming the matrix <spanclass="math inline">\(X^{\mathsf{T}}X\)</span> is invertible. (If thismatrix is non-invertible, then a unique minimum does not exist.)</p><h3 id="generalized-linear-models">Generalized Linear Models</h3><p>Deterministic formulation of the SRM, <span class="math display">\[    u(t)=\int_{0}^{\infty} \eta(s)S(t-s) \mathrm{d}s+\int_{0}^{\infty}\kappa(s)I(t-s) \mathrm{d}s+u_{rest}. \tag{10.9}\]</span></p><p>Suppose that the spike history filter <spanclass="math inline">\(\eta\)</span> extends over a maximum of <spanclass="math inline">\(J\)</span> time steps. Then we can introduce a newparameter vector. <span class="math display">\[    \mathbf{k}=(\kappa(\mathrm{d}t),\kappa(2 \mathrm{d}t),\cdots,\kappa(K \mathrm{d}t),\eta(\mathrm{d}t),\eta(2 \mathrm{d}t),\cdots,\eta(J \mathrm{d}t),u_{rest})\tag{10.10}\]</span></p><p>The spike train in the last <span class="math inline">\(J\)</span>time steps is represented by the spike count sequence <spanclass="math inline">\(n_{t-1},n_{t-2},\cdots ,n_{t-J}\)</span>, where<span class="math inline">\(n_t \in \{0,1\}\)</span>, and included intothe 'input' vector <span class="math display">\[    \mathbf{x}_t=(I_{t-1}\mathrm{d}t,I_{t-2}\mathrm{d}t,\cdots,I_{t-K}\mathrm{d}t,n_{t-1},n_{t-2},\cdots ,n_{t-J},1). \tag{10.11}\]</span> So the discrete-time version <span class="math display">\[    u_t=\sum_{j=1}^{J} k_{K+j}n_{t-j}+\sum_{k=1}^{K}k_{k}I_{t-k}\mathrm{d}t+u_{rest}=\mathbf{k}\cdot \mathbf{x}_t\tag{10.12}\]</span></p><p>We have the firing intensity <span class="math display">\[    \rho(t)=f(u(t)-\theta)=f(\mathbf{k}\cdot \mathbf{x}_t-\theta),\tag{10.13}\]</span></p><h2 id="statistical-formulation-of-encoding-models">StatisticalFormulation of Encoding Models</h2><p>A neural 'encoding model' is a model that assigns a conditionalprobability, <span class="math inline">\(p(D|\mathbf{x})\)</span>, toany possible neural response <span class="math inline">\(D\)</span>given a stimulus <span class="math inline">\(\mathbf{x}\)</span>. Wehypothesize some encoding model, <span class="math display">\[    p(D|\mathbf{x},\theta). \tag{10.16}\]</span> Here <span class="math inline">\(\theta\)</span> is ashort-hand notation for the set of all model parameters. In the exampleof the previous section, the model parameters are <spanclass="math inline">\(\theta=\{\mathbf{k}\}\)</span></p><h3 id="parameter-estimation">Parameter estimation</h3><p>Find a good estimate for <span class="math inline">\(\theta\)</span>for a chosen model class: - Introduce a model that makes sensebiophysically, and incorporates our prior knowledge in a tractablemanner. - Write down the likelihood of the observed data given the modelparameters, along with a prior distribution that encodes our priorbeliefs about the model parameters. - Compute the posterior distributionof the model parameters given the observed data, using Bayes'rule, <spanclass="math display">\[    p(\theta|D)\propto p(D|\theta)p(\theta);\tag{10.18}\]</span></p><p>The maximum likelihood (ML): <span class="math display">\[    ML:\hat{k}_{ML}=\mathop{\arg\max}_\mathbf{k}\{p(D|X,\mathbf{k})\}\]</span></p><p>maximum a posteriori (MAP): <span class="math display">\[    MAP:\hat{k}_{MAP}=\mathop{\arg\max}_\mathbf{k}\{p(D|X,\mathbf{k})p(\mathbf{k})\}\]</span></p><p>where the maximization runs over all possible parameter choices.</p><p>Assume that spike counts per bin follows <spanclass="math display">\[    n_t \sim Poiss[\rho(t)\mathrm{d}t]\]</span></p><p>With the rate parameter of the Poisson distribution given by GLM orSRM model <span class="math inline">\(\rho(t)=f(\mathbf{k}\cdot\mathbf{x}_t)\)</span>, we have <span class="math display">\[    p(D|X,\mathbf{k})=\prod_{t}^{} \left\{ \frac{[f(\mathbf{k}\cdot\mathbf{x}_t) \mathrm{d}t]^{n_t}}{(n_t)!}\exp [-f(\mathbf{x}_t\cdot\mathbf{k})\mathrm{d}t]\right\}\]</span></p><p>For a given observed spike train, the spike count numbers <spanclass="math inline">\(n_t\)</span> are fixed, so we treat them asconstants. We reshuffle the terms and consider the logarithm, <spanclass="math display">\[    \log p(D|X,\mathbf{k})=c_0+\sum_{t} \{ n_t \log f(\mathbf{k}\cdot\mathbf{x}_t)-f(\mathbf{x}_t\cdot \mathbf{k})\mathrm{d}t\} \tag{10.23}\]</span></p><p>If we assume that - <span class="math inline">\(f(u)\)</span> is aconvex (upward-curving) function of its scalar argument <spanclass="math inline">\(u\)</span> - <span class="math inline">\(\logf(u)\)</span> is concave (downward-curving) in <spanclass="math inline">\(u\)</span>, <span class="math display">\[    \mathbf{x}_t=(1,I_{t-1}\mathrm{d}t,I_{t-2}\mathrm{d}t,\cdots,I_{t-K}\mathrm{d}t,n_{t-1},n_{t-2},\cdots ,n_{t-J}). \tag{10.24}\]</span></p><p>the parameter vector <span class="math display">\[    \mathbf{k}=(b,\kappa(\mathrm{d}t),\kappa(2\mathrm{d}t),\cdots,\kappa(K\mathrm{d}t),\eta(\mathrm{d}t),\eta(2\mathrm{d}t),\cdots,\eta(J\mathrm{d}t)); \tag{10.25}\]</span></p><p>here <span class="math inline">\(b=u_{rest}-\theta\)</span> is aconstant offset term which we want to optimize.</p><p>then (10.23) is guaranteed to be a concave function of <spanclass="math inline">\(\mathbf{k}\)</span>.</p><p>Note that, <span class="math inline">\(\rho(t)\)</span> depends onthe past spike trains, therefore <spanclass="math inline">\(D=\{n_t\}\)</span> is no longer a Poissonprocess.</p><p>Finally, we expand the definition of <spanclass="math inline">\(X\)</span> to include observations of other spiketrains. Spike counts are conditionally Poisson distributed given <spanclass="math inline">\(\rho_i(t)\)</span>. <spanclass="math inline">\(n_{i,t}\sim Poiss(\rho_i(t)\mathrm{d}t)\)</span>with a firing rate <span class="math display">\[    \rho_i(t)=f\left(\mathbf{k}_i\cdot \mathbf{x}_t+\sum_{i&#39;\neqi}^{}\sum_{j}^{}  \varepsilon_{i&#39;,j}n_{i&#39;,t-j}\right)\]</span></p><p>these terms are summed over all past spike activity <spanclass="math inline">\(n_{i&#39;,i-j}\)</span> in the population ofcells.</p><p><span class="math inline">\(\rho_i(t)\)</span>: the instantaneousfiring rate of the <span class="math inline">\(i\)</span>-th cell attime <span class="math inline">\(t\)</span> <spanclass="math inline">\(\mathbf{k}_i\)</span>: the cell's linear receptivefield including spike-history effects. <spanclass="math inline">\(\varepsilon_{i&#39;,j}\)</span>: the net effect ofa spike of neuron <span class="math inline">\(i&#39;\)</span> onto themembrane potential of neuron <span class="math inline">\(i\)</span>. Ifwe record from all neurons in the population, <spanclass="math inline">\(\varepsilon_{i&#39;,j}\)</span> can be interpretedas the excitatory or inhibitory postsynaptic potential caused by a spikeof neuron <span class="math inline">\(i&#39;\)</span> a time <spanclass="math inline">\(j \mathrm{d}t\)</span> earlier.</p><h4 id="example-linear-regression-and-voltage-estimation">Example:Linear regression and voltage estimation</h4><p>We want to show that the standard procedure of least-squareminimization can be linked to statitical parameter estimation under theassumption of Gaussian noise.</p><p>Set <span class="math inline">\(\mathbf{x}_t=(I_{t-1},\cdots,I_{t-K})\mathrm{d}t\)</span> and <spanclass="math inline">\(\mathbf{k}=(\kappa(\mathrm{d}t),\cdots,\kappa(K\mathrm{d}t))\)</span>.</p><p>If we assume that the discrete-time voltage measurements have aGaussian distribution around the mean predicted by (10.4), then we needto maximize the likelihood. <span class="math display">\[    \log p(D|X,\mathbf{k})=c_1-c_2 \sum_{t}^{} (u_t-(\mathbf{k}\cdot\mathbf{x}_t))^{2}, \tag{10.28}\]</span> where <span class="math display">\[    X=\begin{pmatrix}    \mathbf{x}_1/\mathrm{d}t \\    \vdots \\    \mathbf{x}_T/\mathrm{d}t    \end{pmatrix}\]</span></p><p><span class="math inline">\(c_1,c_2\)</span> are constants that donot depend on the parameter <spanclass="math inline">\(\mathbf{k}\)</span>. Maximization yields <spanclass="math inline">\(\mathbf{k}_{opt}=(X^{\mathsf{T}}X)^{-1}(\sum_{t}^{}u_t \mathbf{x}_t/\mathrm{d}t)\)</span> which determines the time courseof the filter <span class="math inline">\(\kappa(s)\)</span>. The resultis identical to (10.8): <span class="math display">\[    \hat{\mathbf{k}}_{opt}=\hat{\mathbf{k}}_{LS}\]</span></p><h3 id="regularization-maximum-penalized-likelihood">Regularization:maximum penalized likelihood</h3><p>We want to maximize the posterior <span class="math display">\[    p(\mathbf{k}|X,D)\propto p(D|X,\mathbf{k})p(\mathbf{k}) \tag{10.30}\]</span> here <span class="math inline">\(p(\mathbf{k})\)</span>encodes our a priori beliefs about the true underlying <spanclass="math inline">\(\mathbf{k}\)</span>.</p><p><span class="math display">\[    \log p(k|X,\mathbf{D})=c+\log p(\mathbf{k})+\sum_{t}^{} (n_t \logf(\mathbf{x}_t\cdot \mathbf{k})-f(\mathbf{x}_t\cdot\mathbf{k})\mathrm{d}t). \tag{10.32}\]</span></p><h4 id="example-linear-regression-and-gaussian-prior">Example: Linearregression and Gaussian prior</h4><p>Consider a zero-mean Gaussian <span class="math display">\[    \log p(\mathbf{k})=c-\mathbf{k}^{\mathsf{T}}A \mathbf{k}/2.\]</span> where <span class="math inline">\(A\)</span> is a positivedefinite matrix (the inverse covariance matrix). Combining with (10.28),maximizing the corresponding posterior leads directly to the regularizedleast-square estimator <span class="math display">\[    \hat{\mathbf{k}}_{RLS}=(X^{\mathsf{T}}X+A)^{-1}\left(\sum_{t}^{} u_t\mathbf{x}_t/\mathrm{d}t\right)\]</span></p><h2 id="evaluating-goodness-of-fit">Evaluating Goodness-of-fit</h2><p>In the following we assume that the goodness of fit quantities arecomputed using 'cross-validation': parameters are estimated using thetraining set, and then the goodness of fit quantification is performedon the test set.</p><h3 id="comparing-spiking-membrane-potential-recordings">ComparingSpiking Membrane Potential Recordings</h3><div class="note note-info">            <p>这里提到，如果使用最小平方误差来优化模型，那么实际上隐含了剩余误差呈正态分布的假设。考虑到只有当所有电压轨迹都未达到threshold时，电压分布才是正态的（这里忽略了分布概率趋近于0的那一部分）。</p>          </div><p>下面是[1]中的一段话，也可以部分解释最小二乘法与高斯分布的误差之间的密切关系：</p><hr /><p>The use of the least squares method for extracting information fromimperfect observations assumes a specific a priori probabilitydistribution for the errors, viz. the Gauss distribution. The sameassumption, however, cannot be true for all variable that might be usedto measure the observed quantity (but at most for one variable, and allthose that are linearly connected with it). The method of least squaresapplied in the frequency scale does not lead to the same result as whenapplied to the same observations plotted according to wavelengths. The'best value' for the brightness of a star depends on whether one appliesthe method of least squares to the magnitude or to its intensity inenergy measure. The redeeming feature is, as long as the errors aresmall, that any reasonable transformation is practically linear in therelevant range. But there is no logical foundation for applying it towidely scattered data.</p><hr /><p>The goodness-of-fit in terms of subthreshold membrane potential awayfrom spikes is considered separately from the goodness-of-fit in termsof the spike times only.</p><p>We compute the squared error between the recorded membrane potential<span class="math inline">\(u_t^{exp}\)</span> and model membranepotential <span class="math inline">\(u_t^{mod}\)</span> with forcedspikes at the times of the observed ones. All voltage traces start atthe same point and repeated <span class="math inline">\(N_{rep}\)</span>times. For subthreshold membrane potential, we can average the squarederror over all recorded times <span class="math inline">\(t\)</span>that are not too close to an action potential: <spanclass="math display">\[    RMSE_{nm}=\sqrt{\frac{1}{T_{\Omega_1}N_{rep}}\sum_{i=1}^{N_{rep}}\int_{\Omega_1}^{} (u_i^{exp}(t)-u_i^{mod}(t))^{2} \mathrm{d}t}\tag{10.37}\]</span> where <span class="math inline">\(\Omega_1\)</span> refers tothe ensemble of time bins at least 5 ms before or after any spikes and<span class="math inline">\(T_{\Omega_1}\)</span> is the total time in<span class="math inline">\(\Omega_1\)</span>. <spanclass="math inline">\(RMSE_{nm}\)</span> has index <spanclass="math inline">\(n\)</span> for 'neuron' and index <spanclass="math inline">\(m\)</span> for 'model'.</p><p>For spike times, we find times which are sufficiently away from aspike in any repetition and compute the average squared error <spanclass="math display">\[    RMSE_{nn}=\sqrt{\frac{2}{T_{\Omega_2}N_{rep}(N_{rep}-1)}\sum_{i=1}^{N_{rep}}\sum_{j=1}^{i-1} \int_{\Omega_2}^{} (u_j^{exp}j(t)-u_i^{exp}(t))^{2}\mathrm{d}t} \tag{10.38}\]</span> where <span class="math inline">\(\Omega_2\)</span> refers tothe ensemble of time bins far from the spike times in any repetition and<span class="math inline">\(T_{\Omega_2}\)</span> is the total time in<span class="math inline">\(\Omega_2\)</span>. Typically, 20 ms beforeand 200 ms after the spike is considered sufficiently far (spikeafterpotentials can extend for longer, so its a rather badapproximation). Because the earlier spiking history will affect themembrane potential, the <span class="math inline">\(RMSE_{nn}\)</span>calculated in (10.38) is an overestimate.</p><p>We compute the model error with the intrinsic error by taking theratio <span class="math display">\[    RMSER=\frac{RMSE_{nn}}{RMSE_{nm}} \tag{10.39}\]</span></p><p>The root-mean-squared-error ratio (RMSER) reaches one if the modelprecision is matched with intrinsic error. When smaller than one, theRMSER indicates that the model could be improved. Values larger than oneare possible because <span class="math inline">\(RMSE_{nn}\)</span> isan overestimate of the true intrinsic error.</p><h3 id="spike-train-likelihood">Spike Train Likelihood</h3><p><span class="math display">\[    L^{n}(S|\theta)=\prod_{t^{(i)}\in S}^{} \rho(t^{(i)}|S,\theta)\exp\left[ -\int_{0}^{T} \rho(s|S,\theta) \mathrm{d}s\right] \tag{10.40}\]</span> where we used <spanclass="math inline">\(\rho(t^{(i)}|S,\theta)\)</span> to emphasize thatthe firing intensity of a spike at <spanclass="math inline">\(t^{(i)}\)</span> depends on both the stimulus andspike history as well as the model parameter <spanclass="math inline">\(\theta\)</span>.</p><p>We compare <span class="math inline">\(L^{n}\)</span> with thelikelihood of a homogeneous Poisson model with a constant firingintensity <span class="math inline">\(\rho_0=n/T\)</span>. Thedifference in log-likelihood between the Poisson model and the neuronmodel is finally divided by the total number <spanclass="math inline">\(n\)</span> of observed spikes in order to obtain aquantity with units of 'bits per spike': <span class="math display">\[    \frac{1}{n}\log_{2}\frac{L^{n}(S|\theta)}{\rho_0^{n}\mathrm{e}^{-\rho T} } \tag{10.41}\]</span></p><p>This quantity can be interpreted as an instantaneous mutualinformation between the spike count in a single time bin and thestimulus given the parameters.</p><h3 id="time-rescaling-theorem">Time-rescaling Theorem</h3><p>For a spike train with spikes at <spanclass="math inline">\(t^{(1)}&lt;t^{(2)}&lt;\cdots &lt;t^{(n)}\)</span>and with firing intensity <spanclass="math inline">\(\rho(t|S,\theta)\)</span>, the time-rescalingtransformation <span class="math inline">\(t \to \Lambda(t)\)</span> isdefined as <span class="math display">\[    \Lambda(t)=\int_{0}^{t} \rho(x|S,\theta) \mathrm{d}x. \tag{10.42}\]</span></p><p>Somewhat suprisingly, <spanclass="math inline">\(\Lambda(t^{(k)})\)</span> (evaluated at themeasured firing times) is a Poisson process with unit rate. A correlateof this time-rescaling theorem is that the time intervals <spanclass="math display">\[    \Lambda(t^{(k)})-\Lambda(t^{(k-1)}) \tag{10.43}\]</span> are independent random variables with an exponentialdistribution. Re-scaling again the time axis with the transformation<span class="math display">\[    z_k=1-\exp [-(\Lambda(t^{(k)})-\Lambda(t^{(k-1)}))] \tag{10.44}\]</span> forms independent uniform random variables on the intervalzero to one.</p><p>To verify that the <span class="math inline">\(z_k\)</span>'s areindependent, we can look at the serial correlation of the interspikeintervals or use a scatter plot <spanclass="math inline">\(z_{k+1}\)</span> against <spanclass="math inline">\(z_k\)</span>. Testing whether the <spanclass="math inline">\(z_k\)</span>'s are uniformly distributed can bedone with a <strong>Kolmogorov-Smirnov</strong> (K-S) test. In our case,the reference function is the uniform distribution, so that itscumulative is simply <span class="math inline">\(z\)</span>. Thus, <spanclass="math display">\[    D=\text{sup}_{z}\lvert P(z)-z \rvert . \tag{10.45}\]</span></p><p>The time-rescaling theorem along with the K-S test provide a usefulgoodness-of-fit measure for spike train data with confidence intervalsthat does not require multiple repetitions.</p><h3 id="spike-train-metric">Spike Train Metric</h3><p>Evaluating the goodness-of-fit in terms of log-likelihood or thetime-rescaling theorem requires that we know that the conditional firingintensity <span class="math inline">\(\rho(t|S,\theta)\)</span>accurately.</p><p>Another approach for comparing spike trains involves defining ametric between spike trains.</p><p>Let us consider spike trains as vectors in an abstract vector space,with these vectors denoted with boldface: <spanclass="math inline">\(\bold{S}\)</span>. For now, consider the generalform <span class="math display">\[    (\bold{S}_{i}, \bold{S}_{j})=\int_{0}^{T} \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} K_{\Delta}(s,s&#39;)S_i(t-s)S_j(t-s&#39;)\mathrm{d}s \mathrm{d}s&#39; \mathrm{d}t, \tag{10.46}\]</span> where <span class="math inline">\(K_{\Delta}\)</span> is atwo-dimensional coincidence kernel with a scaling parameter <spanclass="math inline">\(\Delta\)</span>, and <spanclass="math inline">\(T\)</span> is the maximum length of the spiketrains. <span class="math inline">\(K_{\Delta}\)</span> is required tobe a non-negative function with a global maximum at <spanclass="math inline">\(s=s&#39;=0\)</span>. Moreover, <spanclass="math inline">\(K_{\Delta}(s,s&#39;)\)</span> should fall offrapidly so that <spanclass="math inline">\(K_{\Delta}(s,s&#39;)\thickapprox 0\)</span> forall <span class="math inline">\(s,s&#39;&gt;\Delta\)</span>. Examples ofkernels include <spanclass="math inline">\(K_{\Delta}(s,s&#39;)=k_1(s)k_2(s&#39;)=\frac{1}{\Delta^{2}}\exp[-(s+s&#39;)/\Delta]\Theta(s)\Theta(s&#39;)\)</span>. The scalingparameter <span class="math inline">\(\Delta\)</span> must be small,much smaller than the length <span class="math inline">\(T\)</span> ofthe spike train.</p><p>With <spanclass="math inline">\(K_{\Delta}(s,s&#39;)=\delta(s)\delta(s&#39;)\)</span>,we observe that <span class="math inline">\((\bold{S}_{i},\bold{S}_{i})=\int_{0}^{T} S_i^{2}(t) \mathrm{d}t=n_i\)</span> where<span class="math inline">\(n_i\)</span> is the number of spikes in<span class="math inline">\(\bold{S}_i\)</span>.</p><p>Define distance <span class="math inline">\(D_{ij}\)</span>, betweentwo spike-trains <span class="math display">\[    D_{ij}=\left\| \bold{S}_i-\bold{S}_j \right\|_{} \tag{10.47}\]</span> Consider <spanclass="math inline">\(K_{\Delta}(s,s&#39;)=\delta(s)\delta(s&#39;)\)</span>,then <span class="math inline">\(D_{ij}\)</span> is the total number ofspikes in both <span class="math inline">\(\bold{S}_i\)</span> and <spanclass="math inline">\(\bold{S}_j\)</span> reduced by 2 for each spike in<span class="math inline">\(\bold{S}_i\)</span> that coincided with onein <span class="math inline">\(\mathbf{S}_j\)</span>. For the following,it is useful to think of a distance between spike trains as a number ofnon-coincident spikes.</p><p><span class="math display">\[    \cos \theta_{ij}=\frac{(\mathbf{S}_i,\mathbf{S}_j)}{\left\|\mathbf{S}_i \right\|_{}\left\| \mathbf{S}_j \right\|_{}}. \tag{10.48}\]</span></p><h3 id="comparing-sets-of-spike-trains">Comparing Sets of SpikeTrains</h3><p>Let the two sets of spike trains be denoted by <spanclass="math inline">\(X\)</span> and <spanclass="math inline">\(Y\)</span>, containing <spanclass="math inline">\(N_{X}\)</span> and <spanclass="math inline">\(N_{Y}\)</span> spike trains, respectively. Define<span class="math display">\[    \hat{L}_{X}=\frac{1}{N_X}\sum_{i=1}^{N_{X}} \left\|\mathbf{S}_i^{(x)} \right\|_{}^{2}. \tag{10.49}     \]</span> where we have used '^' to denote that the quantity is anexperimental estimate. <span class="math inline">\(\hat{L}_{X}\)</span>is related to the averaged spike count. <spanclass="math inline">\(L_{X}\)</span> is exactly the averaged spike countif the inner product satisfies i)<span class="math inline">\(\int_{}^{}\int_{}^{} K_{\Delta}(s,s&#39;) \mathrm{d}s \mathrm{d}s&#39;=1\)</span>and ii) <span class="math inline">\(K_{\Delta}(s,s&#39;)=0\)</span>whenever $s-s' $ is greater than the minimum interspike interval of anyof the spike trains considered.</p><p>The vector of averaged spike trains <span class="math display">\[    \hat{\nu}_{X}=\frac{1}{N_X}\sum_{i=1}^{N_{X}} \mathbf{S}_i^{(x)}.\tag{10.50}       \]</span> is another occurrence of the spike density. It defines theinstantaneous firing rate of the spiking process, <spanclass="math inline">\(\nu(t)=\langle \hat{\nu}\rangle\)</span>.</p><p>The variability is defined as the variance <spanclass="math display">\[    \hat{V}_{X}=\frac{1}{N_{X}}\sum_{i=1}^{N_{X}} \left\|\mathbf{S}_i^{(x)}-\hat{\nu}_{X} \right\|_{}^{2}. \tag{10.51}\]</span></p><p>Variability relates to reliability. While variability is a positivequantity that cannot exceed <span class="math inline">\(L_X\)</span>,reliabiliy is usually defined between zero and one where one meansperfectly reliable spike timing: <spanclass="math inline">\(\hat{R}_{X}=1-\hat{V}_{X}/\hat{L}_{X}\)</span>.</p><p>Finally, we come to a measure of match between the set of spiketrains <span class="math inline">\(X\)</span> and <spanclass="math inline">\(Y\)</span>. To reproduce the detailed timestructure of the PSTH, we define <span class="math display">\[    \hat{M}=\frac{2(\hat{\nu}_{X},\hat{\nu}_{Y})}{\hat{R}_{X}\hat{L}_{X}+\hat{R}_{Y}\hat{L}_{Y}}.\tag{10.52}\]</span></p><p>We have <span class="math inline">\(M\)</span> (for match) equal toone if <span class="math inline">\(X\)</span> and <spanclass="math inline">\(Y\)</span> have the same instantaneous firingrate. The smaller <span class="math inline">\(M\)</span> the greater themismatch between the spiking processes. The quantity <spanclass="math inline">\(R_{X}L_{X}\)</span> can be interpreted as a numberof reliable spikes. Since <spanclass="math inline">\((\hat{\nu}_{X},\hat{\nu}_{Y})\)</span> isinterpreted as a number of coincident spikes between <spanclass="math inline">\(X\)</span> and <spanclass="math inline">\(Y\)</span>, we can still regard <spanclass="math inline">\(M\)</span> as a factor counting the fraction ofcoincident spikes.</p><p>If the kernel <spanclass="math inline">\(K_{\Delta}(s,s&#39;)\)</span> is chosen to be<span class="math inline">\(k_g(s)k_g(s&#39;)\)</span> and <spanclass="math inline">\(k_g\)</span> is a Gaussian distribution of width<span class="math inline">\(\Delta\)</span>, then <spanclass="math inline">\(M\)</span> relates to a mean square error betweenPSTHs that were filtered with <span class="math inline">\(k_g\)</span>.Therefore, the kernel used in the definition of the inner product(10.46) can be related to the smoothing filter of the PSTH.</p><h2 id="closed-loop-stimulus-design">Closed-loop stimulus design</h2><p>Here we describe how to take advantage of the properties of the GLMto optimize our experiments: the objective is to select, in an online,closed-loop manner, the stimuli that will most efficiently characterizethe neuron's response properties.</p><p>A property of GLMs: not all stimuli will provide the same amount ofinformation about the unknown coefficients <spanclass="math inline">\(\mathbf{k}\)</span>. We need a well-definedobjective function that will rank any given stimuli according to itspotential informativeness.</p><p>When the goal is estimating the unknown parameters of a model, given<span class="math inline">\(D=\{ \mathbf{x}(s),n_s\}_{s&lt;t}\)</span>,the observed data up to the current trial. The posterior uncertainty in<span class="math inline">\(\theta\)</span> can be quantified using theinformation-theoretic notion of 'entropy'.</p><h2 id="summary">Summary</h2><p>For a suitable chosen model class, the likelihood of the data beinggenerated by the model is a concave function of the model parameters,i.e., there are no local maxima.</p><p>Once neuron models are phrased in the language of statistics, theproblems of coding and stimulus design can be formulated in a singleunified framework.</p><h3 id="comparing-psths-and-spike-train-similarity-measures">ComparingPSTHs and spike train similarity measures</h3><p>Experimentally the PSTH is constructed from a set of <spanclass="math inline">\(N_{rep}\)</span> spike trains, <spanclass="math inline">\(S_i(t)\)</span>, measured from repeatedpresentations of the same stimulus. The ensemble average of the recordedspike trains: <span class="math display">\[    \frac{1}{N_{rep}}\sum_{i=1}^{N_{rep}} S_i(t)\]</span> is typically convolved with a Gaussian function <spanclass="math display">\[    h_g(x)=(2\pi \sigma^{2})^{-1/2}\exp (-x^{2}/2\sigma^{2})\]</span> with <span class="math inline">\(\sigma\)</span> around 5 ms,such that <span class="math inline">\(A_1(t)=(h_g *\frac{1}{N_{rep}}\sum_{}^{} S_i)\)</span> is a smoothed PSTH.</p><p>With the kernel <spanclass="math inline">\(K(t,t&#39;)=h_g(t)h_g(t&#39;)\)</span>, We have<span class="math display">\[    \int_{0}^{T} (A_1(t)-A_2(t))^{2} \mathrm{d}t=\left\|\frac{1}{N_{rep}}\sum_{i=1}^{N_{rep}} (S_1(t)-S_2(t)) \right\|_{}^{2}\]</span></p><p>The correlation coefficient between the two smoothed PSTHs can bewritten as a angular separation between the sets of spike trains withkernel <spanclass="math inline">\(K(t,t&#39;)=h_g(t)h_g(t&#39;)\)</span>.</p><h3 id="victor-and-purpura-metric">Victor and Purpura metric</h3><p>Consider the minimum cost <span class="math inline">\(C\)</span>required to transform a spike train <spanclass="math inline">\(S_i\)</span> into another spike train <spanclass="math inline">\(S_j\)</span> if the only transformations availableare - Removing a spike has a cost of one - Adding a spike has a cost ofone - Shifting a spike by a distance <spanclass="math inline">\(d\)</span> has a cost <spanclass="math inline">\(qd\)</span> where <spanclass="math inline">\(q\)</span> is a parameter defining temporalprecision.</p><p>The <span class="math inline">\(C\)</span> defines a metric thatmeasures the dissimilarity between spike train <spanclass="math inline">\(S_i\)</span> and spike train <spanclass="math inline">\(S_j\)</span>.</p><p>For <span class="math inline">\(q=0\)</span> units of cost perseconds, <span class="math inline">\(C\)</span> becomes the differencein number of spikes in spike trains <spanclass="math inline">\(S_i\)</span> and <spanclass="math inline">\(S_j\)</span>.</p><p>For <span class="math inline">\(q&gt;0\)</span>, <spanclass="math inline">\(C\)</span> can be written as a distance <spanclass="math inline">\(D_{ij}^{2}\)</span> with kernel <spanclass="math inline">\(K(t,t&#39;)=h_t(t)\delta(t&#39;)\)</span> andtriangular function <span class="math display">\[    h_t(t)=(1-\lvert t \rvert q/2)\Theta(1-\lvert t \rvert q/2)\]</span> as follows <span class="math display">\[    C(S_i,S_j)=D_{ij}^{2}=\int_{0}^{T} \int_{-\frac{2}{q}}^{\frac{2}{q}}(1-\lvert s \rvert \frac{q}{2})[S_i(t-s)-S_j(t-s)][S_i(t)-S_j(t)]\mathrm{d}s \mathrm{d}t\]</span></p><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>N.G.van Kampen (2007)Stochastic Processes in Physics and Chemistry.<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>神经动力学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>神经科学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (8)</title>
    <link href="/2022/08/26/Neuronal-Dynamics-8/"/>
    <url>/2022/08/26/Neuronal-Dynamics-8/</url>
    
    <content type="html"><![CDATA[<h1 id="noisy-input-models-barrage-of-spike-arrivals">Noisy InputModels: Barrage of Spike Arrivals</h1><p>The online version of this chapter:</p><hr /><p>Chapter 8 Noisy Input Models: Barrage of Spike Arrivalshttps://neuronaldynamics.epfl.ch/online/Ch8.html</p><hr /><h2 id="noise-input">Noise input</h2><p>Modeling the noisiness of the input amounts to splitting the inputcurrent into two components, a deterministic and a stochastic one <spanclass="math display">\[    I(t)=I^{det}(t)+I^{noise}(t), \tag{8.1}\]</span></p><p><span class="math inline">\(I^{det}\)</span>: known or at leastpredictable. <span class="math inline">\(I^{noise}\)</span>:unpredictable or noisy.</p><p>For example, during an in vitro study with intracellular currentinjection, <span class="math inline">\(I^{det}\)</span> would be thecurrent that is set on the switchboard of the current generator, but theactual current fluctuates around the preset value because of finitetemperature.</p><p>A nonlinear integrate-and-fire model with noisy input has the voltageequation <span class="math display">\[    \tau_m\frac{\mathrm{d}}{\mathrm{d}t}u=f(u)+RI^{det}(t)+RI^{noise}(t).\tag{8.2}\]</span> If <span class="math inline">\(u\)</span> reaches thethreshold <span class="math inline">\(\theta_{reset}\)</span>, theintegration is stopped and the membrane potential reset to <spanclass="math inline">\(u_r\)</span>.</p><h3 id="white-noise">White noise</h3><p>Formulate the noise term <spanclass="math inline">\(RI^{noise}\)</span> in the differential equationof the membrane voltage as a 'white noise', <spanclass="math inline">\(RI^{noise}(t)=\xi(t)\)</span>. White noise <spanclass="math inline">\(\xi\)</span> is a stochastic process characterizedby its expectation value, <span class="math display">\[    \langle \xi(t) \rangle =0, \tag{8.3}\]</span> and the autocorrelation <span class="math display">\[    \langle \xi(t)\xi(t&#39;)\rangle=\sigma^{2} \tau_m \delta(t-t&#39;),\tag{8.4}\]</span> where <span class="math inline">\(\sigma\)</span> is theamplitude of the noise (in units of voltage) and <spanclass="math inline">\(\tau_m\)</span> the time constant of (8.2). (8.4)indicates that the process <span class="math inline">\(\xi\)</span> isuncorrelated in time: knowledge of the value <spanclass="math inline">\(\xi\)</span> at time <spanclass="math inline">\(t\)</span> does not enable us to predict its valueat any other time <span class="math inline">\(t&#39;\neq t\)</span>.</p><p>The Fourier transform of the autocorrelation function (8.4) yieldsthe power spectrum. The power spectrum of white noise is flat, i.e., thenoise is equally strong at all frequencies.</p><p>Stochastic differential equation, i.e., an equation for a stochasticprocess, <span class="math display">\[    \tau_m\frac{\mathrm{d}}{\mathrm{d}t}u(t)=f(u(t))+RI^{det}(t)+\xi(t), \tag{8.5}\]</span> also called <strong>Langevin equation</strong>.</p><p>In the mathematical literature, a different style of writing theLangevin equation dominates. <span class="math display">\[    \mathrm{d}u=f(u)\frac{\mathrm{d}t}{\tau_m}+RI^{det}(t)\frac{\mathrm{d}t}{\tau_m}+\sigma\mathrm{d}W_{t}, \tag{8.6}\]</span> where <span class="math inline">\(\mathrm{d}W_{t}\)</span> arethe increments of the <strong>Wiener process</strong> in a short time<span class="math inline">\(\mathrm{d}t\)</span>, that is, <spanclass="math inline">\(\mathrm{d}W_{t}\)</span> are random variablesdrawn from a Gaussian distribution with zero mean and varianceproportional to the step size <spanclass="math inline">\(\mathrm{d}t\)</span> (therefore its standarddeviation is proportional to <spanclass="math inline">\(\sqrt{\mathrm{d}t}\)</span>). White noise which isGaussian distributed is called Gaussian white noise.</p><h4id="example-leaky-integrate-and-fire-model-with-white-noise-input">Example:Leaky integrate-and-fire model with white noise input</h4><p>In the case of the leaky integrate-and-fire model (with voltage scalechosen such that the resting potential is at zero), the stochasticdifferential equation is <span class="math display">\[    \tau_m \frac{\mathrm{d}}{\mathrm{d}t}u(t)=-u(t)+RI^{det}(t)+\xi(t),\tag{8.7}\]</span> which is called the <strong>Ornstein-Uhlenbeckprocess</strong>.</p><p>The fluctuates of the membrane potential have an autocorrelation withcharacteristic time <span class="math inline">\(\tau_m\)</span>.</p><h3 id="noisy-versus-noiseless-membrane-potential">Noisy versusnoiseless membrane potential</h3><p>Consider (8.7) for constant <spanclass="math inline">\(\sigma\)</span>. At <spanclass="math inline">\(t=\hat{t}\)</span> the membrane potential startsat a value <span class="math inline">\(u=u_r=0\)</span>. The solutionfor <span class="math inline">\(t&gt;\hat{t}\)</span> is <spanclass="math display">\[    u(t)=\frac{R}{\tau_m}\int_{0}^{t-\hat{t}} \mathrm{e}^{-s/\tau_m}I^{det}(t-s) \mathrm{d}s+\frac{1}{\tau_m}\int_{0}^{t-\hat{t}}\mathrm{e}^{-s/\tau_m} \xi(t-s) \mathrm{d}s.     \]</span> (8.9)</p><p>Since <span class="math inline">\(\langle \xi(t)\rangle=0\)</span>,the expected trajectory of the membrane potential is <spanclass="math display">\[    u_0(t)=\langleu(t|\hat{t})\rangle=\frac{R}{\tau_m}\int_{0}^{t-\hat{t}}\mathrm{e}^{-s/\tau_m} I^{det}(t-s) \mathrm{d}s.\]</span> (8.10)</p><p>In particular, for constant input current <spanclass="math inline">\(I^{det}(t)\equiv I_0\)</span> we have <spanclass="math display">\[    u_0(t)=u_{\infty}[1-\mathrm{e}^{-(t-\hat{t})/\tau_m}] \tag{8.11}\]</span> with <span class="math inline">\(u_{\infty}=RI_0\)</span>.</p><p>The variance <span class="math inline">\(\langle \Deltau^{2}\rangle\)</span> can be evaluated as <span class="math display">\[    \langle \Delta u^{2}(t)\rangle=\frac{1}{\tau_m^{2}}\int_{0}^{t-\hat{t}} \int_{0}^{t-\hat{t}}\mathrm{e}^{-s/\tau_m} \mathrm{e}^{-s&#39;/\tau_m} \langle\xi(t-s)\xi(t-s&#39;)\rangle \mathrm{d}s&#39; \mathrm{d}s\]</span> (8.12)</p><p>We use <span class="math inline">\(\langle\xi(t-s)\xi(t-s&#39;)\rangle=\sigma^{2}\tau_m\delta(s-s&#39;)\)</span>and perform the integration. The result is <span class="math display">\[    \langle \Deltau^{2}(t)\rangle=\frac{1}{2}\sigma^{2}\left[1-\mathrm{e}^{-2(t-\hat{t})/\tau_m}\right] \tag{8.13}\]</span></p><p>If the threshold is high enough so that firing is a rare event, thetypical distance between the actual trajectory and the mean trajectoryapproaches with time constant <spanclass="math inline">\(\tau_m/2\)</span> a limiting value <spanclass="math display">\[    \sqrt{\langle \Deltau_{\infty}^{2}\rangle}=\frac{1}{\sqrt{2}}\sigma. \tag{8.14}\]</span></p><h3 id="colored-noise">Colored Noise</h3><p>A noise term with a power spectrum which is not flat is calledcolored noise. Colored noise <spanclass="math inline">\(I^{noise}(t)\)</span> can be generated from whitenoise by suitable filtering. For example, low-pass filtering <spanclass="math display">\[    \tau_s\frac{\mathrm{d}I^{noise}(t)}{\mathrm{d}t}=-I^{noise}(t)+\xi(t),\tag{8.15}\]</span> where <span class="math inline">\(\xi(t)\)</span> is the whitenoise process.</p><p>Integrate (8.15) so as to arrive at <span class="math display">\[    I^{noise}(t)=\int_{0}^{\infty} \kappa(s)\xi(t-s) \mathrm{d}s,\tag{8.16}\]</span></p><p>where <span class="math inline">\(\kappa(s)\)</span> is anexponential low-pass filter with time constant <spanclass="math inline">\(\tau_s\)</span> (Actually <spanclass="math inline">\(\kappa(s)=\exp (-s/\tau_s)/\tau_s\)</span>). Theautocorrelation function is therefore <span class="math display">\[    \langle I^{noise}(t)I^{noise}(t&#39;)\rangle =\int_{0}^{\infty}\int_{0}^{\infty} \kappa(s)\kappa(s&#39;)\langle\xi(t-s)\xi(t&#39;-s&#39;)\rangle \mathrm{d}s&#39; \mathrm{d}s.\]</span> (8.17)</p><p>We exploit the definition of the white noise correlation function in(8.4) and find <span class="math display">\[    \langle I^{noise}(t)I^{noist}(t&#39;)\rangle=a \exp\left(-\frac{\lvert t-t&#39; \rvert }{\tau_s}\right) \tag{8.18}\]</span> with a amplitude factor <spanclass="math inline">\(a\)</span>. That means knowledge of the inputcurrent at time <span class="math inline">\(t\)</span> gives us a hintabout the input current shortly afterward, as long as <spanclass="math inline">\(\lvert t&#39;-t \rvert \ll \tau_s\)</span>.</p><p>The noise spectrum is flat for frequencies <spanclass="math inline">\(\omega\ll 1/\tau_s\)</span> and falls off for<span class="math inline">\(\omega&gt;1/\tau_s\)</span>. Sometimes <spanclass="math inline">\(1/\tau_s\)</span> is called the <strong>cut-offfrequency</strong>.</p><h2 id="stochastic-spike-arrival">Stochastic spike arrival</h2><p>If we focus on a leaky integrate-and-fire neuron, shift the voltageso that the resting potential is at zero and concentrate on a singleneuron receiving stochastic input from background neurons hence drop theinput from the network, we arrive at <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}u=-\frac{u}{\tau_m}+\frac{1}{C}I^{ext}(t)+\sum_{k}^{}\sum_{t_k^{(f)}}^{} w_k \delta(t-t_k^{(f)})\]</span> (8.20)</p><p>The membrane potential is reset to <spanclass="math inline">\(u_r\)</span> whenever is reaches the threshold<span class="math inline">\(\theta\)</span>. (8.20) is called<strong>Stein's model</strong>.</p><p>In Stein's model, each input spike generates a postsynaptic potential<span class="math inline">\(\Delta u(t)=w_k\epsilon(t-t^{(f)}_k)\)</span> with <spanclass="math inline">\(\epsilon(s)=\mathrm{e}^{-s/\tau_m}\Theta(s)\)</span>. Integration of (8.20) yields <spanclass="math display">\[    u(t|\hat{t})=u_r \exp(-\frac{t-\hat{t}}{\tau_m})+\frac{1}{C}\int_{0}^{t-\hat{t}} \exp(-\frac{s}{\tau_m})I(t-s) \mathrm{d}s+\sum_{k=1}^{N} \sum_{t_k^{(f)}}^{}w_k \epsilon(t-t_k^{(f)})\]</span> (8.21)</p><p>for <span class="math inline">\(t&gt;\hat{t}\)</span> where <spanclass="math inline">\(\hat{t}\)</span> is the last firing time of theneuron.</p><h3id="membrane-potential-fluctuations-caused-by-spike-arrivals">Membranepotential fluctuations caused by spike arrivals</h3><p>We assume that each input spike evokes a postsynaptic potential <spanclass="math inline">\(w_0 \epsilon(s)\)</span> of the same amplitude andshape, independent of <span class="math inline">\(k\)</span>. The inputstatistics is assumed to be Poisson. Thus, the total input spike train<span class="math display">\[    S(t)=\sum_{k=1}^{N} \sum_{t_k^{(f)}}^{} \delta(t-t_k^{f}),\tag{8.22}\]</span> that arrives at neuron <span class="math inline">\(i\)</span>is a random process with expectation <span class="math display">\[    \langle S(t)\rangle =\nu_0 \tag{8.23}\]</span> and autocorrelation <span class="math display">\[    \langle S(t)S(t&#39;)\rangle=\nu_0^{2}+\nu_0 \delta(t-t&#39;);\tag{8.24}\]</span></p><p>Suppose that we start theh integration of the passive membraneequation at <span class="math inline">\(t=-\infty\)</span> with initialcondition <span class="math inline">\(u_r=0\)</span>. We rewrite (8.21)using the definition of the spike trian in (8.22) <spanclass="math display">\[    u(t)=\frac{1}{C}\int_{0}^{\infty} \exp\left(-\frac{s}{\tau_m}\right)I(t-s) \mathrm{d}s+w_0 \int_{0}^{\infty}\epsilon(s)S(t-s) \mathrm{d}s. \tag{8.25}\]</span></p><p>Using (8.23) and (8.24) we find <span class="math display">\[    \langle u(t)\rangle=u_0(t)=\frac{1}{C}\int_{0}^{\infty} \exp\left(-\frac{s}{\tau_m}\right)I(t-s) \mathrm{d}s+w_0\nu_0\int_{0}^{\infty} \epsilon(s) \mathrm{d}s \tag{8.26}\]</span> and <span class="math display">\[    \langle \Delta u^{2}\rangle=\langle [u(t)-u_0(t)]^{2}\rangle =w_0^{2}\nu_0 \int_{0}^{\infty} \epsilon^{2}(s) \mathrm{d}s.\]</span></p><p>With stochastic spike arrival at excitatory synapses, mean andvariance cannot be changed independently. As we will see next, acombination of excitation and inhibition allows us to increase thevariance while keeping the mean of the potential fixed.</p><h3 id="balanced-excitation-and-inhibition">Balanced excitation andinhibition</h3><p>Suppose we have excitatory input and inhibitory input of the samefrequency. The mean of the stochastic background input vanishes since<span class="math inline">\(\sum_{k}^{} w_k v_k=0\)</span>. Thestochastic arrival of background spikes generates fluctuations of thevoltage with variance <span class="math display">\[    \langle \Delta u^{2}\rangle=\frac{1}{2}\tau_m \sum_{k}^{}w_k^{2}\nu_k\]</span></p><p>Let us now increase all rates by a factor of <spanclass="math inline">\(a&gt;1\)</span> and multiply at the same time thesynaptic efficacies by a factor <spanclass="math inline">\(1/\sqrt{a}\)</span>. Then both mean and varianceof the stochastic background input are the same as before, but the size<span class="math inline">\(w_k\)</span> of the jumps is decreased.</p><p>In the limite of <span class="math inline">\(a\to \infty\)</span> thejump process turns into a diffusion process. The diffusion limit: <spanclass="math display">\[    \frac{w}{\sqrt{a}}S^{exc}-\frac{w}{\sqrt{a}}S^{inh} \to \xi(t)\tag{8.31}\]</span></p><h4 id="example-synaptic-time-constants-and-colored-noise">Example:Synaptic time constants and colored noise</h4><p>In contrast to the previous discussion of balanced input, we nowassume that each spike arrival generated a current pulse <spanclass="math inline">\(\alpha(s)\)</span> of finite duration so that thetotal synaptic input current is <span class="math display">\[    RI(t)=w^{exc}\int_{0}^{\infty} \alpha(s)S^{exc}(t-s) \mathrm{d}s -w^{inh} \int_{0}^{\infty} \alpha(s)S^{inh}(t-s) \mathrm{d}s\]</span> (8.32)</p><p>If the spike arrivel is Poisson with rates <spanclass="math inline">\(a\nu\)</span> and the synaptic weights are <spanclass="math inline">\(w^{exc}=w^{inh}=w/\sqrt{a}\)</span>, then we cantake the limit <span class="math inline">\(a\to \infty\)</span> with nochange of mean or variance. The result is colored noised.</p><p>An instructive case is <spanclass="math inline">\(\alpha(s)=(1/\tau_s)\exp(-s/\tau_s)\Theta(s)\)</span> with synaptic time constant <spanclass="math inline">\(\tau_s\)</span>. In the limit <spanclass="math inline">\(\tau_s\to 0\)</span> we are back to whitenoise.</p><h2 id="subthreshold-vs.-superthreshold-regime">Subthreshold vs.Superthreshold regime</h2><p>An arbitrary time-dependent stimulus <spanclass="math inline">\(I(t)\)</span> is called subthreshold if isgenerates a membrane potential that stays - in the absence of noise -below the firing threshold. Stimuli that induce spikes even in anoise-free neuron are called superthreshold.</p><p>The distinction between sub- and superthreshold stimuli has importantconsequences for the firing behavior of neurons in the presence ofnoise. Consider a leaky integrate-and-fire neuron with constant input<span class="math inline">\(I_0\)</span> for <spanclass="math inline">\(t&gt;0\)</span>. <span class="math display">\[    u_0(t)=u_{\infty}[1-\mathrm{e}^{-t/\tau_m}]+u_r\mathrm{e}^{-t/\tau_m} . \tag{8.33}\]</span> where <span class="math inline">\(u_{\infty}=RI_0\)</span>. If<span class="math inline">\(u_{\infty}&gt;\theta\)</span>, the neuronfires regularly. The interspike interval is <spanclass="math inline">\(s_0\)</span> derived from <spanclass="math inline">\(u_0(s_0)=\theta\)</span>. Thus <spanclass="math display">\[    s_0=\tau \ln \frac{u_{\infty}-u_r}{u_{\infty}-\theta}. \tag{8.34}\]</span></p><p>In the superthreshold regime, the spike train in the presence ofdiffusive noise is simply a noisy version of the regular spike train ofthe noise-free neuron.</p><h4id="example-interval-distribution-in-the-superthreshold-regime">Example:Interval distribution in the superthreshold regime</h4><p>For small noise amplitude <span class="math inline">\(0&lt;\sigma\llu_{\infty}-\theta\)</span>, and superthreshold stimulation, the intervaldistribution is centered at the deterministic interspike interval <spanclass="math inline">\(s_0\)</span>. As long as the mean trajectory isfar away from the threshold, the distribution of membrane potentials hasa Gaussian shape. As time goes on, the distribution of membranepotentials is pushed across the threshold.</p><p>Suppose the membrane potential crosses the threshold with slope <spanclass="math inline">\(u_0&#39;\)</span>, then the interval distributionis approximately given by a Gaussian with mean <spanclass="math inline">\(s_0\)</span> and width <spanclass="math inline">\(\sigma/\sqrt{2}u_0&#39;\)</span>, i.e., <spanclass="math display">\[    P_0(t|0)=\frac{1}{\sqrt{\pi}}\frac{u_0&#39;}{\sigma}\exp\left[-\frac{(u_0&#39;)^{2}(t-s_0)^{2}}{\sigma^{2}}\right] \tag{8.35}\]</span></p><h2 id="diffusion-limit-and-fokker-planck-equation">Diffusion limit andFokker-Planck equation</h2><p>In this section we analyze (8.20) and show how to map it to thediffusion model. The evolution of the probability density as a functionof time is described, in the diffusion limit, by the Fokker-Planckequation. We derive here in the context of a single neuron subject tonoisy input.</p><p>For simplicity we set for the time being <spanclass="math inline">\(I^{ext}=0\)</span> in (8.20). The input spikes atsynapse <span class="math inline">\(k\)</span> are generated by aPoisson process and arrive stochastically with rate <spanclass="math inline">\(\nu_k(t)\)</span>. The probability that no spikearrives in a short time interval <span class="math inline">\(\Deltat\)</span> is therefore <span class="math display">\[    \operatorname{Pr} \{\text{no spike in}\ [t,t+\Deltat]\}=1-\sum_{k}^{} \nu_k(t)\Delta t. \tag{8.36}     \]</span></p><p>Given a value of <span class="math inline">\(u&#39;\)</span> at time<span class="math inline">\(t\)</span>, the probability density offinding a membrane potential <span class="math inline">\(u\)</span> attime <span class="math inline">\(t+\Delta t\)</span> is given by <spanclass="math display">\[    \begin{aligned}        P^{trans}(u,t+\Delta t|u&#39;,t)=\left[1-\Delta t \sum_{k}^{}\nu_k(t)\right]\delta(u-u&#39; \mathrm{e}^{-\Delta t/\tau_m})\\+\Delta t\sum_{k}^{} \nu_k(t)\delta(u-u&#39;\mathrm{e}^{-\Delta t/\tau_m} -w_k).\\    \end{aligned}\]</span> (8.37)</p><p>We will refer to <span class="math inline">\(P^{trans}\)</span> asthe transition law. The evolution of the membrane potential is a MarkovProcess and can be described by <span class="math display">\[    p(u,t+\Delta)=\int_{}^{} P^{trans}(u,t+\Delta t|u&#39;,t)p(u&#39;,t)\mathrm{d}u&#39;. \tag{8.38}\]</span></p><p>Recall that <spanclass="math inline">\(\delta(au)=a^{-1}\delta(u)\)</span>. The result ofthe integration is <span class="math display">\[    \begin{aligned}        p(u,t+\Delta t)=\left[ 1-\Delta t \sum_{k}^{}\nu_k(t)\right]\mathrm{e}^{\Delta t/\tau_m} p(\mathrm{e}^{\Deltat/\tau_m} u,t) \\        +\Delta t\sum_{k}^{} \nu_k(t)\mathrm{e}^{\Delta t/\tau_m}p(\mathrm{e}^{\Delta t/\tau_m}( u-w_k),t).    \end{aligned}\]</span> (8.39)</p><p>So we have <span class="math display">\[    \frac{\partial p(u,t)}{\partialt}==\frac{1}{\tau_m}p(u,t)+\frac{1}{\tau_m}u\frac{\partial }{\partialu}p(u,t)+\sum_{k}^{} \nu_k(t)[p(u-w_k,t)-p(u,t)].\]</span> (8.40)</p><p>Furthermore, if the jump amplitudes <spanclass="math inline">\(w_k\)</span> are small, we can expand the RHS of(8.40) with respect to <span class="math inline">\(u\)</span> about<span class="math inline">\(p(u,t)\)</span>: <spanclass="math display">\[    \tau_m \frac{\partial }{\partial t}p(u,t)=-\frac{\partial }{\partialu}\left[-u+\tau_m \sum_{k}^{}\nu_k(t)w_k\right]p(u,t)+\frac{1}{2}\left[\tau_m \sum_{k}^{}\nu_k(t)w_k^{2}\right]\frac{\partial ^{2}}{\partial u^{2}}p(u,t),\]</span> (8.41)</p><p>where we have neglected terms of order <spanclass="math inline">\(w_k^{3}\)</span> and higher. The expansion in<span class="math inline">\(w_k\)</span> is called the Kramers-Moyalexpansion.</p><p>In (8.41), the first term in rectangular brackets describes thesystematic drift of the membrane potential due to leakage (<spanclass="math inline">\(\propto -u\)</span>) and mean background input(<span class="math inline">\(\propto \sum_{k}^{} \nu_k(t)w_k\)</span>).The second term in rectangular brackets corresponds to a 'diffusionconstant' and accounts for the fluctuations of the membranepotential.</p><p>(8.41) is equivalent to the Langevin equation (8.7) with <spanclass="math inline">\(RI(t)=\tau_m\sum_{k}^{} \nu_k(t)w_k\)</span> andtime-dependent noise amplitude <span class="math display">\[    \sigma^{2}(t)=\tau_m\sum_{k}^{} \nu_k(t)w_k^{2}. \tag{8.42}\]</span></p><p>For the transition from (8.40) to (8.41) we have suppressed <spanclass="math display">\[    \sum_{n=3}^{\infty} \frac{(-1)^{n}}{n!}A_n(t)\frac{\partial^{n}}{\partial u^{n}}p(u,t) \tag{8.43}      \]</span> with <span class="math inline">\(A_n=\tau_m \sum_{k}^{}\nu_k(t)w_k^{n}\)</span>.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x221.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x222.png" /></div></div></div><p>Figures above shows a sequence of models where the size of theweights <span class="math inline">\(w_k\)</span> decreases so that <spanclass="math inline">\(A_n \to 0\)</span> for <spanclass="math inline">\(n\geqslant 3\)</span> while the mean <spanclass="math inline">\(\sum_{k}^{} \nu_k(t)w_k\)</span> and the secondmoment <span class="math inline">\(\sum_{k}^{} \nu_k(t)w_k^{2}\)</span>remain constant. Such a sequence of models does not exist for excitatoryinput alone.</p><h3 id="threshold-and-firing">Threshold and firing</h3><p>We now incorporate a threshold condition into (8.41) and (8.7). Inthe Fokker-Planck equation (8.41), the firing threshold is incorporatedas a boundary condition <span class="math display">\[    p(\theta,t)\equiv 0 \ \text{for all} \ t\]</span></p><div class="note note-warning">            <p>This boundary condition is only for white noise model. For colorednoise the density at threshold is finite, because the effectivefrequency of 'attempts' to push a neuron which has membrane potential<span class="math inline">\(\theta-\delta&lt;u\leqslant \theta\)</span>above threshold is limited by the <strong>cut-off frequency</strong> ofthe noise.</p>          </div><h4 id="example-free-distribution-without-threshold">Example: Freedistribution without threshold</h4><p>The solution of (8.41) with initial condition <spanclass="math inline">\(p(u,\hat{t})=\delta(u-u_r)\)</span> is a Gaussianwith mean <span class="math inline">\(u_0(t)\)</span> and variance <spanclass="math inline">\(\langle \Delta u^{2}(t)\rangle\)</span>, i.e.,<span class="math display">\[    p(u,t)=\frac{1}{\sqrt{2\pi \langle \Delta u^{2}(t)\rangle}}\exp\left\{-\frac{[u(t|\hat{t})-u_0(t)]^{2}}{2\langle \Deltau^{2}(t)\rangle}\right\}\]</span> (8.45)</p><p>In particular, the stationary distribution that is approached in thelimit of <span class="math inline">\(t\to \infty\)</span> for constantinput <span class="math inline">\(I_0\)</span> is <spanclass="math display">\[    p(u,\infty)=\frac{1}{\sqrt{\pi}}\frac{1}{\sigma} \exp\left\{\frac{[u-RI_0]^{2}}{\sigma^{2}}\right\}, \tag{8.46}\]</span> which describes a Gaussian distribution with mean <spanclass="math inline">\(u_{\infty}=RI_0\)</span> and variance <spanclass="math inline">\(\sigma/\sqrt{2}\)</span>.</p><h3 id="interval-distribution-for-the-diffusive-noise-model">Intervaldistribution for the diffusive noise model</h3><p>Let us consider a leaky integrate-and-fire neuron that starts at time<span class="math inline">\(\hat{t}\)</span> with a membrane potential<span class="math inline">\(u_r\)</span> and is driven for <spanclass="math inline">\(t&gt;\hat{t}\)</span> by a known input <spanclass="math inline">\(I(t)\)</span>. We have <spanclass="math display">\[    \operatorname{Pr}\{u_0&lt;u(t)&lt;u_1|u(\hat{t})=u_r\}=\int_{u_0}^{u_1} p(u,t)\mathrm{d}u \tag{8.47}\]</span> where <span class="math inline">\(p(u,t)\)</span> is theprobability density of the membrane potential at time <spanclass="math inline">\(t\)</span>. In the diffusion limit, <spanclass="math inline">\(p(u,t)\)</span> can be found by solution of theFokker-Planck equation (8.41) with initial condition <spanclass="math inline">\(p(u,\hat{t})=\delta(u-u_r)\)</span> and boundarycondition <span class="math inline">\(p(\theta,t)=0\)</span>.</p><p>Imagine that we run 100 simulation trials. The expected fraction ofsimulations that have not yet reached the threshold and therefore still'survive' up to time <span class="math inline">\(t\)</span> is given bythe survivior function, <span class="math display">\[    S_{I}(t|\hat{t})=\int_{-\infty}^{\theta} p(u,t) \mathrm{d}u.\tag{8.48}\]</span></p><p>In view of (7.24), the input-dependent interval distribution istherefore <span class="math display">\[    P_{I}(t|\hat{t})=-\frac{\mathrm{d}}{\mathrm{d}t}\int_{-\infty}^{\theta}p(u,t) \mathrm{d}u. \tag{8.49}\]</span></p><p>In the context of noisy integrate-and-fire neurons <spanclass="math inline">\(P_{I}(t|\hat{t})\)</span> is called thedistribution of 'first passage times'. no general solution is known forthe first passage time problem of the Ornstein-Uhlenbeck process.</p><h4 id="example-numerical-evaluation-of-p_ithatt">Example: Numericalevaluation of <span class="math inline">\(P_{I}(t|\hat{t})\)</span></h4><p>(8.41) has been solved in the absence of a threshold. The transitionprobability from an arbitrary starting value <spanclass="math inline">\(u&#39;\)</span> at time <spanclass="math inline">\(t&#39;\)</span> to a new value <spanclass="math inline">\(u\)</span> at time <spanclass="math inline">\(t\)</span> is <span class="math display">\[    P^{trans}(u,t|u&#39;,t&#39;)=\frac{1}{\sqrt{2\pi\langle \Deltau^{2}(t)\rangle}}\exp \left\{-\frac{[u-u_0(t)]^{2}}{2\langle \Deltau^{2}(t)\rangle}\right\}\]</span> (8.50)</p><p>with <span class="math display">\[    u_0(t)=u&#39;\mathrm{e}^{-(t-t&#39;)/\tau_m} +\int_{0}^{t-t&#39;}\mathrm{e}^{-s&#39;/\tau_m} I(t-s&#39;) \mathrm{d}s \tag{8.51}\]</span></p><p><span class="math display">\[    \langle \Deltau^{2}(t)\rangle=\frac{\sigma^{2}}{2}[1-\mathrm{e}^{-2(t-s)/\tau_m}].\tag{8.52}\]</span></p><p>Because of the Markov property, the probability density to cross thethreshold (not necessarily for the first time) at a time <spanclass="math inline">\(t\)</span>, is equal to the probability to crossit for the first time at <spanclass="math inline">\(t&#39;&lt;t\)</span> and to return back to <spanclass="math inline">\(\theta\)</span> at time <spanclass="math inline">\(t\)</span>, that is, <span class="math display">\[    P^{trans}(\theta,t|u_r,\hat{t})=\int_{\hat{t}}^{t}P_{I}(t&#39;|\hat{t})P^{trans}(\theta,t|\theta,t&#39;) \mathrm{d}t&#39;.\tag{8.53}\]</span></p><h3 id="mean-interval-and-mean-firing-rate-diffusive-noise">Meaninterval and mean firing rate (diffusive noise)</h3><p>Here we express the mean firing rate of a leaky integrate-and-firemodel with diffusive noise as a function of a (constant) input <spanclass="math inline">\(I_0\)</span>.</p><p>For constant input <span class="math inline">\(I_0\)</span> the meaninterspike interval is <span class="math inline">\(\langles\rangle=\int_{0}^{\infty} sP_{I_0}(s|0) \mathrm{d}s=\int_{0}^{\infty}sP_0(s) \mathrm{d}s\)</span>. For the diffusion model (8.7) withthreshold <span class="math inline">\(\theta\)</span>, reset potential<span class="math inline">\(u_r\)</span>, and membrane time constant<span class="math inline">\(\tau_m\)</span>, the mean interval is <spanclass="math display">\[    \langle s\rangle=\tau_m\sqrt{\pi}\int_{\frac{u_r-h_0}{\sigma}}^{\frac{\theta-h_0}{\sigma}} \exp(u^{2})[1+\text{erf}(u)] \mathrm{d}u, \tag{8.54}\]</span> where <span class="math inline">\(h_0=RI_0\)</span> is theinput potential caused by the constant current <spanclass="math inline">\(I_0\)</span>. This expression is sometimes calledthe <strong>Siegert-formula</strong>. The inverse of the mean intervalis the mean firing rate.</p>]]></content>
    
    
    <categories>
      
      <category>神经动力学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>神经科学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (7)</title>
    <link href="/2022/07/24/Neuronal-Dynamics-7/"/>
    <url>/2022/07/24/Neuronal-Dynamics-7/</url>
    
    <content type="html"><![CDATA[<h1 id="variability-of-spike-trains-and-neural-codes">Variability ofSpike Trains and Neural Codes</h1><p>The online version of this chapter:</p><hr /><p>Chapter 7 Variability of Spike Trains and Neural Codeshttps://neuronaldynamics.epfl.ch/online/Ch7.html</p><hr /><h2 id="spiking-train-variability">Spiking train variability</h2><h3 id="are-neurons-noisy">Are neurons noisy?</h3><p>Termal noise: literally omnipresent. Due to the discrete nature ofelectric charge carriers, the voltage <spanclass="math inline">\(u\)</span> across any electrical resistor <spanclass="math inline">\(R\)</span> fluctuates at finite temperature(Johnson noise). The variance of the fluctuations at rest is <spanclass="math inline">\(\langle \Delta u^{2} \rangle \propto RkTB\)</span>where <span class="math inline">\(k\)</span> is the Boltzmann constant,<span class="math inline">\(T\)</span> the temperature and <spanclass="math inline">\(B\)</span> the bandwidth of the system.Fluctuations due to Johnson noise are of minor importance compared toother noise sources in neurons.</p><p>Another source of noise arises from the finite number of ion channelsin a patch of neuronal membrane. For a given constant membrane potential<span class="math inline">\(u\)</span>, a fraction <spanclass="math inline">\(P_i(u)\)</span> of ion channel of type <spanclass="math inline">\(i\)</span> is open on average. The actual numberof open channels fluctuates around <spanclass="math inline">\(N_iP_i(u)\)</span> where <spanclass="math inline">\(N_i\)</span> is the total number of ion channelsof type <span class="math inline">\(i\)</span> in that patch ofmembrane.</p><h3 id="noise-from-the-network">Noise from the Network</h3><p>Extrinsic noise: noise that are due to signal transmission andnetwork effects (extrinsic noise).</p><p>Synaptic transmission failure imposes a substantial limitation tosignal transmisson with a neuronal network.</p><p>Networks of excitatory and inhibitory neurons with fixed randomconnectivity can produce highly irregular spike trains - even in theabsence of any source of noise.</p><h2 id="mean-firing-rate">Mean Firing Rate</h2><h3 id="rate-as-a-spike-count-and-fano-factor">Rate as a Spike Count andFano Factor</h3><p>An experimentalist observes in trial <spanclass="math inline">\(k\)</span> the spikes of a given neuron. Thefiring rate in trial <span class="math inline">\(k\)</span> is the spikecount <span class="math inline">\(n_k^{sp}\)</span> in an interval ofduration <span class="math inline">\(T\)</span> divided by <spanclass="math inline">\(T\)</span>. <span class="math display">\[    \nu_k=\frac{n_k^{sp}}{T}. \tag{7.1}\]</span></p><p><span class="math inline">\(n_k^{sp}\)</span> mean by <spanclass="math inline">\(\langle n^{sp}\rangle\)</span> and deviations fromthe mean as <span class="math inline">\(\Deltan_{k}^{sp}=n_k^{sp}-\langle n^{sp}\rangle\)</span>. Variability of thespike count measure is characterized by the <strong>FanoFactor</strong>, defined as the variance of the spike count <spanclass="math inline">\(\langle (\Delta n^{sp})^{2}\rangle\)</span>divided by its mean <span class="math display">\[    F=\frac{\langle (\Delta n^{sp})^{2}\rangle}{\langle n^{sp}\rangle}\tag{7.2}\]</span></p><p>（Fano因子定义为方差除以均值） In experiments, the mean and varianceare estimated by averaging over <span class="math inline">\(K\)</span>trials <span class="math inline">\(\langle n^{sp}\rangle=(1/K)\sum_{k=1}^{K} n_k^{sp}\)</span> and <span class="math inline">\(\langle(\Delta n^{sp})^{2}\rangle=(1/K) \sum_{k=1}^{K} (\Deltan_k^{sp})^{2}\)</span>.</p><p>The firing rate defined here as spike count divided by themeasurement time <span class="math inline">\(T\)</span> is identical tothe inverse of the mean interspike interval. Since <spanclass="math inline">\(1/\langle x\rangle \neq \langle(1/x)\rangle\)</span>, if we assign a variable <spanclass="math inline">\(\tilde{\nu}(t)=1/(t^{(k+1)}-t^{(k)})\)</span> forall times <span class="math inline">\(t^{(k)}&lt;t\leqslantt^{(k+1)}\)</span>, the temporal average of <spanclass="math inline">\(\tilde{\nu}(t)\)</span> over a much longer time<span class="math inline">\(T\)</span> is not the same as the mean rate<span class="math inline">\(\nu\)</span> defined here as spike countdivided by <span class="math inline">\(T\)</span>.</p><p>Shortback: Too slow for animal!!!</p><h4 id="example-homogeneous-poisson-process">Example: HomogeneousPoisson Process</h4><p>Since the exact firing time of a spike does not matter (as we onlyfocus on <span class="math inline">\(\nu\)</span>), it is tempting todescribe spiking as a Poisson process where spikes occur independentlyand stochastically with a constant rate <spanclass="math inline">\(\nu\)</span>.</p><p>In a homogeneous Poisson process, the probability to find a spike ina short segment of duration <span class="math inline">\(\Deltat\)</span> is <span class="math display">\[    P_{F}(t;t+\Delta t)=\nu \Delta t. \tag{7.3}\]</span></p><p>In other words, spike events are independent of each other and occurwith a constant rate (also called stochastic intensity) defined as <spanclass="math display">\[    \nu= \lim_{\Delta t \to 0} \frac{P_{F}(t;t+\Delta t)}{\Delta t}.\tag{7.4}\]</span></p><p>The expected number of spikes to occur in the measuremet interval<span class="math inline">\(T\)</span> is therefore <spanclass="math display">\[    \langle n^{sp}\rangle=\nu T, \tag{7.5}\]</span></p><p>For a Poisson process, the Fano factor is exactly one.</p><h3id="rate-as-a-spike-density-and-the-peri-stimulus-time-histogram">Rateas a Spike Density and the Peri-Stimulus-Time Histogram</h3><p>Stimulate the neuron with some input sequence, repeated the samesequence several times and the neuronal response is reported in a<strong>Peri-Stimulus-Time Histogram (PSTH)</strong> with bin width<span class="math inline">\(\Delta t\)</span>. <spanclass="math inline">\(t\)</span> is the start of the stimulation and<span class="math inline">\(\Delta t\)</span> defines the time bin forgenerating the histogram.</p><p>The number of occurrences of spikes <spanclass="math inline">\(n_{K}(t;t+\Delta t)\)</span> summed over allrepetitions. The spike density <span class="math display">\[    \rho(t)=\frac{1}{\Delta t}\frac{n_{K}(t;t+\Delta t)}{K}. \tag{7.6}\]</span></p><p>Sometimes the result is smoothed to get a continuous rate variable,usually reported in units of Hz. We call the PSTH the time-dependentfiring rate.</p><p>We have defined the spike train <span class="math display">\[    S(t)=\sum_{f}^{} \delta(t-t^{(f)}) \tag{7.7}\]</span></p><p>If each stimulation can be considered as an independent sample fromthe identical stochastic process, we can define an <strong>instantaneousfiring rate</strong> as an expectation over trials <spanclass="math display">\[    \nu(t)=\langle S(t)\rangle = \frac{1}{K \Delta t}\sum_{k=1}^{K}\int_{t}^{t+\Delta t} S_k(t&#39;) \mathrm{d}t&#39; \tag{7.8-7.9}\]</span></p><p>The PSTH (the right-hand side of (7.9)) provides therefore anempirical estimate of the instantaneous firing rate (the left-handside).</p><p>Shortback: Not possible for animal!!!</p><h4 id="example-inhomogeneous-poisson-process">Example: InhomogeneousPoisson process</h4><p>An inhomogeneous Poisson process can be used to describe the spikedensity measured in a PSTH. In an inhomogeneous Poisson process, spikeevents are independent of each other and occur with an instantaneousfiring rate <span class="math display">\[    \nu(t)=\lim_{\Delta t \to 0} \frac{P_{F}(t;t+\Delta t)}{\Delta t}.\tag{7.10}\]</span></p><p>Therefore, the probability to find a spike in a short segment ofduration <span class="math inline">\(\Delta t\)</span> is <spanclass="math inline">\(P_{F}(t;t+\Delta t)=\nu(t)\Delta(t)\)</span>. Moregenerally, the expected number of spikes in an interval of finiteduration <span class="math inline">\(T\)</span> is <spanclass="math inline">\(\langle n^{sp}\rangle=\int_{0}^{T} \nu(t)\mathrm{d}t\)</span> and the Fano factor is one, as was the case for thehomogeneous Poisson process.</p><p>If a bunch of neurons fire at <spanclass="math inline">\(\hat{t}\)</span>, then the probability to'survive' without firing for <span class="math inline">\(t\)</span>(denoted as <span class="math inline">\(S\)</span>) satisfies <spanclass="math display">\[    \frac{\mathrm{d}S}{\mathrm{d}t}=-\nu (t)\]</span></p><p>so <span class="math display">\[    S(t|\hat{t})=\exp \biggl(-\int_{\hat{t}}^{t} \nu(t&#39;)\mathrm{d}t&#39;\biggr)\]</span></p><p>The probability for a neuron that fires at <spanclass="math inline">\(t\)</span> (the first spike after the spike at<span class="math inline">\(\hat{t}\)</span>) is <spanclass="math display">\[    P(t|\hat{t})=\nu(t)S(t|\hat{t})\]</span></p><h3 id="rate-as-a-population-activity-average-over-several-neurons">Rateas a Population Activity (Average over Several Neurons)</h3><p>Suppose we have a population of neurons with identical properties.The spikes of the neurons in a population <spanclass="math inline">\(m\)</span> are sent off to another population<span class="math inline">\(n\)</span>. The relevant quantity, from thepoint of view of the receiving neuron, is the proportion of activeneurons in the presynaptic population <spanclass="math inline">\(m\)</span>. Formally, we define the<strong>population activity</strong> <span class="math display">\[    A(t)=\frac{1}{\Delta t}\frac{n_{act}(t;t+\Deltat)}{N}=\frac{1}{\Delta t}\frac{\int_{t}^{t+\delta t} \sum_{j}^{}\sum_{f}^{} \delta(t-t_j^{(f)}) \mathrm{d}t}{N}\]</span> (7.11)</p><p>where <span class="math inline">\(N\)</span> is the size of thepopulation, <span class="math inline">\(n_{act}(t;t+\Delta t)\)</span>is the number of spikes (summed over all neurons in the population) thatoccur between <span class="math inline">\(t\)</span> and <spanclass="math inline">\(t+\Delta t\)</span> where <spanclass="math inline">\(\Delta t\)</span> is a small time interval.</p><h2 id="interval-distribution-and-coefficient-of-variation">Intervaldistribution and coefficient of variation</h2><p>Define the estimation of interspike interval (ISI) distributions andinterpreted it as a conditional probability density: <spanclass="math display">\[    P_0(s)=P(t^{(f)}+s|t^{(f)}) \tag{7.12}\]</span> where <span class="math inline">\(\int_{t}^{t+\Delta t}P(t&#39;|t^{(f)}) \mathrm{d}t&#39;\)</span> is the probability that thenext spike occurs in the interval <spanclass="math inline">\([t,t+\Delta t]\)</span> given that the last spikeoccured at time <span class="math inline">\(t^{(f)}\)</span>.</p><p>In ordet to extract the mean firing rate from a stationary intervaldistribution <span class="math inline">\(P_0(s)\)</span>, we start withthe definition of the mean interval, <span class="math display">\[    \langle s \rangle=\int_{0}^{s} sP_0(s) \mathrm{d}s. \tag{7.13}\]</span></p><p>The mean firing rate is the inverse of the mean interval <spanclass="math display">\[    \nu=\frac{1}{\langle s\rangle}=\left[ \int_{0}^{\infty} sP_0(s)\mathrm{d}s \right] ^{-1}\]</span></p><h3 id="coefficient-of-variation-c_v">Coefficient of variation <spanclass="math inline">\(C_{V}\)</span></h3><p>Interspike interval distributions <spanclass="math inline">\(P_0(s)\)</span> derived from a spike train understationary conditions can be broad or sharply peaked. To quantify thewidth of the interval distribution, neuroscientists often evaluate the<strong>coefficient of variation</strong>, short <spanclass="math inline">\(C_{V}\)</span>, defined as the ratio of thestandard deviation and the mean. Therefore the square of the <spanclass="math inline">\(C_{V}\)</span> is <span class="math display">\[    C_{V}^{2}=\frac{\langle \Delta s ^{2}\rangle}{\langle s \rangle^{2}}\]</span> where <span class="math inline">\(\langles\rangle=\int_{0}^{\infty} sP_0(s) \mathrm{d}s\)</span> and <spanclass="math inline">\(\langle \Delta s^{2}\rangle=\int_{0}^{\infty}s^{2}P_0(s) \mathrm{d}s-\langle s \rangle ^{2}\)</span>.</p><p>A Poisson process produces distributions with <spanclass="math inline">\(C_{V}=1\)</span>. A value of <spanclass="math inline">\(C_{V}&gt;1\)</span>, implies that a given spiketrain is less regular that a Poisson process with the same firing rate.If <span class="math inline">\(C_{V}&lt;1\)</span>, then the spike ismore regular.</p><p>Most deterministic integrate-and-fire neurons fire periodically whendriven by a constant stimulus and therefore have <spanclass="math inline">\(C_{V}=0\)</span>. Intrinsically bursting neuronscan have <span class="math inline">\(C_{V}&gt;1\)</span>.</p><h4 id="example-poisson-process-with-absolute-refractoriness">Example:Poisson process with absolute refractoriness</h4><p>We study a Poisson neuron with absolute refractory period <spanclass="math inline">\(\Delta^{abs}\)</span>. For times since last spikelarger than $ ^{abs}$, the neuron is supposed to fire stochasticallywith rate <span class="math inline">\(r\)</span>. The intervaldistribution of a Poisson process with absolute refractoriness is givenby <span class="math display">\[    P_0(s)=    \begin{cases}        0, \quad s&lt;\Delta^{abs} \\        r\exp [-r(s-\Delta_{abs})], \quad s&gt;\Delta^{abs}        \end{cases}\]</span> (7.16)</p><p>(Notice that <span class="math inline">\(\int_{0}^{\infty} P_0(s)\mathrm{d}s=1\)</span>)</p><p>and has a mean <span class="math inline">\(\langle s \rangle=\Delta^{abs}+1/r\)</span> and variance <spanclass="math inline">\(\langle \Delta s^{2}\rangle=1/r^{2}\)</span>. Thecoefficient of variation is therefore <span class="math display">\[    C_{V}=1-\frac{\Delta^{abs}}{\langle s \rangle}\]</span> (7.17)</p><h2 id="autocorrelation-function-and-noise-spectrum">Autocorrelationfunction and noise spectrum</h2><p>Consider a spike train <span class="math inline">\(S_i(t)=\sum_{f}^{}\delta(t-t_i^{(f)})\)</span> of length <spanclass="math inline">\(T\)</span>. We suppose that <spanclass="math inline">\(T\)</span> is sufficiently long so that we canformally consider the limit <span class="math inline">\(T \to\infty\)</span>. The autocorrelation function <spanclass="math inline">\(C_{ii}(s)\)</span> of the spike train is a measurefor the probability to find two spikes at a time interval <spanclass="math inline">\(s\)</span>, i.e. <span class="math display">\[    C_{ii}(s)=\langle S_i(t)S_i(t+s)\rangle _{t}, \tag{7.18}\]</span> where <span class="math display">\[    \langle f(t)\rangle _{t}=\lim_{T \to \infty} \frac{1}{T}\int_{-T/2}^{T/2} f(t) \mathrm{d}t. \tag{7.19}\]</span></p><p>By symmetry, <spanclass="math inline">\(C_{ii}(-s)=C_{ii}(s)\)</span>.</p><p>Define the <strong>power spectrum</strong> (or <strong>power spectraldensity</strong>) of a spike train <span class="math display">\[    \mathscr{P}(\omega)=\lim_{T \to \infty}\mathscr{P}_{T}(\omega),\]</span> where <span class="math inline">\(\mathscr{P}_{T}\)</span> isthe power of a segment of length <span class="math inline">\(T\)</span>of the spike train, <span class="math display">\[    \mathscr{P}_{T}(\omega)=\frac{1}{T}\biggl\lvert \int_{-T/2}^{T/2}S_i(t)\mathrm{e}^{-i\omega t} \mathrm{d}t \biggr\rvert^{2} \tag{7.20}\]</span> The power spectrum <spanclass="math inline">\(\mathscr{P}(\omega)\)</span> of a spike train isequal to the Fourier transform <spanclass="math inline">\(\hat{C}_{ii}(\omega)\)</span> of itsautocorrelation function (Wiener-Khinchin Theorem): <spanclass="math display">\[    \begin{aligned}        \hat{C}_{ii}(\omega) &amp;= \int_{-\infty}^{\infty} \langleS_i(t)S_i(t+s)\rangle \mathrm{e}^{-i\omega s}  \mathrm{d}s \\        &amp;= \lim_{T \to \infty} \frac{1}{T} \int_{-T/2}^{T/2}S_i(t)\int_{-\infty}^{\infty} S_i(t+s)\mathrm{e}^{-i\omegas}  \mathrm{d}s \mathrm{d}t \\        &amp;=\lim_{T \to \infty} \frac{1}{T} \int_{-T/2}^{T/2}  S_i(t)\mathrm{e}^{i\omega t}\mathrm{d}t \int_{-\infty}^{\infty} S_i(s&#39;)\mathrm{e}^{-i\omega s&#39;} \mathrm{d}s&#39; \\        &amp;= \lim_{T \to \infty}\frac{1}{T}\biggl\lvert\int_{-T/2}^{T/2} S_i(t)\mathrm{e}^{-i\omega t} \mathrm{d}t\biggr\rvert^{2}.    \end{aligned}\]</span> (7.21)</p><p>The power spectral density of a spike train during spontaneousactivity is called the noise spectrum of the neuron.</p><hr /><p>Spectral density - Wikipediahttps://en.wikipedia.org/wiki/Spectral_density</p><hr /><h2 id="renewal-statistics">Renewal statistics</h2><p>（相关内容可以看Ross的随机过程的1.6节）</p><p>Poisson processes cannot be used to describe realistic interspikeinterval distributions. Spikes are generated in a renewal process, witha stochastic intensity. <span class="math display">\[    \rho(t|\hat{t})=\rho_0(t-\hat{t}) \tag{7.22}\]</span> where <span class="math inline">\(\hat{t}\)</span> is the timesince the last spike. The central assumption of renewal theory is thatthe state does not depend on earlier events. Renewal theory allows tocalculate the interval distribution <span class="math display">\[    P_0(s)=P(t^{(f)}+s|t^{(f)}) \tag{7.23}  \]</span></p><h3 id="survivor-function-and-hazard">Survivor function and hazard</h3><p>Since <span class="math inline">\(\int_{\hat{t}}^{t}P(t&#39;|\hat{t}) \mathrm{d}t&#39;\)</span> is the probability for asecond action potential between <spanclass="math inline">\(\hat{t}\)</span> and <spanclass="math inline">\(t\)</span>. Thus <span class="math display">\[    S(t|\hat{t})=1-\int_{\hat{t}}^{t} P(t&#39;|\hat{t}) \mathrm{d}t&#39;\]</span> is the probability that the neuron stays quiescent between<span class="math inline">\(\hat{t}\)</span> and <spanclass="math inline">\(t\)</span>. <spanclass="math inline">\(S(t|\hat{t})\)</span> is called the<strong>survivor function</strong>. It has an initial value <spanclass="math inline">\(S(\hat{t}|\hat{t})=1\)</span>. The rate of decayof <span class="math inline">\(S(t|\hat{t})\)</span> is defined by <spanclass="math display">\[    \rho(t|\hat{t})=-\frac{\frac{\mathrm{d}}{\mathrm{d}t}S(t|\hat{t})}{S(t|\hat{t})}=\frac{P(t|\hat{t})}{1-\int_{\hat{t}}^{t}P(t&#39;|\hat{t}) \mathrm{d}t&#39;}.\]</span> (7.25)</p><p><span class="math inline">\(\rho(t|\hat{t})\)</span> is called the'age-dependent death rate' or 'hazard' in renewal theory.</p><p>Integrating <span class="math inline">\(\mathrm{d}S/\mathrm{d}t=-\rhoS\)</span> yields the survivor function <span class="math display">\[    S(t|\hat{t})=\exp \left[ -\int_{\hat{t}}^{t} \rho(t&#39;|\hat{t})\mathrm{d}t&#39; \right] \tag{7.26}\]</span></p><p>The interval distribution is given by <span class="math display">\[    P(t|\hat{t})=-\frac{\mathrm{d}}{\mathrm{d}t}S(t|\hat{t})=\rho(t|\hat{t})S(t|\hat{t}),\tag{7.27}\]</span></p><p>(In order to emit its next spike at <spanclass="math inline">\(t\)</span>, the neuron has to survive the interval<span class="math inline">\((\hat{t},t)\)</span> without firing and thenfire at <span class="math inline">\(t\)</span>.)</p><p><span class="math display">\[    P(t|\hat{t})=\rho(t|\hat{t}) \exp \left[-\int_{\hat{t}}^{t}\rho(t&#39;|\hat{t}) \mathrm{d}t&#39; \right]. \tag{7.28}\]</span></p><p>We focus on stationary renewal systems: <span class="math display">\[    P(t|\hat{t})=P_0(t-\hat{t}) \tag{7.29}\]</span> <span class="math display">\[    S(t|\hat{t})=S_0(t-\hat{t}) \tag{7.30}\]</span> <span class="math display">\[    \rho(t|\hat{t})=\rho_0(t-\hat{t}) \tag{7.31}\]</span></p><h3 id="renewal-theory-and-experiments">Renewal theory andexperiments</h3><p>Under experimental conditions where neuronal adaptation is strong,intervals are not independent. A common measure of memory effects in atime series of events with variable intervals <spanclass="math inline">\(s_j\)</span> is the <strong>serial correlationcoefficients</strong> <span class="math display">\[    c_k=\frac{\langle s_{j+k}s_j\rangle _j-\langle s_j\rangle_j^{2}}{\langle s_j^{2}\rangle- \langle s_j\rangle^{2}}\]</span></p><p>Spike-frequency adaptation causes a negative correlation betweensubsequent intervals <spanclass="math inline">\((c_1&lt;0)\)</span>.</p><h3id="autocorrelation-and-noise-spectrum-of-a-renewal-process">Autocorrelationand noise spectrum of a renewal process</h3><p>Let <span class="math inline">\(\nu_i=\langle S_i \rangle\)</span>denote the mean firing rate (expected number of spikes per unit time) ofthe spike train. For large intervals <spanclass="math inline">\(s\)</span>, firing at time <spanclass="math inline">\(t+s\)</span> is independent from whether or notthere was a spike at time <span class="math inline">\(t\)</span>.Therefore, the expectation to find a spike at <spanclass="math inline">\(t\)</span> and another spike at <spanclass="math inline">\(t+s\)</span> approaches for <spanclass="math inline">\(s \to \infty\)</span> a limiting value <spanclass="math inline">\(\lim_{s \to \infty}\langleS_i(t)S_i(t+s)\rangle=\lim_{s \to \infty}C_{ii}(s)=\nu_i^{2}\)</span>.Substract this baseline value and we get a 'normalized' autocorrelation,<span class="math display">\[    C_{ii}^{0}(s)=C_{ii}(s)-\nu_i^{2}, \tag{7.37}\]</span> with <span class="math inline">\(\lim_{s \to\infty}C_{ii}^{0}(s)=0\)</span>. The Fourier transform of (7.37) yields<span class="math display">\[    \hat{C}_{ii}(\omega)=\hat{C}_{ii}^{0}(\omega)+2\pi \nu_i^{2}\delta(\omega). \tag{7.38}\]</span> Thus <span class="math inline">\(\hat{C}_{ii}(\omega)\)</span>diverges at <span class="math inline">\(\omega=0\)</span>. thedivergence is removed by switching to the normalizedautocorrelation.</p><p>Let us suppose that we have found a first spike at <spanclass="math inline">\(t\)</span>. The correlation function for positive<span class="math inline">\(s\)</span> will be denoted by <spanclass="math inline">\(\nu_i C_{+}(s)\)</span> or <spanclass="math display">\[    C_{+}(s)=\frac{1}{\nu_i} C_{ii}(s) \Theta(s) \tag{7.39}\]</span></p><p>The factor <span class="math inline">\(\nu_i\)</span> in (7.39) takescare of the fact that we expect a first spike at <spanclass="math inline">\(t\)</span> with rate <spanclass="math inline">\(\nu_i\)</span>. <spanclass="math inline">\(C_{+}(s)\)</span> gives the conditionalprobability density that, given a spike at <spanclass="math inline">\(t\)</span>, we will find another spike at <spanclass="math inline">\(t+s&gt;t\)</span>. The spike at <spanclass="math inline">\(t+s\)</span> can be the first spike after <spanclass="math inline">\(t\)</span>, or the second one, or the <spanclass="math inline">\(n\)</span>th one, thus for <spanclass="math inline">\(s&gt;0\)</span> <span class="math display">\[    C_{+}(s)=P_0(s)+\int_{0}^{\infty} P_0(s&#39;)P_0(s-s&#39;)\mathrm{d}s&#39;+\int_{0}^{\infty} \int_{0}^{\infty}P_0(s&#39;)P_0(s&#39;&#39;)P_0(s-s&#39;-s&#39;&#39;) \mathrm{d}s&#39;\mathrm{d}s&#39;&#39;+\cdots\]</span> (7.40) or <span class="math display">\[    C_{+}(s)=P_0(s)+\int_{0}^{\infty} P_0(s&#39;)C_{+}(s-s&#39;)\mathrm{d}s&#39; \tag{7.41}\]</span></p><p>Due to the symmetry of <span class="math inline">\(C_{ii}\)</span>,we have <span class="math inline">\(C_{ii}(s)=\nu C_{+}(-s)\)</span> for<span class="math inline">\(s&lt;0\)</span>. So</p><p><span class="math display">\[    C_{ii}(s)=\nu_i[\delta(s)+C_{+}(s)+C_{+}(-s)]. \tag{7.42}\]</span> (the autocorrelation has a <spanclass="math inline">\(\delta\)</span> peak reflecting the trivialautocorrelation of each spike with itself.</p><p>Take the Fourier transform of (7.41) and find <spanclass="math display">\[    \hat{C}_{+}(\omega)=\frac{\hat{P}_0(\omega)}{1-\hat{P}_0(\omega)}.\tag{7.43}\]</span> Together with the Fourier transform of (7.42), we obtain <spanclass="math display">\[    \hat{C}_{ii}(\omega)=\nu_i \Re\left\{\frac{1+\hat{P}_0(\omega)}{1-\hat{P}_0(\omega)}\right\} +2\pi\nu_i^{2}\delta(\omega) \tag{7.45}\]</span></p><h4 id="example-stationary-poisson-process">Example: Stationary Poissonprocess</h4><p>For a Poisson process, <span class="math display">\[    C_{+}(s)=\nu \mathrm{e}^{-\nu s} [1+\nu s+\frac{1}{2}(\nus)^{2}+\cdots ]=\nu \tag{7.46}\]</span></p><p>So the autocorrelation of a Poisson process is <spanclass="math display">\[    C_{ii}(s)=\nu \delta(s)+\nu^{2} \tag{7.47}\]</span></p><p>The Fourier transform of (7.47) yields a flat spectrum with a <spanclass="math inline">\(\delta\)</span> peak at zero: <spanclass="math display">\[    \hat{C}_{ii}(\omega)=\nu+2\pi\nu^{2}\delta(\omega). \tag{7.48}\]</span></p><h4 id="example-poisson-process-with-absolute-refractoriness-1">Example:Poisson process with absolute refractoriness</h4><p>For a Poisson proccess with absolute refractoriness defined in(7.16). The neuron fires with rate <spanclass="math inline">\(r\)</span>. For <span class="math inline">\(\omega\neq 0\)</span>, (7.45) yields the noise spectrum <spanclass="math display">\[    \hat{C}_{ii}(\omega)=\nu\left\{ 1+2\frac{\nu^{2}}{\omega^{2}}[1-\cos (\omega \Delta^{abs})]+2\frac{\nu}{\omega}\sin (\omega \Delta^{abs})\right\}^{-1},\]</span> (7.49)</p><p>For <span class="math inline">\(\omega \to 0\)</span>, the noisespectrum is decreased by a factor <span class="math inline">\([1+2(\nu\Delta^{abs})+(\nu \Delta^{abs})^{2}]^{-1}\)</span>. Explanation: themean interval of a Poisson neuron with absolute refractoriness is <spanclass="math inline">\(\langle s\rangle=\Delta^{abs}+r^{-1}\)</span>.Hence the mean firing rate is <span class="math display">\[    \nu=\frac{r}{1+\Delta^{abs}r}. \tag{7.50}\]</span></p><p>For finite <span class="math inline">\(\Delta^{abs}\)</span> thefiring is more regular than that of a Poisson process with the same meanrate <span class="math inline">\(\nu\)</span>, and hence the spectrumfor <span class="math inline">\(\omega \to 0\)</span> is less noisy.</p><p>This means that Poisson neurons with absolute refractoriness cantransmit slow signals more reliably than a simple Poisson process.</p><h3 id="input-dependent-renewal-theory">Input dependent renewaltheory</h3><h2 id="the-problem-of-neural-coding">The Problem of Neural Coding</h2><h3 id="limits-of-rate-codes">Limits of rate codes</h3><p><strong>Limitations of the spike count code</strong>. Averaging overa large number of spikes takes a long time. In a changing environment, apostsynaptic neuron does not have the time to perform a temporal averageover many (noisy) spikes.</p><p><strong>Limitations of the PSTH</strong>. It needs several trials tobuild up. Nevertheless, the PSTH measure of the instantaneous firingrate can make sense if there are large populations of similar neuronsthat receive the same stimulus.</p><p><strong>Limitations of rate as a population average</strong>. (7.11)requires a homogeneous population of neurons with identical connectionswhich is hardly realistic. Real populations will always have a certaindegree of heterogeneity both in their internal parameters and in theirconnectivity pattern.</p><p>For inhomogeneous populations, the definition (7.11) may be replacedby a weighted average over the population. Suppose that we are studyinga population of neurons which respond to a stimulus <spanclass="math inline">\(\mathbf{x}\)</span>. We may think of <spanclass="math inline">\(\mathbf{x}\)</span> as the location of thestimulus in input space. Neuron <span class="math inline">\(i\)</span>respond best to stimulus <spanclass="math inline">\(\mathbf{x}_i\)</span>. We may say that the spikesof a neuron <span class="math inline">\(i\)</span> 'represent' an inputvector <span class="math inline">\(\mathbf{x}_i\)</span>.</p><p>In a large population, many neurons will be active simultaneouslywhen a new stimulus <span class="math inline">\(\mathbf{x}\)</span> isrepresented. The location of this stimulus can then be estimated fromthe weighted population average <span class="math display">\[    \mathbf{x}^{est}(t)=\frac{\int_{t}^{t+\Delta t} \sum_{j}^{}\sum_{f}^{} \mathbf{x}_j\delta(t-t_j^{(f)})\mathrm{d}t}{\int_{t}^{t+\Delta t} \sum_{j} \sum_{f}^{}\delta(t-t_j^{(f)}) \mathrm{d}t}\]</span> (7.52)</p><h3 id="candidate-temporal-codes">Candidate temporal codes</h3><h4 id="time-to-first-spike-latency-code">Time-to-first-spike: Latencycode</h4><p>Take saccading as an example. After each saccade, the photo receptorsin the retina receive a new visual input. Information about the onset ofa saccades should easily be available in the brain and could serve as aninternal reference signal.</p><p>Experimental evidences indicate that a coding scheme based on thelatency of the first spike transmit a large amount of information.</p><h4 id="phase">Phase</h4><p>We can apply a code by 'time ot first spike' also in the situationwhere the reference signal is not a single event, but a periodic signal.Oscillations of some global variable (for example the populationactivity) are common in the hippocampus, the olfactory system, and otherareas of the brain. These oscillations could serve as an internalreference signal.</p><h4 id="correlations-and-synchrony">Correlations and Synchrony</h4><p>We can also use spikes from other neurons as the reference signal fora spike code. Neurons which represent the same object in a complex sceneconsisting of several objects could be 'labeled' by the fact that theyfire synchronously.</p><p>Not only synchrony but any precise spatio-temporal pulse patterncould be a meaningful event.</p><h4 id="stimulus-reconstruction-and-reverse-correlation">StimulusReconstruction and Reverse Correlation</h4><p><strong>Reverse correlation</strong>: Every time a spike occurs, wenote the time course of the stimulus in a time window of about 100milliseconds immediately before the spike. Averaging the results overseveral spikes yields the typical time course of the stimulus justbefore a spike.</p><h4 id="rate-versus-temporal-codes">Rate versus temporal codes</h4><p>The stimulus reconstruction with kernels can also be considered as arate code based on spike counts. To see this, consider a spike countmeasure with a running time window <spanclass="math inline">\(K(.)\)</span>. We can estimate the rate <spanclass="math inline">\(\nu\)</span> at time <spanclass="math inline">\(t\)</span> by <span class="math display">\[    \nu(t)=\frac{\int_{-\infty}^{\infty} K(\tau)S(t-\tau)\mathrm{d}\tau}{\int_{-\infty}^{\infty} K(\tau) \mathrm{d}\tau}\tag{7.54}\]</span> where <span class="math inline">\(S(t)=\sum_{f=1}^{n}\delta(t-t^{(f)})\)</span> is the spike train. For a rectangular timewindow <span class="math inline">\(K(\tau)=1\)</span> for <spanclass="math inline">\(-T/2&lt;\tau&lt;T/2\)</span> and zero otherwise,reduces exactly to (7.1). Perform the integration over the <spanclass="math inline">\(\delta\)</span> function and we yield <spanclass="math display">\[    \nu(t)=c \sum_{f=1}^{n} K(t-t^{(f)}) \tag{7.55}\]</span> where <span class="math inline">\(c=[\int_{}^{} K(s)\mathrm{d}s]^{-1}\)</span> is a constant.</p><h4 id="example-towards-a-definition-of-rate-codes">Example: Towards adefinition of rate codes</h4><p>We have seen in (7.55) that stimulus reconstruction with a linearkernel can be seen as a special instance of a rate code. This suggests aformal definition of a rate code via the reconstruction procedure: ifall information contained in a spike train can be recovered by thelinear reconstruction procedure of (7.53), the the neuron is using arate code.</p><h2 id="exercises">Exercises</h2><ol type="1"><li><strong>Poisson process in continuous time</strong>. We consider aPoisson neuron model. In every small time interval <spanclass="math inline">\(\Delta t\)</span>, the probability that the neuronfires is given by <span class="math inline">\(\nu \Deltat\)</span>.</li></ol><ol type="a"><li>The probability that the neuron does not fire during a time ofarbitrarily large length <span class="math inline">\(t\)</span>(survivor function <span class="math inline">\(S_0(t)\)</span>) is <spanclass="math display">\[S_0(t)=\mathrm{e}^{-\nu t}.\]</span></li><li>Suppose that the neuron has fired at time <spanclass="math inline">\(t=0\)</span>, then the distribution of intervals<span class="math inline">\(P(t)\)</span>, i.e., the probability densitythat the neuron fires its next spike at a time <spanclass="math inline">\(t\)</span>, is <span class="math display">\[P_0(s)=\nu \mathrm{e}^{-\nu t}.\]</span></li></ol><ol start="2" type="1"><li><strong>Autocorrelation of a Poisson process</strong>. Find theautocorrelation function <span class="math inline">\(C_0(s)=\langleS_i(t)S_i(t+s)\rangle _{t}\)</span> of the homogeneous Poisson processin continuous time.</li><li><strong>Repeatability and random coincidences</strong>. Whatpercentage of spikes coincide between two trials of a Poisson neuronwith arbitrary rate <span class="math inline">\(\nu_0\)</span> under theassumption that trials are sufficiently long?</li><li><strong>Spike count and Fano Factor</strong>. A homogeneous Poissonprocess has a probability to fire in a very small interval <spanclass="math inline">\(\Delta t\)</span> equal to <spanclass="math inline">\(\nu \Delta t\)</span>.</li></ol><ol type="a"><li>The probability to observe exactly <spanclass="math inline">\(k\)</span> spikes in the time interval <spanclass="math inline">\(T\)</span> is <spanclass="math inline">\(P_{k}(T)=[1/k!](\nu T)^{k}\exp (-\nuT)\)</span>.</li><li>For the inhomogeneous Poisson process the mean spike count in aninterval of duration <span class="math inline">\(T\)</span> is <spanclass="math inline">\(\langle k \rangle=\int_{0}^{T} \nu(t)\mathrm{d}t\)</span>.</li><li>Calculate the variance of the spike count and the Fano factor forthe inhomogeneous Poisson process.</li></ol><ol start="5" type="1"><li><strong>From interval distribution to hazard</strong>. Duringstimulation with a stationary stimulus, interspike intervals in a longspike train are found to be independent and given by the distribution<span class="math display">\[P(t|t&#39;)=\frac{(t-t&#39;)}{\tau^{2}}\exp\left(-\frac{t-t&#39;}{\tau}\right)\]</span> for <span class="math inline">\(t&gt;t&#39;\)</span>.</li></ol><ol type="a"><li>Calculate the survivor function <spanclass="math inline">\(S(t|t&#39;)\)</span>, the hazard function <spanclass="math inline">\(\rho(t|t&#39;)\)</span>. <spanclass="math display">\[S(t|t&#39;)=\left(\frac{t-t&#39;}{\tau}+1\right)\exp\left(-\frac{t-t&#39;}{\tau}\right)\]</span> <span class="math display">\[\rho(t|t&#39;)=\frac{t-t&#39;}{\tau(t-t&#39;)+\tau^{2}}\]</span></li><li>A spike train starts at time <span class="math inline">\(0\)</span>and we observed a first spike at time <spanclass="math inline">\(t_1\)</span>. Calculate the probability density<span class="math inline">\(P(t_{n}|t_1)\)</span> that the <spanclass="math inline">\(n\)</span>th spike occurs around <spanclass="math inline">\(t_n\)</span>. (self-convolution for <spanclass="math inline">\(n-2\)</span> times)</li></ol><ol start="6" type="1"><li><strong>Gamma-distribution</strong> Stationary intervaldistributions can often be fitted by a Gamma distrubution (for <spanclass="math inline">\(s&gt;0\)</span>) <span class="math display">\[P(s)=\frac{1}{\Gamma(k)}\frac{s^{k-1}}{\tau^{k}}\mathrm{e}^{-s/\tau}\]</span></li></ol><ol type="a"><li>Assume that intervals are independent and calculate the powerspectrum.</li><li>Calculate the coefficient of variation <spanclass="math inline">\(C_{V}\)</span> <span class="math display">\[C_{V}=\frac{1}{k}\]</span></li></ol><ol start="7" type="1"><li><strong>Poisson with dead time as a renewal process</strong>.Consider a process where spikes are generated with rate <spanclass="math inline">\(\rho_0\)</span>, but after each spike there is adead time of duration <span class="math inline">\(\Delta^{abs}\)</span>.More precisely, we have a renewal process <span class="math display">\[\rho(t|\hat{t})=\rho_0 \quad \text{for}\ t&gt;\hat{t}+\Delta^{abs}\]</span> and zero otherwise. Calculate the interval distribution andthe Fano factor. <span class="math display">\[P(t|\hat{t})=\begin{cases}     0, \hat{t}&lt;t&lt;\hat{t}+\Delta^{abs} \\     \exp (-t-\hat{t}-\Delta^{abs}), t&gt;\hat{t}+\Delta^{abs}\end{cases}\]</span></li></ol>]]></content>
    
    
    <categories>
      
      <category>神经动力学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>神经科学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Information and Entropy (3)</title>
    <link href="/2022/07/24/Information-and-Entropy-3/"/>
    <url>/2022/07/24/Information-and-Entropy-3/</url>
    
    <content type="html"><![CDATA[<h1 id="principle-of-maximum-entropy">Principle of Maximum Entropy</h1><h2 id="problem-setup">Problem Setup</h2><p>The probelm domain needs to be setup. We'll apply the general case totwo examples, one a business model, and the other a model of physicalsystem.</p><h3 id="bergers-burgers">Berger's Burgers</h3><p>The menu has been extended to include a gourmet low-fat tofumeal.</p><h3 id="magnetic-dipole-model">Magnetic Dipole Model</h3><p>Consider a system containing one dipole and a two-sideenvironment</p><h2 id="probabilities">Probabilities</h2><p>We assume that each of the possible states <spanclass="math inline">\(A_i\)</span> has some probability of occupancy<span class="math inline">\(p(A_i)\)</span> where <spanclass="math inline">\(i\)</span> is an index running over the possiblestates.</p><p><span class="math display">\[    1=\sum_{i}^{} p(A_i) \tag{9.1}\]</span></p><p>Probability, and all quantities that are based on probabilities, aresubjective, or observer-dependent.</p><h2 id="entropy">Entropy</h2><p><span class="math display">\[    S=\sum_{i}^{} p(A_i)\log _{2}\left(\frac{1}{p(A_i)}\right)\]</span> (9.2)</p><p>Information is measured in bits.</p><h2 id="constraints">Constraints</h2><p>We consider only one constraint here. If there is a quantity <spanclass="math inline">\(G\)</span> for which each of the states has avalue <span class="math inline">\(g(A_i)\)</span> then we want toconsider only those probability distributions for which the expectedvalue is a known value <spanclass="math inline">\(\widetilde{G}\)</span> <spanclass="math display">\[    \widetilde{G}=\sum_{i}^{} p(A_i)g(A_i) \tag{9.4}\]</span></p><h2 id="maximum-entropy-single-constraint">Maximum Entropy, SingleConstraint</h2><h3 id="probability-formula">Probability Formula</h3><p>The probability distribution <spanclass="math inline">\(p(A_i)\)</span> we want has been derived byothers. It is a function of the dual variable <spanclass="math inline">\(\beta\)</span>: <span class="math display">\[    p(A_i)=2^{-\alpha}2^{-\beta g(A_i)}\]</span> (9.12)</p><p>which implies <span class="math display">\[    \log _{2}\left(\frac{1}{p(A_i)}\right)=\alpha+\beta g(A_i)\]</span> (9.13)</p><p>where <span class="math display">\[    \alpha=\log _{2}\left(\sum_{i}^{} 2^{-\beta g(A_i)}\right)\]</span> (9.14)</p><p><span class="math display">\[    S=\alpha+\beta G \tag{9.15}\]</span> where <span class="math inline">\(S\)</span>,<spanclass="math inline">\(\alpha\)</span>, and <spanclass="math inline">\(G\)</span> are all functions of <spanclass="math inline">\(\beta\)</span>.</p><h3 id="evaluating-the-dual-variable">Evaluating the Dual Variable</h3><p><span class="math display">\[    0=\sum_{i}^{} [g(A_i)-G(\beta)]2^{-\beta g(A_i)}\]</span> (9.21)</p><p>If this equation is multiplied by <spanclass="math inline">\(2^{\beta G(\beta)}\)</span>, the result is <spanclass="math display">\[    0=f(\beta)\]</span> where the function <spanclass="math inline">\(f(\beta)\)</span> is <span class="math display">\[    f(\beta)=\sum_{i}^{} [g(A_i)-G(\beta)]2^{-\beta[g(A_i)-G(\beta)]}\]</span> (9.23)</p><p>It is not difficult to show that <spanclass="math inline">\(f(\beta)\)</span> is a monotonic function of <spanclass="math inline">\(\beta\)</span> since <spanclass="math inline">\(G(\beta)\)</span> is a monotonic function of <spanclass="math inline">\(\beta\)</span>.</p><h3 id="examples">Examples</h3><p>We only focus on the magnetic dipole example. <spanclass="math display">\[    1=p(U)+p(D)\]</span> (9.33)</p><p><span class="math display">\[    \begin{aligned}        \widetilde{E} &amp;= e(U)p(U)+e(D)p(D) \\        &amp;= m_{d}H[p(U)-p(D)] \\    \end{aligned}\]</span> (9.34)</p><p><span class="math display">\[    S=p(U)\log _{2}\left(\frac{1}{p(A)}\right)+p(D)\log_{2}\left(\frac{1}{p(D)}\right)\]</span> (9.35)</p><p>The entropy is the largest, for the energy <spanclass="math inline">\(\widetilde{E}\)</span> and magnetic field <spanclass="math inline">\(H\)</span>, if</p><p><span class="math display">\[    p(U)=2^{-\alpha}2^{-\beta m_{d}H}       \]</span> (9.36)</p><p><span class="math display">\[    p(D)=2^{-\alpha}2^{\beta m_{d}H}\]</span> (9.37)</p><p>where <span class="math display">\[    \alpha=\log _{2}(2^{-\beta m_{d}H}+2^{\beta m_{d}H})\]</span> (9.38)</p><p>and an according <span class="math inline">\(f(\beta)\)</span>.</p><h1 id="energy">Energy</h1><h2 id="principle-of-maximum-entrophy-for-physical-systems">Principle ofMaximum Entrophy for Physical Systems</h2><p>So far we have <span class="math display">\[    1=\sum_{i}^{} p_i\]</span> <span class="math display">\[    E=\sum_{i}^{} p_i E_i\]</span> <span class="math display">\[    S=k_B \sum_{i}^{} p_i \ln \left(\frac{1}{p_i}\right)\]</span> <span class="math display">\[    p_i=\mathrm{e}^{-\alpha} \mathrm{e}^{-\beta E_i}\]</span> <span class="math display">\[    \alpha=\ln \left(\sum_{i}^{} \mathrm{e}^{-\beta E_i}\right)=\frac{S}{k_B}-\beta E\]</span></p><h3 id="differential-forms">Differential Forms</h3><p>Suppose <span class="math inline">\(E_i\)</span> does not depend onan external parameter. Take differentiation. <spanclass="math display">\[    0=\sum_{i}^{} \mathrm{d}p_i\]</span> <span class="math display">\[    \mathrm{d}E=\sum_{i}^{} E_i \mathrm{d}p_i\]</span> <span class="math display">\[    \mathrm{d}S=k_B \beta \mathrm{d}E   \]</span> <span class="math display">\[    \mathrm{d}\alpha=-E \mathrm{d}\beta\]</span> <span class="math display">\[    \mathrm{d}p_i=-p_i(E_i-E)\mathrm{d}\beta\]</span> from which it is not difficult to show <spanclass="math display">\[    \mathrm{d}E=-\left(\sum_{i}^{} p_i(E_i-E)^{2}\right)\mathrm{d}\beta\]</span> <span class="math display">\[    \mathrm{d}S=-k_B \beta\left(\sum_{i}^{} p_i(E_i-E)^{2}\right)\mathrm{d}\beta\]</span> Note that the formula relating <spanclass="math inline">\(\mathrm{d}E\)</span> and <spanclass="math inline">\(\mathrm{d}\beta\)</span>, implies that if <spanclass="math inline">\(E\)</span> goes up then <spanclass="math inline">\(\beta\)</span> goes down, and vice versa.</p><h3 id="differential-forms-with-external-parameters">Differential Formswith External Parameters</h3><p>Each <span class="math inline">\(E_i\)</span> could be written in theform <span class="math inline">\(E_i(H)\)</span>. <spanclass="math display">\[    E=\sum_{i}^{} p_i E_i(H) \tag{11.20}\]</span> So we have <span class="math display">\[    0=\sum_{i}^{} \mathrm{d}p_i\]</span> (11.21) <span class="math display">\[    \mathrm{d}E=\sum_{i}^{} E_i(H)\mathrm{d}p_i+\sum_{i}^{} p_i\mathrm{d}E_i(H)\]</span> (11.22) <span class="math display">\[    \mathrm{d}S=k_B \beta \mathrm{d}E-k_B \beta \sum_{i}^{} p_i\mathrm{d}E_i(H)\]</span> (11.23) <span class="math display">\[    \mathrm{d}\alpha=-E\mathrm{d}\beta-\beta \sum_{i}^{} p_i\mathrm{d}E_i(H)\]</span> (11.24) <span class="math display">\[    \mathrm{d}p_i=-p_i(E(H)-E)\mathrm{d}\beta-p_i \beta\mathrm{d}E_i(H)+p_i \beta \sum_{j}^{} p_j \mathrm{d}E_j(H)\]</span> (11.25) <span class="math display">\[    \mathrm{d}E=-\left[\sum_{i}^{} p_i(E_i(H)-E)^{2}\right]\mathrm{d}\beta+\sum_{i}^{} p_i(1-\beta(E_i(H)-E))\mathrm{d}E_i(H)\]</span> (11.26) <span class="math display">\[    \mathrm{d}S=-k_B \beta\left[\sum_{i}^{} p_i(E_i(H)-E)^{2}\right]\mathrm{d}\beta-k_B \beta^{2}\sum_{i}^{} p_i(E_i(H)-E)\mathrm{d}E_i(H)\]</span> (11.27)</p><h2 id="system-and-environment">System and Environment</h2><h3 id="partition-model">Partition Model</h3><p>Our model for the way the total combination is partitioned into thesystem and the environment. We will use index <spanclass="math inline">\(i\)</span> for the system and the index <spanclass="math inline">\(j\)</span> for the environment.</p><p>Whether system and environment are isolated or interacting does notaffect the states or the physical properties associated with the states,although it may affect the probability of occupancy of the states.</p><h3 id="interaction-model">Interaction Model</h3><p>Consider two adjacent dipoles that exchange their orientations —— theone on the left ends up with the orientation that the one on the rightstarted with, and vice versa. There are only a few cases.</p><p>First, if the two dipoles started with the same orientation, nothingwould change. On the other hand,if the two dipoles started withdifferent orientations, the effect would be that the pattern oforientationshas changed. This has happened even though the dipolesthemselves have not moved. Since the energy associated with the twopossible alignments is different, there has been a small change in thelocation of the energy, even though the total energy is unchanged.</p><p>Second, if both dipoles are in the system, or both are in theenvironment, then energy may have shifted position within the system orthe environment, but has not moved between them.</p><p>Third, if the two dipoles started with different alignment, and theyare located one on each side of the boundary between the system and theenvironment, then energy has flowed from the system to the environmentor vice versa. This has happened not because the dipoles have moved, butbecause the orientations have moved.</p><h3 id="extensive-and-intensive-quantities">Extensive and IntensiveQuantities</h3><p>The energies of the system state and of the environment state add upto form the energy of the corresponding total state: <spanclass="math display">\[    E_{t,i,j}=E_{s,i}+E_{e,j} \tag{11.37}\]</span></p><p>The probability of occupancy of total state <spanclass="math inline">\(k\)</span> is the product of the two probabilitiesof the two associated states <span class="math inline">\(i\)</span> and<span class="math inline">\(j\)</span>: <span class="math display">\[    p_{t,i,j}=p_{s,i}p_{e,j}\]</span></p><p>The total energy is <span class="math display">\[    E_t=\sum_{i,j}^{} E_{t,i,j}p_{t,i,j}=\sum_{j}^{}E_{e,j}p_{e,j}+\sum_{i}^{} E_{s,i}p_{s,i}=E_e+E_s\]</span> (11.39)</p><p>This result holds whether the system and environment are isolated orinteracting. Similarly, <span class="math display">\[    S_t=\sum_{i,j}^{} p_{t,i,j}\ln\left(\frac{1}{p_{t,i,j}}\right)=\sum_{j}^{} p_{e,j}\ln\left(\frac{1}{p_{e,j}}\right)+\sum_{i}^{} p_{s,i} \ln\left(\frac{1}{p_{s,i}}\right)=S_{e}+S_{s}    \]</span> (11.40)</p><p>This kind of quantity with the property that its total value is thesum of the value for the two (or more) parts is known as an<strong>extensive quantity</strong>.</p><p>Not all quantities are extensive. In particular, <spanclass="math inline">\(\alpha\)</span> and <spanclass="math inline">\(\beta\)</span> are not. If the system andenvironment are isolated, then there is no reason why <spanclass="math inline">\(\beta_s\)</span> and <spanclass="math inline">\(\beta_e\)</span> would be related. However, if thesystem and environment are interacting, the distribution of energybetween the system and the environment may not be known and thereforethe Principle of Maximum Entropy can be applied only to thecombination.</p><p>Quantities like <span class="math inline">\(\beta\)</span> that arethe same throughout a system when analyzed as a whole are called<strong>intensive</strong>.</p><h3 id="equilibrium">Equilibrium</h3><p>After mixing the system and the environment, the total entrophyincreases. The energies of the system and the environment have changed,and as a result the values of <spanclass="math inline">\(\beta_s\)</span> and <spanclass="math inline">\(\beta_e\)</span> have changed, in oppositedirections. Their new values are the same (each is equal to <spanclass="math inline">\(\beta_t\)</span>), and therefore this new valuelies between the two old values.</p><h3 id="energy-flow-work-and-heat">Energy Flow, Work and Heat</h3><p>Energy can be transferred by heat and work at the same time. Work isrepresented by changes in the energy of the individual states <spanclass="math inline">\(\mathrm{d}E_i\)</span>, and heat by changes in theprobabilities <span class="math inline">\(p_i\)</span>. <spanclass="math display">\[    \mathrm{d}E=\sum_{i}^{} E_i(H)\mathrm{d}p_i+\sum_{i}^{} p_i\mathrm{d}E_i(H)\]</span> (11.52)</p><p>where the first term is heat and the second term is work.</p><h3 id="reversible-energy-flow">Reversible Energy Flow</h3><p><span class="math display">\[    \mathrm{d}S_{s}=k_{B} \beta_{s}\mathrm{d}E_s-k_{B}\beta_s\sum_{i}^{}p_{s,i}\mathrm{d}E_{s,i}(H)=k_{B}\beta_s\left[\mathrm{d}E_{s}-\sum_{i}^{}p_{s,i}\mathrm{d}E_{s,i}(H)\right]=k_{B}\beta_{s}\mathrm{d}q_{s}\]</span> (11.55)</p><p>where <span class="math inline">\(\mathrm{d}q_{s}\)</span> stands forthe heat that comes into the system due to the interactionmechanism.</p><p>This formula applies to the system and a similar formula applies tothe environment: <span class="math display">\[    \mathrm{d}S_{e}=k_{B}\beta_{e}\mathrm{d}q_{e} \tag{11.56}\]</span> The two heats are the same except for sign <spanclass="math display">\[    \mathrm{d}q_{s}=-\mathrm{d}q_{e} \tag{11.57}\]</span></p><p>and it therefore follows that the total entropy <spanclass="math inline">\(S_{s}+S_{e}\)</span> is unchanged (i.e., <spanclass="math inline">\(\mathrm{d}S_{s}=-\mathrm{d}S_{e}\)</span> ) if andonly if the two values of <span class="math inline">\(\beta\)</span> forthe system and environment are the same: <span class="math display">\[    \beta_s=\beta_e \tag{11.58}\]</span></p><p>Reversible changes (with no changes in total entrophy) can involvework and heat and therefore changes in energy and entrophy for thesystem, but the system and the environment must have the same value of<span class="math inline">\(\beta\)</span>. Otherwise, the changes areirreversible. Also, reversible changes result in no change to <spanclass="math inline">\(\beta\)</span>.</p><p>The first-order change formulas given earlier can be written toaccount for reversible interactions with the environment by simplysetting <span class="math inline">\(\mathrm{d}\beta=0\)</span> <spanclass="math display">\[    0=\sum_{i}^{} \mathrm{d}p_i\]</span> (11.59) <span class="math display">\[    \mathrm{d}E=\sum_{i}^{} E_i(H)\mathrm{d}p_i+\sum_{i}^{} p_i\mathrm{d}E_i(H)\]</span> (11.60) <span class="math display">\[    \mathrm{d}S=k_{B}\beta \mathrm{d}E-k_{B}\beta \sum_{i}^{} p_i\mathrm{d}E_i(H)\]</span> (11.61) <span class="math display">\[    \mathrm{d}\alpha=-\beta \sum_{i}^{} p_i \mathrm{d}E_i(H)\]</span> (11.62) <span class="math display">\[    \mathrm{d}p_i=-p_i \beta \mathrm{d}E_i(H)+p_i \beta \sum_{j}^{} p_j\mathrm{d}E_j(H)\]</span> (11.63) <span class="math display">\[    \mathrm{d}E=\sum_{i}^{} p_i(1-\beta(E_i(H)-E))\mathrm{d}E_i(H)\]</span> (11.64) <span class="math display">\[    \mathrm{d}S=-k_{B} \beta^{2}\sum_{i}^{}p_i(E_i(H)-E)\mathrm{d}E_i(H)\]</span> (11.65)</p><h1 id="temperature">Temperature</h1><p>In this chapter we will interpret <spanclass="math inline">\(\beta\)</span> further, and will define itsreciprocal as the temperature of the material.</p><h2 id="temperature-scales">Temperature Scales</h2><p>Recall in Chapter 11 <span class="math display">\[    \frac{\mathrm{d}E}{\mathrm{d}S}=\frac{1}{k_{B}\beta}\]</span></p><p>Define the "absolute temperature" as <span class="math display">\[    T=\frac{1}{k_{B}\beta}\]</span></p><h2 id="heat-engine">Heat Engine</h2><p>The change in energy can be attributed to the effects of work <spanclass="math inline">\(\mathrm{d}w\)</span> and heat <spanclass="math inline">\(\mathrm{d}q\)</span> <span class="math display">\[    \mathrm{d}w=\left(\frac{E}{H}\right) \mathrm{d}H\]</span> <span class="math display">\[    \mathrm{d}q=\sum_{i}^{} E_i(H) \mathrm{d}p_i =T \mathrm{d}S\]</span></p><h1 id="quantum-information">Quantum Information</h1><h2 id="quantum-information-storage">Quantum Information Storage</h2><h3 id="model-1-tiny-classical-bits">Model 1: Tiny Classical Bits</h3><p>Like the magnetic dipole model, this model of the quantum bit behavesessentially like a classical bit except that its size may be very smalland it mamy be able to be measured rapidly.</p><h3 id="model-2-superposition-of-states-the-qubit">Model 2:Superposition of States (the Qubit)</h3><p><span class="math display">\[    \psi=\alpha \psi_0+\beta \psi_1\]</span> When a measurement is made, the values of <spanclass="math inline">\(\alpha\)</span> and <spanclass="math inline">\(\beta\)</span> change so that one of them is <spanclass="math inline">\(1\)</span> and the other is <spanclass="math inline">\(0\)</span>.</p><h3 id="model-3-multiple-qubits-with-entanglement">Model 3: MultipleQubits with Entanglement</h3><p><span class="math display">\[    \psi=\alpha_{00} \psi_{00}+\alpha_{01}\psi_{01}+\alpha_{10}\psi_{10}+\alpha_{11} \psi_{11}\]</span></p><p>This can be thought as two qubits, one corresponding to each of thetwo measurements. These qubits are not independent, but rather are<strong>entangled</strong> in some way. A measurement of, for example,the first qubit will return <span class="math inline">\(0\)</span> withprobability <span class="math inline">\(\lvert \alpha_{00} \rvert^{2}+\lvert \alpha_{01} \rvert ^{2}\)</span> and if it does the wavefuntion collapses to only those stationary states that are consistentwith this measured value <span class="math display">\[    \psi=\frac{\alpha_{00}\psi_{00}+\alpha_{01} \psi_{01}}{\sqrt{\lvert\alpha_{00} \rvert ^{2}+\lvert \alpha_{01} \rvert ^{2}}}\]</span></p><h2 id="bracket-notation-for-qubits">Bracket Notation for Qubits</h2><h3 id="kets-bras-brackets-and-operators">Kets, Bras, Brackets, andOperators</h3><p>A ket is a column vector composed of complex numbers.</p><p><span class="math display">\[    |k\rangle =    \begin{pmatrix}    k_1 \\    k_2 \\    \end{pmatrix}    =    \vec k\]</span></p><p>The two kets <span class="math inline">\(|0\rangle\)</span> and <spanclass="math inline">\(| 1 \rangle\)</span> are used to represent the twological states of qubits, and have a standard vector representation<span class="math display">\[    |0 \rangle =\begin{pmatrix}    1 \\    0 \\    \end{pmatrix}    \quad    |1 \rangle =    \begin{pmatrix}    0 \\    1 \\    \end{pmatrix}\]</span></p><p>The superposition of <span class="math inline">\(|0 \rangle\)</span>and <span class="math inline">\(|1 \rangle\)</span> can be written as<span class="math display">\[    |\psi\rangle =\alpha |0\rangle+\beta|1 \rangle=\begin{pmatrix}    \alpha \\    \beta    \end{pmatrix}\]</span></p><hr /><p>Bras are the Hermitian conjugates of kets. <spanclass="math display">\[    \langle \psi |=(|\psi\rangle)^{\dag}=(\alpha^{*} \ \beta^{*})\]</span> (<span class="math inline">\(^{*}\)</span>) is theconventional notation for the conjugate of a complex number.</p><hr /><p>The dot product is the product of a bra by a ket. It is calledbracket. <span class="math display">\[    \langle q | k \rangle =\sum_{j}^{} q_j^{*}k_j\]</span></p><p>We have the very important property of kets.</p><p><span class="math display">\[    \langle \psi | \psi \rangle =\lvert \alpha \rvert ^{2}+\lvert \beta\rvert ^{2}=1\]</span></p><p>The dot product can be used to compute the probability of a qubit ofbeing in either one of the possible states <spanclass="math inline">\(|0\rangle\)</span> and <spanclass="math inline">\(|1\rangle\)</span>. <span class="math display">\[    \operatorname{Pr}(|0\rangle)=\lvert \langle 0 | \psi\rangle \rvert^{2}=\lvert \alpha \rvert ^{2}   \]</span></p><hr /><p>Operators are objects that transform one ket <spanclass="math inline">\(|k\rangle\)</span> into another ket <spanclass="math inline">\(| q\rangle\)</span>. Operators are representedwith hats: <span class="math inline">\(\widehat{O}\)</span>. <spanclass="math display">\[    \widehat{O} | k\rangle =\begin{pmatrix}    o_{11} &amp; o_{12} \\    o_{21} &amp; o_{22} \\    \end{pmatrix}    \begin{pmatrix}    k_1 \\    k_2 \\    \end{pmatrix}    =    \begin{pmatrix}    q_1 \\    q_2 \\    \end{pmatrix}    =    |q\rangle\]</span></p><p>Operators act on bras in a similar manner <spanclass="math display">\[    \langle k| \hat{O}^{\dag}=\langle q |\]</span></p><p>All quantum operators must be unitary!</p><p>If we know the input and output kets, we have a easy way to constructan operator <span class="math inline">\(\widehat{O}\)</span> using theexterior product of a ket by a bra.</p><p><span class="math display">\[    \widehat{O} | k\rangle=(|q \rangle \langle k|)|k\rangle\]</span></p><h3 id="tensor-product-composite-systems">Tensor Product —— CompositeSystems</h3><p>If we have two qubits <spanclass="math inline">\(|\psi\rangle\)</span> and <spanclass="math inline">\(|\phi \rangle\)</span>, the system composed bythese two qubits is represented by <spanclass="math inline">\(|\psi\rangle \otimes |\phi\rangle\)</span>.</p><p>The following four representations of the tensor product are madeequivalent <span class="math display">\[    |q_1 \rangle \otimes |q_2\rangle \equiv |q_1\rangle | q_2 \rangle\equiv |q_1, q_2 \rangle \equiv |q_1q_2\rangle\]</span></p><p>For <span class="math inline">\(n\)</span> qubits, <spanclass="math display">\[    |q_1 \rangle \otimes |q_2 \rangle \otimes \cdots \otimes |q_n\rangle= \bigotimes_{j=1}^{n}|q_j\rangle\]</span></p><p>The dual of a tensor product of kets is the tensor product of thecorresponding bras. <span class="math display">\[    (|q_1q_2\rangle)^{\dag}=(|q_1\rangle \otimes |q_2 \rangle)^{\dag}=\langle q_1 | \otimes \langle q_2|=\langle q_1q_2|\]</span></p><p>The result of the dot product of two composite systems is themultiplication of the individual dot products taken in order. <spanclass="math display">\[    \langle q_1q_2 | w_1w_2 \rangle=(\langle q_1 | \otimes \langleq_2|)(|w_1\rangle \otimes |w_2\rangle)=\langle q_1| w_1\rangle \otimes\langle q_2 | w_2 \rangle\]</span></p><h2 id="no-cloning-theorem">No Cloning Theorem</h2><p>Qubits cannot be cloned.</p><p>To show taht cloning is not possible, let us assume that an operator<span class="math inline">\(\widehat{C}\)</span> takes the informationof one qubit <span class="math inline">\(|\phi_1\rangle\)</span> andcopies it into another "blank" qubit, the result is a qubit <spanclass="math inline">\(|\psi_1\rangle\)</span> identical to <spanclass="math inline">\(|\phi_1\rangle\)</span>, and the original <spanclass="math inline">\(|\phi_1\rangle\)</span> is unmodified. <spanclass="math inline">\(\widehat{C}\)</span> must be unitary. Thus wedefine <span class="math inline">\(\widehat{C}\)</span> <spanclass="math display">\[    | Original \rangle \otimes | Blank \rangle\stackrel{\widehat{C}}{\longrightarrow} |Original \rangle \otimes  |clone \rangle\]</span></p><p>We are now ready to clone two arbitrary qubits <spanclass="math inline">\(|\phi_1\rangle\)</span> and $| _2 $ separately.<span class="math display">\[    \widehat{C} | \phi_1 \rangle | blank \rangle =| \phi_1 \rangle |\psi_1 \rangle\]</span> <span class="math display">\[    \widehat{C} | \phi_2 \rangle | blank \rangle =| \phi_2 \rangle |\psi_2 \rangle\]</span> where it is understood that $| _1 =| _1 $ and <spanclass="math inline">\(| \phi_2 \rangle =| \psi_2 \rangle\)</span>, andwe have given them different names to distinguish original fromcopy.</p><p>Since the cloning machine is unitary, it preserves the dot products,so we can compare the dot product before and after cloning <spanclass="math display">\[    \langle \phi_2 | \langle blank | | \phi_1 \rangle | blank \rangle =\langle \phi_2 | \langle \psi_2 | | \phi_1 \rangle | \psi_1 \rangle\]</span> therefore <span class="math display">\[    \langle \phi_2 | \phi_1  \rangle \langle blank | blank \rangle=\langle \phi_2 | \phi_1 \rangle \langle \psi_2 | \psi_1 \rangle\]</span> The requirements that kets be normalized imposes that <spanclass="math inline">\(\langle blank | blank \rangle =1\)</span>. theabove equation can only be true in two cases: - <spanclass="math inline">\(\langle \phi_2 | \phi_1 \rangle =0\)</span>, whichmeans that $| _1 $ and $| _2 $ are orthogonal. This means that we canclone states chosen at random from a set of orthogonal states. And isequivalent to say that we can clone $| 0 $ and $| 1 $, which we alreadyknew since we do that classically all the time. - <spanclass="math inline">\(\langle \psi_2 | \psi_2 \rangle =1\)</span>, whichmeans that <span class="math inline">\(\psi_2=\psi_1\)</span>, that is,that clones obtained in each operation are identical. If the twooriginals were different, as we had assumed, what this result says isthat the clone is independent from the original, which is quite abizarre property for a clone!</p><h2 id="representation-of-qubits">Representation of Qubits</h2><h3 id="qubits-in-the-bloch-sphere">Qubits in the Bloch sphere</h3><p><span class="math display">\[    | \psi \rangle =\mathrm{e}^{ia} \left( \cos \frac{\theta}{2}| 0\rangle +\sin \frac{\theta}{2} \mathrm{e}^{i\varphi} | 1 \rangle \right)\]</span></p><p>If we ignore the global phase factor $^{ia} $, the two angles <spanclass="math inline">\(\theta\)</span> and <spanclass="math inline">\(\varphi\)</span> define a point in a unit sphere.This sphere is called the Bloch Sphere.</p><h3 id="qubits-and-symmetries">Qubits and symmetries</h3><p>Pauli matrices: <span class="math display">\[    \mathbb{I}=    \begin{pmatrix}    1 &amp; 0 \\    0 &amp; 1 \\    \end{pmatrix}    \quad    \sigma_{x}=    \begin{pmatrix}    0 &amp; 1 \\    1 &amp; 0 \\    \end{pmatrix}    \quad    \sigma_y=    \begin{pmatrix}    0 &amp; -i \\    i &amp; 0 \\    \end{pmatrix}       \quad    \sigma_{z}=    \begin{pmatrix}    1 &amp; 0 \\    0 &amp; -1 \\    \end{pmatrix}\]</span></p><h4 id="action-of-pauli-matrices-on-an-arbitrary-qubit">Action of Paulimatrices on an arbitrary qubit</h4><p>An arbitrary superposition state <span class="math display">\[    | \psi \rangle =\alpha| 0 \rangle +\beta | 1 \rangle =\cos\frac{\theta}{2} | 0 \rangle +\sin \frac{\theta}{2}\mathrm{e}^{i\varphi} | 1 \rangle\]</span></p><p>So <span class="math display">\[    \sigma_x | \psi \rangle =    \begin{pmatrix}    \beta \\    \alpha \\    \end{pmatrix}\]</span> (Rotation of <span class="math inline">\(\pi\)</span> about<span class="math inline">\(x\)</span> axis)</p><p><span class="math display">\[    \sigma_y | \psi \rangle =i    \begin{pmatrix}    -\beta \\    \alpha \\    \end{pmatrix}    \]</span> (Rotation of <span class="math inline">\(\pi\)</span> about<span class="math inline">\(y\)</span> axis)</p><p><span class="math display">\[    \sigma_z | \psi \rangle =    \begin{pmatrix}    \alpha \\    -\beta \\    \end{pmatrix}    \]</span> (Rotation of <span class="math inline">\(\pi\)</span> about<span class="math inline">\(z\)</span> axis)</p><p>Hence Pauli matrices are rotations of <spanclass="math inline">\(\pi\)</span> about each of the axis of the blochsphere.</p><p><span class="math display">\[    \mathrm{e}^{i \sigma_x \theta/2} =\cos \frac{\theta}{2} \mathbb{I}+i\sin \frac{\theta}{2}\sigma_x\]</span></p><p>This result shows us how to do arbitrary rotations of an angle <spanclass="math inline">\(\theta\)</span> about the <spanclass="math inline">\(x\)</span> axis, the resulting operator is oftencalled $R_x()=^{i_x /2} $. The cases of <spanclass="math inline">\(R_y\)</span> and <spanclass="math inline">\(R_z\)</span> are completely analogous.</p><h3 id="quantum-gates">Quantum Gates</h3><h4 id="elementary-quantum-gates">Elementary Quantum Gates</h4><ul><li>Pauli <span class="math inline">\(X\)</span>: <spanclass="math inline">\(X=\begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \\\end{pmatrix}\equiv \sigma_x\)</span>. It is equivalent to doing a <spanclass="math inline">\(NOT\)</span> or bit flip.</li><li>Pauli <span class="math inline">\(Y\)</span>: <spanclass="math inline">\(Y=\begin{pmatrix} 0 &amp; -i \\ i &amp; 0 \\\end{pmatrix}\equiv \sigma_y\)</span>.</li><li>Pauli <span class="math inline">\(Z\)</span>: <spanclass="math inline">\(Z=\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; -1 \\\end{pmatrix}\equiv \sigma_z\)</span>. Changes the internal phase.</li><li>Hadamard: <spanclass="math inline">\(H=\frac{1}{\sqrt{2}}\begin{pmatrix} 1 &amp; 1 \\ 1&amp; 1 \\ \end{pmatrix}\)</span>.</li><li>Phase: <span class="math inline">\(S=\begin{pmatrix} 1 &amp; 0 \\ 0&amp; i \\ \end{pmatrix}\)</span>.</li></ul><p>Below we enumerates some of the properties of the elementary quantumgates.</p><p><span class="math display">\[    H=\frac{1}{\sqrt{2}}(X+Z) \quad HXH=Z\]</span> <span class="math display">\[    XYX=-Y \quad HYH=-Y\]</span> <span class="math display">\[    XZX=-Z \quad HZH=X\]</span> <span class="math display">\[    XR_{y}(\theta) X=R_y(-\theta) \quad XR_z(\theta)X=R_y(-\theta)\]</span></p><h4 id="two-qubit-gates.-controlled-gates">Two-qubit gates. ControlledGates</h4><p>Operators are unitary and square, so quantum gates will always havethe same number of inputs and outputs. Another way to say it is that allquantum gates are naturally reversible.</p><p>The most important two qubit gates are the controlled gates. In acontrolled gate the first input qubit is a control qubit. If is in the$| 1 $ state, it will trigger the gate that acts on the second qubit,otherwise, it will not trigger it and the second qubit will remainunaltered. Say, a control qubit <span class="math inline">\(| \psi\rangle =\begin{pmatrix} \alpha \\ \beta \\ \end{pmatrix}\)</span> withthe second qubit $| $ and the gate <spanclass="math inline">\(U\)</span> results in $| +U | $.</p><p>There are two controlled gates that are very relevant to thealgorithms we will describe later on, the <spanclass="math inline">\(C-X\)</span> also known as <spanclass="math inline">\(C-NOT\)</span> and the <spanclass="math inline">\(C-Z\)</span> also known as <spanclass="math inline">\(C-Phase\)</span>.</p><h2 id="quantum-communication">Quantum Communication</h2><h3 id="teleportation---alice-and-bobs-story">Teleportation - Alice andBob's story</h3><p>Alice and Bob entangled a pair of qubits $| _{AB} $ when they firstmet, <span class="math display">\[    | \phi_{AB} \rangle =\frac{1}{\sqrt{2}} (| 0_{A} \rangle \otimes  |0_{B} \rangle +| 1_{A} \rangle \otimes | 1_{B} \rangle )\]</span></p><p>Life took each of them through separate paths. However, Alice decidesto send Bob a letter in a qubit $| <em>{L} =| 0</em>{L} +| 1_{L} $.</p><p>Alice puts the qubit of the pair she once entangled with Bob in acomposite system with $| _{L} $. The complete three-qubit system can berepresented using tensor products <span class="math display">\[    | \phi_{A}\psi_{L}\phi_{B} \rangle =\frac{1}{\sqrt{2}}\biggl( |0_{A} \rangle  \otimes (\alpha | 0_{L} \rangle +\beta | 1_{L}\rangle )\otimes  | 0_{B} \rangle + | 1_{A} \rangle \otimes (\alpha | 0_{L}\rangle +\beta | 1_{L} \rangle ) \otimes | 1_{B} \rangle \biggr)\]</span></p><p>In practice what the <span class="math inline">\(C-NOT\)</span> doesis transfer the superposition to Alice's Qubit <spanclass="math display">\[    \begin{aligned}        &amp;= \frac{1}{\sqrt{2}}\alpha\biggl( | 0_{A} \rangle \otimes |0_{B} \rangle + | 1_{A} \rangle  \otimes | 1_{B} \rangle \biggr) \otimes| 0_{L} \rangle  \\        &amp;+ \frac{1}{\sqrt{2}}\beta\biggl( | 1_{A} \rangle \otimes |0_{B} \rangle +| 0_{A} \rangle \otimes | 1_{B} \rangle \biggr) \otimes |1_{L} \rangle      \end{aligned}\]</span></p><p>At this point Alice's and Bob's qubit have both the information ofthe superposition that was originally in the letter. The Hadamard gateproduces a new superposition out of the letter as follows <spanclass="math display">\[    \begin{aligned}        &amp;= \frac{1}{\sqrt{2}}\alpha\biggl( | 0_{A} \rangle \otimes |0_{B} \rangle + | 1_{A} \rangle  \otimes | 1_{B} \rangle \biggr) \otimes\frac{1}{\sqrt{2}}(| 0_{L} \rangle+| 1_{L} \rangle )  \\        &amp;+ \frac{1}{\sqrt{2}}\beta\biggl( | 1_{A} \rangle \otimes |0_{B} \rangle +| 0_{A} \rangle \otimes | 1_{B} \rangle \biggr) \otimes\frac{1}{\sqrt{2}}(| 0_{L} \rangle-| 1_{L} \rangle )      \end{aligned}\]</span></p><p>At this point the information about the superposition in the originalletter is no longer in Alice's hands. To appreciate that it is so, weneed to make some manipulations and reordering of the corss products.<span class="math display">\[    \begin{aligned}        &amp;= \frac{1}{2}| 0_{A} \rangle \otimes | 0_{L} \rangle\otimes (\alpha | 0_{B} \rangle +\beta | 1_{B} \rangle ) \\        &amp;+ \frac{1}{2}| 0_{A} \rangle \otimes | 1_{L} \rangle\otimes (\alpha | 0_{B} \rangle -\beta | 1_{B} \rangle ) \\        &amp;+ \frac{1}{2}| 1_{A} \rangle \otimes | 0_{L} \rangle\otimes (\alpha | 1_{B} \rangle +\beta | 0_{B} \rangle ) \\        &amp;+ \frac{1}{2}| 1_{A} \rangle \otimes | 1_{L} \rangle\otimes (\alpha | 1_{B} \rangle -\beta | 0_{B} \rangle ) \\    \end{aligned}\]</span></p><p>The next steps are the key to faultless teleportation - Alicemeasures her two qubits, she will obtain either of $| 0_{A}0_{L} <spanclass="math inline">\(,\)</span>| 0_{A}1_{L} $, $| 1_{A}0_{L} $, or $|1_{A}1_{L} $ with equal probability. - Upon Alice's measurement, Bob'squbit takes the value of one of the four possible superpositions. Andso, the result of her measurement can help Bob unscramble his bit. - Ifshe measured $| 0_{A}0_{L} $, she will tell Bob not to do anything tohis qubit. If she measured $| 0_{A}1_{L} $, Bob will have to correct forthe phase (that can be done with a <spanclass="math inline">\(Z\)</span> gate). If she measured $| 1_{A}0_{L} $,the states in the message have been flipped, and to unflip them Bob willhave to use a bit-flip (a.k.a not, a.k.a <spanclass="math inline">\(X\)</span>) gate. Finally if she measured $|1_{A}1_{L} $, Bob will have to correct both for the phase and the bitflip.</p><p>Alice needs to communicate to bob 2 classical bits. The first bit ofAlice informs about bit-flip errors and the second about phaseerrors.</p>]]></content>
    
    
    <categories>
      
      <category>信息论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>常微分方程（11）</title>
    <link href="/2022/07/19/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%8811%EF%BC%89/"/>
    <url>/2022/07/19/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%8811%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="常微分方程拾遗">常微分方程拾遗</h1><p>我是个没有感情的抄书机器~</p><h2 id="green-函数相关">Green 函数相关</h2><h3 id="二阶线性齐次常微分方程的跃度">二阶线性齐次常微分方程的跃度</h3><p>首先我们知道任何一个二阶线性齐次常微分方程可以整理成如下形式： <spanclass="math display">\[    \frac{\mathrm{d}}{\mathrm{d}x}\left[p(x)\frac{\mathrm{d}y(x)}{\mathrm{d}x}\right]+q(x)y(x)=0,\quad a&lt;x&lt;b.      \]</span> 考虑非奇异情形，即假设 <spanclass="math inline">\(p(x)\)</span>，<spanclass="math inline">\(p&#39;(x)\)</span> 和 <spanclass="math inline">\(q(x)\)</span> 都是定义域 <spanclass="math inline">\(a&lt;x&lt;b\)</span> 上的实连续函数，<spanclass="math inline">\(p(x)\)</span> 无零点. 因此该方程的两个线性无关解<span class="math inline">\(y_1(x)\)</span> 和 <spanclass="math inline">\(y_2(x)\)</span> 都在定义域 <spanclass="math inline">\(a&lt;x&lt;b\)</span> 上连续. 而对非齐次项为 <spanclass="math inline">\(\delta\)</span> 函数的二阶线性常微分方程 <spanclass="math display">\[    \frac{\mathrm{d}}{\mathrm{d}x}\left[p(x)\frac{\mathrm{d}g(x;t)}{\mathrm{d}x}\right]+q(x)g(x;t)=\delta(x-t),\quad a&lt;x&lt;b, a&lt;t&lt;b    \]</span> 当 <span class="math inline">\(x\neq t\)</span> 时，它的解由<span class="math inline">\(y_1(x)\)</span> 和 <spanclass="math inline">\(y_2(x)\)</span> 叠加得到： <spanclass="math display">\[    \begin{aligned}        g(x;t)&amp;=        \begin{cases}        c_1(t)y_1(x)+c_2(t)y_2(x) \equiv g_{&lt;}, \quad x&lt;t \\        d_1(t)y_1(x)+d_2(t)y_2(x) \equiv g_{&gt;}, \quad x&gt;t        \end{cases} \\        &amp;= g_{&lt;}+(g_{&gt;}-g_{&lt;})\eta(x-t), \\    \end{aligned}\]</span></p><p>代回原方程，整理得到对于任何检验函数 <spanclass="math inline">\(f(x)\)</span>， <span class="math display">\[    \left[p&#39;(t)(g_{&gt;}(t)-g_{&lt;}(t))+2p(t) \left(\frac{\mathrm{d}g_{&gt;}}{\mathrm{d}x}-\frac{\mathrm{d}g_{&lt;}}{\mathrm{d}x}\right)_{x=t}-1\right]f(t)= p(t)(g_{&gt;}(t)-g_{&lt;}(t))f&#39;(t).\]</span> 由 <span class="math inline">\(f(x)\)</span> 的任意性，得<span class="math inline">\(g(x;t)\)</span> 在 <spanclass="math inline">\(x=t\)</span> 处连续，而 <spanclass="math inline">\(\frac{\mathrm{d}g(x;t)}{\mathrm{d}x}\)</span> 在<span class="math inline">\(x=t\)</span> 处有跃度 <spanclass="math inline">\(\frac{1}{p(t)}\)</span>，即 <spanclass="math display">\[    \frac{\mathrm{d}g(x;t)}{\mathrm{d}x}\bigg|^{x=t+}_{x=t-}=\frac{1}{p(t)}\]</span></p><h3 id="常微分方程初值问题的-green-函数">常微分方程初值问题的 Green函数</h3><p>初值问题 <span class="math display">\[    \frac{\mathrm{d}^{2}g}{\mathrm{d}t^{2}}=\delta(t-\tau), \quadt&gt;0, \tau&gt;0,\]</span> <span class="math display">\[    g|_{t=0}=0, \quad \frac{\mathrm{d}g}{\mathrm{d}t}\bigg|_{t=0}=0\]</span> 对原方程积分两次得到通解 <span class="math display">\[    g(t;\tau)=(t-\tau)\eta(t-\tau)+\alpha(\tau)t+\beta(\tau).\]</span></p><p>由初值条件定出 <span class="math display">\[    g(t;\tau)=(t-\tau)\eta(t-\tau).\]</span></p><hr /><p>那么初值问题 <span class="math display">\[    \frac{\mathrm{d}^{2}y}{\mathrm{d}t^{2}}=f(t), \quad t&gt;0,\]</span> <span class="math display">\[    y(0)=0, \quad y&#39;(0)=0.\]</span> 就有解 <span class="math display">\[    y(t)=\int_{0}^{\infty} g(t;\tau)f(\tau) \mathrm{d}\tau=\int_{0}^{t}(t-\tau)f(\tau) \mathrm{d}\tau.\]</span></p><hr /><p>初值问题 <span class="math display">\[    \frac{\mathrm{d}^{2}g(t;\tau)}{\mathrm{d}t^{2}}+k^{2}g(t;\tau)=\delta(t-\tau),\quad t&gt;0, \tau &gt;0,\]</span> <span class="math display">\[    g(0;\tau)=0, \quad\frac{\mathrm{d}g(t;\tau)}{\mathrm{d}t}\bigg|_{t=0}=0.\]</span></p><p>有解 <span class="math display">\[    g(t;\tau)=\frac{1}{k} \sin k(t-\tau)\eta(t-r)+C(r)\sin kt+D(r)\coskt.\]</span></p><hr /><p>初值问题 <span class="math display">\[    \frac{\mathrm{d}^{2}y}{\mathrm{d}t^{2}}+k^{2}y(t)=f(t), \quadt&gt;0,\]</span> <span class="math display">\[    y(0)=0, \quad y&#39;(0)=0\]</span> 有解 <span class="math display">\[    y(t)=\frac{1}{k}\int_{0}^{t} f(\tau)\sin k(t-\tau) \mathrm{d}\tau.\]</span></p><hr /><p>一般的初值问题 <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}\left[ p(t)\frac{\mathrm{d}g(t;\tau)}{\mathrm{d}t}\right]+q(t)g(t;\tau)=\delta(t-\tau), \quad t&gt;0,\tau&gt;0;\]</span> <span class="math display">\[    g(0;\tau)=0, \quad\frac{\mathrm{d}g(t;\tau)}{\mathrm{d}t}\bigg|_{t=0}=0,\]</span> 其中 <span class="math inline">\(p(t)\)</span>,<spanclass="math inline">\(p&#39;(t)\)</span> 和 <spanclass="math inline">\(q(t)\)</span> 都是 <spanclass="math inline">\([0,\infty)\)</span> 上的实连续函数， <spanclass="math inline">\(p(t)\)</span> 无零点.</p><p>定义 <span class="math inline">\(y_1(t)\)</span> 和 <spanclass="math inline">\(y_2(t)\)</span>是相应齐次线性方程的两个线性无关解. Wronsky 行列式 <spanclass="math display">\[    W[y_1(t),y_2(t)]\equiv    \begin{vmatrix}    y_1(t) &amp; y_2(t) \\    y_1&#39;(t) &amp; y_2&#39;(t) \\    \end{vmatrix}    \neq 0\]</span></p><p>原方程的解为 <span class="math display">\[    g(t;\tau)=\frac{1}{p(\tau)}\frac{y_1(\tau)y_2(t)-y_2(\tau)y_1(t)}{W[y_1(\tau),y_2(\tau)]}\eta(t-\tau).\]</span> 实际上可以去掉 <spanclass="math inline">\(t,\tau&gt;0\)</span> 的限制，补充条件 <spanclass="math display">\[    p(t)=p(-t), \quad q(t)=q(-t),\]</span> 并将初值条件变为 <span class="math display">\[    g(t;\tau)|_{t&lt;\tau}=0, \quad\frac{\mathrm{d}g(t;\tau)}{\mathrm{d}t}\bigg|_{t&lt;\tau}=0.\]</span></p><hr /><p>进一步，初值问题 <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}\left[ p(t)\frac{\mathrm{d}y(t)}{\mathrm{d}t}\right]+q(t) y(t)=f(t), \quad t&gt;0,\]</span> <span class="math display">\[    g(0;\tau)=0, \quad y&#39;(0)=0\]</span> 有解 <span class="math display">\[    y(t)=\int_{0}^{t} g(t;\tau)f(\tau) \mathrm{d}\tau.\]</span></p><hr /><p>更一般的，初值问题 <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}\left[ p(t)\frac{\mathrm{d}y(t)}{\mathrm{d}t}\right]+q(t) y(t)=f(t), \quad t&gt;0,\]</span> <span class="math display">\[    g(0;\tau)=A, \quad y&#39;(0)=B\]</span></p><p>有解（需要用到 <spanclass="math inline">\(g(t;\tau)=g(-\tau;-t)\)</span>）</p><p><span class="math display">\[    \begin{aligned}    y(t)&amp;=\int_{0}^{\infty} g(t;\tau)f(\tau) \mathrm{d}\tau +\left\{ p(r)\left[ y(\tau)\frac{\mathrm{d}g(t;\tau)}{\mathrm{d}\tau}-g(t;\tau)\frac{\mathrm{d}y(\tau)}{\mathrm{d}\tau}\right]\right\}^{\infty}_{\tau=0}         \\    &amp;=\int_{0}^{t} g(t;\tau) f(\tau) \mathrm{d}\tau-p(0)\left[ A\frac{\mathrm{d}g(t;\tau)}{\mathrm{d}\tau}-Bg(t;\tau)\right]_{\tau=0}    \end{aligned}\]</span></p><h3 id="常微分方程边值问题的-green-函数">常微分方程边值问题的 Green函数</h3><p>边值问题 <span class="math display">\[    \frac{\mathrm{d}^{2}g(x;\xi)}{\mathrm{d}x^{2}}=\delta(x-\xi), \quada&lt;x,\xi&lt;b,\]</span> <span class="math display">\[    g(a,\xi)=0, \quad g(b;\xi)=0.\]</span></p><p>有解 <span class="math display">\[    g(x;\xi)=(x-\xi)\eta(x-\xi)-\frac{b-\xi}{b-a}(x-a).\]</span></p><hr /><p>边值问题 <span class="math display">\[    \frac{\mathrm{d}^{2}g(x;\xi)}{\mathrm{d}x^{2}}+k^{2}g(x;\xi)=\delta(x-\xi), \quad a&lt;x,\xi&lt;b,\]</span> <span class="math display">\[    g(a;\xi)=0, \quad g(b;\xi)=0.\]</span></p><p>有解 <span class="math display">\[    g(x;\xi)=    \begin{cases}        \displaystyle -\frac{1}{k}\frac{\sin k(b-\xi)}{\sin k(b-a)}\sink(x-a), \quad a&lt;x&lt;\xi, \\        \displaystyle -\frac{1}{k}\frac{\sin k(\xi-a)}{\sin k(b-a)} \sink(b-x), \quad \xi&lt;x&lt;b.    \end{cases}\]</span></p><hr /><p>一般的边值问题</p><p><span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}x}\left[ p(x)\frac{\mathrm{d}g(x;\xi)}{\mathrm{d}x}\right]+q(x)g(x;\xi)=\delta(x-\xi), \quad a&lt;x,\xi&lt;b.\]</span> <span class="math display">\[    g(a;\xi)=0, \quad g(b;\xi)=0\]</span></p><p>有解 <span class="math display">\[    \begin{aligned}    g(x;\xi)=&amp;-\frac{1}{p(\xi)}\frac{y_2(b)y_1(\xi)-y_1(b)y_2(\xi)}{y_1(b)y_2(a)-y_1(a)y_2(b)}\frac{y_2(a)y_1(x)-y_1(a)y_2(x)}{W[y_1(\xi),y_2(\xi)]}         \\    &amp;+\frac{1}{p(\xi)}\frac{y_1(\xi)y_2(x)-y_2(\xi)y_1(x)}{W[y_1(\xi),y_2(\xi)]}\eta(x-\xi).    \end{aligned}\]</span></p><p>从该表达式或者构造新的 Green 函数可以得到 <spanclass="math display">\[    g(x;\xi)=g(\xi;x).\]</span></p><hr /><p>边值问题 <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}x}\left[ p(x)\frac{\mathrm{d}y(x)}{\mathrm{d}x}\right]+q(x) y(x)=f(x), \quada&lt;x&lt;b,\]</span> <span class="math display">\[    y(a)=A, \quad y(b)=B,\]</span> 有解 <span class="math display">\[    \begin{aligned}    y(x)&amp;=\int_{a}^{b} g(x;\xi)f(\xi)\mathrm{d}\xi+\left\{p(\xi)\left[y(\xi)\frac{\mathrm{d}g(x;\xi)}{\mathrm{d}\xi}-g(x;\xi)\frac{\mathrm{d}y(\xi)}{\mathrm{d}\xi}\right]\right\}^{\xi=b}_{\xi=a}         \\    &amp;=\int_{a}^{b} g(x;\xi)f(\xi) \mathrm{d}\xi+Bp(b)\frac{\mathrm{d}g(x;\xi)}{\mathrm{d}\xi}\bigg|_{\xi=b}-Ap(a)\frac{\mathrm{d}g(x;\xi)}{\mathrm{d}\xi}\bigg|_{\xi=a}    \end{aligned}\]</span></p><hr /><p>无界区间上的边值问题 <span class="math display">\[    \frac{\mathrm{d}^{2}g(x;\xi)}{\mathrm{d}x^{2}}-k^{2}g(x;\xi)=\delta(x-\xi),\quad -\infty&lt;x,\xi&lt;\infty,\]</span> <span class="math display">\[    g(x;\xi)\big|_{x\to \pm \infty} 有界，\]</span> 有解（其中 <span class="math inline">\(k&gt;0\)</span>） <spanclass="math display">\[    g(x;\xi)=-\frac{1}{2k}\mathrm{e}^{-k\lvert x-\xi \rvert }\]</span></p><hr /><p>无界区间上的边值问题 <span class="math display">\[    \frac{\mathrm{d}^{2}y(x)}{\mathrm{d}x^{2}}-k^{2}y(x)=f(x), \quad-\infty&lt;x&lt;\infty,\]</span> <span class="math display">\[    y(x)\big|_{x\to \pm \infty} 有界，\]</span> 有解（其中 <span class="math inline">\(k&gt;0\)</span>） <spanclass="math display">\[    y(x)=-\frac{1}{2k}\int_{-\infty}^{\infty} \mathrm{e}^{-k\lvert x-\xi\rvert } f(\xi) \mathrm{d}\xi\]</span></p>]]></content>
    
    
    <categories>
      
      <category>分析学</category>
      
      <category>微分方程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (6)</title>
    <link href="/2022/07/11/Neuronal-Dynamics-6/"/>
    <url>/2022/07/11/Neuronal-Dynamics-6/</url>
    
    <content type="html"><![CDATA[<h1 id="adaptation-and-firing-patterns">Adaptation and FiringPatterns</h1><p>The online version of this chapter:</p><hr /><p>Chapter 6 Adaptation and Firing Patternshttps://neuronaldynamics.epfl.ch/online/Ch6.html</p><hr /><h2 id="adaptive-exponential-integrate-and-fire">Adaptive ExponentialIntegrate-and-Fire</h2><p>A single equation is not sufficient to describe the variety of firingpatterns that neurons exhibit in response to a step current. We couplethe voltage equation to abstract current variables <spanclass="math inline">\(w_k\)</span>. The set of equation is <spanclass="math display">\[    \tau_m \frac{\mathrm{d}u}{\mathrm{d}t}=f(u)-R \sum_{k}^{} w_k+RI(t)\]</span> (6.1) <span class="math display">\[    \tau_k\frac{\mathrm{d}w_k}{\mathrm{d}t}=a_k(u-u_{rest})-w_k+b_k\tau_k\sum_{t^{(f)}}^{}\delta(t-t^{(f)}).\]</span> (6.2)</p><p>the adaptation current is fed back to the voltage equation withresistance <span class="math inline">\(R\)</span>. The voltage variable<span class="math inline">\(u\)</span> is reset if the membranepotential reaches the numerical threshold <spanclass="math inline">\(\Theta_{reset}\)</span>. The moment <spanclass="math inline">\(u(t)=\Theta_{reset}\)</span> defines the firingtime <span class="math inline">\(t^{(f)}=t\)</span>. After firing,integration of the voltage restarts at <spanclass="math inline">\(u=u_r\)</span>. The parameters <spanclass="math inline">\(b_k\)</span> are the 'jump' of the spike-triggeredadaptation.</p><div class="note note-info">            <p>One possible biophysical interpretation of the increase is thatduring the action potential calcium enters into the cell so that theamplitude of a calcium-dependent potassium current is increased.</p>          </div><p>The adaptive Exponential Integrate-and-Fire model (AdEx) consists ofan exponential nonlinearity in the voltage equation coupled to a singleadaptation variable <span class="math inline">\(w\)</span> <spanclass="math display">\[    \tau_m \frac{\mathrm{d}u}{\mathrm{d}t}=-(u-u_{rest})+\Delta_{T} \exp\left(\frac{u-\theta_{rh}}{\Delta_{T}}\right)-Rw+RI(t)\]</span> (6.3) <span class="math display">\[    \tau_w \frac{\mathrm{d}w}{\mathrm{d}t}=a(u-u_{rest})-w+b\tau_w\sum_{t^{(f)}}^{} \delta(t-t^{(f)}).\]</span> (6.4)</p><p>At each threshold crossing the voltage is reset to <spanclass="math inline">\(u=u_r\)</span> and the adaptation variable <spanclass="math inline">\(w\)</span> is increased by an amount <spanclass="math inline">\(b\)</span>. Adaptation is characterized by twoparameters: <span class="math inline">\(a\)</span> couples adaptation tothe voltage and is the source of subthreshold adaptation.Spike-triggered adaptation is controlled by a combination of <spanclass="math inline">\(a\)</span> and <spanclass="math inline">\(b\)</span>. The choice of <spanclass="math inline">\(a\)</span> and <spanclass="math inline">\(b\)</span> largely determines the firing patternsof the neuron and can be related to the dynamics of ion channels.</p><h4 id="example-izhikevich-model">Example: Izhikevich model</h4><p>This model uses the quadratic integrate-and-fire model for the firstequation <span class="math display">\[    \tau_m\frac{\mathrm{d}u}{\mathrm{d}t}=-(u-u_{rest})(u-\theta)-Rw+RI(t)\]</span> <span class="math display">\[    \tau_w \frac{\mathrm{d}w}{\mathrm{d}t}=a(u-u_{rest})-w+b\tau_w\sum_{t^{(f)}}^{} \delta(t-t^{(f)}).\]</span></p><p>If <span class="math inline">\(u=\theta_{reset}\)</span>, the voltageis reset to <span class="math inline">\(u=u_r\)</span> and theadaptation variable <span class="math inline">\(w\)</span> is increasedby an amount <span class="math inline">\(b\)</span>. Normally <spanclass="math inline">\(b\)</span> is positive, but <spanclass="math inline">\(b&lt;0\)</span> is also possible.</p><p><img src="/img/neu_dyn/x150.png" /> &gt; Multiple firing patterns incortical neurons. For each type, the neuron is stimulated with a stepcurrent with low or high amplitude.</p><p><img src="/img/neu_dyn/x151.png" /> &gt; Multiple firing patterns inthe AdEx neuron model. The spiking response can be classified by thesteady-state firing behavior (vertical axis: tonic, adapting, bursting)and by its transient initiation pattern as shown along the horizontalaxis: tonic (i.e. no special transient behavior), initial burst, ordelayed spike initiation.</p><h4 id="example-leaky-model-with-adaptation">Example: Leaky model withadaptation</h4><p>Adaptation variables <span class="math inline">\(w_k\)</span> canalso be combined with a standard leaky integrate-and-fire model <spanclass="math display">\[    \tau_m \frac{\mathrm{d}u}{\mathrm{d}t}=-(u-u_{rest})-R\sum_{k}^{}w_k +RI(t)\]</span> (6.7)</p><p><span class="math display">\[    \tau_k \frac{\mathrm{d}w_k}{\mathrm{d}t}=a(u-u_{rest})-w_k+b_k\tau_k\sum_{t^{(f)}}^{} \delta(t-t^{(f)})\]</span> (6.8)</p><p>At the moment of firing, defined by the threshold condition <spanclass="math inline">\(u(t^{(f)})=\theta_{reset}\)</span>, the voltage isreset to <span class="math inline">\(u=u_r\)</span> and the adaptationvariable <span class="math inline">\(w_k\)</span> are increased by anamount <span class="math inline">\(b_k\)</span>.</p><h2 id="firing-patterns">Firing Patterns</h2><h3 id="classification-of-firing-patterns">Classification of FiringPatterns</h3><p>It is advisable to separate the steady-state pattern from the initialtransient phase. The initiation phase refers to the firing pattern rightafter the onset of the current step.</p><p>Three main initiation pattern: the initiation can not bedistinguished from the rest of the spiking response (tonic); the neuronresponds with a significantly greater spike frequency in the transient(initial burst) than in the steady state; the neuronal firing startswith a delay (delay).</p><p>Three main steady state patterns: regularly spaced spikes (tonic);gradually increasing interspike intervals (adapting); or regularalternations between short and long interspike intervals (bursting).</p><p>Irregular firing patterns are also possible in the AdEx model, butthe discussion below is restricted to deterministic models.</p><h4 id="example-tonic-adapting-and-facilitating">Example: Tonic,Adapting and Facilitating</h4><p>When the subthreshold coupling <span class="math inline">\(a\)</span>is small and the voltage reset is low (<span class="math inline">\(u_r\thickapprox u_{rest}\)</span>), the AdEx response is either tonic oradapting. This depends on the jump <spanclass="math inline">\(b\)</span> and the time scale <spanclass="math inline">\(\tau_w\)</span>.</p><p>A large jump with a small time scale creates evenly spaced spikes atlow frequency.On the other hand, a small spike-triggered currentdecaying on a long timescale can accumulate strength over several spikesand therefore successively decreases the net driving current <spanclass="math inline">\(I-w\)</span>. In general, weak but long-lastingspike-triggered currents cause spike-frequency adadptation while shortbut strong currents lead to a prolongation of the refactory period.There is a continuum between purely tonic spiking and stronglyadapting.</p><p>When the spike-triggered curent is depolarizing <spanclass="math inline">\((b&lt;0)\)</span> the interspike interval maygradually decreases, leading to spike-frequency facilitation.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x152.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x153.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x154.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x155.png" /></div></div></div><h3id="phase-plane-analysis-of-non-linear-integrate-and-fire-models-in-two-dimensions">Phaseplane analysis of non-linear integrate-and-fire models in twodimensions</h3><p>In the AdEx model, the <spanclass="math inline">\(u-\)</span>nullcline is again linear in thesubthreshold regime and rises exponentially when <spanclass="math inline">\(u\)</span> is close to <spanclass="math inline">\(\theta\)</span>. Upon current injection, the <spanclass="math inline">\(u-\)</span>nullcline is shifted vertically by anamount proportional to the magnitude of the current <spanclass="math inline">\(I\)</span>.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x156.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x157.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x158.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x159.png" /></div></div></div><p>Each time the trajectory reaches <spanclass="math inline">\(u=\theta_{reset}\)</span>, it will bere-initialized at a reset value <spanclass="math inline">\((u_r,w+b)\)</span>.</p><p>There are three regions of the phase plane with qualitativelydifferent ensuing dynamics. A 'detour reset' corresponds to a downswingof the membrane potential after the end of the action potential. Thedistinction between detour and direct resets is helpful to understandhow different firing patterns arise.</p><h4 id="example-bursting">Example: Bursting</h4><p>Initial burst: a neuron first fires a group of spikes at aconsiderably higher spiking frequency than the steady-statefrequency.</p><p>The shape of the voltage trajectory after the end of the actionpotential (downswing or not) can be used to distinguish between adapting(strictly detour or strictly direct resets) and initial bursting (firstdirect then detour resets).</p><p>By alternation between direct and detour resets, regular bursting canarise.</p><p>AdEx can also produce an irregular alternation. The parameters forirregular firing occupies a small and patchy volume in parameterspace.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x160.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x161.png" /></div></div></div><h3 id="exploring-the-space-of-reset-parameters">Exploring the Space ofReset Parameters</h3><p>Boundaries in the parameter space mark transitions between differenttypes of firing pattern - which are often correlated with types ofcells.</p><p>Apply a step current with an amplitude twice as large as the minimalcurrent necessary to elicit a spike and study the dependence of theobserved firing pattern on the reset parameter <spanclass="math inline">\(u_r\)</span> and <spanclass="math inline">\(b\)</span>. All the other parameters are keptfixed.</p><p><img src="/img/neu_dyn/x162.png" /></p><ul><li>The line separating initial bursting and tonic firing resembles theshape of the <span class="math inline">\(u-\)</span>nullcline. That'sbecause the <span class="math inline">\(u-\)</span>nullcline plays animportant role in determining whether the reset is 'direct' or leads toa 'detour'.</li><li>Regular bursting is possible, if the voltage reset <spanclass="math inline">\(u_r\)</span> is located above the voltagethreshold <span class="math inline">\(\theta\)</span>. Irregular firingpatterns are found within the bursting region of the parameterspace.</li><li>Adapting firing patterns occur only over a restricted range of jumpamplitudes <span class="math inline">\(b\)</span> of the spike-triggeredadaptation current.</li></ul><h4 id="example-piecewise-linear-model">Example: Piecewise-LinearModel</h4><p>Consider a piecewise linear version of the AdEx model <spanclass="math display">\[    f(u)=    \begin{cases}        -(u-u_{rest}) \quad u\leqslant \theta_{rh} \\        \Delta_{T}(u-u_p) \quad \text{otherwise}    \end{cases}\]</span> (6.9)</p><p>with <span class="math display">\[    u_p=\theta_{rh}+\frac{\theta_{rh}-u_{rest}}{\Delta_T},\]</span> (6.10) which we insert into the voltage equation <spanclass="math inline">\(\tau_m\mathrm{d}u/\mathrm{d}t=f(u)+RI-Rw\)</span>. Note that the <spanclass="math inline">\(u-\)</span>nullcline is given by <spanclass="math inline">\(w=f(u)/R+I\)</span> and takes at <spanclass="math inline">\(u=\theta_{rh}\)</span> its minimum value <spanclass="math inline">\(w_{min}=f(\theta_{rh})/R+I\)</span>.</p><p>We assume separation of timescale (<spanclass="math inline">\(\tau_m/\tau_w \ll 1\)</span>) and exploit the factthat the trajectories in the phase plane are nearly horizontal - unlessthey approach the <span class="math inline">\(u-\)</span>nullcline.</p><p>Map the initial condition <spanclass="math inline">\((u_r,w_r)\)</span> after a first reset to thevalue <span class="math inline">\(w_e\)</span> of the adaptationvariable at the end of the trajectory: <spanclass="math inline">\(w_e=M(u_r,w_r)\)</span>. The next reset startsthen from <span class="math inline">\((u_r,w_e+b)\)</span>. Alltrajectories with <span class="math inline">\(w_r&lt;w_{min}\)</span>remain horizontal, so that <spanclass="math inline">\(w_e=w_r\)</span>.</p><p>If <span class="math inline">\(w_r&gt;w_{min}\)</span>, wedistinguish two possible cases. The first one corresponds to a voltagereset below the threshold, <spanclass="math inline">\(u_r&lt;\theta_{rh}\)</span>. A trajectoryinitiated at <span class="math inline">\(u_r&lt;\theta_{rh}\)</span>evolves horizontally until it comes close to the left branch of the<span class="math inline">\(u-\)</span>nullcline. It then follows the<span class="math inline">\(u-\)</span>nullcline at a small distance<span class="math inline">\(x(u)\)</span> below it. The distance can beshown to be <span class="math display">\[    x(u)=\frac{\tau_m}{\tau_w}[I-(a+R^{-1})(u-u_{rest})],       \]</span> (6.11) which vanishes in the limit <spanclass="math inline">\(\tau_m/\tau_w \to 0\)</span>. For <spanclass="math inline">\(u_r&lt;\theta_{rh}\)</span>. <spanclass="math display">\[    M(u_r,w_r)=    \begin{cases}        w_r \quad w_r&lt;f(u_r)/R+I \\        f(\theta_{rh})/R+I \quad \text{otherwise}    \end{cases}\]</span> (6.12) If <spanclass="math inline">\(u_r&gt;\theta_{rh}\)</span> then we have a directreset (i.e.,movement starts to the right) if <spanclass="math inline">\((u_r,w_r)\)</span> lands below the right branch ofthe <span class="math inline">\(u-\)</span>nullcline and a detour resetotherwise <span class="math display">\[    M(u_r,w_r)=    \begin{cases}        w_r \quad w_r&lt;f(u_r)/R+I \\        f(\theta_{rh})/R+I \quad \text{otherwise}    \end{cases}\]</span> (6.13)</p><p><img src="/img/neu_dyn/x163.png" /> &gt; Fig.6.7</p><p>The map <span class="math inline">\(M\)</span> uniquely defines thefiring pattern. Regular bursting is possible only if <spanclass="math inline">\(u_r&gt;\theta_{rh}\)</span> and <spanclass="math inline">\(b&lt;f(u_r)-f(\theta_{rh})\)</span> so that atleast one reset in each burst lands below the <spanclass="math inline">\(u-\)</span>nullcline. For <spanclass="math inline">\(u_r&gt;\theta_{rh}\)</span>, we have tonic spikingwith detour resets when <spanclass="math inline">\(b&gt;f(u_r)+I\)</span> and initial bursting if<spanclass="math inline">\(f(u_r)+I&gt;b&gt;f(u_r)-f(\theta_{rh})+x(\theta_{rh})\)</span>.</p><p>If <span class="math inline">\(u_r\leqslant \theta\)</span> we havetonic spiking with detour resets when <spanclass="math inline">\(b&gt;f(u_r)+I\)</span>, tonic spiking with directreset when <spanclass="math inline">\(b&lt;f(u_r)-f(\theta_{rh})\)</span> and initialbursting if <spanclass="math inline">\(f(u_r)+I&gt;b&gt;f(u_r)-f(\theta_{rh})\)</span>.Note that the rough layout of the parameter regions in Fig.6.7A.</p><h3 id="exploring-the-space-of-subthreshold-parameters">Exploring theSpace of Subthreshold Parameters</h3><p>While the exponential integrate-and-fire model losses stabilityalways via a saddle-node bifurcation, the AdEx can become unstableeither via a Hopf or a saddle-node bifurcation.</p><p>In the AdEx, an eigenvalue analysis shows that the stable fixed pointlooses stability via a Hopf bifurcation if <spanclass="math inline">\(aR&gt;\tau_m/\tau_w\)</span>.</p><div class="note note-info">            <p>Otherwise, when the coupling from voltage to adaptation (parameter<span class="math inline">\(a\)</span>) and back from adaptation tovoltage (parameter <span class="math inline">\(R\)</span>) are bothweak, an increase in the current causes the stable fixed point to mergewith the unstable one, so that both disappear via a saddle-nodebifurcation.</p>          </div><div class="note note-warning">            <p>The type of bifurcation has no influence on the firing pattern(bursting, adapting, tonic) which depends mainly on the choice of resetparameters.</p>          </div><p>However, the subthreshold parameters do control the presence orabsence of oscillations in response to a short current pulse.</p><p><strong>resonator</strong>: a model showing damped oscillations.</p><p><strong>integrator</strong>: a model without damped oscillations.</p><p>The presence of damped oscillations depends non-linearly on <spanclass="math inline">\(a/g_{L}\)</span> and <spanclass="math inline">\(\tau_m/\tau_w\)</span> as summarized in the figurebelow.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x164.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x165.png" /></div></div></div><p>The frequency of the damped oscillation is given by <spanclass="math display">\[    \omega=\frac{4}{\tau_w}\left[aR-\frac{2\tau_w}{\tau_m}\left(1-\frac{\tau_m}{\tau_w}\right)^{2}\right]\]</span> (6.14)</p><h4 id="example-transient-spiking">Example: Transient Spiking</h4><p>Upon the onset of a current step, some neurons may fire a smallnumber of spikes and then remain silent, even if the stimulus ismaintained for a very long time. An AdEx model with subthresholdcoupling <span class="math inline">\(a&gt;0\)</span> can explain thisphenomenon whereas pure spike-triggered adaptation (<spanclass="math inline">\(a=0;b&gt;0\)</span>) cannot account for it,because adaptation would eventually decay back to zero so that theneuron fires another spike.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x166.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x167.png" /></div></div></div><p>Choose parameter <span class="math inline">\(a\)</span> and <spanclass="math inline">\(\tau_w\)</span> such that the neuron is in theresonator regime. The voltage response to a step input then exhibitsdamped oscillations. Phase plane analysis reveals that sometimes severalresets are needed before the trajectory is attracted towards the fixedpoint.</p><h2 id="biophysical-origin-of-adaptation">Biophysical Origin ofAdaptation</h2><p>We now show that the variables <spanclass="math inline">\(w_k\)</span> can be linked to the biophysics ofion channels and dendrites.</p><h3 id="subthreshold-adaptation-by-a-single-slow-channel">Subthresholdadaptation by a single slow channel</h3><p>First our aim is to give a biophysical interpretation of theparameters <span class="math inline">\(a\)</span>, <spanclass="math inline">\(\tau_w\)</span>, and the variable <spanclass="math inline">\(w\)</span> that show up in the adaptation equation<span class="math display">\[    \tau_w \frac{\mathrm{d}w}{\mathrm{d}t}=a(u-E_0)-w.\]</span> (6.15) Here and in the following we write <spanclass="math inline">\(E_0\)</span> instead of <spanclass="math inline">\(u_{rest}\)</span> in order to simplify notationand keep the treatment slightly more general.</p><p>We know rapid activation of the sodium channels, important during theupswing of action potentials, is well approximated by the exponentialnonlinearity in the voltage equation of the AdEx model. We will see nowthat the subthreshold current <span class="math inline">\(w\)</span> islinked to the dynamics of other ion channels with a slower dynamics.</p><p>Let us focus on the model of a membrane with a leak current and asingle, slow, ion channel, say a potassium channel of the Hodgkin-Huxleytype <span class="math display">\[    \tau_m\frac{\mathrm{d}u}{\mathrm{d}t}=-(u-E_{L})-R_{L}g_{K}n^{p}(u-E_k)+R_{L}I_{ext}\]</span> (6.16)</p><p>where <span class="math inline">\(R_{L}\)</span> and <spanclass="math inline">\(E_{L}\)</span> are the resistance and reversalpotential of the leak current, <spanclass="math inline">\(\tau_m=R_{L}C\)</span> the membrane time constant,<span class="math inline">\(g_{K}\)</span> the maximal conductance ofthe open channel and <span class="math inline">\(n\)</span> the gatingvariable (which appears with arbitrary power <spanclass="math inline">\(p\)</span>) with dynamics <spanclass="math display">\[    \frac{\mathrm{d}n}{\mathrm{d}t}=-\frac{n-n_0(u)}{\tau_n(u)}.\]</span> (6.17)</p><p>As long as the membrane potential stays below threshold, we canlinearize the equations (6.16) and (6.17) aroung the resting voltage<span class="math inline">\(E_0\)</span>, given by the fixed pointcondition <span class="math display">\[    E_0=\frac{E_{L}+(R_{L}g_{K})n_0^{p}(E_0)E_{K}}{1+(R_{L}g_{K})n_0^{p}(E_0)}.\]</span> (6.18)</p><p>The resting potential is shifted with respect to the leak reversalpotential if the channel is partially open at rest, <spanclass="math inline">\(n_0(E_0)&gt;0\)</span>.</p><p>We introduce the parameter <span class="math inline">\(\beta=g_{K}pn_0^{p-1}(E_0)(E_0-E_{K})\)</span> and expand <spanclass="math inline">\(n_0(u)=n_0(E_0)+n_0&#39;(u-E_0)\)</span> where<span class="math inline">\(n_0&#39;\)</span> is the derivative <spanclass="math inline">\(\mathrm{d}n_0/\mathrm{d}u\)</span> evaluated at<span class="math inline">\(E_0\)</span>.</p><p>The variable <span class="math inline">\(w=\beta[n-n_0(E_0)]\)</span>then follows the linear equation <span class="math display">\[    \tau_n(E_0) \frac{\mathrm{d}w}{\mathrm{d}t}=a(u-E_0)-w.\]</span> (6.19)</p><p>Note that the time constant of the variable <spanclass="math inline">\(w\)</span> is given by the time constant of thechannel at the resting potential. The parameter <spanclass="math inline">\(a=\beta n_0&#39;(E_0)\)</span> is proportional tothe sensitivity of the channel to a change in the membrane voltage, asmeasured by the slope <spanclass="math inline">\(\mathrm{d}n_0/\mathrm{d}u\)</span> at <spanclass="math inline">\(E_0\)</span>.</p><p>The adaptation variable <span class="math inline">\(w\)</span> iscoupled into the voltage equation in the standart form <spanclass="math display">\[    \tau_m^{eff}=-(u-E_0)-Rw+RI_{ext}.\]</span> (6.20)</p><p>Note that the membrane time constant and the resistance are rescaledby a factor <spanclass="math inline">\(\lambda=1+(R_{L}g_{K})n_0^{p}(E_0)\)</span> withrespect to their values in the passive membrane equation (6.16). Namely,<span class="math inline">\(\tau_m^{eff}=\tau_m/\lambda\)</span>, <spanclass="math inline">\(R=R_{L}/\lambda\)</span>. In fact, both aresmaller because of partial opening of the channel at rest.</p><p>In summary, each channel with nonzero slope <spanclass="math inline">\(\mathrm{d}n_0/\mathrm{d}u\)</span> at <spanclass="math inline">\(E_0\)</span> gives rise to an effective adaptationvariable <span class="math inline">\(w\)</span>. Since there are manychannels, we can expect many variables <spanclass="math inline">\(w_k\)</span>. Those with similar time constantscan be summed and grouped into a single equation. But if time constantsare different by an order of magnitude or more than several adaptationvariables are needed.</p><h3id="spike-triggered-adaptation-arising-from-a-biophysical-ion-channel">Spike-triggeredadaptation arising from a biophysical ion-channel</h3><p>We have seen that some ion channels are partially open at the restingpotential, while others react only when the membrane potential is wellabove the firing threshold. The second group gives a biophysicalinterpretation of the jump amplitude <spanclass="math inline">\(b\)</span> og a spike-triggered adaptationcurrent.</p><p>We now study the change in the state of the ion channel inducedduring the large-amplitude excursion of the voltage trajectory during aspike. During the spike, the target <spanclass="math inline">\(n_0(u)\)</span> of the gating variable is close toone; but since the time constant <spanclass="math inline">\(\tau_n\)</span> is long, the target is not reachedduring the short time that the voltage stays above the activationthreshold.</p><p>Nevertheless, the ion channel is partially activated by the spike.Unless the neuron is firing at a very large firing rate, each additionalspike activate the channel further, always by the same amount <spanclass="math inline">\(\Delta_{n}\)</span>, which depends on the durationof the spike and the activation threshold of the current. Thespike-triggered jump in the adapting current <spanclass="math inline">\(w\)</span> is then <span class="math display">\[    b=\beta \Delta_n. \tag{6.21}\]</span> where <span class="math inline">\(\beta=g_{K}pn_0^{p-1}(E_0)(E_0-E_K)\)</span> has been defined before.</p><h4id="example-calculating-the-jump-b-of-the-spike-triggered-adaptation-current">Example:Calculating the jump <span class="math inline">\(b\)</span> of thespike-triggered adaptation current</h4><p>We consider a gating dynamics <span class="math display">\[    \frac{\mathrm{d}n}{\mathrm{d}t}=-\frac{n-n_0(u)}{\tau_n(u)}.\]</span> (6.22)</p><p>with the steplike activation function <spanclass="math inline">\(n_0(u)=\Theta(u-u_0^{act})\)</span> where <spanclass="math inline">\(u_{0}^{act}=-30\)</span> mV and <spanclass="math inline">\(\tau_n(u)=100\)</span> ms independent of <spanclass="math inline">\(u\)</span>. The activation threshold <spanclass="math inline">\(-30\)</span> mV is above the firing threshold(typically in the range of <span class="math inline">\(-40\)</span> mV).Assuming that, during an action potential the voltage remains for <spanclass="math inline">\(\Delta_{t}=1\)</span> ms above <spanclass="math inline">\(u_0^{act}\)</span>, we found <spanclass="math display">\[    n_{after}-n_{before}\thickapprox \ln(1+\frac{n_{after}-n_{before}}{1-n_{after}})=\frac{\Delta_{t}}{\tau_n}=\Delta_{n}\]</span></p><h3id="subthreshold-adaptation-caused-by-passive-dendrites">Subthresholdadaptation caused by passive dendrites</h3><p>Here we show that a passive dendrite can also give rise to asubthreshold coupling of the form of (6.15).</p><p>We focus on a simple neuron model with two compartments, representingthe soma and the dendrite, superscript <spanclass="math inline">\(s\)</span> and <spanclass="math inline">\(d\)</span> respectively. The two compartments areboth passive with membrane potential <spanclass="math inline">\(V^{s}\)</span>, <spanclass="math inline">\(V^{d}\)</span>, transversal resistance <spanclass="math inline">\(R_{T}^{s}\)</span>, <spanclass="math inline">\(R_{T}^{d}\)</span>, capacity <spanclass="math inline">\(C^{s}\)</span>, <spanclass="math inline">\(C^{d}\)</span> and resting potential <spanclass="math inline">\(u_{rest}\)</span>, <spanclass="math inline">\(E^{d}\)</span>. The two compartments are linked bya longitudinal resistance <span class="math inline">\(R_{L}\)</span>. Ifcurrent is injected only in the soma, then the two-compartment modelwith passive dendrites corresponds to <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}V^{s}=\frac{1}{C^{s}}\left[-\frac{(V^{s}-u_{rest})}{R_{T}^{s}}-\frac{V^{s}-V^{d}}{R_{L}}+I(t)\right],     \]</span> (6.23)</p><p><span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}V^{d}=\frac{1}{C^{d}}\left[-\frac{(V^{d}-E^{d})}{R_{T}^{d}}-\frac{V^{d}-V^{s}}{R_{L}}\right].\]</span> (6.24)</p><p>We assume that <span class="math inline">\(E^{d}=u_{rest}=E\)</span>.In this case the adaptation current is <spanclass="math inline">\(w=-(V^{d}-u_{rest})/R_{L}\)</span> and the twoequations above reduce to <span class="math display">\[    \tau^{eff}\frac{\mathrm{d}V^{s}}{\mathrm{d}t}=-(V^{s}-E)-R^{eff}w\]</span> (6.25) <span class="math display">\[    \tau_w \frac{\mathrm{d}w}{\mathrm{d}t}=a(V^{s}-E)-w\]</span> (6.26) with an effective input resistance <spanclass="math inline">\(R^{eff}=1/[1/R_{T}^{s}+1/R_{L}]\)</span>, aneffective somatic time constant <spanclass="math inline">\(\tau^{eff}=C^{s}R^{eff}\)</span> and effectiveadaptation time constant <spanclass="math inline">\(\tau_w=R_{L}C^{d}/[1+(R_{L}/R_{T}^{d})]\)</span>and a coupling between somatic voltage and adaptation current <spanclass="math inline">\(a=-[R_{L}+(R_{L}^{2}/R_{T}^{d})]^{-1}\)</span>.</p><ul><li><span class="math inline">\(a\)</span> is always negative, whichmeans that passive dendrites introduce a facilitating subthresholdcoupling.</li><li>Facilitation is particularly strong with a small longitudinalresistance.</li><li>The timescale of the facilitation <spanclass="math inline">\(\tau_w\)</span> is smaller than the dendritic timeconstant <span class="math inline">\(R_{T}^{d}C^{d}\)</span> - so that,compared to other 'adaptation' currents, the dendritic current is arelatively fast one.</li></ul><h4 id="example-bursting-with-a-passive-dendrite-and-i_m">Example:Bursting with a Passive Dendrite and <spanclass="math inline">\(I_{M}\)</span></h4><p>Suppose that the action potential can be approximated by aone-millisecond pulse at <span class="math inline">\(0\)</span> mV. Theneach spike brings an increase in the dendritic membrane potential. Interms of the current <span class="math inline">\(w\)</span>, theincrease is <spanclass="math inline">\(b=-aE_0(1-\mathrm{e}^{-1/\tau_w})\)</span>. Again,the spike-triggered jump is always negative, leading to spike-triggeredfacilitation.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x168.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x169.png" /></div></div></div><p>The above figures shows an example where we combined a dendriticcompartment with the linearized effects of the M-current to result inregular bursting.</p><p>The bursting is meditated by the dendritic facilitation which iscounterbalanced by the adapting effects of <spanclass="math inline">\(I_{M}\)</span>.</p><p>This example suggests that the dynamics of spike-triggered currentson multiple timescales can be understood in terms of their stereotypicaleffect on the membrane potential.</p><h2 id="spike-response-model-srm">Spike response model (SRM)</h2><p>We introduced 'filter picture' in section 1.3.5. In this picture, theparameters of the model are replaced by (parametric) functions oftime.</p><p>In contrast to nonlinear integrate-and-fire models, the SRM has no'intrinsic' firing threshold but only the sharp numerical threshold forreset.</p><p>The subthreshold behavior of the SRM is richer than that of theintegrate-and-fire model discussed so far and can account for variousaspects of refractoriness and adaptation.</p><h3 id="definition-of-the-srm">Definition of the SRM</h3><p>In the framework of SRM the state of a neuron is described by asingle variable <span class="math inline">\(u\)</span> which weinterpret as the membrane potential.</p><p>After a short current pulse perturbing <spanclass="math inline">\(u\)</span>, it takes some time before <spanclass="math inline">\(u\)</span> returns to rest. The function <spanclass="math inline">\(\kappa(s)\)</span> describes the time course ofthe voltage response to a short current pulse at time <spanclass="math inline">\(s=0\)</span>. Because the subthreshold behavior ofthe membrane potential is taken as linear, the voltage response <spanclass="math inline">\(h\)</span> to an arbitrary time-dependentstimuating current <span class="math inline">\(I^{ext}(t&#39;)\)</span>is given by the integral <spanclass="math inline">\(h(t)=\int_{0}^{\infty} \kappa(s)I^{ext}(t-s)\mathrm{d}s\)</span>.</p><p>Spike firing is defined by a threshold process. The form of theaction potential and the after-potential is described by a function<span class="math inline">\(\eta\)</span>. The evolution of <spanclass="math inline">\(u\)</span> is given by <spanclass="math display">\[    u(t)=\sum_{f} \eta(t-t^{(f)})+\int_{0}^{\infty}\kappa(s)I^{ext}(t-s) \mathrm{d}s + u_{rest}    \]</span> (6.27)</p><p>Introducing the spike train <spanclass="math inline">\(S(t)=\sum_{f}^{} \delta(t-t^{(f)})\)</span>,(6.27) can be also written as a convolution <spanclass="math display">\[    u(t)=\int_{0}^{\infty} \eta(s)S(t-s) \mathrm{d}s +\int_{0}^{\infty}\kappa(s)I^{ext}(t-s) \mathrm{d}s+u_{rest}\]</span> (6.28)</p><p>The threshold <span class="math inline">\(\theta\)</span> is notfixed, but time-dependent <span class="math display">\[    \theta \to \theta(t)\]</span> (6.29)</p><p>Firing occurs whenever the membrane potential <spanclass="math inline">\(u\)</span> reaches the dynamic threshold <spanclass="math inline">\(\theta(t)\)</span> from below <spanclass="math display">\[    t=t^{(f)} \Leftrightarrow u(t)=\theta(t)\ \text{and}\\frac{\mathrm{d}[u(t)-\theta(t)]}{\mathrm{d}t}&gt;0.\]</span> (6.30)</p><p><img src="/img/neu_dyn/x170.png" /></p><h4 id="example-dynamic-threshold---and-how-to-get-rid-of-it">Example:Dynamic threshold - and how to get rid of it</h4><p>A standard model of the dynamic threshold is <spanclass="math display">\[    \theta(t)=\theta_0+\sum_{f}^{}\theta_1(t-t^{(f)})=\theta_0+\int_{0}^{\infty} \theta_1(s)S(t-s)\mathrm{d}s\]</span> (6.31)</p><p>where <span class="math inline">\(\theta_0\)</span> is the 'normal'threshold of neuron <span class="math inline">\(i\)</span> in theabsence of spiking. After each output spike, the firing threshold of theneuron is increased by an amount <spanclass="math inline">\(\theta_1(t-t^{(f)})\)</span> where <spanclass="math inline">\(t^{(f)}&lt;t\)</span> denote the firing times inthe past.</p><p>For example, during an absolute refractory period <spanclass="math inline">\(\Delta^{abs}\)</span>, we may set <spanclass="math inline">\(\Delta_{\theta}\)</span> for a few milliseconds toa large and positive value so as to avoid any firing and let it relaxback to zero over the next few hundred milliseconds.</p><p>There is no need to interpret the variable <spanclass="math inline">\(u\)</span> as the membrane potential. It is oftenconvenient to transform the variable <spanclass="math inline">\(u\)</span> so as to remove the time-dependence ofthe threshold. <span class="math display">\[    \eta(t-t^{(f)}) \to\eta^{eff}(t-t^{(f)})=\eta(t-t^{(f)})-\theta_1(t-t^{(f)})\]</span> (6.32)</p><p>The argument can also be turned the other way round, so as to removethe spike after-potential and only work with a dynamic threshold.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x171.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x172.png" /></div></div></div><h3 id="interpretation-of-eta-and-kappa">Interpretation of <spanclass="math inline">\(\eta\)</span> and <spanclass="math inline">\(\kappa\)</span></h3><p>(6.27) and (6.30) defines a mathematical model. We'll give abiological interpretation of the terms.</p><p>The kernel <span class="math inline">\(\kappa(s)\)</span> is thelinear response of the membrane potential to an input current. Itdescribes the time course of a deviation of the membrane potential fromits resting value that is caused by a short current pulse ("impulseresponse").</p><p>The kernel <span class="math inline">\(\eta\)</span> describes thestandard form of an action potential of neuron <spanclass="math inline">\(i\)</span> including the negative overshoot whichtypically follows a spike (the spike afterpotential). Graphicallyspeaking, a contribution <span class="math inline">\(\eta\)</span> is'pasted in' each time membrane potential reaches the threshold <spanclass="math inline">\(\theta\)</span>.</p><p>In a simplified model, the form of the action potential may beneglected as long as we keep track of the firing times <spanclass="math inline">\(t^{(f)}\)</span>. The kernel <spanclass="math inline">\(\eta\)</span> describes then simply the 'reset' ofthe membrane potential to a lower value after the spike at <spanclass="math inline">\(t^{(f)}\)</span>. <span class="math display">\[    \eta(t-t^{(f)})=-\eta_0 \exp\left(-\frac{t-t^{(f)}}{\tau_{recov}}\right)\]</span> (6.33)</p><p>with a parameter <span class="math inline">\(\eta_0&gt;0\)</span> anda recovery time constant <spanclass="math inline">\(\tau_{recov}\)</span>. The leakyintegrate-and-fire model is in fact a special case of the SRM, withparameter <span class="math inline">\(\eta_0=(\theta-u_{r})\)</span> and<span class="math inline">\(\tau_{recov}=\tau_m\)</span>.</p><h4 id="example-refractoriness">Example: Refractoriness</h4><p>Absolute refractoriness can be incorporated in the SRM by setting thedynamic threshold during a time <spanclass="math inline">\(\Delta^{abs}\)</span> to an extremely high valuethat cannot be attained.</p><p>Relative refractoriness can be mimicked in various ways. - After aspike the firing threshold returns only slowly back to its normal value(increase in firing threshold). - After the spike the membranepotential, and hence <span class="math inline">\(\eta\)</span>, passesthrough a regime of hyperpolarization (spike after-potential) where thevoltage is below the resting potential. During this phase, morestimulation than usual is needed to drive the membrane potential abovethe threshold. This is equivalent to a transient increase of the firingthreshold. - The responsiveness of the neuron is reduced immediatelyafter a spike. In the SRM we can model the reduced responsiveness bymaking the shape of <span class="math inline">\(\varepsilon\)</span> and<span class="math inline">\(\kappa\)</span> depend on the time since thelast spike timing <span class="math inline">\(\hat{t}\)</span>.</p><p>A slightly more general version of SRM is <spanclass="math display">\[    u(t)=\sum_{f}^{} \eta(t-t^{(f)})+\int_{0}^{\infty}\kappa(t-\hat{t},s)I^{ext}(t-s) \mathrm{d}s+u_{rest}.\]</span> (6.34)</p><h3 id="mapping-the-integrate-and-fire-model-to-the-srm">Mapping theIntegrate-and-Fire Model to the SRM</h3><p>Recall that the leaky integrate-and-fire model follows the equation<span class="math display">\[    \tau_m \frac{\mathrm{d}u_i}{\mathrm{d}t}=-(u_i-E_0)-R\sum_{k}^{} w_k+RI_i(t)\]</span> (6.35)</p><p><span class="math display">\[\{t_i^{(f)}\} \in \{t | u_i(t)=\theta\}.\]</span> (6.36)</p><p><span class="math display">\[\tau_k \frac{\mathrm{d}w_k}{\mathrm{d}t}=a_k(u_i-E_0)-w_k+\tau_k b_k\sum_{t^{(f)}}^{} \delta(t-t^{(f)})\]</span><br />(6.37)</p><p>Let us consider a short current pulse <spanclass="math inline">\(I_i^{out}=-q\delta(t)\)</span> applied to the<span class="math inline">\(RC\)</span> circuit. Let <spanclass="math inline">\(q=C(\theta-u_r)\)</span>, the total reset currentis <span class="math display">\[    I_i^{out}(t)=-C(\theta-u_r)\sum_{f}^{} \delta(t-t_i^{(f)})   \]</span> (6.38)</p><p>We add the output current (6.38) on the right-hand side of (6.35),<span class="math display">\[    \tau_m\frac{\mathrm{d}u_i}{\mathrm{d}t}=-(u_i-E_0)-R\sum_{k}^{} w_k+RI_i(t)-RC(\theta-u_r) \sum_{f}^{} \delta(t-t_i^{(f)}),\]</span> (6.39)</p><p><span class="math display">\[    \tau_k \frac{\mathrm{d}w_k}{\mathrm{d}t}=a_k(u_i-E_0)-w_k+\tau_k b_k\sum_{f}^{} \delta(t-t^{(f)})\]</span> (6.40)</p><p>Now we solve (6.39) and (6.40). - First, we shift the voltage so asto set <span class="math inline">\(E_0\)</span> to zero. - Second, wecalculate the eigenvalues and eigenvectors of the 'free' equations inthe absence of input (and therefore no spikes). If there are <spanclass="math inline">\(K\)</span> adaptation variables, we have a totalof <span class="math inline">\(K+1\)</span> eigenvalues (including <spanclass="math inline">\(\pm \sqrt{R(-a_1-\cdots -a_K)}-1\)</span> and<span class="math inline">\(-1\)</span> with a multiplicity of <spanclass="math inline">\(K-1\)</span>). The associated eigenvectors are<span class="math inline">\(\mathbf{e}_k\)</span> with components <spanclass="math inline">\((e_{k0},\cdots ,e_{kK})^{\mathsf{T}}\)</span>. -Third, we express the response to an impulse <spanclass="math inline">\(\Delta u=1\)</span> in the voltage (noperturbation in the adaptation variables) in terms of the <spanclass="math inline">\(K+1\)</span> eigenvectors: <spanclass="math inline">\((1,0,\cdots ,0)^{\mathsf{T}}=\sum_{k=0}^{K}\beta_k \mathbf{e}_k\)</span>. - Finally, we express the pulse caused bya reset of voltage and adaptation variables in terms of the eigenvectors<span class="math inline">\((-\theta+u_r,b_1,\cdots,b_{K})^{\mathsf{T}}=\sum_{k=0}^{K} \gamma_k \mathbf{e}_k\)</span>.</p><p>The response to the reset pulses yields the kernel <spanclass="math inline">\(\eta\)</span> while the response to voltage pulsesyields the filter <span class="math inline">\(\kappa(s)\)</span> of theSRM <span class="math display">\[    u_i(t)=\sum_{f}^{} \eta(t-t_i^{(f)})+\int_{0}^{\infty}\kappa(s)I_i(t-s) \mathrm{d}s,\]</span> (6.41)</p><p>with kernels <span class="math display">\[    \eta(s)=\sum_{k=0}^{K} \gamma_k e_{k0} \exp (\lambda_k s)\Theta(s),\]</span> (6.42)</p><p><span class="math display">\[    \kappa(s)=\sum_{k=0}^{K} \beta_k e_{k0} \exp (\lambda_k s)\Theta(s).\]</span> (6.43)</p><blockquote><p>我大概能理解，但是求导的一些细节不是很了解</p></blockquote><h4 id="example-adaptation-and-bursting">Example: Adaptation andBursting</h4><p>Study a leaky integrate-and-fire model with a single slow adaptationvariable <span class="math inline">\(\tau_w\gg \tau_m\)</span> which iscoupled to the voltage in the threshold regime (<spanclass="math inline">\(a&gt;0\)</span>) and increased during spiking byan amount <span class="math inline">\(b\)</span>.</p><p>With a parameter <span class="math inline">\(\delta=\tau_m/\tau_w \ll1\)</span>, the eigenvalues are <spanclass="math inline">\(\lambda_1=-\tau_w[1-a\delta]\)</span> and <spanclass="math inline">\(\lambda_2=-\tau_w\delta[1+a]\)</span>, associatedto eigenvectors <spanclass="math inline">\(\mathbf{e}_1=(1,a\delta)^{\mathsf{T}}\)</span> and<spanclass="math inline">\(\mathbf{e}_2=(1,-1+\delta+a\delta)^{\mathsf{T}}\)</span>.The resulting spike after-potential kernel <spanclass="math inline">\(\eta(s)\)</span> is shown below. Because of theslow constant <span class="math inline">\(\tau_w\gg \tau_m\)</span>, thekernel <span class="math inline">\(\eta\)</span> has a longhyperpolarizing tail. The neuron model responds to a step current withadaptation, because of accumulation of hyperpolarizing spike-afterpotentials over many spikes.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x173.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x174.png" /></div></div></div><p>As a second example, consider four adaptation currents with differenttime constants <spanclass="math inline">\(\tau_1&lt;\tau_2&lt;\tau_3&lt;\tau_4\)</span>. Weassume pure spike-triggered coupling (<spanclass="math inline">\(a=0\)</span>) so that the integration of thedifferential equations of <span class="math inline">\(w_k\)</span> giveseach an exponential current <span class="math display">\[    w_k(t)=\sum_{f}^{} b_k \exp \left( -\frac{t-t^{(f)}}{\tau_k}\right)\Theta(t-t^{(f)})   \]</span> (6.44)</p><p>Choose as follows: - The time constant of the first current is veryshort and <span class="math inline">\(b_1&lt;0\)</span> (inward current)so as to model the upswing of the action potential (a candidate currentwould be sodium). - A second current (e.g. a fast potassium channel)with a slightly longer time constant is outgoing (<spanclass="math inline">\(b_2&gt;0\)</span>) and leads to the downswing andrapid reset of the membrane potential. - The third current, with a timeconstant of tens of milliseconds is inward (<spanclass="math inline">\(b_3&lt;0\)</span>). - The slowest current is againhyperpolarizing (<span class="math inline">\(b_4&gt;0\)</span>).</p><p>The spike after-potential <span class="math inline">\(\eta\)</span>is shown below.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x639.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x640.png" /></div></div></div><p>Because of the depolarizing spike after-potential induced by theinward current <span class="math inline">\(w_3\)</span>, the neuronmodel responds to a step current of appropriate amplitude with bursts.The bursts end because of the accumulation of the hyperpolarizing effectof the slowest current.</p><h3id="multi-compartment-integrate-and-fire-model-as-a-srm">Multi-compartmentintegrate-and-fire model as a SRM</h3><p>In this section, we want to show that neurons with a linear dendritictree and a voltage threshold for spike firing at the soma can be mappedto the SRM.</p><p>We study an integrate-and-fire model with a passive dendritic treedescribed by <span class="math inline">\(n\)</span> compartments.Membrane resistance, core resistance, and capacity of compartment <spanclass="math inline">\(\mu\)</span> are denoted by <spanclass="math inline">\(R_{T}^{\mu}\)</span>, <spanclass="math inline">\(R_{L}^{\mu}\)</span>, and <spanclass="math inline">\(C^{\mu}\)</span>, respectively. The longitudinalcore resistance between compartment <spanclass="math inline">\(\mu\)</span> and a neighboring compartment <spanclass="math inline">\(\nu\)</span> is <span class="math inline">\(r^{\mu\nu}=(R_{L}^{\mu}+R_{L}^{\nu})/2\)</span>. Compartment <spanclass="math inline">\(\mu=1\)</span> represents the soma and is equippedwith a simple mechanism for spike generation, i.e., with a thresholdcriterion as in the standard integrate-and-fire model. The remainingdendritic compartments (<span class="math inline">\(2\leqslant\mu\leqslant n\)</span>) are passive.</p><p>Each compartment <span class="math inline">\(1\leqslant \mu\leqslantn\)</span> of neuron <span class="math inline">\(i\)</span> may receiveinput <span class="math inline">\(I_i^{\mu}(t)\)</span> from presynapticneurons. As a result of spike generation, there is an additional resetcurrent <span class="math inline">\(\Omega_i(t)\)</span> at the soma.The membrane potential <span class="math inline">\(V_i^{\mu}\)</span> ofcompartment <span class="math inline">\(\mu\)</span> is given by <spanclass="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}V_i^{\mu}=\frac{1}{C_i^{\mu}}\left[-\frac{V_i^{\mu}}{R_{T,i}^{\mu}}-\sum_{\nu}^{}\frac{V_i^{\mu}-V_i^{\nu}}{r_{i}^{\mu \nu}}+I_i^{\mu}(t)-\delta^{\mu1}\Omega_i(t)\right]  \]</span> (6.45)</p><p>where the sum runs over all neighbors of compartment <spanclass="math inline">\(\mu\)</span>. <spanclass="math inline">\(\delta^{\mu \nu}\)</span> is the Kronecker symbol.Below we will identify the somatic voltage <spanclass="math inline">\(V_i^{1}\)</span> with the potential <spanclass="math inline">\(u_i\)</span> of the SRM.</p><p>The solution of (6.45) can be formulated by means of Green'sfunctions <span class="math inline">\(G_{i}^{\mu \nu}(s)\)</span> thatdescribe the impact of an current pulse injected in compartment <spanclass="math inline">\(\nu\)</span> on the membrane potential ofcompartment <span class="math inline">\(\mu\)</span>. The solution is ofthe form <span class="math display">\[    V_{i}^{\mu}(t)=\sum_{\nu}^{} \frac{1}{C_i^{\nu}}\int_{0}^{\infty}G_i^{\mu \nu}(s)[I_i^{\nu}(t-s)-\delta^{\nu 1}\Omega_i(t-s)]\mathrm{d}s.\]</span> (6.46)</p><blockquote><p>这里我完全不理解上面的解法，什么是作用于 compartment <spanclass="math inline">\(\nu\)</span> 上的电流对于 compartment <spanclass="math inline">\(\mu\)</span> 的影响？另外我对于 Green函数法解二阶线性齐次常微分方程也有问题：既然 <spanclass="math inline">\(f(t)\)</span> 也可以写成 <spanclass="math inline">\(\int_{0}^{\infty} \delta(s)f(t-s)\mathrm{d}s\)</span>（积分上下限无所谓），那么为什么在数学物理方法的书上一定要求出非齐次项为<span class="math inline">\(\delta(t-s)\)</span> 的关于两个元 <spanclass="math inline">\(t,s\)</span> 的 Green 函数 <spanclass="math inline">\(G(t,s)\)</span>？</p></blockquote><hr /><p>This part is based on Bressloff and Taylor'spaper<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="P. C. Bressloff and J. G. Taylor (1994) Dynamics of compartmental model neurons. Neural Networks 7, pp. 1153–1165.">[1]</span></a></sup>.</p><p>(6.45) may be written as a linear matrix equation of the form <spanclass="math display">\[    \frac{\mathrm{d}\mathbf{V}}{\mathrm{d}t}=\mathbf{Q}\mathbf{V}(t)+\mathbf{I}(t)\]</span> <span class="math display">\[    \mathbf{Q}_{\mu \nu}=-\frac{\delta^{\mu\nu}}{\tau_{\mu}}+\sum_{\nu&#39;}^{}\frac{\delta^{\nu\nu&#39;}}{\tau_{\mu\nu&#39;}}\]</span> <span class="math display">\[    \mathbf{I}_{\mu}(t)=\frac{1}{C_i^{\mu}}[I_{i}^{\mu}-\delta^{\mu1}\Omega_i(t)]\]</span> where <span class="math display">\[    \frac{1}{\tau_{\mu}}=\frac{1}{C_i^{\mu}}\left[ \sum_{\nu&#39;}^{}\frac{1}{r_i^{\mu\nu&#39;}}+\frac{1}{R^{\mu}_{T,i}}\right], \quad\frac{1}{\tau_{\mu\nu}}=\frac{1}{C_i^{\mu}r_i^{\mu\nu}}\]</span> (the sum runs over all neighbors of compartment <spanclass="math inline">\(\mu\)</span>)</p><p>The above equation may be solved as (a bit different from (6.46))</p><p><span class="math display">\[    V_i^{\mu}(t)=\sum_{\nu}^{} \int_{t_0}^{t}G_i^{\mu\nu}(t-s)I_{\nu}(s) \mathrm{d}s +\sum_{\nu}^{}G_i^{\mu\nu}(t-t_0)V_i^{\nu}(t_0) \quad t\geqslant t_0\]</span> (maybe the second sum stands for initial condition?) with<span class="math display">\[    G_i^{\mu\nu}(t)= [\mathrm{e}^{t \mathbf{Q}}] _{\mu\nu}\]</span></p><p>This coincides with the <spanclass="math inline">\(G_i^{\mu\nu}\)</span> in the textbook except for aconstant (maybe <span class="math inline">\(C_i^{\mu}\)</span> orsomething)</p><hr /><p>We consider a network made up of a set of neurons described by (6.45)and a simple threshold criterion for generating spikes. We assume thateach spike <span class="math inline">\(t_j^{(f)}\)</span> of apresynaptic neuron <span class="math inline">\(j\)</span> evokes, for<span class="math inline">\(t&gt;t_j^{(f)}\)</span>, a synaptic currentpulse <span class="math inline">\(\alpha(t-t_j^{(f)})\)</span> into thepostsynaptic neuron <span class="math inline">\(i\)</span>. The actualamplitude of the current of the current pulse depends on the strength<span class="math inline">\(W_{ij}\)</span> of the synapse that connectsneuron <span class="math inline">\(j\)</span> to neuron <spanclass="math inline">\(i\)</span>. The total input to compartment <spanclass="math inline">\(\mu\)</span> of neuron <spanclass="math inline">\(i\)</span> is thus <span class="math display">\[    I_i^{\mu}(t)=\sum_{j \in \Gamma_i^{\mu}}^{} W_{ij} \sum_{f}^{}\alpha(t-t_j^{(f)}).\]</span> (6.47)</p><p>Here, <span class="math inline">\(\Gamma_i^{\mu}\)</span> denotes theset of all neurons that have a synapse with compartment <spanclass="math inline">\(\mu\)</span> of neuron <spanclass="math inline">\(i\)</span>. The firing times of neuron <spanclass="math inline">\(j\)</span> are denoted by <spanclass="math inline">\(t_j^{(f)}\)</span>.</p><p>We assume that spikes are generated at the soma in the manner of theintegrate-and-fire model. The reset voltage is <spanclass="math inline">\(V_i^{1}=u_r&lt;\theta\)</span>. This is equivalentto a current pulse <span class="math display">\[    \gamma_i(s)=C_i^{1}(\theta-u_r)\delta(s),\]</span> (6.48)</p><p>so that the overall current due to the firing of action potentials atthe soma of neuron <span class="math inline">\(i\)</span> amounts to<span class="math display">\[    \Omega_i(t)=\sum_{f}^{} \gamma_i(t-t_i^{(f)}).\]</span> (6.49)</p><p>Using the above specializations for the synaptic input current andthe somatic reset current the membrane potential (6.46) of compartment<span class="math inline">\(\mu\)</span> in neuron <spanclass="math inline">\(i\)</span> can be written as <spanclass="math display">\[    V_i^{\mu}(t)=\sum_{f}^{} \eta_i^{\mu}(t-t_i^{(f)})+\sum_{\nu}^{}\sum_{j \in \Gamma_i^{\nu}}^{} W_{ij} \sum_{f}^{} \varepsilon_i^{\mu\nu}(t-t_j^{(f)}).\]</span> (6.50)</p><p>with <span class="math display">\[    \varepsilon_i^{\mu \nu}(s)=\frac{1}{C_i^{\nu}}\int_{0}^{\infty}G_i^{\mu \nu}(s&#39;)\alpha(s-s&#39;) \mathrm{d}s&#39;,\]</span> (6.51) <span class="math display">\[    \eta_i^{\mu}(s)=\frac{1}{C_i^{1}}\int_{0}^{\infty} G_i^{\mu1}(s&#39;)\gamma_i(s-s&#39;) \mathrm{d}s&#39;.\]</span> (6.52)</p><p>The kernel <span class="math inline">\(\varepsilon_i^{\mu\nu}(s)\)</span> describes the effect of a presynaptic action potentialarriving at compartment <span class="math inline">\(\nu\)</span> on themembrane potential of compartment <spanclass="math inline">\(\mu\)</span>. Similarly, <spanclass="math inline">\(\eta_i^{\mu}(s)\)</span> describes the response ofcompartment <span class="math inline">\(\mu\)</span> to an actionpotential generated at the soma.</p><p>The triggering of action potentials depends on the somatic membranepotential only. We define <spanclass="math inline">\(u_i=V_i^{1}\)</span>, <spanclass="math inline">\(\eta_i(s)=\eta_i^{1}(s)\)</span> and, for <spanclass="math inline">\(j \in \Gamma_i^{\nu}\)</span>, we set <spanclass="math inline">\(\varepsilon_{ij}=\varepsilon_i^{1\nu}\)</span>.This yields the equation of the SRM <span class="math display">\[    u_i(t)=\sum_{f}^{} \eta_i(t-t_i^{(f)})+\sum_{j}^{} W_{ij}\sum_{f}^{}\varepsilon_{ij}(t-t_j^{(f)}).\]</span> (6.53)</p><h4 id="example-two-compartment-integrate-and-fire-model">Example:Two-compartment integrate-and-fire model</h4><p>Two compartments are characterized by a somatic capacitance <spanclass="math inline">\(C^{1}\)</span> and a dendritic capacitance <spanclass="math inline">\(C^{2}=aC^{1}\)</span>. The membrane time constantis <span class="math inline">\(\tau_0=R^{1}C^{1}=R^{2}C^{2}\)</span> andthe longitudinal time constant <spanclass="math inline">\(\tau_{12}=r^{12}C^{1}C^{2}/(C^{1}+C^{2})\)</span>.The neuron fires, if <spanclass="math inline">\(V^{1}(t)=\theta\)</span>. After each firing thesomatic potential is reset to <span class="math inline">\(u_r\)</span>.This is equivalent to a current pulse <span class="math display">\[    \gamma(s)=q\delta(s),\tag{6.54}\]</span> where <spanclass="math inline">\(q=C^{1}[\theta-u_r]\)</span>. The dendritereceives spike trains from other neurons <spanclass="math inline">\(j\)</span> and we assume that each spike evokes acurrent pulse with time course <span class="math display">\[    \alpha(s)=\frac{1}{\tau_s}\exp \left(-\frac{s}{\tau_s}\right)\Theta(s).\]</span> (6.55)</p><p>With the Green's function we can calculate the response kernels <spanclass="math inline">\(\eta_0(s)=\eta_i^{(1)}\)</span> and <spanclass="math inline">\(\varepsilon_0(s)=\varepsilon_i^{12}\)</span> asdefined in (6.51) and (6.52).</p><hr /><p>Calculate <span class="math inline">\(\eta_0(s)\)</span> (omit thesubscript <span class="math inline">\(i\)</span>): <spanclass="math display">\[    \begin{bmatrix}    \frac{\mathrm{d}V^{1}}{\mathrm{d}t} \\    \frac{\mathrm{d}V^{2}}{\mathrm{d}t}    \end{bmatrix}    =    \begin{bmatrix}    -\frac{1}{C^{1}}\left(\frac{1}{\tau^{12}}+\frac{1}{R^{1}}\right)&amp; \frac{1}{C^{1}\tau^{12}} \\    \frac{1}{aC^{1}\tau^{12}} &amp;-\frac{1}{aC^{1}}\left(\frac{1}{\tau^{12}}+\frac{1}{R^{2}}\right)    \end{bmatrix}    \begin{bmatrix}    V^{1} \\    V^{2} \\    \end{bmatrix}    +    \begin{bmatrix}    -q[\theta-u_r]\delta(t) \\    \frac{1}{\tau_s}\exp \left(-\frac{s}{\tau_s}\right) \Theta(s) \\    \end{bmatrix}\]</span></p><p>which is the form of <span class="math inline">\(\displaystyle\frac{\mathrm{d}\mathbf{V}}{\mathrm{d}t}=\mathbf{Q}\mathbf{V}(t)+\mathbf{I}(t)\)</span>. <span class="math display">\[    \mathbf{Q}=    \begin{bmatrix}    -\frac{a}{(1+a)\tau^{12}}-\frac{1}{\tau_0} &amp;\frac{a}{(1+a)\tau^{12}} \\    \frac{1}{(1+a)\tau^{12}} &amp;-\frac{1}{(1+a)\tau^{12}}-\frac{1}{\tau_0} \\    \end{bmatrix}\]</span></p><p>The eigenvalues of <span class="math inline">\(\mathbf{Q}\)</span>are <span class="math inline">\(\displaystyle -\frac{1}{\tau_0}\)</span>and <span class="math inline">\(\displaystyle-\frac{1}{\tau_0}-\frac{1}{\tau^{12}}\)</span>. The correspondingeigenvectors are <span class="math inline">\((1,1)^{\mathsf{T}}\)</span>and <span class="math inline">\(\displaystyle (1,-\frac{1}{a})^{\mathsf{T}}\)</span>. <span class="math display">\[    G^{11}(t)=\left[\int_{0}^{t} \mathrm{e}^{(t-s)\mathbf{Q}} \delta(s)\mathrm{d}s\right]_{11}=[\mathrm{e}^{t\mathbf{Q}}]_{11}=\frac{1}{1+a}\exp\left(-\frac{s}{\tau_0}\right)\left[1+a\exp\left(-\frac{s}{\tau_{12}}\right)\right]\]</span></p><p>So <span class="math display">\[    \eta_0(s)=-\frac{1}{C^{1}}\int_{0}^{\infty}G^{11}(s&#39;)q\delta(s-s&#39;) \mathrm{d}s&#39;\]</span></p><p>Similarly, <span class="math display">\[    G^{12}(t)=\frac{a}{1+a}\exp\left(-\frac{t}{\tau_0}\right)\left[1-\exp\left(-\frac{t}{\tau^{12}}\right)\right]\]</span> So <span class="math display">\[    \varepsilon_0(s)=\varepsilon^{12}(s)=\frac{1}{C^{2}}\int_{0}^{\infty}G^{12}(s&#39;)\frac{1}{\tau_s}\exp\left(-\frac{s-s&#39;}{\tau_s}\right)\Theta(s-s&#39;) \mathrm{d}s&#39;\]</span></p><hr /><p>We find <span class="math display">\[    \eta_0(s)=-\frac{\theta-u_r}{(1+a)}\exp\left(-\frac{s}{\tau_0}\right)\left[1+a\exp\left(-\frac{s}{\tau_{12}}\right)\right],\]</span></p><p><span class="math display">\[    \varepsilon_0(s)=\frac{1}{(1+a)C^{1}}\exp\left(-\frac{s}{\tau_0}\right)\left[\frac{1-\mathrm{e}^{-\delta_1s}}{\tau_s\delta_1}-\exp\left(-\frac{s}{\tau_{12}}\right)\frac{1-\mathrm{e}^{-\delta_2s}}{\tau_s\delta_2}\right],\]</span> (6.56)</p><p>with <spanclass="math inline">\(\delta_1=\tau_s^{-1}-\tau_0^{-1}\)</span> and<spanclass="math inline">\(\delta_2=\tau_s^{-1}-\tau_0^{-1}-\tau_{12}^{-1}\)</span>.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x176.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x177.png" /></div></div></div><p>The kernel <span class="math inline">\(\varepsilon_0(s)\)</span>describes the voltage response of the soma to an input at the dendrite.It shows the typical time course of an excitatory or inhibitorypostsynaptic potential. The time course of the kernel <spanclass="math inline">\(\eta_0(s)\)</span> is a double exponential andreflects the dynamics of the reset in a two-compartment model.</p><h2 id="summary">Summary</h2><p>For the understanding of the Spike Response Model, we introduce</p><hr /><p>Spike-response model -Scholarpediahttp://www.scholarpedia.org/article/Spike-response_model#Leaky_Integrate-and-fire_model</p><hr /><hr /><p>Adaptive exponential integrate-and-fire model -Scholarpediahttp://www.scholarpedia.org/article/Adaptive_exponential_integrate-and-fire_model</p><hr /><h3 id="exercises">Exercises</h3><ol type="1"><li><strong>Timescale of firing rate decay</strong>. The characteristicfeature of adaptation is that, after the onset of a superthreshold stepcurrent, interspike-intervals become successively longer, or,equivalently, that the momentary firing rate drops. The aim is to make aquantitative prediction of the decay of the firing rate of a leakintegrate-and-fire model with a single adaptation current.</li></ol><ol type="a"><li>Show that the firing rate of (6.7) and (6.8) with constant <spanclass="math inline">\(I\)</span>, constant <spanclass="math inline">\(w\)</span> and <spanclass="math inline">\(a=0\)</span> is <span class="math display">\[f(I,w)=-\left[\tau_m \log\left(1-\frac{\theta_{rh}-u_{reset}}{R(I-w)}\right)\right]^{-1}.      \]</span> (6.57)</li><li>For each spike (i.e., once per interspike interval), <spanclass="math inline">\(w\)</span> jumps by an amount <spanclass="math inline">\(b\)</span>. Show that for <spanclass="math inline">\(I\)</span> constant and <spanclass="math inline">\(w\)</span> averaged over one interspike, (6.8)becomes <span class="math display">\[\tau_w \frac{\mathrm{d}w}{\mathrm{d}t}=-w+b\tau_w f(I,w).\]</span> (6.58)</li><li>At time <span class="math inline">\(t_0\)</span>, a strong currentof amplitude <span class="math inline">\(I_0\)</span> is switched onthat causes transiently a firing rate <span class="math inline">\(f\gg\tau_w\)</span>. Afterward the firing rate decays. Find the effectivetime constant of the firing rate for the case of strong input current.<strong>Solution</strong>: For a) I find that <spanclass="math display">\[f(I,w)=\left[ \tau \log \left(1+\frac{\tau_m(\theta_{rh}-u_{reset})}{R(I-w)-\tau_m(\theta_{rh}-u_{rest})}\right) \right]^{-1}\]</span> which must satisfy <span class="math display">\[\theta_{rh}-u_{rest}&lt;\frac{R(I-w)}{\tau_m}\]</span></li></ol><ol start="2" type="1"><li><strong>Subthreshold resonance</strong>. We study a leakyintegrate-and-fire model with a single adaptation variable <spanclass="math inline">\(w\)</span>.</li></ol><ol type="a"><li>Assume <span class="math inline">\(E_0=u_{rest}\)</span> and castequation (6.7) and (6.8) in the form of (6.27). Set <spanclass="math inline">\(\varepsilon=0\)</span> and calculate <spanclass="math inline">\(\eta\)</span> andn <spanclass="math inline">\(\kappa\)</span>. Show that <spanclass="math inline">\(\kappa(t)\)</span> can be written as a linearcombination <spanclass="math inline">\(\kappa(t)=k_{+}\mathrm{e}^{\lambda_{+}t}+k_{-}\mathrm{e}^{\lambda_{-}t}\)</span> with <spanclass="math display">\[\lambda_{\pm}=\frac{1}{2\tau_m \tau_w}(-(\tau_m+\tau_w)\pm\sqrt{\tau_m^{2}+\tau_w^{2}-2\tau_m\tau_w(1+2aR)})   \]</span> (6.59) and <span class="math display">\[k_{\pm}=\pm \frac{R(\lambda_{\pm}\tau_w+1)}{\tau_m\tau_w(\lambda_{+}-\lambda_{-})}\]</span> (6.60)</li><li>What are the parameters of (6.7),(6.8) that lead to oscillations in<span class="math inline">\(\kappa(t)\)</span>?</li><li>What is the frequency of the oscillation?</li><li>Take the Fourier transform of (6.7),(6.8) and find the function<span class="math inline">\(\hat{R}(w)\)</span> that relates the current<span class="math inline">\(\hat{I}(w)\)</span> at frequency <spanclass="math inline">\(\omega\)</span> to the voltage <spanclass="math inline">\(\hat{u}(w)\)</span> at the same frequency, i.e.,<span class="math inline">\(\hat{u}(w)=\hat{R}(w)\hat{I}(w)\)</span>.Show that, in the case where <span class="math inline">\(\kappa\)</span>has oscillations, the function <spanclass="math inline">\(\hat{R}(w)\)</span> has a global maximum. What isthe frequency where this happens? <strong>Solution</strong>: (6.7),(6.8)can be expressed as <span class="math display">\[\begin{bmatrix}\frac{\mathrm{d}u}{\mathrm{d}t} \\\frac{\mathrm{d}w}{\mathrm{d}t} \\\end{bmatrix}=\begin{bmatrix}-\frac{1}{\tau_m} &amp; -\frac{R}{\tau_m} \\\frac{a}{\tau_w} &amp; -\frac{1}{\tau_w} \\\end{bmatrix}\begin{bmatrix}u \\w \\\end{bmatrix}+\begin{bmatrix}\frac{E_0+RI(t)}{\tau_m} \\-\frac{aE_0}{\tau_w} \\\end{bmatrix}\]</span> Under the assumption that <spanclass="math inline">\(I(t)=\delta(t)\)</span>, <spanclass="math inline">\(E_0=0\)</span>, <spanclass="math inline">\(w(0)=0\)</span>, we get what we want. <spanclass="math inline">\(\tau_m^{2}+\tau_w^{2}-2\tau_m\tau_w(1+2aR)&lt;0\)</span>leads to oscillation of <span class="math inline">\(\kappa(t)\)</span>.The frequency is <span class="math display">\[\sqrt{-\tau_m^{2}-\tau_w^{2}+2\tau_m\tau_w(1+2aR)}/2\pi\]</span> For d), <span class="math display">\[\hat{R}(\omega)=\frac{(2\pi i\tau_w \omega+1)R}{-8\pi^{3}\tau_m\tau_w\omega^{2}+1+aR+2\pi i\omega(2\pi \tau_m+\tau_w)}\]</span> &gt; I give up! Ask 1 and 2.</li></ol><ol start="3" type="1"><li><strong>Integrate-and-fire model with slow adaptation</strong>. Theaim is to relate the leaky integrate-and-fire model with a singeladaptation variable, defined in (6.7) and (6.8) to the Spike ResponseModel in the form of (6.27). Adaptation is slow so that <spanclass="math inline">\(\tau_m/\tau_w=\delta\ll 1\)</span> and allcalculations can be done to first order in <spanclass="math inline">\(\delta\)</span>.</li></ol><ol type="a"><li>Show that the spike after-potential is given by <spanclass="math display">\[\eta(t)=\gamma_1\mathrm{e}^{\lambda_1t} +\gamma_2\mathrm{e}^{\lambda_2t}\]</span> (6.61) <span class="math display">\[\gamma_1=\Delta u (1-\delta-\delta a)-b(1+\delta)\]</span> (6.62) <span class="math display">\[\gamma_2=\Delta u -\gamma_1\]</span> (6.63)</li><li>Derive the input response kernel <spanclass="math inline">\(\kappa(s)\)</span>. &gt; The initial conditionsare <span class="math inline">\(\eta(0)=\Delta(u)\)</span> and <spanclass="math inline">\(w(0)=b\)</span> ? I cannot get the desiredresult.</li></ol><ol type="1"><li><strong>Integrate-and-fire model with time-dependent timeconstant</strong>. Since many channels are open immediately after aspike, the effective membrane time constant after a spike is smallerthan the time constant at rest. Consider an integrate-and-fire modelwith spike-time dependent time constant, i.e., with a membrane timeconstant <span class="math inline">\(\tau\)</span> that is a function ofthe time since the last postsynaptic spike, <spanclass="math display">\[\frac{\mathrm{d}u}{\mathrm{d}t}=-\frac{u}{\tau(t-\hat{t})}+\frac{1}{C}I^{ext}(t);\]</span> (6.64) As usual, <span class="math inline">\(\hat{t}\)</span>denotes the last firing time of the neuron. The neuron fires if <spanclass="math inline">\(u(t)\)</span> hits a fixed threshold <spanclass="math inline">\(\theta\)</span> and integration restarts with areset value <span class="math inline">\(u_r\)</span>.</li></ol><ol type="a"><li>Suppose that the time constant is <spanclass="math inline">\(\tau(t-\hat{t})=2\)</span> ms for <spanclass="math inline">\(t-\hat{t}&lt;10\)</span> ms and <spanclass="math inline">\(\tau(t-\hat{t})=20\)</span> ms for <spanclass="math inline">\(t-\hat{t}\geqslant 10\)</span> ms. Set <spanclass="math inline">\(u_r=-10\)</span> mV. Sketch the time course of themembrane potential for an input current <spanclass="math inline">\(I(t)=q\delta(t-t&#39;)\)</span> arriving at <spanclass="math inline">\(t&#39;=5\)</span> ms or <spanclass="math inline">\(t&#39;=15\)</span> ms. What are the differencesbetween the two cases?</li><li>Integrate (6.64) for arbitrary input with <spanclass="math inline">\(u(\hat{t})=u_r\)</span> as initial condition andinterpret the result.</li></ol><ol start="5" type="1"><li><strong>Spike-triggered adaptation currents</strong>. Consider aleaky integrate-and-fire model. A spike at time <spanclass="math inline">\(t^{(f)}\)</span> generates several adaptationcurrents <spanclass="math inline">\(\mathrm{d}w_k/\mathrm{d}t=-\frac{w_k}{\tau_k}+b_k\delta(t-t^{(f)})\)</span>with <span class="math inline">\(k=1,\cdots ,K\)</span>.</li></ol><ol type="a"><li>Calculate the effect of the adaptation current on the voltage.</li><li>Construct a combination of spike-triggered currents that couldgenerates slow adaptation.</li><li>Construct a combination of spike-triggered currents that couldgenerates bursts.</li></ol><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>P. C. Bressloff and J. G.Taylor (1994) Dynamics of compartmental model neurons. Neural Networks7, pp. 1153–1165.<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>神经动力学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>神经科学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (5)</title>
    <link href="/2022/07/09/Neuronal-Dynamics-5/"/>
    <url>/2022/07/09/Neuronal-Dynamics-5/</url>
    
    <content type="html"><![CDATA[<h1 id="nonlinear-integrate-and-fire-models">NonlinearIntegrate-and-Fire Models</h1><p>The online version of this chapter:</p><hr /><p>Chapter 5 Nonlinear Integrate-and-Fire Modelshttps://neuronaldynamics.epfl.ch/online/Ch5.html</p><hr /><h2 id="thresholds-in-a-nonlinear-integrate-and-fire-model">Thresholdsin a nonlinear integrate-and-fire model</h2><p>In a general nonlinear integrate-and-fire model with a singlevariable <span class="math inline">\(u\)</span>, the membrane potentialevolves according to <span class="math display">\[    \tau \frac{\mathrm{d}}{\mathrm{d}t}u=f(u)+R(u)I.\]</span> (5.2)</p><p>The dynamics is stopped if <span class="math inline">\(u\)</span>reaches the threshold <spanclass="math inline">\(\theta_{reset}\)</span>. In this case the firingtime <span class="math inline">\(t^{(f)}\)</span> is noted andintegration of the membrane potential equation restarts at time <spanclass="math inline">\(t^{(f)}+\Delta^{abs}\)</span> with initialcondition <span class="math inline">\(u_{r}\)</span>. If not specifiedotherwise, we always assume a constant input resistance <spanclass="math inline">\(R(u)=R\)</span> independent of voltage.</p><h4 id="example-rescaling-and-standard-forms">Example: Rescaling andstandard forms</h4><p>Introduce a new variable <spanclass="math inline">\(\tilde{u}\)</span> by the transformation <spanclass="math display">\[    u(t) \to \tilde{u}(t)=\tau \int_{0}^{u(t)}\frac{\mathrm{d}x}{R(x)}         \]</span> which is possible if <span class="math inline">\(R(x)\neq0\)</span> for all <span class="math inline">\(x\)</span> in theintegration range. In terms of <spanclass="math inline">\(\tilde{u}\)</span> we have <spanclass="math display">\[    \frac{\mathrm{d}\tilde{u}}{\mathrm{d}t}=d(\tilde{u})+I(t)\]</span> with <spanclass="math inline">\(d(\tilde{u})=f(u)/R(u)\)</span>.</p><h3 id="where-is-the-firing-threshold">Where is the firingthreshold?</h3><p>The voltage threshold <span class="math inline">\(\theta\)</span>determined with pulse-like input currents is different from the voltagethreshold determined with prolonged step currents.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x136.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x137.png" /></div></div></div><p>The critical current for initiation of repetitive firing correspondsto the voltage where the stable fixed point disappears, or <spanclass="math inline">\(\theta_{rh}=I_{c}R\)</span>. In the experimentalliterature, the critical current <spanclass="math inline">\(I_c=\theta_{rh}/R\)</span> is called the'rheobase' current. In the mathematical literature, it is called thebifurcation point.</p><h2 id="exponential-integrate-and-fire-model">ExponentialIntegrate-and-Fire Model</h2><p>In the experimental integrate-and-fire model, the differentialequation for the membrane potential is given by <spanclass="math display">\[    \tau \frac{\mathrm{d}}{\mathrm{d}t}u=-(u-u_{rest})+\Delta_{T}\exp\left(\frac{u-\theta_{rh}}{\Delta_{T}}\right)+RI;\]</span> (5.6)</p><p>The first term describe the leak of a passive membrane. The secondterm is an exponential nonlinearity with 'sharpness' parameter <spanclass="math inline">\(\Delta_{T}\)</span> and 'threshold' <spanclass="math inline">\(\theta_{rh}\)</span>.</p><p>If the numerical threshold is chosen sufficiently high, <spanclass="math inline">\(\theta_{reset}\gg \theta+\Delta_{T}\)</span>, theupswing of the action potential for <span class="math inline">\(u\gg\theta+\Delta_{T}\)</span> is so rapid, that it goes to infinity in anincredibly short time, so the exact value of <spanclass="math inline">\(\theta_{reset}\)</span> does not play any role.<span class="math inline">\(\theta_{rh}\)</span> is the threshold foundwith constant (rheobase) current.</p><h3 id="extracting-the-nonlinearity-from-data">Extracting theNonlinearity from Data</h3><p>Why we choose an exponential nolinearity rather than any othernonlinear dependence? After rescaling with the time constant <spanclass="math inline">\(\tau\)</span>, the nonlinearity <spanclass="math inline">\(\tilde{f}(u)=-f(u)/\tau\)</span> is <spanclass="math display">\[    \tilde{f}(u(t))=\frac{1}{C}I(t)-\frac{\mathrm{d}}{\mathrm{d}t}u(t);\]</span></p><p>where <span class="math inline">\(C=\tau/R\)</span> can be intepretedas the capacity of the membrane.</p><p>The slope of the curve at the resting potential is related to themembrane time constant <span class="math inline">\(\tau\)</span> whilethe threshold parameter <span class="math inline">\(\theta_{rh}\)</span>is the voltage at which the function <spanclass="math inline">\(\tilde{f}\)</span> goes through its minimum.</p><h3 id="from-hodgkin-huxley-to-exponential-integrate-and-fire">FromHodgkin-Huxley to Exponential Integrate-and-Fire</h3><p>As long as we are only interested in the initiation phase of theaction potential we can assume a fixed value <spanclass="math inline">\(w=w_{rest}\)</span>.</p><p>For constant <span class="math inline">\(w\)</span>, <spanclass="math display">\[    \frac{\mathrm{d}u}{\mathrm{d}t}=F(u,w_{rest})+I=f(u)+I\]</span> It has three zero-crossings: the first one (left) at <spanclass="math inline">\(u_{rest}\)</span>, corresponding to a stable fixedpoint; a second one (middle) which acts as a threshold <spanclass="math inline">\(\theta\)</span>; and a third one to the right,which is again a stable fixed point and limits the upswing of the actionpotential. The value of the reset threshold <spanclass="math inline">\(\theta_{reset}&gt;\theta\)</span> must be chosenbetween the second and third fixed point.</p><p>Replace the downswing of the action potential in the nonlinearintegrate-and-fire model by an artificial reset of the voltage variableto a value <span class="math inline">\(u_{r}\)</span> whenever <spanclass="math inline">\(u\)</span> hits <spanclass="math inline">\(\theta_{reset}\)</span>.</p><h4 id="example-exponential-activation-of-sodium-channels">Example:Exponential Activation of Sodium Channels</h4><p><span class="math display">\[    C\frac{\mathrm{d}u}{\mathrm{d}t}=-g_{Na}[m_0(u)]^{3}h_{rest}(u-E_{Na})-g_{K}(n_{rest})^{4}(u-E_{K})-g_{L}(u-E_{L})+I,\]</span> (5.15) Potassium and leak currents can now be summed up to anew effective leak term <spanclass="math inline">\(g^{eff}(u-E^{eff})\)</span>. In the voltage rangeclose to the resting potential the driving force <spanclass="math inline">\((u-E_{Na})\)</span> of the sodium current can bewell approximated by <spanclass="math inline">\((u_{rest}-E_{Na})\)</span>. Then the onlyremaining nonlinearity on the right-hand-side of (5.15) arises from the<span class="math inline">\(m_0(u)\)</span>. For voltages around rest,<span class="math inline">\(m_0(u)\)</span> has an exponentialshape.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x144.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x145.png" /></div></div></div><h2 id="quadratic-integrate-and-fire">Quadratic Integrate and Fire</h2><p>A specific instance of a nonlinear integrate-and-fire model is thequadratic model, <span class="math display">\[    \tau\frac{\mathrm{d}}{\mathrm{d}t}u=a_0(u-u_{rest})(u-u_c)+RI,\]</span> (5.16)</p><p>with <span class="math inline">\(a_0&gt;0\)</span> and <spanclass="math inline">\(u_c&gt;u_{rest}\)</span>. The quadraticintegrate-and-fire model is closely related to the so-called <spanclass="math inline">\(\Theta-\)</span>neuron, a canonical type-I neuronmodel.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x146.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x147.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x148.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x149.png" /></div></div></div><h3 id="canonical-type-i-model">Canonical Type I model</h3><p>We'll show a one-to-one relation between the quadraticintegrate-and-fire model and the canonical type I phase model, <spanclass="math display">\[    \frac{\mathrm{d}\phi}{\mathrm{d}t}=[1-\cos \phi]+\Delta I[1+\cos\phi];\]</span> (5.17) Denote by <spanclass="math inline">\(I_{\theta}\)</span> the minimal current necessaryfor repetitive firing of the quadratic integrate-and-fire neuron. With asuitable shift of the voltage scale and constant current <spanclass="math inline">\(I=I_{\theta}+\Delta I\)</span> the equation of thequadratic neuron model can then by cast into the form <spanclass="math display">\[    \frac{\mathrm{d}u}{\mathrm{d}t}=u^{2}+\Delta I.     \tag{5.18}\]</span></p><p>For <span class="math inline">\(\Delta I&gt;0\)</span> the voltageincreases until it reaches the firing threshold <spanclass="math inline">\(\theta \gg 1\)</span> where it is reset to a value<span class="math inline">\(u_r\ll -1\)</span>.</p><p>By the transformation <span class="math display">\[    u(t)=\tan \left( \frac{\phi(t)}{2}\right). \tag{5.19}\]</span> The differential equation (5.18) can be transformed into(5.17). Thus (5.19) with <span class="math inline">\(\phi(t)\)</span>given by (5.17) is a solution to the differential equation of thequadratic integrate-and-fire neuron. The quadratic integrate-and-fireneuron is therefore (in the limit <span class="math inline">\(\theta \to\infty\)</span> and <span class="math inline">\(u_r \to-\infty\)</span>) equivalent to the genetic type I neuron (5.17).</p>]]></content>
    
    
    <categories>
      
      <category>神经动力学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>神经科学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Information and Entropy (2)</title>
    <link href="/2022/07/08/Information-and-Entropy-2/"/>
    <url>/2022/07/08/Information-and-Entropy-2/</url>
    
    <content type="html"><![CDATA[<h1 id="communications">Communications</h1><h2 id="source-model">Source Model</h2><p>The source is assumed to produce symbols at a rate of <spanclass="math inline">\(R\)</span> symbols per second. The event of theselection of symbol <span class="math inline">\(i\)</span> will bedenoted <span class="math inline">\(A_i\)</span>.</p><p>Suppose that each event <span class="math inline">\(A_i\)</span> isrepresented by a different codeword <spanclass="math inline">\(C_i\)</span> with a length <spanclass="math inline">\(L_i\)</span>.</p><p>An important property of such codewords is that none can be the sameas the first portion of another, longer, codeword. A code that obeysthis property is called a <strong>prefix-condition code</strong>, orsometimes an <strong>instantaneous code</strong>.</p><h3 id="kraft-inequality">Kraft Inequality</h3><p>An important limitation on the distribution of code lengths <spanclass="math inline">\(L_i\)</span> was given by L.G.Kraft, which isknown as the Kraft inequality: <span class="math display">\[    \sum_{i}^{} \frac{1}{2^{L_i}}\leqslant 1 \tag{6.1}\]</span></p><p>Any valid set of distinct codewords obeys this inequality, andconversely for any proposed <span class="math inline">\(L_i\)</span>that obey it, a code can be found.</p><p>Proof: Let <span class="math inline">\(L_{max}\)</span> be the lengthof the longest codeword of a prefix-condition code. There are exactly<span class="math inline">\(2^{L_{max}}\)</span> different patterns of<span class="math inline">\(0\)</span> and <spanclass="math inline">\(1\)</span> of this length. Thus <spanclass="math display">\[    \sum_{i}^{} \frac{1}{2^{L_{max}}}=1 \tag{6.2}\]</span> where this sum is over these patterns. For each shortercodeword of length <span class="math inline">\(k(k&lt;L_{max})\)</span>there are exactly <span class="math inline">\(2^{L_{max}-k}\)</span>patterns that begin with this codeword, and none of those is a validcodeword. In the sum of (6.2) replace the terms corresponding to thosepatterns by a single term equal to <spanclass="math inline">\(1/2^{k}\)</span>. The sum is unchanged. Continuethis process with other short codewords. Some terms that are notcodewords are eliminated. Q.E.D.</p><h3 id="source-entropy">Source Entropy</h3><p>The uncertainty of the identity of the next symbol chosen <spanclass="math inline">\(H\)</span> is the average information gained whenthe next symbol is made known: <span class="math display">\[    H=\sum_{i} p(A_i)\log _{2}\left(\frac{1}{p(A_i)}\right) \tag{6.3}\]</span></p><p>This quantity is also known as the entropy of the source. Theinformation rate, in bits per second, is <spanclass="math inline">\(H\cdot R\)</span> where <spanclass="math inline">\(R\)</span> is the rate at which the source selectsthe symbols, measured in symbols per second.</p><h3 id="gibbs-inequality">Gibbs Inequality</h3><p>This inequality states that the entropy is smaller than or equal toany other average formed using the same probabilities but a differentbut a different function in the logarithm. Specifically, <spanclass="math display">\[    \sum_{i}^{} p(A_i)\log _{2}\left(\frac{1}{p(A_i)}\right)\leqslant\sum_{i}^{} p(A_i)\log _{2}\left(\frac{1}{p&#39;(A_i)}\right) \tag{6.4}\]</span> where <span class="math inline">\(p(A_i)\)</span> is anyprobability distribution and <spanclass="math inline">\(p&#39;(A_i)\)</span> is any other probabilitydistribution, namely, <span class="math display">\[    0\leqslant p&#39;(A_i)\leqslant 1 \tag{6.5}\]</span> and <span class="math display">\[    \sum_{i}^{} p&#39;(A_i)\leqslant 1. \tag{6.6}\]</span> As is true for all probability distributions, <spanclass="math display">\[    \sum_{i}^{} p(A_i)=1. \tag{6.7}\]</span> Proof: <span class="math display">\[    \begin{aligned}        \sum_{i}^{} p(A_i)\log_{2}\left(\frac{1}{p(A_i)}\right)-\sum_{i}^{} p(A_i)\log_{2}\left(\frac{1}{p&#39;(A_i)}\right)&amp;= \sum_{i}^{} p(A_i)\log_{2}\left(\frac{p&#39;(A_i)}{p(A_i)}\right)  \\        &amp;\leqslant \log _{2}\mathrm{e} \sum_{i}^{}p(A_i)\left[\frac{p&#39;(A_i)}{p(A_i)}-1\right] \\        &amp;= \log _{2} \mathrm{e} \left(\sum_{i}^{} p&#39;(A_i)-1\right) \\        &amp;\leqslant 0    \end{aligned}\]</span></p><h2 id="source-coding-theorem">Source Coding Theorem</h2><p>The codewords have an average length, in bits per symbol, <spanclass="math display">\[    L=\sum_{i}^{} p(A_i)L_i \tag{6.11}\]</span></p><p>The Source Coding Theorem states that the average information persymbol is always less than or equal to the average length of a codeword:<span class="math display">\[    H\leqslant L \tag{6.12}\]</span></p><p>This inequality is easy to prove using the Gibbs and Kraftinequalities. Use the Gibbs inequality with <spanclass="math inline">\(p&#39;(A_i)=1/2^{L_i}\)</span>. Thus <spanclass="math display">\[    \begin{aligned}        H&amp;=\sum_{i}^{} p(A_i)\log _{2}\left( \frac{1}{p(A_i)}\right)\\        &amp;\leqslant  \sum_{i}^{} p(A_i)\log_{2}\left(\frac{1}{p&#39;(A_i)}\right) \\        &amp;= \sum_{i}^{} p(A_i)\log _{2}2^{L_i}\\        &amp;= \sum_{i}^{} p(A_i)L_i \\        &amp;= L    \end{aligned}\]</span></p><p>The Source Coding Theorem can also be expressed in terms of rates oftransmission in bits per second by multiplying (6.12) by the symbols persecond <span class="math inline">\(R\)</span>: <spanclass="math display">\[    HR\leqslant LR \tag{6.14}\]</span></p><h2 id="channel-model">Channel Model</h2><p>If the channel perfectly changes its output state in conformance withits input state, it is said to be <strong>noiseless</strong> and in thatcase nothing affects the output except the input.</p><p>Suppose that the channel has a certain maximum rate <spanclass="math inline">\(W\)</span> at which its output can follow changesat the input.</p><p>The <strong>binary</strong> channel has two mutually exclusive inputstates.</p><p>The maximum rate at which information supplied to the input canaffect the output is called the <strong>channel capacity</strong> <spanclass="math inline">\(C=W \log _{2}n\)</span> bits per second. For thebinary channel, <span class="math inline">\(C=W\)</span>.</p><h2 id="noiseless-channel-theorem">Noiseless Channel Theorem</h2><p>It may be necessary to provide temporary storage buffers toaccommodate bursts of adjacent infrequently occurring symbols with longcodewords, and the symbols may not materialize at the output of thesystem at a uniform rate.</p><p>Also, to encode the symbols efficiently it may be necessary toconsider several of them together, in which case the first symbol wouldnot be available at the output until several symbols had been presentedat the input. Therefore high speed operation may lead to highlatency.</p><h2 id="noisy-channel">Noisy Channel</h2><p>For every possible input there may be more than one possible outputoutcome. Denote <strong>transition probabilities</strong> <spanclass="math inline">\(c_{ji}\)</span> the probability of the outputevent <span class="math inline">\(B_j\)</span> occuring when event <spanclass="math inline">\(A_i\)</span> happens. So <spanclass="math display">\[    0\leqslant c_{ji}\leqslant 1 \tag{6.15}\]</span> and <span class="math display">\[    1=\sum_{j}^{} c_{ji} \tag{6.16}\]</span> If the channel is noiseless, for each value of <spanclass="math inline">\(i\)</span> exactly one of the various <spanclass="math inline">\(c_{ji}\)</span> is equal to <spanclass="math inline">\(1\)</span> and all others are <spanclass="math inline">\(0\)</span>. <span class="math display">\[    p(B_j|A_i)=c_{ji} \tag{6.17}\]</span> The unconnditional probability of each output <spanclass="math inline">\(p(B_j)\)</span> is <span class="math display">\[    p(B_j)=\sum_{i}^{} c_{ji}p(A_i) \tag{6.18}\]</span> So by Bayes' Theorem: <span class="math display">\[    p(A_i|B_j)=\frac{p(A_i)c_{ji}}{p(B_j)} \tag{6.19}\]</span> The simplest noisy channel is the symmetric binary channel,for which there is a probability <spanclass="math inline">\(\varepsilon\)</span> of an error, so <spanclass="math display">\[    \begin{bmatrix}    c_{00} &amp; c_{01} \\    c_{10} &amp; c_{11} \\    \end{bmatrix}    =    \begin{bmatrix}    1-\varepsilon &amp; \varepsilon \\    \varepsilon &amp; 1-\varepsilon \\    \end{bmatrix}\]</span></p><p>Define the information that we have learned about the input as aresult of knowing the output as the <strong>mutualinformation</strong>.</p><p>Before we know the output, our uncertainty <spanclass="math inline">\(U_{before}\)</span> about the identity of theinput event is he entropy of the input: <span class="math display">\[    U_{before}=\sum_{i}^{} p(A_i)\log _{2}\left(\frac{1}{p(A_i)}\right)\tag{6.21}\]</span> After some particular output event <spanclass="math inline">\(B_j\)</span> has been observed, the residualuncertainty <span class="math inline">\(U_{after}(B_j)\)</span> aboutthe input event is: <span class="math display">\[    U_{after}(B_j)=\sum_{i}^{} p(A_i|B_j)\log_{2}\left(\frac{1}{p(A_i|B_j)}\right) \tag{6.22}\]</span></p><p>The mutual information <span class="math inline">\(M\)</span> isdefined as the average, over all outputs, of the amount so learned,<span class="math display">\[    M=U_{before}-\sum_{j}^{} p(B_j)U_{after}(B_j) \tag{6.23}\]</span> It is not difficult to prove that <spanclass="math inline">\(M\geqslant 0\)</span>. To prove this, the Gibbsinequality is used, for each <span class="math inline">\(j\)</span>:<span class="math display">\[    \begin{aligned}        U_{after}(B_j)&amp;=\sum_{i}^{} p(A_i|B_j)\log_{2}\left(\frac{1}{p(A_i|B_j)}\right) \\        &amp;\leqslant \sum_{i}^{} p(A_i|B_j)\log_{2}\left(\frac{1}{p(A_i)}\right)         \end{aligned}\]</span> (6.24) So <span class="math display">\[    \begin{aligned}        \sum_{j}^{} p(B_j)U_{after}(B_j) &amp;\leqslant \sum_{j}^{}p(B_j)\sum_{i}^{} p(A_i|B_j)\log _{2}\left(\frac{1}{p(A_i)}\right)  \\        &amp;= \sum_{ji}^{} p(B_j)p(A_i|B_j)\log_{2}\left(\frac{1}{p(A_i)}\right) \\        &amp;= \sum_{ij}^{} p(B_j|A_i)p(A_i)\log_{2}\left(\frac{1}{p(A_i)}\right) \\        &amp;=\sum_{i}^{} p(A_i)\log _{2}\left(\frac{1}{p(A_i)}\right)\\        &amp;=U_{before}              \end{aligned}\]</span> (6.25)</p><p>Substitution in (6.23) and simplification leads to <spanclass="math display">\[    M=\sum_{j}^{} \left(\sum_{i}^{} p(A_i)c_{ji}\right)\log_{2}\left(\frac{1}{\sum_{i}^{} p(A_i)c_{ji}}\right)-\sum_{ij}^{}p(A_i)c_{ji}\log _{2}\left(\frac{1}{c_{ji}}\right)\]</span> (6.26)</p><p>(6.26) was derived for the case where the input "causes" the output.However, such a cause-and-effect relationship is not necessary. The term<strong>mutual information</strong> suggests that it is just as valid toview the output as causing the input, or to ignore completely thequestion of what causes what. Two alternate formulas for <spanclass="math inline">\(M\)</span> shows that <spanclass="math inline">\(M\)</span> can be interpreted in either direction:<span class="math display">\[    \begin{aligned}        M &amp;= \sum_{i}^{} p(A_i)\log_{2}\left(\frac{1}{p(A_i)}\right)-\sum_{j}^{} p(B_j)\sum_{i}^{}p(A_i|B_j)\log _{2}\left(\frac{1}{p(A_i|B_j)}\right) \\        &amp;= \sum_{j}^{} p(B_j)\log_{2}\left(\frac{1}{p(B_j)}\right)-\sum_{i}^{} p(A_i)\sum_{j}^{}p(B_j|A_i)\log _{2}\left(\frac{1}{p(B_j|A_i)}\right)    \end{aligned}\]</span> (6.27)</p><p>So (6.26) is easily checked.</p><h2 id="noisy-channel-capacity-theorem">Noisy Channel CapacityTheorem</h2><p>It is more useful to define the channel capacity so that it dependsonly on the channel, so <span class="math inline">\(M_{max}\)</span>,the maximum mutual information that results from any possible inputprobability distribution, is used.</p><p>Generally speaking, going away from the symmetric case offers few ifany advantages in engineered systems, and in particular the fundamentallimits given by the theorems in this chapter cannot be evaded throughsuch techniques. Therefore the symmetric case gives the right intuitiveunderstanding.</p><p>The channel capacity is defined as <span class="math display">\[    C=M_{max}W \tag{6.29}\]</span> where <span class="math inline">\(W\)</span> is the maximumrate at which the output state can follow changes at the input. Thus<span class="math inline">\(C\)</span> is expressed in bits persecond.</p><p>The channel capacity theorem states that: if the input informationrate in bits per decond <span class="math inline">\(D\)</span> is lessthan <span class="math inline">\(C\)</span> then it is possible (perhapsby dealing with long sequences of inputs together) to code the data insuch a way that the error rate is as low as desired.</p><p>The proof is not a constructive proof. However, there is not yet anygeneral theory of how to design codes from scratch.</p><h2 id="reversibility">Reversibility</h2><p>Some Boolean operations had the property that the input could not bededuced from the output. The <span class="math inline">\(AND\)</span>and <span class="math inline">\(OR\)</span> gates are examples. Otheroperations were reversible——the <spanclass="math inline">\(EXOR\)</span> gate, when the output is augmentedby one of the two inputs, is an example.</p><h2 id="detail-communication-system-requirements">Detail: CommunicationSystem Requirements</h2><p>The systems are characterized by four measures: throughput, latency,tolerance of errors, and tolerance to nonuniform rate (bursts).Throghput is simply the number of bits per second that such a systemshould, to be successful, accommodate. Latency is the time delay of themessage; it could be defined either as the delay of the start of theoutput after the source begins, or a similar quantity about the end ofthe message (or, for that matter, about any particular features in themessage). The numbers for throughput, in MB (megabytes) or kb (kilobits)are approximate.</p><hr /><p>Channel capacityhttp://web.archive.org/web/20080126223204/http://www.cs.ucl.ac.uk/staff/S.Bhatti/D51-notes/node31.html</p><hr /><h1 id="processes">Processes</h1><p>We know the model of a communication system: - Input (Symbols) -Source Encoder - Compressor - Channel Encoder - Channel - ChannelDecoder - Expander - Source Decoder - Output(Symbols)</p><p>Because each of these steps processes information in some way, it iscalled a <strong>processor</strong> and what it does is called a<strong>process</strong>. The processes we consider here are -<strong>Discrete:</strong> The inputs are members of a set of mutuallyexclusive possibilities, only one of which occurs at a time, and theoutput is one of another discrete set of mutually exclusive events. -<strong>Finite:</strong> The set of possible inputs is finite in number,as is the set of possible outputs. - <strong>Memoryless:</strong> Theprocess acts on the input at some time and produces an output based onthat input, ignoring any prior inputs. -<strong>Nondeterministic:</strong> The process may produce a differentoutput when presented with the same input a second time (the model isalso valid for deterministic processes). Because the process isnondeterministic the output may contain random <strong>noise</strong>. -<strong>Lossy:</strong> It may not be possible to "see" the input fromthe output, i.e., determine the input by observing the output. Suchprocesses are called <strong>lossy</strong> because knowledge about theinput is lost when the output is created (the model is also valid forlossless processes).</p><h2 id="types-of-process-diagrams">Types of Process Diagrams</h2><p>We may use <strong>block diagram</strong>, <strong>circuitdiagram</strong>, <strong>probability diagram</strong> and<strong>information diagram</strong>.</p><h2 id="probability-diagrams">Probability Diagrams</h2><p>We suppose the probability model of a process with <spanclass="math inline">\(n\)</span> inputs and <spanclass="math inline">\(m\)</span> outputs. The <spanclass="math inline">\(n\)</span> inputs are mutually exclusive, as arethe <span class="math inline">\(m\)</span> output states.</p><p>For each <span class="math inline">\(i\)</span> denote theprobability that this input leads to the output <spanclass="math inline">\(j\)</span> as <spanclass="math inline">\(c_{ji}\)</span>. Denote the event associated withthe selection of input <span class="math inline">\(i\)</span> as <spanclass="math inline">\(A_i\)</span> and the event associated with output<span class="math inline">\(j\)</span> as <spanclass="math inline">\(B_j\)</span>.</p><h3 id="example-and-gates">Example: AND Gates</h3><p>The <span class="math inline">\(AND\)</span> gate is deterministicbut is lossy.</p><h3 id="example-binary-channel">Example: Binary Channel</h3><p>Symmetric Binary Channel (SBC), symmetric in the sense that theerrors in the two directions are equally likely.</p><p>It is possible for processes to introduce noise but have no loss, orvice versa.</p><p>Loss of information happens because it is no longer possible to tellwith certainty what the input signal is, when the output is observed.Loss shows up in probability diagram where two or more paths converge onthe same output.</p><p>Noise happens because the output is not determined precisely by theinput. Noise shows up in probability diagram where two or more pathsdiverge from the same input.</p><h2 id="information-loss-and-noise">Information, Loss, and Noise</h2><p>We now return to our model of a general discrete memorylessnondeterministic lossy process, and derive formulas for noise, loss, andinformation transfer.</p><p>The information at the input <span class="math inline">\(I\)</span>is the same as the entropy of this source. <span class="math display">\[    I=\sum_{i}^{} p(A_i)\log _{2}\left(\frac{1}{p(A_i)}\right)\]</span> The output information <span class="math inline">\(J\)</span>can also be expressed in terms of the input probability distribution andthe channel transition matrix: <span class="math display">\[    \begin{aligned}        J &amp;= \sum_{j}^{} p(B_j)\log_{2}\left(\frac{1}{p(B_j)}\right) \\        &amp;= \sum_{j}^{} \left( \sum_{i}^{} c_{ji}p(A_i)\right)\log_{2}\left(\frac{1}{\sum_{i}^{} c_{ji}p(A_i)}\right)    \end{aligned}\]</span></p><p>Note that this measure of information at the output <spanclass="math inline">\(J\)</span> refers to the identity of the outputstate, not the input state. If we've got an output state <spanclass="math inline">\(B_j\)</span>, then the uncertainty of ourknowledge of the input state is <span class="math display">\[    \sum_{i}^{} p(A_i|B_j)\log _{2}\left(\frac{1}{p(A_i|B_j)}\right)\]</span> So the average uncertainty about the input after learning theoutput is <span class="math display">\[    \begin{aligned}        L&amp;=\sum_{j}^{} p(B_j)\sum_{i}^{} p(A_i|B_j)\log_{2}\left(\frac{1}{p(A_i|B_j)}\right) \\        &amp;=\sum_{ij}^{} p(A_i,B_j)\log_{2}\left(\frac{1}{p(A_i|B_j)}\right)    \end{aligned}\]</span></p><p>We have denoted this average uncertainty by <spanclass="math inline">\(L\)</span> and will call it "loss." In the specialcase that the process allows the input state to be identified uniquelyfor each possible output state, the process is "lossless" and <spanclass="math inline">\(L=0\)</span>.</p><p>Denote <span class="math inline">\(M=I-L\)</span> the "mutualinformation". This is an important quantity because it is the amount ofinformation tha gets through the process.</p><p>Some processes have loss but are deterministic. An example is the<span class="math inline">\(AND\)</span> logic gate.</p><p>There is a quantity similar to <span class="math inline">\(L\)</span>that characterizes a nondeterministic process, whether or not it hasloss. Define the noise <span class="math inline">\(N\)</span> of aprocess as the uncertainty in the output, given the input state,averaged over all input states. <span class="math display">\[    \begin{aligned}        N &amp;= \sum_{i}^{} p(A_i)\sum_{j}^{} p(B_j|A_i)\log_{2}\left(\frac{1}{p(B_j|A_i)}\right) \\        &amp;= \sum_{i}^{} p(A_i)\sum_{j}^{} c_{ji}\log_{2}\left(\frac{1}{c_{ji}}\right)    \end{aligned}\]</span></p><p>What may not be obvious, but can be proven easily, is that the mutualinformation <span class="math inline">\(M\)</span> plays exactly thesame sort of role for noise as it does for loss. Since <spanclass="math display">\[    J=\sum_{i}^{} p(B_j)\log _{2}\left(\frac{1}{p(B_j)}\right)\]</span> we have <span class="math display">\[    M=J-N \tag{7.24}\]</span> It follows from these results that <spanclass="math display">\[    J-I=N-L \tag{7.27}\]</span></p><h3 id="example-symmetric-binary-channel">Example: Symmetric BinaryChannel</h3><p>For the SBC with bit error probability <spanclass="math inline">\(\varepsilon\)</span>, these formulas can beevaluated, even if the two input probabilities <spanclass="math inline">\(p(A_0)\)</span> and <spanclass="math inline">\(p(A_1)\)</span> are not equal. If they happen tobe equal (each 0.5), then <span class="math display">\[    I=1\ \text{bit}\]</span> <span class="math display">\[    J=1\ \text{bit}\]</span> <span class="math display">\[    L=N=\varepsilon \log_{2}\left(\frac{1}{\varepsilon}\right)+(1-\varepsilon) \log_{2}\left(\frac{1}{1-\varepsilon}\right)\]</span> <span class="math display">\[    M=1-\varepsilon \log_{2}\left(\frac{1}{\varepsilon}\right)-(1-\varepsilon)\log_{2}\left(\frac{1}{1-\varepsilon}\right)\]</span></p><h2 id="deterministic-examples">Deterministic Examples</h2><p>p88-89</p><h3 id="error-correcting-example">Error Correcting Example</h3><p>p89-90</p><h2 id="capacity">Capacity</h2><p>Call <span class="math inline">\(W\)</span> the maximum rate at whichthe input state of the process can be detected at the output. Then therate at which information flows through the process can be as large as<span class="math inline">\(WM\)</span>. However, this product is not aproperty of the process itself, but on how it is used. The<strong>process capacity</strong> <span class="math inline">\(C\)</span>is defined as <span class="math display">\[    C=WM_{max} \tag{7.32}\]</span></p><h2 id="information-diagrams">Information Diagrams</h2><p>p91</p><h2 id="cascaded-processes">Cascaded Processes</h2><p><img src="/img/inf_and_ent/2022-07-11-17-42-53.png" /></p><p>Consider two processes in <strong>cascade</strong>. This term refersto having the output from one process serve as the input to anotherprocess.</p><p>The matrix of transition probabilities is merely the matrix productof the two transition probability matrices for process 1 and process2.</p><p>Now we seek formulas for <span class="math inline">\(I,J,L,N\)</span>and <span class="math inline">\(M\)</span> of the overall process interms of the corresponding quantities for the component processes.</p><p><span class="math inline">\(I=I_1\)</span> and <spanclass="math inline">\(J=J_2\)</span>. <spanclass="math inline">\(L\)</span> and <spanclass="math inline">\(N\)</span> cannot generally be found exactly from<span class="math inline">\(L_1,L_2,N_1\)</span> and <spanclass="math inline">\(N_2\)</span>, it is possible to find upper andlower bounds for them. <span class="math display">\[    L-N=(L_1+L_2)-(N_1+N_2)\tag{7.33}\]</span> The loss <span class="math inline">\(L\)</span> for theoverall process is not always equal to the sum of the losses for the twocomponents <span class="math inline">\(L_1+L_2\)</span>, but instead<span class="math display">\[    0\leqslant L_1\leqslant L\leqslant L_1+L_2 \tag{7.34}\]</span> so that the loss is bound from above and below. Also, <spanclass="math display">\[    L_1+L_2-N_1\leqslant L\leqslant L_1+L_2 \tag{7.35}\]</span> so that if the first process is noise-free then <spanclass="math inline">\(L\)</span> is exactly <spanclass="math inline">\(L_1+L_2\)</span>.</p><p>There are similar formulas for <span class="math inline">\(N\)</span>in terms of <span class="math inline">\(N_1+N_2\)</span>: <spanclass="math display">\[    0\leqslant N_2\leqslant N\leqslant N_1+N_2 \tag{7.36}\]</span> <span class="math display">\[    N_1+N_2-L_2\leqslant N\leqslant N_1+N_2 \tag{7.37}\]</span> Similar formulas for the mutual information of the cascade<span class="math inline">\(M\)</span> follow from these results: <spanclass="math display">\[    M_1-L_2\leqslant M\leqslant M_1\leqslant I \tag{7.38}\]</span> <span class="math display">\[    M_1-L_2\leqslant M\leqslant M_1+N_1-L_2 \tag{7.39}\]</span> <span class="math display">\[    M_2-N_1\leqslant M\leqslant M_2\leqslant J \tag{7.40}\]</span> <span class="math display">\[    M_2-N_1\leqslant M\leqslant M_2+L_2-N_1 \tag{7.41}\]</span> Other formulas for <span class="math inline">\(M\)</span> areeasily derived from <span class="math inline">\(0\leqslant M\leqslantI\)</span> applied to the first process and the cascade, and <spanclass="math inline">\(M=J-N\)</span> applied to the second process andthe cascade: <span class="math display">\[    \begin{aligned}        M &amp;= M_1+L_1-L \\        &amp;=M_1+N_1+N_2-N-L_2 \\        &amp;=M_2+N_2-N \\        &amp;=M_2+L_2+L_1-L-N_1         \end{aligned}\]</span> where the second formula in each case comes from the use of(7.33).</p><p><span class="math inline">\(M\)</span> cannont exceed either <spanclass="math inline">\(M_1\)</span> or <spanclass="math inline">\(M_2\)</span>. If the second process is lossless,<span class="math inline">\(L_2=0\)</span> and then <spanclass="math inline">\(M=M_1\)</span>. Similarly if the first process isnoiseless, then <span class="math inline">\(N_1=0\)</span> and <spanclass="math inline">\(M=M_2\)</span>.</p><p>The channel capacity <span class="math inline">\(C\)</span> of thecascade satisfies <span class="math inline">\(C\leqslant C_1\)</span>and <span class="math inline">\(C\leqslant C_2\)</span>. However, otherresults relating the channel capacities are not a trivial consequence ofthe formulas above.</p><h1 id="inference">Inference</h1><h2 id="estimation">Estimation</h2><p>We now try to determine the input event when the output has beenobserved. This is the case for communication systems and memorysystems.</p><p>The conditional output probabilities <spanclass="math inline">\(c_{ji}\)</span> are a property of the process, anddo not depend on the input probabilities <spanclass="math inline">\(p(A_i)\)</span>.</p><p>The unconditional probability <spanclass="math inline">\(p(B_j)\)</span> of each output event <spanclass="math inline">\(B_j\)</span> is <span class="math display">\[    p(B_j)=\sum_{i}^{} c_{ji}p(A_i)  \tag{8.1}\]</span> and <span class="math display">\[    p(A_i,B_j)=p(A_i)c_{ji} \tag{8.2}       \]</span> so <span class="math display">\[    p(A_i|B_j)=\frac{p(A_i)c_{ji}}{p(B_j)} \tag{8.3}\]</span> If the process has no loss (<spanclass="math inline">\(L=0\)</span>) then for each <spanclass="math inline">\(j\)</span> exactly one of the input events <spanclass="math inline">\(A_i\)</span> has nonzero probability, andtherefore its probability <spanclass="math inline">\(p(A_i|B_j)\)</span> is <spanclass="math inline">\(1\)</span>.</p><p>We know <span class="math display">\[    U_{before}=\sum_{i}^{} p(A_i)\log _{2}\left(\frac{1}{p(A_i)}\right)\]</span> and the residual uncertainty after some particular outputevent is <span class="math display">\[    U_{after}(B_j)=\sum_{i}^{} p(A_i|B_j)\log_{2}\left(\frac{1}{p(A_i|B_j)}\right)\]</span></p><p>The question is whether <spanclass="math inline">\(U_{after}(B_j)\leqslant U_{before}\)</span>. Theanswer is often, but not always, yes.</p><p><strong>On average</strong>, out uncertainty about the input state isnever increased by learning something about the output state.</p><h3 id="non-symmetric-binary-channel">Non-symmetric Binary Channel</h3><p>For a rare family genetic disease,</p><table><thead><tr class="header"><th></th><th><span class="math inline">\(p(A)\)</span></th><th><span class="math inline">\(p(B)\)</span></th><th><span class="math inline">\(I\)</span></th><th><span class="math inline">\(L\)</span></th><th><span class="math inline">\(M\)</span></th><th><span class="math inline">\(N\)</span></th><th><span class="math inline">\(J\)</span></th></tr></thead><tbody><tr class="odd"><td>Family history</td><td>0.5</td><td>0.5</td><td>1.00000</td><td>0.11119</td><td>0.88881</td><td>0.11112</td><td>0.99993</td></tr><tr class="even"><td>Unknown Family history</td><td>0.9995</td><td>0.0005</td><td>0.00620</td><td>0.00346</td><td>0.00274</td><td>0.14141</td><td>0.14416</td></tr></tbody></table><h3 id="inference-strategy">Inference Strategy</h3><p>One simple strategy for inference is "maximum likelihood". However,sometimes it does not work at all (rare family genetic diseasetest).</p><h2 id="principle-of-maximum-entropy-simple-form">Principle of MaximumEntropy: Simple Form</h2><p>Before the Principle of Maximum Entropy can be used the problemdomain needs to be set up. It is not assumed in this step whichparticular state the system is in (or which state is actually"occupied"); indeed it is assumed that we do not know and cannot knowthis with certainty, and so we deal instead with the probability of eachof the states being occupied.</p><h3 id="bergers-burgers">Berger's Burgers</h3><p>The Principle of Maximum Entropy will be introduced by means of anexample. A fast-food restaurant, Berger's Burgers, offers three meals:burger, chicken, and fish. The price, Calorie count, and probability ofeach meal being delivered cold are as listed below.</p><table><thead><tr class="header"><th>Item</th><th>Entree</th><th>Cost</th><th>Calories</th><th>Probability of arriving hot</th><th>probability of arriving cold</th></tr></thead><tbody><tr class="odd"><td>Value Meal 1</td><td>Burger</td><td>1.00</td><td>1000</td><td>0.5</td><td>0.5</td></tr><tr class="even"><td>Value Meal 2</td><td>Chicken</td><td>2.00</td><td>600</td><td>0.8</td><td>0.2</td></tr><tr class="odd"><td>Value Meal 3</td><td>Fish</td><td>3.00</td><td>400</td><td>0.9</td><td>0.1</td></tr></tbody></table><h3 id="probabilities">Probabilities</h3><p>If we do not know the outcome we may still have some knowledge, andwe use probabilities to express this knowledge.</p><h3 id="entropy">Entropy</h3><p>Our uncertainty is expressed as <span class="math display">\[    S=p(B)\log _{2}\left(\frac{1}{p(B)}\right)+p(C)\log_{2}\left(\frac{1}{p(C)}\right)+p(F)\log _{2}\left(\frac{1}{p(F)}\right)\]</span> In the context of physical systems this uncertainty is knownas the entropy. In communication systems the uncertainty regarding whichactual message is to be transmitted is also known as the entropy of thesource.</p><p>In general the entropy, because it is expressed in terms ofprobabilities, depends on the observer. One person may have differentknowledge of the system from another, and therefore would calculate adifferent numerical value for entropy.</p><p>The Principle of Maximum Entropy is used to discover the probabilitydistribution which leads to the highest value for this uncertainty,thereby assuring that no information is inadvertently assumed. Theresulting probability distribution is not observer-dependent.</p><h3 id="constraints">Constraints</h3><p>If we have additional information then we ought to be able to find aprobability distribution that is better in the sense that it has lessuncertainty.</p><p>We consider we know the expected value of some quantity (thePrinciple of Maximum Entropy can handle multiple constraints but themathematical procedures and formulas become more complicated) If thereis an attribute for which each of the states has a value <spanclass="math inline">\(g(A_i)\)</span> and for which we know the actualvalue <span class="math inline">\(G\)</span>, then we should consideronly those probability distributions for which the expected value isequal to <span class="math inline">\(G\)</span>.</p><p><span class="math display">\[    G=\sum_{i}^{} p(A_i)g(A_i) \tag{8.15}\]</span></p><p>In the case that the average cost is 1.75, <spanclass="math inline">\(S\)</span> attains its maximum when <spanclass="math inline">\(p(B)=0.466\)</span>, <spanclass="math inline">\(p(C)=0.318\)</span>, <spanclass="math inline">\(p(C)=0.318\)</span>, and <spanclass="math inline">\(S=1.517\)</span> bits.</p>]]></content>
    
    
    <categories>
      
      <category>信息论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (4)</title>
    <link href="/2022/07/04/Neuronal-Dynamics-4/"/>
    <url>/2022/07/04/Neuronal-Dynamics-4/</url>
    
    <content type="html"><![CDATA[<h1id="dimensionality-reduction-and-phase-plane-analysis">DimensionalityReduction and Phase Plane Analysis</h1><p>The online version of this chapter:</p><hr /><p>Chapter 4 Dimensionality Reduction and Phase Plane Analysishttps://neuronaldynamics.epfl.ch/online/Ch4.html</p><hr /><h2 id="threshold-effects">Threshold effects</h2><p>In this section we use current pulses ans steps in order to explorethe threshold behavior of the Hodgkin-Huxley model.</p><h3 id="pulse-input">Pulse Input</h3><p>The threshold depends on the stimulation protocol.</p><h3 id="step-current-input">Step Current Input</h3><p>We study the response of the Hodgkin-Huxley model to a step currentof the form <span class="math display">\[    I(t)=I_1+\Delta I \mathscr{H}(t)\]</span> where <span class="math inline">\(\mathscr{H}\)</span> denotesthe Heaviside step function.</p><p><img src="/img/neu_dyn/x87.png" /></p><p>When probing with step currents, there is neither a unique currentthreshold for spike initiation nor for repetitive firing. The triggermechanism for action potentials depends not only on <spanclass="math inline">\(I_2\)</span> but also on the size of the currentstep <span class="math inline">\(\Delta I\)</span>.</p><p>Biologically, the dependence upon the step size arises from thedifferent time constants of activation and inactivation of the ionchannels. We'll see it below.</p><h2 id="reduction-to-two-dimensions">Reduction to two dimensions</h2><h3 id="general-approach">General approach</h3><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x26.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x27.png" /></div></div></div><p>Note that the time scale of <span class="math inline">\(m\)</span> ismuch faster than <span class="math inline">\(n\)</span> and <spanclass="math inline">\(h\)</span>. Moreover, <spanclass="math inline">\(m\)</span> is fast compared to the membrane timeconstant <span class="math inline">\(\tau=C/g_{L}\)</span> of a passivemembrane, which characterizes the evolution of the voltage <spanclass="math inline">\(u\)</span> when all channels are closed. So we maytreat <span class="math inline">\(m\)</span> as an instantaneousvariable, therefore it can be replaced by its steady-state value, <spanclass="math inline">\(m(t) \to m_0[u(t)]\)</span>. (<strong>quasi steadystate approximation</strong>)</p><p>Note that the time constants <spanclass="math inline">\(\tau_n(u)\)</span> and <spanclass="math inline">\(\tau_{h}(u)\)</span> have similar dynamics overthe voltage <span class="math inline">\(u\)</span>. Moreover, the graphsof <span class="math inline">\(n_0(u)\)</span> and <spanclass="math inline">\(1-h_0(u)\)</span> are also similar.</p><p>We use a linear approximation <spanclass="math inline">\((b-h)\thickapprox an\)</span> with some constants<span class="math inline">\(a,b\)</span> and set <spanclass="math inline">\(w=b-h=an\)</span>. With <spanclass="math inline">\(h=b-w\)</span>, <spanclass="math inline">\(n=w/a\)</span>, and <spanclass="math inline">\(m=m_0(u)\)</span>, equations (2.4)-(2.5) become<span class="math display">\[    C\frac{\mathrm{d}u}{\mathrm{d}t}=-g_{Na}[m_0(u)]^{3}(b-w)(u-E_{Na})-g_{K}(\frac{w}{a})^{4}(u-E_{K})-g_{L}(u-E_{L})+I,\tag{4.3}\]</span> or <span class="math display">\[    \frac{\mathrm{d}u}{\mathrm{d}t}=\frac{1}{\tau}[F(u,w)+RI], \tag{4.4}\]</span> with <span class="math inline">\(R=g_{L}^{-1}\)</span>, <spanclass="math inline">\(\tau=RC\)</span> and some function <spanclass="math inline">\(F\)</span>. We also have <spanclass="math display">\[    \frac{\mathrm{d}w}{\mathrm{d}t}=\frac{1}{\tau_{w}}G(u,w), \tag{4.5}\]</span></p><h4 id="example-morris-lecar-model">Example: Morris-Lecar model</h4><p>The Morris-Lecar equations read <span class="math display">\[    C\frac{\mathrm{d}u}{\mathrm{d}t}=-g_1 \hat{m}_0(u)(u-V_1)-g_2\hat{w}(u-V_2)-g_{L}(u-V_{L})+I, \tag{4.6}\]</span> <span class="math display">\[    \frac{\mathrm{d}\hat{w}}{\mathrm{d}t}=-\frac{1}{\tau(u)}[\hat{w}-w_0(u)].\tag{4.7}\]</span></p><p>(4.6) doesn't have the factor <spanclass="math inline">\((b-w)\)</span> which closes the channel for highvoltage. Another difference is that neither <spanclass="math inline">\(\hat{m}_0\)</span> nor <spanclass="math inline">\(\hat{w}\)</span> have exponents. In the followingwe consider (4.6) and (4.7) as a model in its own right and drop thehats over <span class="math inline">\(m_0\)</span> and <spanclass="math inline">\(w\)</span>.</p><p>It's reasonable to approximate the voltage dependence by <spanclass="math display">\[    m_0(u)=\frac{1}{2}\left[ 1+ \tanh \left(\frac{u-u_1}{u_2}\right)\right] \tag{4.8}\]</span> <span class="math display">\[    w_0(u)=\frac{1}{2}\left[ 1+ \tanh \left(\frac{u-u_3}{u_4}\right)\right] \tag{4.9}\]</span> with parameters <span class="math inline">\(u_1,\cdots,u_4\)</span>, and to approximate the time constant by <spanclass="math display">\[    \tau(u)=\frac{\tau_{w}}{\cosh(\frac{u-u_3}{2u_4})} \tag{4.10}\]</span> with a further parameter <spanclass="math inline">\(\tau_{w}\)</span>.</p><p><strong>Example: FitzHugh-Nagumo model</strong></p><p>FitzHugh and Nagumo obtained sharp pulse-like oscillationsreminiscent of trains of action potentials by defining the function<span class="math inline">\(F(u,w)\)</span> and <spanclass="math inline">\(G(u,w)\)</span> as<br /><span class="math display">\[    \begin{cases}        F(u,w)=u-\frac{1}{3}u^{3}-w, \\        G(u,w)=b_0+b_1u-w,    \end{cases}    \tag{4.11}\]</span></p><p>where <span class="math inline">\(u\)</span> is the membrane voltageand <span class="math inline">\(w\)</span> is a recovery variable.</p><h3 id="mathematical-steps">Mathematical steps</h3><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x92.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x93.png" /></div></div></div><p>The overall aim of the approach is to replace the variables <spanclass="math inline">\(n\)</span> and <spanclass="math inline">\(h\)</span> in the Hodgkin-Huxley model by a singleeffective variable <span class="math inline">\(w\)</span>. During anaction potential, the variables <spanclass="math inline">\(n(t)\)</span> and <spanclass="math inline">\(h(t)\)</span> stay close to a straight line.</p><p>A minimal condition for the projection is that the approximation isperfect while the neuron is at rest. First, we introduces new variables<span class="math display">\[    x=n-n_0(u_{rest}) \tag{4.12}\]</span> <span class="math display">\[    y=h-h_0(u_{rest}). \tag{4.13}\]</span> At rest, we have <spanclass="math inline">\(x=y=0\)</span>.</p><p>Second, the points <spanclass="math inline">\((n_0(u),h_0(u))\)</span> as a function of <spanclass="math inline">\(u\)</span> define a curve. The slope of the curveat <span class="math inline">\(u=u_{rest}\)</span> yields the rotationangle <span class="math inline">\(\alpha\)</span> via <spanclass="math display">\[    \tan \alpha=\frac{ \frac{\mathrm{d}h_0}{\mathrm{d}u}\big|_{u_{rest}}}{ \frac{\mathrm{d}n_0}{\mathrm{d}u}\big |_{u_{rest}}}\]</span> Rotate the coordinates <span class="math display">\[    \begin{pmatrix}    z_1 \\    z_2 \\    \end{pmatrix}    =    \begin{pmatrix}    \cos \alpha &amp; \sin \alpha \\    -\sin \alpha &amp; \cos \alpha \\    \end{pmatrix}    \begin{pmatrix}    x \\    y \\    \end{pmatrix}.\]</span></p><p><img src="/img/neu_dyn/x94.png" /></p><p>The inverse transform <span class="math display">\[    \begin{pmatrix}    x \\    y \\    \end{pmatrix}    =    \begin{pmatrix}    \cos \alpha &amp; -\sin \alpha \\    \sin \alpha &amp; \cos \alpha \\    \end{pmatrix}    \begin{pmatrix}    z_1 \\    z_2 \\    \end{pmatrix}.\]</span></p><p>Project <span class="math inline">\((n,h)\)</span> to the lineexpanded by <span class="math inline">\(z_1\)</span>, namely, <spanclass="math inline">\(z_2=0\)</span>. <span class="math display">\[    n&#39;=n_0(u_{rest})+z_1\cos \alpha, \tag{4.17}\]</span> <span class="math display">\[    h&#39;=h_0(u_{rest})+z_1\sin \alpha. \tag{4.18}\]</span></p><p>From (4.15) we find <span class="math display">\[    \frac{\mathrm{d}z_1}{\mathrm{d}t}=\cos \alpha\frac{\mathrm{d}n}{\mathrm{d}t}+\sin \alpha\frac{\mathrm{d}h}{\mathrm{d}t}. \tag{4.19}\]</span> Since <span class="math display">\[    \frac{\mathrm{d}n}{\mathrm{d}t}=-\frac{1}{\tau_n(u)}[n-n_0(u)]\]</span> and an similar equation for <spanclass="math inline">\(h\)</span>, we have <span class="math display">\[    \frac{\mathrm{d}z_1}{\mathrm{d}t}=-\cos \alpha \frac{z_1 \cos\alpha+n_0(u_{rest})-n_0(u)}{\tau_{n}(u)}- \sin \alpha \frac{z_1 \sin\alpha+h_0(u_{rest})-h_0(u)}{\tau_{h}(u)}. \tag{4.20}\]</span> which is of the form <spanclass="math inline">\(\mathrm{d}z_1/\mathrm{d}t=G(u,z_1)\)</span>, asdesired.</p><p>It is convenient to rescale <span class="math inline">\(z_1\)</span>and define <span class="math display">\[    w=- n_0(u_{rest})\tan \alpha-z_1 \sin \alpha. \tag{4.21}\]</span> If we introduce <span class="math inline">\(a=-\tan\alpha\)</span> and <span class="math inline">\(b=an_0(u_{rest})+h_0(u_{rest})\)</span>, we find from (4.17) <spanclass="math inline">\(n&#39;=w/a\)</span> and from (4.18) <spanclass="math inline">\(h&#39;=b-w\)</span>. We also have <spanclass="math inline">\(\mathrm{d}w/\mathrm{d}t=G(u,w)\)</span>. Actually,<span class="math display">\[    \frac{\mathrm{d}w}{\mathrm{d}t}=-\frac{1}{\tau(u)}[w-w_0(u)].\tag{4.22}    \]</span> with <span class="math display">\[    w_0(u)=-\sin \alpha[n_0(u)\cos \alpha+h_0(u)\sin \alpha-b\sin\alpha] \tag{4.23}\]</span> In practice, <span class="math inline">\(w_0(u)\)</span> and<span class="math inline">\(\tau(u)\)</span> can be fitted by (4.9) and(4.10), respectively.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x95.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x96.png" /></div></div></div><h2 id="phase-plane-analysis">Phase plane analysis</h2><p>We know <span class="math display">\[    \begin{pmatrix}    \Delta u \\    \Delta w \\    \end{pmatrix}    =    \begin{pmatrix}    \dot{u} \\    \dot{v} \\    \end{pmatrix}    \Delta t. \tag{4.24}        \]</span></p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x97.png" /></div><div class="group-image-wrap"><img src="/img/neu_dyn/x98.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x99.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x100.png" /></div></div></div><p>Case A is stable, Case C and D are unstable. Stability in case Bcannot be decided with the information available from the picture alone.C and D are saddle points.</p><h3 id="nullclines">Nullclines</h3><p>The set of points with <span class="math inline">\(\dot{u}=0\)</span>is called the <span class="math inline">\(u-\)</span>nullcline.</p><h3 id="stability-of-fixed-points">Stability of Fixed Points</h3><p>The local stability of a fixed point <spanclass="math inline">\((u_{FP},w_{FP})\)</span> is determined bylinearization of the dynamics at the intersection. With <spanclass="math inline">\(\mathbf{x}=(u-u_{FP},w-w_{FP})^{\mathsf{T}}\)</span>,we have after the linearization <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}\mathbf{x}=    \begin{pmatrix}    F_u &amp; F_w \\    G_u &amp; G_w \\    \end{pmatrix}    \mathbf{x}\tag{4.25}\]</span></p><p>Stability of the fixed point <spanclass="math inline">\(\mathbf{x}=0\)</span> in (4.25) requires that thereal part of both eigenvalues be negative. The necessary and sufficientcondition for stability is therefore <span class="math display">\[    F_u+G_w&lt;0 \quad \text{and} \quad F_u G_w-F_w G_u&gt;0.\]</span> If <span class="math inline">\(F_u G_w-F_w G_u&lt;0\)</span>,the fixed point is then called a saddle point.</p><h4 id="example-linear-model">Example: Linear model</h4><p><span class="math display">\[    \dot{u}=au-w \\    \dot{w}=\varepsilon(bu-w), \tag{4.27}\]</span> with positive constants <spanclass="math inline">\(b,\varepsilon&gt;0\)</span>. The <spanclass="math inline">\(u-\)</span>nullcline is <spanclass="math inline">\(w=au\)</span>, the <spanclass="math inline">\(w-\)</span>nullcline is <spanclass="math inline">\(w=bu\)</span>. For the moment we assume <spanclass="math inline">\(a&lt;0\)</span>.</p><p>For the sake of completeness we also study the linear system <spanclass="math display">\[    \dot{u}=-au+w \\    \dot{w}=\varepsilon(bu-w), 0&lt;a&lt;b, \tag{4.28}\]</span> with positive constants <spanclass="math inline">\(a,b\)</span>, and <spanclass="math inline">\(\varepsilon\)</span>.</p><p>Recall the theorem of Poincare-Bendixson.</p><p>In dimensionless variables the FitzHugh-Nagumo model is <spanclass="math display">\[    \frac{\mathrm{d}u}{\mathrm{d}t}=u-\frac{1}{3}u^{3}-w+I \tag{4.29}\]</span> <span class="math display">\[    \frac{\mathrm{d}w}{\mathrm{d}t}=\varepsilon(b_0+b_1u-w). \tag{4.30}\]</span> Time is measured in units of <spanclass="math inline">\(\tau\)</span> and <spanclass="math inline">\(\varepsilon=\tau/\tau_w\)</span> is the ratio ofthe two time scales.</p><h2 id="type-i-and-type-ii-neuron-models">Type I and Type II neuronmodels</h2><p>The onset of repetitive firing under constant current injection ischaracterized by a minimal current <spanclass="math inline">\(I_{\theta}\)</span>, also called the rheobasecurrent. Mathematically speaking, the point <spanclass="math inline">\(I_{\theta}\)</span> where the transition in thenumber or stability of fixed points occurs is called a bifurcation pointand <span class="math inline">\(I\)</span> is the bifurcationparameter.</p><h3 id="type-i-models-and-saddle-node-onto-limit-cycle-bifurcation">TypeI Models and Saddle-Node-onto-Limit-Cycle Bifurcation</h3><p>Neuron models with a continuous gain function. Mathematically, asaddle-node-onto-limit-cycle bifurcation generically gives rise to atype I behavior.</p><p><img src="/img/neu_dyn/x108.png" /></p><h4 id="example-morris-lecar-model-1">Example: Morris-Lecar model</h4><p>Depending on the choice of parameters, the Morris-Lecar model is ofeither type I or type II.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x109.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x110.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x111.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x112.png" /></div></div></div><h4 id="example-hodgkin-huxley-model-reduced-to-two-dimensions">Example:Hodgkin-Huxley Model Reduced to Two Dimensions</h4><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x113.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x114.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x115.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x116.png" /></div></div></div><p>(4.14)</p><h3 id="type-ii-models-and-saddle-node-off-limit-cycle-bifurcation">TypeII Models and Saddle-Node-off-Limit-Cycle Bifurcation</h3><p>There is no reason why a limit cycle should appear directly at thebifurcation point - it can also exist before the bifurcation point isreached. In this case, the limit cycle does not pass through the ruinsof the fixed point and therefore has finite frequency. This gives riseto a type II neuron model. (Saddle-Node-off-Limit-Cycle)</p><h4id="example-hodgkin-huxley-model-reduced-to-two-dimensions-1">Example:Hodgkin-Huxley Model Reduced to Two Dimensions</h4><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x117.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x118.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x119.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x120.png" /></div></div></div><p>(4.15)</p><p>(4.15) shows the same neuron model as (4.14) except for one singlechange in parameter: the time scale <spanclass="math inline">\(\tau_{w}\)</span> in (4.5) for the <spanclass="math inline">\(w-\)</span>dynamics is slightly faster.</p><h4 id="example-saddle-node-without-limit-cycle">Example: Saddle-nodewithout limit cycle</h4><p>Not all saddle-node bifurcations lead to a limit cycle. If the slopeof the <span class="math inline">\(w-\)</span>nullcline of theFitzHugh-Nagumo model defined in (4.29) and (4.30) is smaller than one,it is possible to have three fixed points, one of them unstable and theother two stable. The system is therefore bistable. If <spanclass="math inline">\(I&gt;0\)</span> is big enough so that the leftstable fixed point and the saddle merge and disappear. Since the rightfixed point remains stable, no oscillation occurs.</p><h3 id="type-ii-models-and-hopf-bifurcation">Type II Models and HopfBifurcation</h3><p>From the solution of the stability problem in (4.25) we know that theeigenvalues <span class="math inline">\(\lambda_{+/-}\)</span> form acomplax conjugate pair with a real part <spanclass="math inline">\(\gamma\)</span> and a imaginary part <spanclass="math inline">\(+/-\omega\)</span> (Fig. 4.16). The fixed point isstable if <span class="math inline">\(\gamma&lt;0\)</span>. At thetransition point, the real part vanishes and the eigenvalues are <spanclass="math display">\[    \lambda_{\pm}=\pm i \sqrt{F_u G_w-G_u F_w}. \tag{4.31}\]</span> These eigenvalues correspond to an oscillatory solution with afrequency given by <span class="math inline">\(\omega=\sqrt{F_u G_w-G_uF_w}\)</span>.</p><p><img src="/img/neu_dyn/x121.png" /> (4.16)</p><p>Whenever we have a Hopf bifurcation, be it subcritical orsupercritical, the limit cycle starts with finite frequency.</p><p>In a supercritical Hopf bifurcation, the amplitude of the oscillationgrows with the stimulation <span class="math inline">\(I\)</span>. Suchperiodic oscillations of small amplitude should be intepreted asspontaneous subthreshold oscillations.</p><p>Only models with a subcritical Hopf-bifurcation give rise tolarge-amplitude oscillations close to the bifurcation point.</p><h4 id="example-fitzhugh-nagumo-model">Example: FitzHugh-Nagumomodel</h4><p>In Fig. 4.10, if the slope of the <spanclass="math inline">\(w-\)</span>nullcline is larger than one, there isonly one fixed point, whatever <span class="math inline">\(I\)</span>.With increasing current <span class="math inline">\(I\)</span>, thefixed point moves to the right. Eventually it loses stability via a Hopfbifurcation.</p><h2 id="threshold-and-excitability">Threshold and excitability</h2><p>The Hodgkin-Huxley model does not have a clear-cut firingthreshold.</p><p>For stimulation with a short current pulse of variable amplitude,models with saddle-node bifurcation (on or off a limit cycle) indeedhave a threshold whereas models where firing arises via a Hopfbifurcation have not. However, even models with Hopf bifurcation canshow threshold-like behavior for current pulse if the dynamics of <spanclass="math inline">\(w\)</span> are consideraby slower than that of<span class="math inline">\(u\)</span>.</p><h3 id="type-i-models">Type I models</h3><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x124.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x125.png" /></div></div></div><p>The stable manifold of an saddle point. All trajectories with initialcondition to the right of the stable manifold must make a detour aroundthe unstable fixed point before they can reach the stable fixed point.Trajectories with initial conditions to the left of the stable manifoldreturn immediately toward the stable fixed point.</p><p>The stable manifold acts as a threshold for spike initiation, if theneuron model is probed with a isolated current pulse.</p><h4 id="example-canonical-type-i-model">Example: Canonical type Imodel</h4><p>Consider the one-dimensional model <span class="math display">\[    \frac{\mathrm{d}\phi}{\mathrm{d}t}=q(1-\cos \phi)+I(1+\cos \phi)\tag{4.32}\]</span> where <span class="math inline">\(q&gt;0\)</span> is aparameter and <span class="math inline">\(I\)</span> with <spanclass="math inline">\(0&lt;\lvert I \rvert &lt;q\)</span> the appliedcurrent. The variable <span class="math inline">\(\phi\)</span> is thephase along the limit cycle trajectory. Formally, a spike is said tooccur whenever <span class="math inline">\(\phi=\pi\)</span>.</p><p>For <span class="math inline">\(I&lt;0\)</span>, the phase equation<span class="math inline">\(\mathrm{d}\phi/\mathrm{d}t\)</span> has twofixed points. The resting state is at the stable fixed point <spanclass="math inline">\(\phi=\phi_r\)</span>. The unstable fixed point at<span class="math inline">\(\phi=\theta\)</span> acts as athreshold.</p><p>For all currents <span class="math inline">\(I&gt;0\)</span>, we have<span class="math inline">\(\mathrm{d}\phi/\mathrm{d}t&gt;0\)</span>, sothat the system is circling along the limit cycle. For <spanclass="math inline">\(I \to 0\)</span>, the velocity along thetrajectory around <span class="math inline">\(\phi=0\)</span> tends tozero. Therefore the frequency of the oscillation <spanclass="math inline">\(\nu=1/T(I)\)</span> decreases to zero.</p><h3 id="hopf-bifurcations">Hopf Bifurcations</h3><p>There is a continuum of trajectories.</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x127.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x128.png" /></div></div></div><p>Nevertheless, if the time scale of the <spanclass="math inline">\(u\)</span> dynamics is much faster than that ofthe <span class="math inline">\(w-\)</span>dynamics, then there is acritical regime where the sensitivity to the amplitude of the inputcurrent pulse can be extremely high.</p><p>In models with Hopf bifurcation, the peak of the response is alwaysreached with roughly the same delay, independently of the size of theinput pulse. It is the amplitude of the response that increases rapidlybut continuously.</p><h2id="separation-of-time-scales-and-reduction-to-one-dimension">Separationof time scales and reduction to one dimension</h2><p>We measure time in units of <span class="math inline">\(\tau\)</span>and take <span class="math inline">\(R=1\)</span> in (4.4) and (4.5).Then <span class="math display">\[    \frac{\mathrm{d}u}{\mathrm{d}t}=F(u,w)+I \tag{4.33}\]</span> <span class="math display">\[    \frac{\mathrm{d}w}{\mathrm{d}t}=\varepsilon G(u,w) \tag{4.34}\]</span> where <spanclass="math inline">\(\varepsilon=\tau/\tau_w\)</span>. If <spanclass="math inline">\(\tau_w\gg \tau\)</span>, then <spanclass="math inline">\(\varepsilon\ll 1\)</span>. In this situation thetime scale the governs the evolution of <spanclass="math inline">\(u\)</span> is much faster than that of <spanclass="math inline">\(w\)</span>. In the mathematical literature thelimit of <span class="math inline">\(\varepsilon \to 0\)</span> iscalled 'singular perturbation'. Oscillatory behavior for small <spanclass="math inline">\(\varepsilon\)</span> is called a 'relaxationoscillation'. Trajectories slowly follow the <spanclass="math inline">\(u-\)</span>nullcline, except at the knees of thenullcline where they jump to a different branch.</p><p><img src="/img/neu_dyn/x129.png" /></p><p>In the above figure the middle branch of the <spanclass="math inline">\(u-\)</span>nullcline (where <spanclass="math inline">\(\dot{u}&gt;0\)</span>) acts as a threshold forspike initiation.</p><p>We can exploit the separation of times scales for a further reductionof the two-dimensional system of equations to a single variable. Aninput current <span class="math inline">\(I(t)\)</span> acts on thevoltage dynamics, but has no direct influence on the variable <spanclass="math inline">\(w\)</span>. Moreover, in the limit of <spanclass="math inline">\(\varepsilon\ll 1\)</span>, the influence of thevoltage <span class="math inline">\(u\)</span> on the <spanclass="math inline">\(w-\)</span>variable via (4.34) is negligible.Hence, we can set <span class="math inline">\(w=w_{rest}\)</span> andsummarize the voltage dynamics of spike initiation by a single equation<span class="math display">\[    \frac{\mathrm{d}u}{\mathrm{d}t}=F(u,w_{rest})+I. \tag{4.35}\]</span></p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/img/neu_dyn/x130.png" /></div><div class="group-image-wrap"><imgsrc="/img/neu_dyn/x131.png" /></div></div></div><h4 id="example-piecewise-linear-nullclines">Example: Piecewise linearnullclines</h4><p><img src="/img/neu_dyn/x132.png" /></p><p><span class="math display">\[    \frac{\mathrm{d}u}{\mathrm{d}t}=f(u)-w+I \tag{4.36}\]</span> <span class="math display">\[    \frac{\mathrm{d}w}{\mathrm{d}t}=\varepsilon(bu-w) \tag{4.37}\]</span></p><p>with <span class="math inline">\(f(u)=au\)</span> for <spanclass="math inline">\(u&lt;0.5\)</span>, <spanclass="math inline">\(f(u)=a(1-u)\)</span> for <spanclass="math inline">\(0.5&lt; u&lt; 1.5\)</span> and <spanclass="math inline">\(f(u)=c_0+c_1u\)</span> for <spanclass="math inline">\(u&gt;1.5\)</span> where <spanclass="math inline">\(a\)</span>,<spanclass="math inline">\(c_1&lt;0\)</span> are parameters and <spanclass="math inline">\(c_0=-0.5a-1.5c_1\)</span>. Furthermore, <spanclass="math inline">\(b&gt;0\)</span> and <spanclass="math inline">\(0&lt;\varepsilon\ll 1\)</span>.</p><p>The rest state is at <span class="math inline">\(u=w=0\)</span>.<span class="math inline">\(u=1\)</span> acts as a threshold. Let us nowsuppose that neuron receives a weak and constant background currentduring our threshold-search experiments. A constant current shifts the<span class="math inline">\(u-\)</span>nullcline vertically upward.Hence the point where <span class="math inline">\(\dot{u}=0\)</span>shifts leftward and therefore the voltage threshold for pulsestimulation sits now at a lower value.</p>]]></content>
    
    
    <categories>
      
      <category>神经动力学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>神经科学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Information and Entropy (1)</title>
    <link href="/2022/07/01/Information-and-Entropy-1/"/>
    <url>/2022/07/01/Information-and-Entropy-1/</url>
    
    <content type="html"><![CDATA[<h1 id="bits">Bits</h1><h2 id="the-boolean-bit">The Boolean Bit</h2><h2 id="the-circuit-bit">The Circuit Bit</h2><h2 id="the-control-bit">The Control Bit</h2><h2 id="the-physical-bit">The Physical Bit</h2><h2 id="the-quantum-bit">The Quantum Bit</h2><p>There are three features of quantum mechanics, reversibility,superposition, and entanglement, that make qubits or collections ofqubits different from Boolean bits.</p><h2 id="the-classical-bit">The Classical Bit</h2><h1 id="codes">Codes</h1><h2 id="symbol-space-size">Symbol Space Size</h2><h2 id="use-of-spare-capacity">Use of Spare Capacity</h2><p>There are many strategies to deal with unused code patterns. Here aresome: - Ignore - Map to other values - Reserve for future expansion -Use for control codes - Use for common abbreviations</p><h3 id="binary-coded-decimal-bcd">Binary Coded Decimal (BCD)</h3><h3 id="genetic-code">Genetic Code</h3><h3 id="telephone-area-codes">Telephone Area Codes</h3><h3 id="ip-addresses">IP Addresses</h3><h3 id="ascii">ASCII</h3><h2 id="extension-of-codes">Extension of Codes</h2><h2 id="fixed-length-and-variable-length-codes">Fixed-Length andVariable-Length Codes</h2><h3 id="morse-code">Morse Code</h3><h2 id="detail-ascii">Detail: ASCII</h2><h2 id="detail-integer-codes">Detail: Integer Codes</h2><h3 id="binary-code">Binary Code</h3><h3 id="binary-gray-code">Binary Gray Code</h3><h3 id="s-complement">2's Complement</h3><h3 id="signmagnitude">Sign/Magnitude</h3><h3 id="s-complement-1">1's Complement</h3><h2 id="detail-the-genetic-code">Detail: The Genetic Code</h2><h2 id="detail-ip-addresses">Detail: IP Addresses</h2><h2 id="detail-morse-code">Detail: Morse Code</h2><h3 id="problem-2-universality">Problem 2: Universality</h3><p>A Boolean function <span class="math inline">\(F(A,B)\)</span> issaid to be universal if any arbitrary boolean if any arbitrary booleanfunction can be constructed by using nested <spanclass="math inline">\(F(A,B)\)</span> functions.</p><ul><li><p><span class="math inline">\(OR\)</span> and <spanclass="math inline">\(AND\)</span> are not universal functions since<span class="math inline">\(OR\)</span> (<spanclass="math inline">\(AND\)</span>) is monotonicincreasing(decreasing)</p></li><li><p><span class="math inline">\(XOR\)</span> is not universal. Wedefine a nested function <span class="math inline">\(XOR\)</span>s to bean expression <span class="math inline">\(f\)</span> drawn from the set<spanclass="math inline">\(EXPR=\{0,1,A,B,XOR(f_{\alpha},f_{\beta})\}\)</span>,where <span class="math inline">\(f_{\alpha}\)</span> and <spanclass="math inline">\(f_{\beta}\)</span> are also drawn from the set<span class="math inline">\(EXPR\)</span>, but only a finite number oftimes. Consider the property <span class="math inline">\(E\)</span>: afunction <span class="math inline">\(f_{\gamma}\)</span> has property<span class="math inline">\(E\)</span> if and only if <spanclass="math inline">\(0,2\)</span>, or <spanclass="math inline">\(4\)</span> of these values is <spanclass="math inline">\(1\)</span>. If function <spanclass="math inline">\(f\)</span> is equal to <spanclass="math inline">\(AND(A,B)\)</span>, then it does not have property<span class="math inline">\(E\)</span>. <spanclass="math inline">\(f\)</span> cannot be equal to <spanclass="math inline">\(A,B,0\)</span> or <spanclass="math inline">\(1\)</span>, and so it must be of the form <spanclass="math inline">\(XOR(f_{\alpha},f_{\beta})\)</span>. Noting that<span class="math inline">\(XOR\)</span> produces a <spanclass="math inline">\(1\)</span> only upon input of a <spanclass="math inline">\(1\)</span> and a <spanclass="math inline">\(0\)</span>, i.e., an 'unpaired' <spanclass="math inline">\(1\)</span>. We obtain that exactly one of <spanclass="math inline">\(f_{\alpha}\)</span> and <spanclass="math inline">\(f_{\beta}\)</span> has odd output, namely notsatisfying property <span class="math inline">\(E\)</span>. (assume<span class="math inline">\(f_{\alpha}\)</span> has odd output) So <spanclass="math inline">\(f_{\alpha}\)</span> is another <spanclass="math inline">\(XOR\)</span> function. Eventually you run out offunctions and thus <span class="math inline">\(f\)</span> cannot beequal to <span class="math inline">\(AND(A,B)\)</span>.</p></li><li><p><span class="math inline">\(NAND\)</span> is a universalfunction. Actually: <figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">b_15</span>=N <br><span class="hljs-attr">b_14</span>=NAND(A,B)<br><span class="hljs-attr">b_13</span>=NAND(b_14,A)<br><span class="hljs-attr">b_12</span>=NAND(A,<span class="hljs-number">1</span>)<br><span class="hljs-attr">b_11</span>=NAND(b_12,B)<br><span class="hljs-attr">b_10</span>=NAND(B,<span class="hljs-number">1</span>)<br><span class="hljs-attr">b_9</span>=NAND(NAND(A,B),NAND(b_12,b_10))<br><span class="hljs-attr">b_8</span>=NAND(NAND(b_10,b_12),<span class="hljs-number">1</span>)<br><span class="hljs-attr">b_7</span>=NAND(b_8,<span class="hljs-number">1</span>)<br><span class="hljs-attr">b_6</span>=NAND(b_9,<span class="hljs-number">1</span>)<br><span class="hljs-attr">b_5</span>=NAND(b_10,<span class="hljs-number">1</span>)<br><span class="hljs-attr">b_4</span>=NAND(b_11,<span class="hljs-number">1</span>) <br><span class="hljs-attr">b_3</span>=NAND(b_12,<span class="hljs-number">1</span>) <br><span class="hljs-attr">b_2</span>=NAND(b_14,<span class="hljs-number">1</span>) <br><span class="hljs-attr">b_1</span>=NAND(b_13,<span class="hljs-number">1</span>) <br><span class="hljs-attr">b_0</span>=Z <br></code></pre></td></tr></table></figure></p></li></ul><h1 id="compression">Compression</h1><p>two types of compression: - Lossless or reversible compression -Lossy or irreversible compression</p><h2 id="variable-length-encoding">Variable-Length Encoding</h2><p>We put off the discussion of this technique until Chapter 5.</p><h2 id="run-length-encoding">Run Length Encoding</h2><p>Ex: "a B B B B B a a a B B a a a a" could be encoded as "a 1 B 5 a 3B 2 a 4".</p><h2 id="static-dictionary">Static Dictionary</h2><h2 id="semi-adaptive-dictionary">Semi-adaptive Dictionary</h2><h2 id="dynamic-dictionary">Dynamic Dictionary</h2><p><strong>LZW compression technique</strong></p><h3 id="the-lzw-patent">The LZW Patent</h3><h2 id="irreversible-techniques">Irreversible Techniques</h2><h2 id="detail-lzw-compression">Detail: LZW Compression</h2><hr /><p>LZW编码的学习与实现https://blog.csdn.net/krossford/article/details/49157531?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3&amp;utm_source=qq&amp;utm_medium=social&amp;utm_oi=808074114588893184</p><hr /><p>LZW算法可以实现动态字典，可以做到实时编码、传输、解码（而不是将输入全部分析一遍后利用字符出现频率什么的来编码压缩）。一般来说传输会比输入延迟一个字符，这也意味着输入相比传输要提前看一个字符从而确定动态字典的编写。通常是输入<span class="math inline">\(\Rightarrow\)</span> （输入方）添加入字典<span class="math inline">\(\Rightarrow\)</span> 传输 <spanclass="math inline">\(\Rightarrow\)</span> （输出方）添加入字典 <spanclass="math inline">\(\Rightarrow\)</span>输出。在ASCII码的语境下，输入使用8位字符（0-255），而传输使用9位字符（0-511）</p><p>关于LZW算法还有一个特殊点：当输入的字符串第一次出现连续三个相同字符时（比如说rrr），将会出现传输某个（输出方）字典中没有的编码的编码。当输入第二个r之后，才会传输第一个r；输入第三个r后并不会传输（因为rr在输入方字典中出现过）；输入第三个r后的字符（比如说i）后传输rr的编码，当然此时这个编码不存在于输出方的字典中，输出方需要对该情况作特殊处理。书上给出的我看不懂&gt; Normally, on receipt of a transmitted codeword, the decoder canlook up its string in the dictionary and then output it and use itsfirst character to complete the partially formed last dictionary entry,and then start the next dictionary entry. Thus only one dictionarylookup is needed. However, the algorithm presented above uses twolookups, one for the first character, and a later one for the entirestring. Why not use just one lookup for greater efficiency? &gt;&gt;There is a special case illustrated by the transmission of code 271in this example, where the string corresponding to the received code isnot complete. The first character can be found but then before theentire string is retrieved, the entry must be completed. This happenswhen a character or a string appears for the first time three times in arow, and is therefore rare. The algorithm above works correctly, at acost of an extra lookup that is seldom needed and may slow the algorithmdown.</p><h2 id="detail-2-d-discrete-cosine-transformation">Detail: 2-D DiscreteCosine Transformation</h2><h3 id="discrete-linear-transformation">Discrete LinearTransformation</h3><h3 id="discrete-cosine-transformation">Discrete CosineTransformation</h3><p>这部分看TimothySauer的数值分析p409-p449，但注意这本书的DFT插值好像有非常大的问题.</p><h1 id="errors">Errors</h1><h2 id="extension-of-system-model">Extension of System Model</h2><p>Extend the model for information handling to include "channelcoding." The channel encoder adds bits to the message so that in case itgets corrupted in some way, the channel encoder will know that andpossibly even be able to repair the damage.</p><h2 id="how-do-errors-happen">How do Errors Happen?</h2><p>In the usual case, we will usually assume that different bits getcorrupted independently, but in some cases errors in adjacent bits arenot independent of each other, but instead have a common underlyingcause (i.e., the errors may happen in bursts).</p><h2 id="detection-vs.-correction">Detection vs. Correction</h2><p>detect the error and then let the person or system that uses theoutput know that an error has occurred.</p><p>Have the channel decoder attempt to repair the message by correctingthe error.</p><p>In both cases, extra bits are added to the messages to make themlonger, so the message contains redundancy. The channel, by allowingerrors to occur, actually introduces information. The decoder isirreversible in that it discards some information but keep the originalinformation if well designed.</p><h2 id="hamming-distance">Hamming Distance</h2><p>The number of bits that are different between the two.</p><h2 id="single-bits">Single Bits</h2><h2 id="multiple-bits">Multiple Bits</h2><h3 id="parity">Parity</h3><p>Detect: Chang the 8-bit string into 9 bits. The added bit would be 1if the number of bits equal to 1 is odd, and 0 otherwise. It is mostoften used when the likelihood of an error is very small, and there isno reason to suppose that errors of adjacent bits occur together, andthe receiver is able to request a retransmission of the data.</p><h3 id="rectangular-codes">Rectangular Codes</h3><p>Rectangular codes can provide single error correction and doubleerror detection simultaneously. Refer to p48.</p><h3 id="hamming-code">Hamming Code</h3><hr /><p>汉明码（Hamming Code）原理及实现https://www.cnblogs.com/Philip-Tell-Truth/p/6669854.html</p><hr /><h2 id="block-codes">Block Codes</h2><p>If the number of data bits in the block is <spanclass="math inline">\(k\)</span>, the the number of parity bits is <spanclass="math inline">\(n-k\)</span>, and it is customary to call such acode an <span class="math inline">\((n,k)\)</span> block code. Thus theHamming Code just described is <spanclass="math inline">\((7,4)\)</span>.</p><p>It is also customary to include in the parentheses the minimumHamming distance <span class="math inline">\(d\)</span> between any twovalid codewords, or original data items, in the form <spanclass="math inline">\((n,k,d)\)</span>. We have following blocktypes</p><table><thead><tr class="header"><th>Parity bits</th><th>Block size</th><th>Payload</th><th>Code rate</th><th>Block code type</th></tr></thead><tbody><tr class="odd"><td>2</td><td>3</td><td>1</td><td>0.33</td><td>(3,1,3)</td></tr><tr class="even"><td>3</td><td>7</td><td>4</td><td>0.57</td><td>(7,4,3)</td></tr><tr class="odd"><td>4</td><td>15</td><td>11</td><td>0.73</td><td>(15,11,3)</td></tr><tr class="even"><td>5</td><td>31</td><td>26</td><td>0.84</td><td>(31,26,3)</td></tr><tr class="odd"><td>6</td><td>63</td><td>57</td><td>0.90</td><td>(63,57,3)</td></tr><tr class="even"><td>7</td><td>127</td><td>120</td><td>0.94</td><td>(127,120,3)</td></tr><tr class="odd"><td>8</td><td>255</td><td>247</td><td>0.97</td><td>(255,247,3)</td></tr></tbody></table><h2 id="advanced-codes">Advanced Codes</h2><p>Bose-Chaudhuri-Hocquenhem (BCH) codes.</p><p>Irving S. Reed and Gustave Solomon of MIT Lincoln Laboratory. The<span class="math inline">\((256,224,5)\)</span> and <spanclass="math inline">\((224,192,5)\)</span> Reed-Solomon codes are usedin CD players.</p><h2 id="detail-check-digits">Detail: Check Digits</h2><h3 id="credit-cards">Credit Cards</h3><h3 id="isbn">ISBN</h3><h3 id="issn">ISSN</h3><h1 id="probability">Probability</h1><h2 id="events">Events</h2><p><strong>Outcome</strong> is the symbol selected, whether or not it isknown to us. <strong>Event</strong> is a subset of the possible outcomesof an experiment.</p><p>When a selection is made, then, there are several events. One is theoutcome itself. This is called a <strong>fundamental event</strong>.</p><p>The special event in which any symbol at all is selected is called<strong>universal event</strong>. The special "event" in which no symbolis seleted is called the <strong>null event</strong>. The null eventcannot happen because an outcome is only defined after a selection ismade.</p><p>A set of events which do not overlap is said to be <strong>mutuallyexclusive</strong>. A set of events, one of which is sure to happen, isknown as <strong>exhaustive</strong>. A set of events that are bothmutually exclusive and exhaustive is known as a<strong>partition</strong>. The partition that consists of all thefundamental events will be called the <strong>fundamentalpartition</strong>.</p><p>A partition consisting of a small number of events, some of which maycorrespond to many symbols, is known as a <strong>coarse-grainedpartition</strong> whereas a partition with many events is a<strong>fine-grained partition</strong>.</p><h2 id="known-outcomes">Known Outcomes</h2><h2 id="unknown-outcomes">Unknown Outcomes</h2><h2 id="joint-events-and-conditional-probabilities">Joint Events andConditional Probabilities</h2><p><span class="math display">\[    p(A,B)=p(B)p(A|B)=p(A)p(B|A) \tag{5.5}\]</span></p><p>This formula is known as Bayes' Theorem.</p><h2 id="averages">Averages</h2><h2 id="information">Information</h2><p><span class="math display">\[    I=\sum_{i}^{} p(A_i)\log _{2}\left( \frac{1}{p(A_i)}\right)\tag{5.14}\]</span></p><p>This quantity is called the <strong>entropy of a source</strong>.</p><h2 id="properties-of-information">Properties of Information</h2><p>If there are two events in the partition with probabilities <spanclass="math inline">\(p\)</span> and <spanclass="math inline">\((1-p)\)</span>, the information per symbol is<span class="math display">\[    I=p\log _{2}(\frac{1}{p})+(1-p)\log _{2}(\frac{1}{1-p}) \tag{5.16}\]</span> which attains its largest (1 bit) for <spanclass="math inline">\(p=0.5\)</span>.</p><p>For partitions with more than two possible events the information persymbol can be higher. If there are <spanclass="math inline">\(n\)</span> possible events the information persymbol lies between <span class="math inline">\(0\)</span> and <spanclass="math inline">\(\log _{2}(n)\)</span> bits, the maximum valuebeing achieved when all probabilities are equal.</p><h2 id="efficient-source-coding">Efficient Source Coding</h2><p>If a source has <span class="math inline">\(n\)</span> possiblesymbols then a fixed-length code for it would require <spanclass="math inline">\(\log _{2}(n)\)</span> bits per symbol.</p><p>There is a general procedure for constructing codes of this sortwhich are very efficient (in fact, they require an average of less tha<span class="math inline">\(I+1\)</span> bits per symbol, even if <spanclass="math inline">\(I\)</span> is considerably below <spanclass="math inline">\(\log _{2}(n)\)</span>). We'll introduce Huffmancodes below.</p><h2 id="detail-efficient-source-code">Detail: Efficient Source Code</h2><p>Huffman code: p66-p68</p>]]></content>
    
    
    <categories>
      
      <category>信息论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>常微分方程（10）</title>
    <link href="/2022/06/28/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%8810%EF%BC%89/"/>
    <url>/2022/06/28/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%8810%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="首次积分">首次积分</h1><h2 id="首次积分的定义">首次积分的定义</h2><p>考虑一般的 <span class="math inline">\(n\)</span> 阶微分方程 <spanclass="math display">\[    \frac{\mathrm{d}y_i}{\mathrm{d}x}=f_i(x,y_1,\cdots,y_n)\quad(i=1,\cdots,n),\tag{10.14}\]</span> 其中右端函数 <span class="math inline">\(f_1,\cdots,f_n\)</span> 在某个区域 <span class="math inline">\(D \subset\mathbb{R}^{n+1}\)</span> 内对 <span class="math inline">\((x,y_1,\cdots,y_n)\)</span> 是连续的，而且对 <span class="math inline">\(y_1,\cdots,y_n\)</span> 是连续可微的.</p><p>设函数 <span class="math inline">\(V=V(x,y_1,\cdots ,y_n)\)</span> 在<span class="math inline">\(D\)</span> 的某一子区域 <spanclass="math inline">\(G\)</span> 内连续，而且对 <spanclass="math inline">\(x,y_1,\cdots ,y_n\)</span> 是连续可微的. 又设<span class="math inline">\(V(x,y_1,\cdots ,y_n)\)</span>不是常数，但沿着微分方程（10.14）在区域 <spanclass="math inline">\(G\)</span> 内的任一积分曲线 <spanclass="math display">\[    \Gamma\colon \quad y_1=y_1(x),\cdots ,y_n=y_n(x) \quad (x \in J)\]</span> 函数 <span class="math inline">\(V\)</span> 取常值；亦即 <spanclass="math display">\[    V(x,y_1(x),\cdots ,y_n(x))=常数 \quad (x \in J)\]</span> 或当 <span class="math inline">\((x,y_1,\cdots ,y_n) \in\Gamma\)</span> 时，有 <span class="math display">\[    V(x,y_1,\cdots ,y_n)=常数，\]</span> 这里的常数随积分曲线 <spanclass="math inline">\(\Gamma\)</span> 而定. 则称 <spanclass="math display">\[    V(x,y_1,\cdots ,y_n)=C \tag{10.15}\]</span> 为（10.14）在区域 <span class="math inline">\(G\)</span>的<strong>首次积分</strong>，其中 <span class="math inline">\(C\)</span>是一个任意常数.</p><p>首次积分的定义可以自然地移植到高阶微分方程.</p><h2 id="首次积分的性质">首次积分的性质</h2><p><strong>定理 10.1</strong> 设函数 <spanclass="math inline">\(\Phi(x,y_1,\cdots ,y_n)\)</span> 在区域 <spanclass="math inline">\(G_1\)</span> 内是连续可微的，而且它不是常数. 则<span class="math display">\[    \Phi(x,y_1,\cdots ,y_n)=C \tag{10.18}\]</span> 是微分方程（10.14）在区域 <spanclass="math inline">\(G_1\)</span> 内的首次积分的充要条件为： <spanclass="math display">\[    \frac{\partial \Phi}{\partial x}+\frac{\partial \Phi}{\partialy_1}f_1+\cdots +\frac{\partial \Phi}{\partial y_n}f_n=0\tag{10.19}      \]</span> 是关于变量 <span class="math inline">\((x,y_1,\cdots ,y_n)\inG_1\)</span> 的一个恒等式.</p><p><strong>定理 10.2</strong>若已知（10.14）的一个首次积分（10.18），则可把微分方程（10.17）降低一阶.</p><p><strong>证</strong>：注意到首次积分 <spanclass="math inline">\(\Phi\)</span> 的偏导数不能都恒等于0，设 <spanclass="math inline">\(\displaystyle \frac{\partial \Phi}{\partialy_n}\neq 0\)</span>. 可以利用隐函数定理由首次积分（10.18）解出 <spanclass="math display">\[    y_n=g(x,y_1,\cdots ,y_{n-1},C), \tag{10.23}\]</span> 把它代入（10.14）的前 <span class="math inline">\(n-1\)</span>个式子，消去 <span class="math inline">\(y_n\)</span>. 证明得到的 <spanclass="math inline">\(n-1\)</span> 阶的微分方程的解加上 <spanclass="math inline">\(y_n=g(x,u_1(x),\cdots ,u_{n-1}(x),C)\)</span>就是（10.14）的解，只需证明满足（10.14）的最后一个等式，结合首次积分的充要条件，就得到所需结论.</p><p>设（10.14）有 <span class="math inline">\(n\)</span> 个首次积分 <spanclass="math display">\[    \Phi_i(x,y_1,\cdots ,y_n)=C_i \tag{10.28}\]</span> <span class="math inline">\((i=1,\cdots ,n)\)</span>.如果在某区域 <span class="math inline">\(G_1\)</span> 内它们的 Jacobi行列式 <span class="math display">\[    \frac{D(\Phi_1,\cdots ,\Phi_n)}{D(y_1,\cdots ,y_n)}\neq 0,\tag{10.29}\]</span> 则称它们在区域 <span class="math inline">\(G_1\)</span>内为<strong>互相独立</strong>的.</p><p><strong>定理 10.3</strong> 设已知（10.14）在区域 <spanclass="math inline">\(G_1\)</span> 内的 <spanclass="math inline">\(n\)</span> 个互相独立的首次积分（10.28）.则可由它们得到（10.14）在区域 <span class="math inline">\(G_1\)</span>内的通解 <span class="math display">\[    y_1=\varphi_1(x,C_1,\cdots ,C_n),\cdots ,y_n=\varphi_n(x,C_1,\cdots,C_n), \tag{10.30}\]</span> 其中 <span class="math inline">\(C_1,\cdots ,C_n\)</span> 为<span class="math inline">\(n\)</span>个任意常数（在允许的范围内）；而且上述通解表示了（10.14）在 <spanclass="math inline">\(G_1\)</span> 内的所有解.</p><p><strong>证</strong>：（10.29）成立，可以用隐函数定理从（10.28）解出<span class="math inline">\(y_1,\cdots ,y_n\)</span>.令其表达式为（10.30）. 将其代入（10.28），然后对 <spanclass="math inline">\(x\)</span>求导，并且利用首次积分的充要条件，就得到 <span class="math display">\[    \varphi_1&#39;=f_1,\cdots ,\varphi_n&#39;=f_n,\]</span> 这说明（10.30）确实是（10.14）的解.</p><p>对（10.28）关于 <span class="math inline">\(C_j\)</span>求导，可以得到 <span class="math display">\[    \frac{D(\varphi_1,\cdots ,\varphi_n)}{D(C_1,\cdots ,C_n)}=\left[\frac{D(\Phi_1,\cdots ,\Phi_n)}{D(y_1,\cdots ,y_n)}\right]^{-1} \neq0,        \]</span> 这说明 <span class="math inline">\(C_1,\cdots ,C_n\)</span>是互相独立的.</p><p>任取（10.14）在区域 <span class="math inline">\(G_1\)</span>内的解，考虑其在一点的初值，然后用解的唯一性定理推出解是（10.30）.证毕.</p><p>反之若已知（10.14）的通解，则由它可得到 <spanclass="math inline">\(n\)</span> 个互相独立的首次积分.</p><p>类似于秩定理，如果已知（10.14）的 <spanclass="math inline">\(k(1\leqslant k\leqslant n)\)</span>个互相独立的首次积分 <span class="math display">\[    V_i(x,y_1,\cdots ,y_n)=C_i \quad(i=1,\cdots ,k). \tag{10.35}\]</span> 即矩阵 <span class="math inline">\(\displaystyle \left(\frac{\partial V_i}{\partial y_j}\right)_{k \times n}\)</span> 的秩等于<span class="math inline">\(k\)</span>. 则利用这 <spanclass="math inline">\(k\)</span>个互相独立的首次积分（10.35）可以把（10.14）降低 <spanclass="math inline">\(k\)</span> 阶.</p><p>如果给定首次积分（10.35），又设 <spanclass="math inline">\(H(z_1,\cdots ,z_k)\)</span>是连续可微的函数，而且它不是常数，则 <span class="math display">\[    H(V_1(x,y_1,\cdots ,y_n),\cdots ,V_k(x,y_1,\cdots ,y_n))=C\]</span> 是（10.14）的一个首次积分. （但是好像并不能保证它与诸 <spanclass="math inline">\(V_i\)</span> 互相独立）</p><h2 id="首次积分的存在性">首次积分的存在性</h2><p><strong>定理 10.4</strong> 设 <spanclass="math inline">\(P_0=(x_0,y_1^{0},\cdots ,y_n^{0})\in G\)</span>.则存在 <span class="math inline">\(P_0\)</span> 点的一个领域 <spanclass="math inline">\(G_0 \subsetG\)</span>，使得微分方程（10.14）在区域 <spanclass="math inline">\(G_0\)</span> 内有 <spanclass="math inline">\(n\)</span> 个互相独立的首次积分.</p><p><strong>证</strong>：取 <span class="math inline">\(P_0\)</span>点附近的初值条件. 然后利用隐函数定理得到 <spanclass="math inline">\(n\)</span> 个互相独立的首次积分.</p><p><strong>定理 10.5</strong> 微分方程（10.14）最多只有 <spanclass="math inline">\(n\)</span> 个独立的首次积分.</p><p><strong>定理 10.6</strong> 设（10.28）是微分方程（10.14）在区域 <spanclass="math inline">\(G_0\)</span> 内的 <spanclass="math inline">\(n\)</span> 个独立的首次积分. 则在区域 <spanclass="math inline">\(G_0\)</span> 内微分方程（10.14）的任何首次积分<span class="math display">\[    V(x,y_1,\cdots ,y_n)=C\]</span> 可以用（10.28）来表达，亦即 <span class="math display">\[    V(x,y_1,\cdots ,y_n)=h[\Phi_1(x,y_1,\cdots ,y_n),\cdots,\Phi_n(x,y_1,\cdots ,y_n)], \tag{10.41}\]</span> 其中 <span class="math inline">\(h\)</span>是某个连续可微的函数.</p><p><strong>证</strong>：注意到 <span class="math display">\[    J:=\frac{\partial (\Phi_1,\cdots ,\Phi_n)}{\partial (y_1,\cdots,y_n)}\neq 0,    \]</span> 再次由隐函数定理得到， <span class="math display">\[    y_i=y_i(x,\Phi_1,\cdots ,\Phi_n),i=1,\cdots ,n,\]</span> 设 <span class="math inline">\(V\)</span>是任意一个首次积分，定义函数 <span class="math inline">\(H\)</span><span class="math display">\[    H(x,\Phi_1,\cdots ,\Phi_n) :=V(x,y_1,\cdots ,y_n).\]</span> 下证 <span class="math inline">\(H\)</span> 与 <spanclass="math inline">\(x\)</span> 无关. 计算 <spanclass="math display">\[    \frac{\partial H}{\partial x}=\frac{\partial V}{\partialx}+\sum_{i=1}^{n} \frac{\partial V}{\partial y_i}\frac{\partialy_i}{\partial x}.\]</span> 然后由隐函数定理 <span class="math display">\[    \frac{\partial y_i}{\partial x}=-\frac{1}{J} \frac{\partial(\Phi_1,\cdots ,\Phi_i,\cdots ,\Phi_n)}{\partial (y_1,\cdots ,x,\cdots,y_n)}.\]</span></p><p>根据行列式展开的规律可以得到 <span class="math display">\[    \frac{\partial H}{\partial x}=\frac{\partial V}{\partialx}-\frac{1}{J}\sum_{i=1}^{n} \frac{\partial V}{\partial y_i}\frac{\partial (\Phi_1,\cdots ,\Phi_i,\cdots ,\Phi_n)}{\partial(y_1,\cdots ,x,\cdots ,y_n)}=\frac{1}{J}\frac{\partial (V,\Phi_1,\cdots,\Phi_n)}{\partial (x,y_1,\cdots ,y_n)}\]</span> 利用首次积分的充要条件 <span class="math display">\[    \frac{\partial V}{\partial x}+\sum_{i=1}^{n} \frac{\partialV}{\partial y_i}f_i=0,\]</span> 可以发现上面的行列式为零（对 <spanclass="math inline">\(V\)</span> 和诸 <spanclass="math inline">\(\Phi\)</span> 都用充要条件，并看成一个有非零解<span class="math inline">\((1,f_1,\cdots ,f_n)\)</span>的线性方程组），所以 <span class="math inline">\(\displaystyle\frac{\partial H}{\partial x}=0\)</span>，得证.</p><p>也就是 <span class="math inline">\(V=H(\Phi_1,\cdots,\Phi_n)\)</span>.</p>]]></content>
    
    
    <categories>
      
      <category>分析学</category>
      
      <category>微分方程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Through the Heat Equation</title>
    <link href="/2022/06/24/Through-the-Heat-Equation/"/>
    <url>/2022/06/24/Through-the-Heat-Equation/</url>
    
    <content type="html"><![CDATA[<h1 id="the-heat-equation">The Heat Equation</h1><h2 id="derivation-of-the-heat-equation">Derivation of the heatequation</h2><p>Consider an infinite metal plate which we model as the plane <spanclass="math inline">\(\mathbb{R}^{2}\)</span>, and suppose we are givenan initial heat distribution at time <spanclass="math inline">\(t=0\)</span>. Let the temperature at the point<span class="math inline">\((x, y)\)</span> at time <spanclass="math inline">\(t\)</span> be denoted by <spanclass="math inline">\(u(x,y,t)\)</span>.</p><p>Consider a small square centered at <spanclass="math inline">\((x_0,y_0)\)</span> with sides parallel to the axisand of side length <span class="math inline">\(h\)</span>. The amount ofheat energy in <span class="math inline">\(S\)</span> at time <spanclass="math inline">\(t\)</span> is given by <spanclass="math display">\[    H(t)= \sigma \iint_{S} u(x,y,t) \mathrm{d}x \mathrm{d}y,\]</span> where <span class="math inline">\(\sigma&gt;0\)</span> is aconstant called the specific heat of the material. Therefore, the heatflow into <span class="math inline">\(S\)</span> is <spanclass="math display">\[    \frac{\partial H}{\partial t}= \sigma \iint_{S} \frac{\partialu}{\partial t} \mathrm{d}x\mathrm{d}y,\]</span> which is approximately equal to <span class="math display">\[    \sigma h^{2} \frac{\partial u}{\partial t}(x_0,y_0,t),\]</span> since the area of <span class="math inline">\(S\)</span> is<span class="math inline">\(h^{2}\)</span>. Now we apply Newton's law ofcooling, which states that heat flows from the higher to lowertemperature at a rate proportional to the difference, that is, thegradient.</p><p>The heat flow through the vertical side on the right is therefore<span class="math display">\[    - \kappa h \frac{\partial u}{\partial x} (x_0+\frac{h}{2},y_0,t),\]</span> where <span class="math inline">\(\kappa&gt;0\)</span> is theconductivity of the material. A similar argument for the other sidesshows that the total heat flow through the square <spanclass="math inline">\(S\)</span> is given by <spanclass="math display">\[    \kappa h \left[ \frac{\partial u}{\partialx}(x_0+\frac{h}{2},y_0,t)- \frac{\partial u}{\partialx}(x_0-\frac{h}{2},y_0,t)+\frac{\partial u}{\partialy}(x_0,y_0+\frac{h}{2},t)-\frac{\partial u}{\partialy}(x_0,y_0-\frac{h}{2},t)\right].\]</span> Applying the mean value theorem and letting <spanclass="math inline">\(h\)</span> tend to zero, we find that <spanclass="math display">\[    \frac{\sigma}{\kappa} \frac{\partial u}{\partial t}= \frac{\partial^{2}u}{\partial x^{2}}+ \frac{\partial ^{2}u}{\partial y^{2}};\]</span> this is called the <strong>time-dependent heatequation</strong>, often abbreviated to the heat equation.</p><h2 id="steady-state-heat-equation-in-the-disc">Steady-state heatequation in the disc</h2><p>After a long period of time, there is no more heat exchange, so thatthe system reaches thermal equilibrium and <spanclass="math inline">\(\displaystyle \frac{\partial u}{\partialt}=0\)</span>. In this case, the time-dependent heat equation reduces tothe <strong>steady-state heat equation</strong> <spanclass="math display">\[    \frac{\partial ^{2}u}{\partial x^{2}}+\frac{\partial ^{2}u}{\partialy^{2}}=0. \tag{1}\]</span> The operator <span class="math inline">\(\displaystyle\frac{\partial ^{2}}{\partial x^{2}}+\frac{\partial ^{2}}{\partialy^{2}}\)</span> is of such importance in mathematics and physics that itis often abbreviated as <span class="math inline">\(\Delta\)</span> andgiven a name: the Laplace operator or <strong>Laplacian</strong>. So thesteady-state heat equation is written as <span class="math display">\[    \Delta u=0,\]</span> and solutions to this equation are called <strong>harmonicfunctions</strong>.</p><p>Consider the unit disc in the plane <span class="math display">\[    D=\{(x, y) \in \mathbb{R}^{2}\colon x^{2}+y^{2}&lt;1\},\]</span> whose boundary is the unit circle <spanclass="math inline">\(C\)</span>. In polar coordinates <spanclass="math inline">\((r,\theta)\)</span>, with <spanclass="math inline">\(0\leqslant r\)</span> and <spanclass="math inline">\(0\leqslant \theta&lt;2\pi\)</span>, we have <spanclass="math display">\[    D=\{(r,\theta) \colon 0\leqslant r&lt;1\} \quad\text{and} \quadC=\{(r,\theta)\colon r=1\}.\]</span> The problem. often called the <strong>Dirichletproblem</strong> (for the Laplacian on the unit disc), is to solve thesteady-state heat equation in the unit disc subject to the boundarycondition <span class="math inline">\(u=f\)</span> on <spanclass="math inline">\(C\)</span>. This corresponds to fixing apredetermined temperature distribution on the circle, waiting a longtime, and then looking at the temperature distribution inside thedisc.</p><p>While the method of separation of variables will turn out to beuseful for equation (1), a difficulty comes from the fact that theboundary condition is not easily expressed in terms of rectangularcoordinates. Since this boundary condition is best described by thecoordinates <span class="math inline">\((r,\theta)\)</span>, namely<span class="math inline">\(u(1,\theta)=f(\theta)\)</span>, we rewritethe Laplacian in polar coordinates. An application of the chain rulegives: <span class="math display">\[    \Delta u= \frac{\partial ^{2}u}{\partial r^{2}}+\frac{1}{r}\frac{\partial u}{\partial r}+\frac{1}{r^{2}} \frac{\partial^{2}u}{\partial \theta^{2}}.\]</span> We now multiply both sides by <spanclass="math inline">\(r^{2}\)</span>, and since <spanclass="math inline">\(\Delta u=0\)</span>, we get <spanclass="math display">\[    r^{2}\frac{\partial ^{2}u}{\partial r^{2}}+r\frac{\partialu}{\partial r}=-\frac{\partial ^{2}u}{\partial \theta^{2}}.\]</span> Separating these variables, and looking for a solution of theform <span class="math inline">\(u(r,\theta)=F(r)G(\theta)\)</span>, wefind <span class="math display">\[    \frac{r^{2}F&#39;&#39;(r)+rF&#39;(r)}{F(r)}=-\frac{G&#39;&#39;(\theta)}{G(\theta)}.\]</span> Since the two sides depend on different variables, they mustboth be constant, say equal to <spanclass="math inline">\(\lambda\)</span>. We therefore get the followingequations: <span class="math display">\[    \begin{cases}        G&#39;&#39;(\theta)+\lambda G(\theta)=0, \\        r^{2} F&#39;&#39;(r)+rF&#39;(r)-\lambda F(r)=0.    \end{cases}\]</span> Since <span class="math inline">\(G\)</span> must be periodicof period <span class="math inline">\(2\pi\)</span>, this implies that<span class="math inline">\(\lambda\geqslant 0\)</span> and (as we haveseen before) that <span class="math inline">\(\lambda=m^{2}\)</span>where <span class="math inline">\(m\)</span> is an integer; hence <spanclass="math display">\[    G(\theta)= \tilde{A} \mathrm{e}^{im\theta} +\tilde{B}\mathrm{e}^{-im\theta} .\]</span> An application of Euler's identity, <spanclass="math inline">\(\mathrm{e}^{ix} =\cos x+ i \sin x\)</span>, allowsone to rewrite <span class="math inline">\(G\)</span> in terms ofcomplex exponentials, <span class="math display">\[    G(\theta)= A \mathrm{e}^{im\theta} +B \mathrm{e}^{-im \theta} .\]</span></p><p>With <span class="math inline">\(\lambda=m^{2}\)</span> and <spanclass="math inline">\(m\neq 0\)</span>, two simple solution of theequation in <span class="math inline">\(F\)</span> are <spanclass="math inline">\(F(r)=r^{m}\)</span> and <spanclass="math inline">\(F(r)=r^{-m}\)</span>. If <spanclass="math inline">\(m=0\)</span>, then <spanclass="math inline">\(F(r)=1\)</span> and <spanclass="math inline">\(F(r)=\log r\)</span> are two solutions. If <spanclass="math inline">\(m&gt;0\)</span>, we note that <spanclass="math inline">\(r^{-m}\)</span> grows unboundedly large as <spanclass="math inline">\(r\)</span> tends to zero, so <spanclass="math inline">\(F(r)G(\theta)\)</span> is unbounded at theoriginl; the same occurs when <span class="math inline">\(m=0\)</span>and <span class="math inline">\(F(r)= \log r\)</span>. We reject thesesolutions as countrary to our intuition. Therefore, we are left with thefollowing special functions: <span class="math display">\[    u_{m}(r,\theta)= r^{\lvert m \rvert }\mathrm{e}^{im\theta} , \quad m\in \mathbb{Z}.\]</span> We now make the important observation that (1) is linear, andso as in the case of the vibrating string, we may superpose the abovespecial solutions to obtain the presumed general solution: <spanclass="math display">\[    u(r,\theta)= \sum_{m=-\infty}^{\infty} a_m r^{\lvert m \rvert}\mathrm{e}^{im \theta} .\]</span> If this expression gave all the solutions to the steady-stateheat equation, then for a reasonable <spanclass="math inline">\(f\)</span> we should have <spanclass="math display">\[    u(1,\theta)= \sum_{m=-\infty}^{\infty} a_m \mathrm{e}^{im \theta} =f(\theta).\]</span> We therefore ask again in this context: given any reasonablefunction <span class="math inline">\(f\)</span> on <spanclass="math inline">\([0,2\pi]\)</span> with <spanclass="math inline">\(f(0)= f(2\pi)\)</span>, can we find coefficients<span class="math inline">\(a_m\)</span> so that <spanclass="math display">\[    f(\theta)= \sum_{m=-\infty}^{\infty} a_m \mathrm{e}^{im \theta}\]</span></p><blockquote><p>solution to Euler equation: <span class="math display">\[r^{2}F&#39;&#39;(r)+rF&#39;(r)-n^{2}F(r)=0,\]</span> which are twice differentiable when <spanclass="math inline">\(r&gt;0\)</span>, are given by linear combinationsof <span class="math inline">\(r^{n}\)</span> ad <spanclass="math inline">\(r^{-n}\)</span> when <spanclass="math inline">\(n\neq 0\)</span>, and <spanclass="math inline">\(1\)</span> and <span class="math inline">\(\logr\)</span> when <span class="math inline">\(n=0\)</span>. Write <spanclass="math inline">\(F(r)=g(r)r^{n}\)</span>, then <spanclass="math inline">\(rg&#39;(r)+2ng(r)=c\)</span> where <spanclass="math inline">\(c\)</span> is a constant.</p></blockquote><p><strong>Dirichlet problem in the rectangle</strong> <spanclass="math inline">\(R=\{(x, y)\colon 0\leqslant x\leqslant\pi,0\leqslant y\leqslant 1\}\)</span> and <span class="math display">\[    u(x,0)=f_0(x) \quad u(x,1)=f_1(x) \quad u(0,y)=u(\pi,y)=0,\]</span> If <span class="math inline">\(f_0\)</span> and <spanclass="math inline">\(f_1\)</span> have Fourier expansions <spanclass="math display">\[    f_0(x)=\sum_{k=1}^{\infty} A_k \sin kx \quad f_1(x)=\sum_{k=1}^{\infty} B_k \sin kx,\]</span> then <span class="math display">\[    u(x, y)= \sum_{k=1}^{\infty} \left( \frac{\sinh k(1-y)}{\sinh k}A_k+\frac{\sinh ky}{\sinh k}B_k\right) \sin kx\]</span></p><h2 id="the-poisson-kernel-and-dirichlets-problem-in-the-unit-disc">ThePoisson kernel and Dirichlet's problem in the unit disc</h2><p>To adapt Abel summability to the context of Fourier series, we definethe Abel means of the function $f() _{n=-}^{} a_n ^{in } $ by <spanclass="math display">\[    A_r(f)(\theta)= \sum_{n=-\infty}^{\infty} r^{\lvert n \rvert }a_n\mathrm{e}^{in \theta} .\]</span> It is natural to write <spanclass="math inline">\(c_0=a_0\)</span>, and <spanclass="math inline">\(c_n= a_n \mathrm{e}^{in \theta}+a_{-n}\mathrm{e}^{-in \theta}\)</span> for <spanclass="math inline">\(n&gt;0\)</span>.</p><p>Since <span class="math inline">\(f\)</span> is integrable, $a_n $ isuniformly bounded in <span class="math inline">\(n\)</span>, so that<span class="math inline">\(A_r(f)\)</span> converges absolutely anduniformly for each <span class="math inline">\(0\leqslantr&lt;1\)</span>. <span class="math display">\[    A_r(f)(\theta)=(f * P_r)(\theta),\]</span> where <span class="math inline">\(P_{r}(\theta)\)</span> isthe <strong>Poisson kernel</strong> given by <spanclass="math display">\[    P_{r}(\theta)= \sum_{n=-\infty}^{\infty} r^{\lvert n \rvert}\mathrm{e}^{in \theta}\]</span> Actually, if <span class="math inline">\(0\leqslantr&lt;1\)</span>, then <span class="math display">\[    P_{r}(\theta)= \frac{1-r^{2}}{1-2r \cos \theta+r^{2}}.\]</span> The Poisson kernel is a good kernel, as <spanclass="math inline">\(r\)</span> tends to <spanclass="math inline">\(1\)</span> from below.</p><p><strong>Theorem</strong> The Fourier series of an integrable functionon the circle is Abel summable to <span class="math inline">\(f\)</span>at every point of continuity. Moreover, if <spanclass="math inline">\(f\)</span> is continuous on the circle, then theFourier series of <span class="math inline">\(f\)</span> is uniformlyAbel summable to <span class="math inline">\(f\)</span>.</p><p>We now return to Dirichlet problem in the unit disc with boundarycondition <span class="math inline">\(u=f\)</span> on the circle. Weexpected that a solution was given by <span class="math display">\[    u(r,\theta)= \sum_{m=-\infty}^{\infty} a_mr^{\lvert m \rvert}\mathrm{e}^{im \theta} ,\]</span> where <span class="math inline">\(a_m\)</span> was the <spanclass="math inline">\(m^{th}\)</span> Fourier coefficient of <spanclass="math inline">\(f\)</span>. In other words, we were led to take<span class="math display">\[    u(r,\theta)= A_{r}(f)(\theta)= \frac{1}{2\pi} \int_{-\pi}^{\pi}f(\varphi)P_{r}(\theta-\varphi) \mathrm{d}\varphi.\]</span></p><p><strong>Theorem</strong> Let <span class="math inline">\(f\)</span>be an integrable function defined on the unit circle. Then the function<span class="math inline">\(u\)</span> defined in the unit disc by thePoisson integral <span class="math display">\[    u(r,\theta)=(f*P_{r})(\theta)\]</span> has the following properties: - u has two continuousderivatives in the unit disc and satisfies <spanclass="math inline">\(\Delta u=0\)</span>. - If <spanclass="math inline">\(\theta\)</span> is any point of continuity of<span class="math inline">\(f\)</span>, then <spanclass="math display">\[    \lim_{r \to 1}u(r,\theta)=f(\theta).\]</span> If <span class="math inline">\(f\)</span> is continuouseverywhere, then this limit is uniform. - If <spanclass="math inline">\(f\)</span> is continuous, then <spanclass="math inline">\(u(r,\theta)\)</span> is the unique solution to thesteady-state heat equation in the disc which satisfies the above twoconditions.</p><p>We only prove the last property. Suppose <spanclass="math inline">\(v\)</span> solves the steady-state heat equationin the disc and converges to <span class="math inline">\(f\)</span>uniformly as <span class="math inline">\(r\)</span> tends to <spanclass="math inline">\(1\)</span> from below. For each fixed <spanclass="math inline">\(r\)</span> with <spanclass="math inline">\(0&lt;r&lt;1\)</span>, the function <spanclass="math inline">\(v(r,\theta)\)</span> has a Fourier series <spanclass="math display">\[    \sum_{n=-\infty}^{\infty} a_n(r)\mathrm{e}^{in \theta}\quad\text{where}\quad a_n(r)=\frac{1}{2\pi} \int_{-\pi}^{\pi}v(r,\theta)\mathrm{e}^{-in \theta}  \mathrm{d}\theta.     \]</span> Taking into account that <spanclass="math inline">\(v(r,\theta)\)</span> solves the equation <spanclass="math display">\[    \frac{\partial ^{2}v}{\partial r^{2}}+\frac{1}{r}\frac{\partialv}{\partial r}+\frac{1}{r^{2}}\frac{\partial ^{2}v}{\partial\theta^{2}}=0, \tag{7}\]</span> we find that <span class="math display">\[    a&#39;&#39;_n(r)+\frac{1}{r}a_n&#39;(r)-\frac{n^{2}}{r^{2}}a_n(r)=0. \tag{8}\]</span> Indeed, we may first multiply (7) by $^{-in } $ and integratein <span class="math inline">\(\theta\)</span>. Then, since <spanclass="math inline">\(v\)</span> is periodic, two integrations by partsgive <span class="math display">\[    \frac{1}{2\pi} \int_{-\pi}^{\pi}  \frac{\partial ^{2}v}{\partial\theta^{2}}(r,\theta)\mathrm{e}^{-in \theta}  \mathrm{d}\theta = -n^{2}a_n(r).\]</span> Finally, we may interchange the order of differentiation andintegration, which is permissible since <spanclass="math inline">\(v\)</span> has two continuous derivatives; thisyields (8).</p><p>Therefore, we must have <span class="math inline">\(a_n(r)= A_nr^{n}+B_n r^{-n}\)</span> for some constants <spanclass="math inline">\(A_n\)</span> and <spanclass="math inline">\(B_n\)</span>, when <span class="math inline">\(n\neq 0\)</span>. To evaluate the constants, we first observe that eachterm <span class="math inline">\(a_n(r)\)</span> is bounded because<span class="math inline">\(v\)</span> is bounded, therefore <spanclass="math inline">\(B_n=0\)</span>. Tho find <spanclass="math inline">\(A_n\)</span> we let <span class="math inline">\(r\to 1\)</span>. Since <span class="math inline">\(v\)</span> convergesuniformly to <span class="math inline">\(f\)</span> we find that <spanclass="math display">\[    A_n= \frac{1}{2\pi} \int_{-\pi}^{\pi} f(\theta)\mathrm{e}^{-in\theta}  \mathrm{d}\theta.\]</span> By a similar argument, this formula also holds when <spanclass="math inline">\(n=0\)</span>. Our conclusion is that for each<span class="math inline">\(0&lt;r&lt;1\)</span>, the Fourier series of<span class="math inline">\(v\)</span> is given by the series of <spanclass="math inline">\(u(r,\theta)\)</span>, so by the uniqueness ofFourier series for continuous functions, we must have <spanclass="math inline">\(u=v\)</span>.</p><p><strong>Remark.</strong> By part (iii) of the theorem, we mayconclude that if <span class="math inline">\(u\)</span> solves <spanclass="math inline">\(\Delta u=0\)</span> in the disc, and converges to<span class="math inline">\(0\)</span> uniformly as <spanclass="math inline">\(r \to 1\)</span>, then <spanclass="math inline">\(u\)</span> must be identically <spanclass="math inline">\(0\)</span>. However, if uniform convergence isreplaced by pointwise convergence, this conclusion may fail.</p><p>Actually, If <span class="math inline">\(P_{r}(\theta)\)</span>denotes the Poisson kernel, then the function <spanclass="math display">\[    u(r,\theta)= \frac{\partial P_r}{\partial \theta}=\frac{2(r^{2}-1)r\sin \theta}{(1-2r\cos \theta+r^{2})^{2}}\]</span> defined for <span class="math inline">\(0\leqslantr&lt;1\)</span> and <span class="math inline">\(\theta \in\mathbb{R}\)</span>, satisfies: - <span class="math inline">\(\Deltau=0\)</span> in the disc. - <span class="math inline">\(\lim_{r \to1}u(r,\theta)=0\)</span> for each <spanclass="math inline">\(\theta\)</span>.</p><p>However, <span class="math inline">\(u\)</span> is not identicallyzero.</p><hr /><p>Solve Laplace's equation <span class="math inline">\(\Deltau=0\)</span> in the semi infinite strip <span class="math display">\[    S=\{(x, y)\colon 0&lt;x&lt;1,0&lt;y\},\]</span> subject to the following boundary conditions <spanclass="math display">\[    \begin{cases}        u(0,y)=0 \quad y\geqslant 0, \\        u(1,y)=0 \quad y\geqslant 0,        u(x,0)=f(x) \quad 0\leqslant x\leqslant 1       \end{cases}\]</span> where <span class="math inline">\(f\)</span> is a givenfunction, with of course <spanclass="math inline">\(f(0)=f(1)=0\)</span>. Write <spanclass="math display">\[    f(x)= \sum_{n=1}^{\infty} a_n \sin (n \pi x)\]</span> and expand the general solution in terms of the specialsoluions given by <span class="math display">\[    u_n(x,y)= \mathrm{e}^{-n\pi y} \sin (n\pi x).\]</span> Express <span class="math inline">\(u\)</span> as an integralinvolving <span class="math inline">\(f\)</span>, analogous to thePoisson integral formula.</p><p>Answer:</p><p>By considering the odd extension of <spanclass="math inline">\(f\)</span> and following the derivation ofPoisson's kernel with $^{-y} $ and $^{i t} $ replacing <spanclass="math inline">\(r\)</span> and $^{it} $, respectively, we obtain<span class="math display">\[    u(x,y)= \frac{1}{2} \int_{-1}^{1} f(t)Q_y(x-t) \mathrm{d}t\]</span> where <span class="math display">\[    Q_y(t)= \frac{1-\mathrm{e}^{-2\pi y} }{1-2\mathrm{e}^{-\pi y} \cos\pi t+ \mathrm{e}^{-2\pi y} }.\]</span> or, using the fact that <span class="math inline">\(f\)</span>is odd, we have the alternate form <span class="math display">\[    u(x,y)=\frac{1}{2} \int_{0}^{1} f(t)Q(x,t) \mathrm{d}t\]</span> where <span class="math display">\[    Q(x,t)=\frac{1-\mathrm{e}^{-2\pi y} }{1-2\mathrm{e}^{-\pi y} \cos\pi (x-t)+ \mathrm{e}^{-2\pi y} }-\frac{1-\mathrm{e}^{-2\pi y}}{1-2\mathrm{e}^{-\pi y} \cos \pi (x+t)+ \mathrm{e}^{-2\pi y} }.\]</span></p><hr /><p>Consider the Dirichlet problem in the annulus defined by <spanclass="math inline">\(\{(r,\theta)\colon \rho&lt;r&lt;1\}\)</span>,where <span class="math inline">\(0&lt;\rho&lt;1\)</span> is the innerradius. The problem is to solve <span class="math display">\[    \frac{\partial ^{2}u}{\partial r^{2}}+\frac{1}{r}\frac{\partialu}{\partial r}+\frac{1}{r^{2}} \frac{\partial ^{2}u}{\partial\theta^{2}}=0\]</span> subject to the boundary conditions <spanclass="math display">\[    \begin{cases}        u(1,\theta)=f(\theta), \\        u(\rho,\theta)=g(\theta),    \end{cases}\]</span> where <span class="math inline">\(f\)</span> and <spanclass="math inline">\(g\)</span> are given continuous functions.</p><p>Arguing as we have previously for the Dirichlet problem in the disc,we can hope to write <span class="math display">\[    u(r,\theta)= \sum_{}^{} c_n(r) \mathrm{e}^{in \theta}\]</span> with <spanclass="math inline">\(c_n(r)=A_nr^{n}+B_nr^{-n},n\neq 0\)</span>. Set<span class="math display">\[    f(\theta) \sim \sum_{}^{} a_n \mathrm{e}^{in \theta}  \quad\text{and}\quad g(\theta) \sim \sum_{}^{} b_n \mathrm{e}^{in \theta}.       \]</span> We want <span class="math inline">\(c_n(1)=a_n\)</span> and<span class="math inline">\(c_n(\rho)=b_n\)</span>. This leads to thesolution <span class="math display">\[    u(r,\theta)= \sum_{n\neq 0}^{} \left(\frac{1}{\rho^{n}-\rho^{-n}}\right)[((\rho/r)^{n}-(r/\rho)^{n})a_n+(r^{n}-r^{-n})b_n]\mathrm{e}^{in \theta}+a_0+(b_0-a_0) \frac{\log r}{\log \rho}.  \]</span></p><p>Show that as a result we have <span class="math display">\[    u(r,\theta)-(P_{r}*f)(\theta)\to 0 \quad \text{as}\ r\to 1\\text{uniformly in}\ \theta\]</span> and <span class="math display">\[    u(r,\theta)-(P_{\rho/r}*g)(\theta)\to 0 \quad \text{as}\ r\to \rho\\text{uniformly in}\ \theta\]</span></p><h2 id="the-heat-equation-on-the-circle">The heat equation on thecircle</h2><p>As a final illustration, we return to the original problem of heatdiffusion considered by Fourier.</p><p>Suppose we are given an initial temperature distribution at <spanclass="math inline">\(t=0\)</span> on a ring and that we are asked todescribe the temperature at points on the ring at times <spanclass="math inline">\(t&gt;0\)</span>.</p><p>The ring is modeled by the unit circle. A point on this circle isdescribed by its angle <span class="math inline">\(\theta=2\pix\)</span>, where the variable <span class="math inline">\(x\)</span>lies between <span class="math inline">\(0\)</span> and <spanclass="math inline">\(1\)</span>. If <spanclass="math inline">\(u(x,t)\)</span> denotes the temperature at time<span class="math inline">\(t\)</span> of a point described by the angle<span class="math inline">\(\theta\)</span>, then consideration similarto the ones given in Chapter 1 show that <spanclass="math inline">\(u\)</span> satisfies the differential equation<span class="math display">\[    \frac{\partial u}{\partial t}= c\frac{\partial ^{2}u}{\partialx^{2}}.  \tag{9}\]</span></p><p>The constant <span class="math inline">\(c\)</span> is a positivephysical constant which depends on the material of which thhe ring ismade. After rescaling the time variable, we may assume that <spanclass="math inline">\(c=1\)</span>. If <spanclass="math inline">\(f\)</span> is our initial data, we impose thecondition <span class="math display">\[    u(x,0)=f(x).\]</span> To solve the problem, we separate variables and look forspecial solutions of the form <span class="math display">\[    u(x,t)=A(x)B(t).\]</span> Then inserting this expression for <spanclass="math inline">\(u\)</span> into the heat equation we get <spanclass="math display">\[    \frac{B&#39;(t)}{B(t)}= \frac{A&#39;&#39;(x)}{A(x)}.\]</span> Both sides are therefore constant, say equal to <spanclass="math inline">\(\lambda\)</span>. Since <spanclass="math inline">\(A\)</span> must be periodic of period <spanclass="math inline">\(1\)</span>, we see that the only possibility is<span class="math inline">\(\lambda=-4\pi^{2}n^{2}\)</span>, where <spanclass="math inline">\(n \in \mathbb{Z}\)</span>. Then <spanclass="math inline">\(A\)</span> is a linear combination of theexponentials $^{2inx} $ and $^{-2inx} $, and <spanclass="math inline">\(B(t)\)</span> is a multiple of$<sup>{-4</sup>{2}n^{2}t} $. By superposing these solutions, we are ledto <span class="math display">\[    u(x,t)=\sum_{n=-\infty}^{\infty} a_n \mathrm{e}^{-4\pi^{2}n^{2}t}\mathrm{e}^{2\pi inx}, \tag{10}\]</span> where, setting <span class="math inline">\(t=0\)</span>, wesee that <span class="math inline">\(\{a_n\}\)</span> are the Fouriercoefficients of <span class="math inline">\(f\)</span>.</p><p>Note that when <span class="math inline">\(f\)</span> is Riemannintegrable, the coefficients <span class="math inline">\(a_n\)</span>are bounded, and since the factor <spanclass="math inline">\(\mathrm{e}^{-4\pi^{2}n^{2}t}\)</span> tends tozero extremely fast, the series defining <spanclass="math inline">\(u\)</span> converges. In fact, in this case, <spanclass="math inline">\(u\)</span> is twice differentiable and solvesequation (9).</p><p>For a better understanding of the properties of our solution (10), wewrite it as <span class="math display">\[    u(x,t)=(f*H_t)(x),\]</span> where <span class="math inline">\(H_t\)</span> is the<strong>heat kernel for the circle</strong>, given by <spanclass="math display">\[    H_{t}(x)=\sum_{n=-\infty}^{\infty} \mathrm{e}^{-4\pi^{2}n^{2}t}\mathrm{e}^{2\pi inx} , \tag{11}\]</span> and where the convolution for functions with period <spanclass="math inline">\(1\)</span> is defined by <spanclass="math display">\[    (f*g)(x)= \int_{0}^{1} f(x-y)g(y) \mathrm{d}y.\]</span></p><p>The natural question with regard to the boundary condition is thefollowing: do we have $u(x,t) f(x) $ as <spanclass="math inline">\(t\)</span> tends to <spanclass="math inline">\(0\)</span>, and in what sense? A simpleapplication of the Parsevel identity shows that this limit holds in themean square sense, namely <span class="math display">\[    \int_{0}^{1} \lvert u(x,t)-f(x) \rvert ^{2} \mathrm{d}x \to 0 \quad\text{as}\ t\to 0.\]</span></p><p>An analogy between the heat kernel and the Poisson kernel:</p><p><span class="math display">\[    u(\theta,\tau)= \sum_{}^{} a_n \mathrm{e}^{-n^{2}\tau}\mathrm{e}^{in \theta} =(f* h_{\tau})(\theta)\]</span> of the equation <span class="math display">\[    \frac{\partial u}{\partial \tau}=\frac{\partial ^{2}u}{\partial\theta^{2}} \quad \text{with}\ 0\leqslant \theta\leqslant 2\pi \\text{and} \ \tau&gt;0,\]</span> with boundary condition $u(,0)= f() <em>{}^{} a_n ^{in } $.Here <span class="math inline">\(h_{\tau}(\theta)=\sum_{n=-\infty}^{\infty} \mathrm{e}^{-n^{2}\tau} \mathrm{e}^{in\theta}\)</span>. This version of the heat kernel on <spanclass="math inline">\([0,2\pi]\)</span> is the analogue of the Poissonkernel, which can be written as $P</em>{r}()=_{n=-}^{} e^{-n }^{in } $with $r= ^{-} $(and so <span class="math inline">\(0&lt;r&lt;1\)</span>corresponds to <span class="math inline">\(\tau&gt;0\)</span>).</p><p>Unlike in the case of the Poisson kernel, there is no elementaryformula for the heat kernel. Nevertheless, it turns out that it is agood kernel. The proof is not obvious and requires the use of thecelebrated Poisson summation formula. As a corollary, we will also findthat <span class="math inline">\(H_t\)</span> is everywhere positive, afact that is also not obvious from its defining expression (11). We can,however, give the following heuristic argument for the positivity of<span class="math inline">\(H_t\)</span>. Suppose that we begin with aninitial temperature distribution <span class="math inline">\(f\)</span>which is everywhere <span class="math inline">\(\leqslant 0\)</span>.Then it is physically reasonable to expect <spanclass="math inline">\(u(x,t)\leqslant 0\)</span> for all <spanclass="math inline">\(t\)</span> since heat travels from hot to cold.Now <span class="math display">\[    u(x,t)= \int_{0}^{1} f(x-y)H_t(y) \mathrm{d}y.\]</span> If <span class="math inline">\(H_{t}\)</span> is negative forsome <span class="math inline">\(x_0\)</span>, then we may choose <spanclass="math inline">\(f\leqslant 0\)</span> supported near <spanclass="math inline">\(0\)</span>, and this would imply <spanclass="math inline">\(u(x_0,t)&gt;0\)</span>, which is acontradiction.</p><h2 id="the-time-dependent-heat-equation-on-the-real-line">Thetime-dependent heat equation on the real line</h2><p>Here we study the analogous problem on the real line.</p><p>Consider an infinite rod, which we model by the real line, andsuppose that we are given an initial temperature distribution <spanclass="math inline">\(f(x)\)</span> on the rod at time <spanclass="math inline">\(t=0\)</span>. We wish now to determine thetemperature <span class="math inline">\(u(x,t)\)</span> at a point <spanclass="math inline">\(x\)</span> at time <spanclass="math inline">\(t&gt;0\)</span>. Considerations similar to theones given in Chapter 1 show that when <spanclass="math inline">\(u\)</span> is appropriately normalized, it solvesthe following PDE: <span class="math display">\[    \frac{\partial u}{\partial t}= \frac{\partial ^{2}u}{\partialx^{2}}, \tag{12}\]</span> called the <strong>heat equation</strong>. The initialcondition we impose is <spanclass="math inline">\(u(x,0)=f(x)\)</span>.</p><p>Just as in the case of the circle, the solution is given in terms ofa convolution. Indeed, define the <strong>heat kernel</strong> of theline by <span class="math display">\[    \mathcal{H}_{t}(x)= K_{\delta}(x), \quad \text{with} \ \delta=4\pit,\]</span> so that <span class="math display">\[    \mathcal{H}_{t}(x)=\frac{1}{(4\pi t)^{1/2}} \mathrm{e}^{-x^{2}/4t}\quad \text{and} \quad \hat{\mathcal{H}}_{t}(\xi)= \mathrm{e}^{-4\pi^{2}t \xi ^{2}} .\]</span></p><p>Taking the Fourier transform of equation (12) in the <spanclass="math inline">\(x\)</span> variable leads to <spanclass="math display">\[    \frac{\partial \hat{u}}{\partial t}(\xi, t)= -4\pi^{2} \xi^{2}\hat{u}(\xi,t).\]</span> Fixing <span class="math inline">\(\xi\)</span>, this is anODE in the variable <span class="math inline">\(t\)</span>, so thereexists a constant <span class="math inline">\(A(\xi)\)</span> so that<span class="math display">\[    \hat{u}(\xi,t)= A(\xi) \mathrm{e}^{-4\pi^{2}\xi^{2}t} .\]</span> We may also take the Fourier transform of the initialcondition and obtain <span class="math inline">\(\hat{u}(\xi,0)=\hat{f}(\xi)\)</span>, hence <spanclass="math inline">\(A(\xi)=\hat{f}(\xi)\)</span>. This leads to thefollowing theorem.</p><p><strong>Theorem 2.1</strong> Given <span class="math inline">\(f\in\mathcal{S}(\mathbb{R})\)</span>, let <span class="math display">\[    u(x,t)=(f*\mathcal{H}_{t})(x) \quad t&gt;0\]</span> where <span class="math inline">\(\mathcal{H}_{t}\)</span> isthe heat kernel. Then: 1. The function <spanclass="math inline">\(u\)</span> is <spanclass="math inline">\(C^{2}\)</span> when <span class="math inline">\(x\in \mathbb{R}\)</span> and <span class="math inline">\(t&gt;0\)</span>,and <span class="math inline">\(u\)</span> solves the heat equation. 2.<span class="math inline">\(u(x,t) \to f(x)\)</span> uniformly in <spanclass="math inline">\(x\)</span> as <span class="math inline">\(t \to0\)</span>. Hence if we set <spanclass="math inline">\(u(x,0)=f(x)\)</span>, then <spanclass="math inline">\(u\)</span> is continuous on the closure of theupper half-plane <spanclass="math inline">\(\overline{\mathbb{R}_{+}^{2}}=\{(x,t)\colon x\in\mathbb{R}, t\geqslant 0\}\)</span>. 3. <spanclass="math inline">\(\int_{-\infty}^{\infty} \lvert u(x,t)-f(x) \rvert^{2} \mathrm{d}x \to 0\)</span> as <span class="math inline">\(t \to0\)</span>.</p><p><strong>proof</strong> Because <span class="math inline">\(u= f*\mathcal{H}_{t}\)</span>, taking the Fourier transform in the <spanclass="math inline">\(x-variable\)</span> gives <spanclass="math inline">\(\hat{u}=\hat{f} \hat{\mathcal{H}}_{t}\)</span>,and so $(,t)=() <sup>{-4</sup>{2}^{2}t} $. The Fourier inversion formulagives <span class="math display">\[    u(x,t)= \int_{-\infty}^{\infty} \hat{f}(\xi)\mathrm{e}^{-4\pi^{2}t\xi^{2}} \mathrm{e}^{2\pi i \xi x}  \mathrm{d}\xi .\]</span> By differentiating under the integral sign, one verifies 1. Infact, one observes that <span class="math inline">\(u\)</span> isindefinitely differentiable. Note that 2 is an immediate consequence ofCorollary 1.7. Finally, by Plancherel's formula, we have <spanclass="math display">\[    \begin{aligned}        \int_{-\infty}^{\infty} \lvert u(x,t)-f(x) \rvert ^{2}\mathrm{d}x &amp;= \int_{-\infty}^{\infty} \lvert\hat{u}(\xi,t)-\hat{f}(\xi) \rvert ^{2} \mathrm{d}\xi \\        &amp;= \int_{-\infty}^{\infty} \lvert \hat{f}(\xi) \rvert ^{2}\lvert \mathrm{e}^{-4\pi^{2}t \xi^{2}} -1 \rvert ^{2} \mathrm{d}\xi.    \end{aligned}\]</span></p><p>To see that this last integral goes to <spanclass="math inline">\(0\)</span> as <span class="math inline">\(t \to0\)</span>, we argue as follows: since <spanclass="math inline">\(\lvert \mathrm{e}^{-4\pi^{2}t \xi ^{2}} -1 \rvert\leqslant 2\)</span> and <span class="math inline">\(f\in\mathcal{S}(\mathbb{R})\)</span>, we can find <spanclass="math inline">\(N\)</span> so that <span class="math display">\[    \int_{\lvert \xi \rvert \geqslant N}^{} \lvert \hat{f}(\xi) \rvert^{2}\lvert \mathrm{e}^{-4\pi^{2}t \xi^{2}} -1 \rvert  \mathrm{d}\xi&lt;\varepsilon,\]</span> and for all small <span class="math inline">\(t\)</span> wehave <span class="math inline">\(\sup_{\lvert \xi \rvert \leqslantN}\lvert \hat{f}(\xi) \rvert ^{2}\lvert \mathrm{e}^{-4\pi^{2}t \xi^{2}}-1 \rvert ^{2}&lt;\varepsilon/2N\)</span> since <spanclass="math inline">\(\hat{f}\)</span> is bounded. Thus <spanclass="math display">\[    \int_{\lvert \xi \rvert \leqslant N}^{} \lvert \hat{f}(\xi) \rvert^{2}\lvert \mathrm{e}^{-4\pi^{2}t \xi^{2}} -1 \rvert ^{2} \mathrm{d}\xi&lt; \varepsilon \quad \text{for all small}\ t.     \]</span> This completes the proof of the theorem.</p><h2 id="the-steady-state-heat-equation-in-the-upper-half-plane">Thesteady-state heat equation in the upper half-plane</h2><p>The equation we are now concerned with is <spanclass="math display">\[    \Delta u= \frac{\partial ^{2}u}{\partial x^{2}}+\frac{\partial^{2}u}{\partial y^{2}}=0  \tag{13}\]</span> in the upper half-plane <spanclass="math inline">\(\mathbb{R}_{+}^{2}=\{(x, y)\colon x\in\mathbb{R},y&gt;0\}\)</span>. The boundary condition we require is <spanclass="math inline">\(u(x,0)=f(x)\)</span>. The kernel that solves thisproblem is called the <strong>Poisson kernel</strong> for the upperhalf-plane, and is given by <span class="math display">\[    \mathcal{P}_{y}(x)=\frac{1}{\pi}\frac{y}{x^{2}+y^{2}}\quad\text{where} \ x\in \mathbb{R}\ \text{and} y&gt;0.\]</span> This is the analogue of the Poisson kernel for the disc.</p><p>Note that for each fixed <span class="math inline">\(y\)</span> thekernel <span class="math inline">\(\mathcal{P}_{y}\)</span> is only ofmoderate decrease as a function of <spanclass="math inline">\(x\)</span>, so we will use the theory of theFourier transform appropriate for these types of functions.</p><p>We proceed as in the case of the time-dependent heat equation, bytaking the Fourier transform of equation (13) in the <spanclass="math inline">\(x\)</span> variable, thereby obtaining <spanclass="math display">\[    -4\pi^{2}\xi^{2}\hat{u}(\xi,y)+\frac{\partial ^{2}\hat{u}}{\partialy^{2}}(\xi,y)=0     \]</span> with the boundary condition <spanclass="math inline">\(\hat{u}(\xi,0)=\hat{f}(\xi)\)</span>. The generalsolution of this ODE in <span class="math inline">\(y\)</span> (with<span class="math inline">\(\xi\)</span> fixed) takes the form <spanclass="math display">\[    \hat{u}(\xi,y)= A(\xi) \mathrm{e}^{-2\pi \lvert \xi \rvert y}+B(\xi) \mathrm{e}^{2\pi\lvert \xi \rvert y} .\]</span> If we disregard the second term because of its rapidexponential increase we find, after setting <spanclass="math inline">\(y=0\)</span>, that <span class="math display">\[    \hat{u}(\xi,y)=\hat{f}(\xi) \mathrm{e}^{-2\pi \lvert \xi \rvert y} .\]</span> Therefore <span class="math inline">\(u\)</span> is given interms of the convolution of <span class="math inline">\(f\)</span> witha kernel whose Fourier transform is $^{-2y} $. This is precisely thePoisson kernel given above, as we prove next.</p><p><strong>Lemma 2.4</strong> The following two identities hold: <spanclass="math display">\[    \int_{-\infty}^{\infty} \mathrm{e}^{-2\pi\lvert \xi \rvert y}\mathrm{e}^{2\pi i \xi x}  \mathrm{d}\xi = \mathcal{P}_y(x),\]</span> <span class="math display">\[    \int_{-\infty}^{\infty} \mathcal{P}_y(x)\mathrm{e}^{-2\pi ix \xi}\mathrm{d}x= \mathrm{e}^{-2\pi\lvert \xi \rvert y} .\]</span></p><p><strong>Lemma 2.5</strong> The Poisson kernel is a good kernel on<span class="math inline">\(\mathbb{R}\)</span> as <spanclass="math inline">\(y \to 0\)</span>.</p><p>The following theorem establishes the existence of a solution to ourproblem.</p><p><strong>Theorem 2.6</strong> Given <span class="math inline">\(f \in\mathcal{S}(\mathbb{R})\)</span>,let <span class="math inline">\(u(x,y)=(f*\mathcal{P}_{y})(x)\)</span>. Then: 1. <spanclass="math inline">\(u (x, y)\)</span> is <spanclass="math inline">\(C^{2}\)</span> in <spanclass="math inline">\(\mathbb{R}^{2}_{+}\)</span> and <spanclass="math inline">\(\Delta u=0\)</span>. 2. <spanclass="math inline">\(u(x, y) \to f(x)\)</span> uniformly as <spanclass="math inline">\(y \to 0\)</span>. 3. <spanclass="math inline">\(\int_{-\infty}^{\infty} \lvert u(x, y)-f(x) \rvert^{2} \mathrm{d}x \to 0\)</span> as <span class="math inline">\(y \to0\)</span>. 4. If <span class="math inline">\(u(x,0)=f(x)\)</span>, then<span class="math inline">\(u\)</span> is continuous on the closure<span class="math inline">\(\overline{\mathbb{R}^{2}_{+}}\)</span> ofthe upper half-plane, and vanishes at infinity in the sense that <spanclass="math display">\[    u(x, y) \to 0 \quad \text{as} \ \lvert x \rvert +y \to \infty.\]</span></p><h2 id="heat-kernel">Heat kernel</h2><p>Another application related to the Poisson summation formula and thetheta function is the time-dependent heat equation on the circle. Asolution to the equation <span class="math display">\[    \frac{\partial u}{\partial t}=\frac{\partial ^{2}u}{\partial x^{2}}\]</span> subject to <span class="math inline">\(u(x,0)=f(x)\)</span>,where <span class="math inline">\(f\)</span> is periodic of period <spanclass="math inline">\(1\)</span>, was given in the previous chapter by<span class="math display">\[    u(x,t)=(f*H_t)(x)\]</span> where <span class="math inline">\(H_t(x)\)</span> is the heatkernel on the circle, that is, <span class="math display">\[    H_t(x)= \sum_{n=-\infty}^{\infty} \mathrm{e}^{-4\pi^{2}n^{2}t}\mathrm{e}^{2\pi inx} .\]</span> Note in particular that with our definition of the generalizedtheta function in the previous section, we have <spanclass="math inline">\(\Theta(x|4\pi it)=H_t(x)\)</span>. Also, recallthat the heat equation on <spanclass="math inline">\(\mathbb{R}\)</span> gave rise to the heat kernel.<span class="math display">\[    \mathcal{H}_{t}(x)=\frac{1}{(4\pi t)^{1/2}}\mathrm{e}^{-x^{2}/4t}\]</span> where <spanclass="math inline">\(\hat{\mathcal{H}}_{t}(\xi)=\mathrm{e}^{-4\pi^{2}\xi^{2}t}\)</span>.The fundamental relation between these two objects is an immediateconsequence of the Poisson summation formula:</p><p><strong>Theorem 3.3</strong> The heat kernel on the circle is theperiodization of the heat kernel on the real line: <spanclass="math display">\[    H_t(x)= \sum_{n=-\infty}^{\infty} \mathcal{H}_{t}(x+n).\]</span> Although the proof that <spanclass="math inline">\(\mathcal{H}_t\)</span> is a good kernel on <spanclass="math inline">\(\mathbb{R}\)</span> was fairly straightforward, weleft open the harder problem that <spanclass="math inline">\(H_t\)</span> is a good kernel on the circle. Theabove results allow us to resolve this matter.</p>]]></content>
    
    
    <categories>
      
      <category>分析学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>常微分方程（9）</title>
    <link href="/2022/06/23/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%889%EF%BC%89/"/>
    <url>/2022/06/23/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%889%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="边值问题">边值问题</h1><h2 id="sturm-比较定理">Sturm 比较定理</h2><p>考虑二阶线性微分方程 <span class="math display">\[    y&#39;&#39;+p(x)y&#39;+q(x)y=0 \tag{9.1}\]</span> 其中 <span class="math inline">\(p(x)\)</span> 和 <spanclass="math inline">\(q(x)\)</span> 在区间 <spanclass="math inline">\(J\)</span> 上是连续的.</p><p><strong>引理9.1</strong>：（9.1）的任何非零解在区间 <spanclass="math inline">\(J\)</span> 内的零点都是孤立的.</p><p>由解的唯一性易知.</p><p><strong>定理9.1</strong>：设 <spanclass="math inline">\(y=\varphi_1(x)\)</span> 和 <spanclass="math inline">\(y=\varphi_2(x)\)</span> 是（9.1）的两个非零解，则- 它们线性无关，当且仅当它们有相同的零点； -它们线性无关，当且仅当它们的零点互相交错.</p><p><strong>证</strong>；利用 Wronsky 行列式和 Liouville公式，考虑相邻零点处的导数值即可.</p><p><strong>定理9.2（Sturm 比较定理）</strong>：两个齐次线性微分方程<span class="math display">\[    y&#39;&#39;+P(x)y&#39;+Q(x)y=0 \tag{9.6}\]</span> 和 <span class="math display">\[    y&#39;&#39;+p(x)y&#39;+R(x)y=0 \tag{9.7}\]</span> 这里系数函数 <span class="math inline">\(p(x),Q(x)\)</span> 和<span class="math inline">\(R(x)\)</span> 在区间 <spanclass="math inline">\(J\)</span> 上是连续的，而且假设不等式 <spanclass="math display">\[    R(x)\geqslant Q(x) \quad (x \in J) \tag{9.8}\]</span> 成立. 又设 <span class="math inline">\(y=\varphi(x)\)</span>是方程（9.6）的一个非零解，而且 <span class="math inline">\(x_1\)</span>和 <span class="math inline">\(x_2\)</span> 是它的两个相邻的零点.则（9.7）的任何非零解 <span class="math inline">\(y=\psi(x)\)</span> 在<span class="math inline">\(x_1\)</span> 和 <spanclass="math inline">\(x_2\)</span> 之间至少有一个零点 <spanclass="math inline">\(x_0\)</span>（<span class="math inline">\(x_0 \in[x_1,x_2]\)</span>）.</p><p>如果（9.8）变为 <span class="math display">\[    R(x)&gt;Q(x) \tag{9.8*}\]</span> 那么可以加强为 <span class="math inline">\(x_0 \in(x_1,x_2)\)</span></p><p><strong>证</strong>：关键是构造出函数 <spanclass="math inline">\(v(x)=\psi(x)\varphi&#39;(x)-\varphi(x)\psi&#39;(x)\)</span>并讨论 <span class="math inline">\(v&#39;(x)+p(x)v(x)\)</span> 在 <spanclass="math inline">\([x_1,x_2]\)</span> 上恒正或恒负.</p><p><strong>推论</strong>：方程（9.1）的任何两个线性无关的解的零点是相互交错的.（令 <span class="math inline">\(R(x)=Q(x)\)</span>）</p><p>设 <span class="math inline">\(y=\varphi(x)\)</span>是（9.1）的一个非零解. 若 <spanclass="math inline">\(y=\varphi(x)\)</span> 在区间 <spanclass="math inline">\(J\)</span> 上最多只有一个零点，则称它在 <spanclass="math inline">\(J\)</span>上是<strong>非振动的</strong>；否则，称它在 <spanclass="math inline">\(J\)</span> 上是<strong>振动的</strong>. 如果 <spanclass="math inline">\(y=\varphi(x)\)</span> 在区间 <spanclass="math inline">\(J\)</span> 上有无限个零点，则称它在 <spanclass="math inline">\(J\)</span> 上是<strong>无限振动的</strong>.</p><p><strong>判别法1</strong>：设（9.1）中的系数函数 <spanclass="math display">\[    q(x)\leqslant 0 \quad(x \in J)\]</span> 则它的一切非零解都是非振动的. （与 <spanclass="math inline">\(y&#39;&#39;+p(x)y&#39;=0\)</span> 的非零解 <spanclass="math inline">\(y=\psi(x) \equiv 1\)</span> 比较即可）</p><p><strong>判别法2</strong>：设微分方程 <span class="math display">\[    y&#39;&#39;+Q(x)y=0 \tag{9.13}\]</span> 其中 <span class="math inline">\(Q(x)\)</span> 在区间 <spanclass="math inline">\(a\leqslant x&lt;\infty\)</span>上是连续的，而且满足不等式 <span class="math display">\[    Q(x)\geqslant m&gt;0 \quad(m 是常数)\]</span> 则（9.13）的任何非零解 <spanclass="math inline">\(y=\varphi(x)\)</span> 在区间 <spanclass="math inline">\([a,\infty)\)</span>上是无限振动的；而且它的任何两个相邻零点的间距不大于 <spanclass="math inline">\(\displaystyle \frac{\pi}{\sqrt{m}}\)</span>.</p><p>由方程 <span class="math display">\[    y&#39;&#39;+\frac{1}{4x^{2}}y=0 \quad(1\leqslant x&lt;\infty)\]</span> 的非零解 <span class="math display">\[    y=\sqrt{x}(C_1+C_2 \ln x)\]</span> 知判别法2不能减弱为 <spanclass="math inline">\(Q(x)&gt;0\quad(a\leqslantx&lt;\infty)\)</span></p><p>与判别法相对偶的有：如果 <span class="math inline">\(Q(x)\)</span>满足不等式 <span class="math display">\[    Q(x)\leqslant M \quad(a\leqslant x&lt;\infty)\]</span> 其中常数 <span class="math inline">\(M&gt;0\)</span>.则它的任何非零解 <span class="math inline">\(y=\varphi(x)\)</span>的相邻零点的间距不小于常数 <span class="math inline">\(\displaystyle\frac{\pi}{\sqrt{M}}\)</span></p><p>证明时与 <span class="math inline">\(y&#39;&#39;+My=0\)</span>的非零解 <span class="math inline">\(y=\sin [\sqrt{M}(x-c)]\)</span> 且<span class="math inline">\(c\)</span> 可以任意跑比较.</p><h2 id="s-l-边值问题的特征值">S-L 边值问题的特征值</h2><p>考虑比较一般的二阶齐次线性微分方程 <span class="math display">\[    [p(x)y&#39;]&#39;+[q(x)+\lambda r(x)]y=0 \tag{9.16}\]</span></p><p>其中 <span class="math inline">\(\lambda\)</span>是一个参数，系数函数 <span class="math inline">\(p(x),q(x)\)</span> 和<span class="math inline">\(r(x)\)</span> 在区间 <spanclass="math inline">\(a\leqslant x\leqslant b\)</span> 上是连续的，<spanclass="math inline">\(p(x)\)</span> 是可微的，而且 <spanclass="math inline">\(p(x)&gt;0\)</span> 和 <spanclass="math inline">\(r(x)&gt;0\)</span>. 另外，设边值条件 <spanclass="math display">\[    Ky(a)+Ly&#39;(a)=0,\quad My(b)+Ny&#39;(b)=0, \tag{9.17}\]</span> 其中常数 <span class="math inline">\(K,L,M,N\)</span> 满足条件<span class="math display">\[    K^{2}+L^{2}&gt;0, \quad M^{2}+N^{2}&gt;0.\]</span></p><p>上述形式的边值问题通常称为 <strong>Sturm-Liouville 问题</strong>. 设<span class="math inline">\(\lambda= \lambda_0\)</span>时边值问题有非零解 <span class="math inline">\(y=\varphi_0(x)\)</span>.则称 <span class="math inline">\(\lambda_0\)</span>为该问题的<strong>特征值</strong>，<spanclass="math inline">\(y=\varphi_0(x)\)</span>为相应的<strong>特征函数</strong>，注意那么对于任何常数 <spanclass="math inline">\(C \neq 0\)</span>，<span class="math inline">\(y=C\varphi_0(x)\)</span> 仍是相应的特征函数.</p><p>作 Prüfer变换<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="邓宗琦. 常微分方程边值问题和Sturm比较理论引论. 湖北：华中师范大学出版社，1987">[1]</span></a></sup><span class="math display">\[    y(x)=\rho(x) \sin \theta(x), \quad p(x)y&#39;(x)=\rho(x) \cos\theta(x)\]</span> （9.16）变为 <span class="math display">\[    \theta&#39;(x)= \frac{\cos ^{2}\theta(x)}{p(x)}+[q(x)+\lambda r(x)]\sin ^{2}\theta(x),\]</span> <span class="math display">\[    \rho&#39;(x)= \rho(x)[\frac{1}{p(x)}-q(x)- \lambda r(x)] \cos\theta(x) \sin \theta(x);\]</span></p><p>又（9.16）即 <span class="math display">\[    y&#39;&#39;(x)=-\frac{\rho(x)}{p(x)}[q(x)+\lambda r(x)] \sin\theta(x)- \frac{\rho(x)p&#39;(x)}{p^{2}(x)} \cos \theta(x)\]</span></p><p>而边界条件变为 <span class="math display">\[    y(a) \cos \theta_a-p(a)y&#39;(a) \sin \theta_a=0, \quad y(b) \cos\theta_b- p(b)y&#39;(b) \sin \theta_b=0.\]</span></p><p>将（9.16）化成如下形式： <span class="math display">\[    y&#39;&#39;+(\lambda+q(x))y=0, \tag{9.20}       \]</span> 其中 <span class="math inline">\(q(x)\)</span> 在区间 <spanclass="math inline">\([0,1]\)</span> 上连续，而且把边值条件（9.17）化成<span class="math display">\[    y(0)\cos \alpha - y&#39;(0) \sin \alpha=0, \quad y(1)\cos \beta-y&#39;(1) \sin \beta=0, \tag{9.21}\]</span> 这里规定常数 <span class="math inline">\(0\leqslant\alpha&lt;\pi, 0&lt;\beta\leqslant \pi\)</span>.</p><blockquote><p>搞不定了，到时候问老师.</p></blockquote><p>希望存在（9.20）的形如 <spanclass="math inline">\(y=\varphi(x,\lambda)\)</span> 的解，满足初值条件<span class="math display">\[    \varphi(0,\lambda)= \sin \alpha, \quad \varphi&#39;(0,\lambda)=\cos\alpha. \tag{9.22}\]</span> 想让它也满足（9.21）的第二式. 令 <span class="math display">\[    \varphi(x,\lambda)= \rho(x,\lambda)\sin \theta(x,\lambda), \quad\varphi&#39;(x,\lambda)=\rho(x,\lambda) \cos \theta(x,\lambda),\]</span> 其中 <span class="math display">\[    \begin{cases}        \rho(x,\lambda)=\sqrt{[\varphi(x,\lambda)]^{2}+[\varphi&#39;(x,\lambda)]^{2}}\quad(&gt;0), \\        \theta(x,\lambda)= \arctan\frac{\varphi(x,\lambda)}{\varphi&#39;(x,\lambda)} \quad (0\leqslantx\leqslant 1).              \end{cases}\]</span> 由（9.21）的第一式，有 <span class="math display">\[    \theta(0,\lambda)= \arctan \frac{\sin \alpha}{\cos\alpha}=\alpha+j\pi, \tag{9.23}\]</span> 这里 <span class="math inline">\(j\)</span> 是某个整数.满足第二式，只要使 <spanclass="math inline">\(\theta=\theta(x,\lambda)\)</span> 满足条件 <spanclass="math display">\[    \theta(1,\lambda)= \beta+ k\pi, \tag{9.24}\]</span></p><p><span class="math inline">\(\theta(x)\)</span> 显然满足 <spanclass="math display">\[    \theta&#39;=\cos ^{2}\theta+[\lambda+q(x)]\sin ^{2}\theta \tag{9.25}\]</span></p><p>且在（9.23）中取 <span class="math inline">\(j=0\)</span>，即 <spanclass="math inline">\(\theta(0,\lambda)=\alpha\)</span>.</p><p><strong>引理 9.2</strong>：令 <spanclass="math inline">\(\omega(\lambda)=\theta(1,\lambda)\)</span>. 则函数<span class="math inline">\(\omega(\lambda)\)</span> 在区间 <spanclass="math inline">\(-\infty&lt;\lambda&lt;\infty\)</span>上是连续的，而且是严格上升的.</p><p><strong>证</strong>：（9.25）得到 <spanclass="math inline">\(\lambda\)</span> 的变分方程为 <spanclass="math display">\[    \frac{\mathrm{d}}{\mathrm{d}x} \frac{\partial \theta}{\partial\lambda}= [\lambda+q(x)-1] \sin 2\theta \frac{\partial \theta}{\partial\lambda} + \sin ^{2} \theta, \tag{9.27}\]</span></p><p>又由 <span class="math inline">\(\theta(0,\lambda)=\alpha\)</span> 知<span class="math display">\[    \frac{\partial \theta}{\partial \lambda}(0,\lambda)=0. \tag{9.28}\]</span> 而方程（9.27）关于 <span class="math inline">\(\displaystyle\frac{\partial \theta}{\partial \lambda}\)</span> 是一阶线性的.因此，再利用初值条件（9.28），得到 <span class="math display">\[    \frac{\partial \theta}{\partial \lambda}(x,\lambda)= \int_{0}^{x}\mathrm{e}^{\int_{t}^{x} E(s,\lambda) \mathrm{d}s} \sin^{2}\theta(t,\lambda) \mathrm{d}t\]</span> 其中 <span class="math display">\[    E(s,\lambda)= [\lambda+q(s)-1] \sin 2\theta(s,\lambda).\]</span></p><p>易知 <span class="math inline">\(\sin ^{2} \theta(x,\lambda)\)</span>不恒为零（<span class="math inline">\(0\leqslant x\leqslant1\)</span>）. 因此上式可见 <span class="math display">\[    \omega&#39;(\lambda)=\frac{\partial \theta}{\partial\lambda}(1,\lambda)&gt;0,\]</span> 引理证毕.</p><p><strong>引理 9.3</strong>：当 <spanclass="math inline">\(-\infty&lt;\lambda&lt;\infty\)</span> 时，<spanclass="math inline">\(\omega(\lambda)&gt;0\)</span>，并且 <spanclass="math display">\[    \lim_{\lambda \to -\infty} \omega(\lambda)=0.\]</span> <strong>证</strong>：先利用（9.25）证明 <spanclass="math inline">\(\theta(x,\lambda)&gt;0, 0&lt;x\leqslant1\)</span>. 那么就有 <spanclass="math inline">\(\omega(\lambda)=\theta(1,\lambda)&gt;0,-\infty&lt;\lambda&lt;\infty\)</span>.</p><p>考虑到 <span class="math inline">\(0\leqslant\alpha&lt;\pi\)</span>，在 <spanclass="math inline">\((x,\theta)\)</span> 平面上取两点 <spanclass="math inline">\(A(0,\pi-\varepsilon)\)</span> 和 <spanclass="math inline">\(B(1,\varepsilon)\)</span>. 如果积分曲线 <spanclass="math inline">\(\theta=\theta(x,\lambda)\)</span> 与直线第一次交于<span class="math inline">\(x= \bar{x}_1\)</span>，则斜率 <spanclass="math inline">\(\theta&#39;(\bar{x}_1,\lambda)\geqslant 直线 AB的斜率 K=2 \varepsilon-\pi\)</span>. 又注意到由（9.25） <spanclass="math display">\[    \theta&#39;(\bar{x}_1,\lambda)=\cos ^{2}\theta+[\lambda+q(\bar{x}_1)]\sin ^{2}\theta\]</span> 考虑到要让 <span class="math inline">\(\lambda \to-\infty\)</span>，想让 <spanclass="math inline">\(\theta&#39;(\bar{x}_1,\lambda)&lt;2\varepsilon-\pi\)</span> 从而导出矛盾. 而 <spanclass="math inline">\(\sin \theta \geqslant \varepsilon\)</span>，故只要<span class="math display">\[    1+[\lambda+M]\sin ^{2}\varepsilon&lt;2 \varepsilon-\pi\]</span> 其中 <span class="math inline">\(M= \max\{q(x)\colon0\leqslant x\leqslant 1\}\)</span>，就有积分曲线不可能与 <spanclass="math inline">\(AB\)</span> 相交，解出 <spanclass="math inline">\(\lambda\)</span> 的取值. 这说明当 <spanclass="math inline">\(\lambda \to -\infty\)</span> 时，<spanclass="math inline">\(\omega(\lambda)\to 0\)</span>.</p><p>也可以参考</p><hr /><p>(万年老坑)常微分方程学习笔记(10)https://zhuanlan.zhihu.com/p/151401565</p><hr /><p><strong>引理 9.4</strong>：当 <span class="math inline">\(\lambda \to\infty\)</span> 时， <span class="math inline">\(\omega(\lambda)\to\infty\)</span>.</p><p><strong>证</strong>：关键是注意到 <span class="math display">\[    \int_{0}^{1} \frac{\theta&#39;}{\cos ^{2}\theta+[\lambda+q(x)]\sin^{2}\theta}\mathrm{d}x=\int_{\alpha}^{\omega(\lambda)}  \frac{\mathrm{d}\theta}{\cos^{2}\theta+[\lambda+q(x)]\sin ^{2}\theta}=1\]</span> 如果 <span class="math inline">\(\omega(\lambda)\)</span>有界，令 <span class="math inline">\(\lambda \to\infty\)</span>，可以预想中间式子积分其实应该很小，分 <spanclass="math inline">\(\sin \theta\)</span>的零点附近（注意只有有限个零点）和剩下的部分分别计算积分，导出矛盾.</p><p><strong>定理 9.3</strong>：S-L边值问题有无限多个（简单的）特征值，而且可排列如下： <spanclass="math display">\[    \lambda_0&lt;\lambda_1&lt;\cdots &lt;\lambda_k&lt;\cdots\]</span> 其中 <span class="math display">\[    \lim_{k \to \infty} \lambda_{k} = \infty.\]</span></p><blockquote><p>习题 9-2 第二题为什么不与定理矛盾？</p></blockquote><h2 id="特征函数系的正交性">特征函数系的正交性</h2><p>S-L 边值问题 <span class="math display">\[    y&#39;&#39;+[\lambda+ q(x)]y=0, \tag{9.33}\]</span> 和 <span class="math display">\[    \begin{cases}        y(0)\cos \alpha- y&#39;(0) \sin \alpha=0, \\        y(1)\cos \beta- y&#39;(1) \sin \beta=0, \tag{9.34}    \end{cases}\]</span> 其中 <span class="math inline">\(\lambda\)</span>是参数，而函数 <span class="math inline">\(q(x)\)</span> 在区间 <spanclass="math inline">\(0\leqslant x\leqslant 1\)</span> 是连续；又设常数<span class="math inline">\(\alpha\)</span> 和 <spanclass="math inline">\(\beta\)</span> 满足不等式 <spanclass="math display">\[    0\leqslant \alpha&lt;\pi, \quad 0&lt;\beta\leqslant \pi.\]</span></p><p>考虑对于每个特征值 <span class="math inline">\(\lambda_n\)</span>的特征函数 <span class="math inline">\(\varphi(x,\lambda_n)\)</span>. 当<span class="math inline">\(C \neq 0\)</span> 时， <spanclass="math inline">\(C\varphi(x,\lambda_n)\)</span> 也是特征函数.</p><p><strong>引理 9.4</strong>：对应于每个特征值，S-L边值问题有且只有一个线性无关的特征函数.</p><p>考虑两个解在 <span class="math inline">\(x=0\)</span> 处的 Wronsky行列式，直接得到它们线性相关.</p><p>下面令 <span class="math display">\[    \varphi_n(x)= \varphi(x, \lambda_n) \quad (n=0,1,2,\cdots ).\tag{9.35}\]</span></p><p><strong>引理 9.5</strong> 特征函数系（9.35）在区间 <spanclass="math inline">\(0\leqslant x\leqslant 1\)</span>上组成一个正交系，即 <span class="math display">\[    \int_{0}^{1} \varphi_n(x)\varphi_k(x) \mathrm{d}x=    \begin{cases}        0, \quad n\neq k; \\        \delta_k&gt;0, \quad n\neq k.    \end{cases}\]</span></p><p><strong>证</strong>：关键是得到 <span class="math display">\[    (\lambda_n-\lambda_k) \varphi_n(x) \varphi_k(x)=\frac{\mathrm{d}}{\mathrm{d}x}[\varphi_n(x)\varphi&#39;_k(x)-\varphi&#39;_n(x)\varphi_k(x)],\]</span> 然后积分并利用边值条件.</p><p><strong>定理 9.6</strong>：<spanclass="math inline">\(\varphi_n(x)\)</span> 在 <spanclass="math inline">\([0,1]\)</span> 上有 <spanclass="math inline">\(n\)</span> 个零点.</p><p>注意到 <span class="math inline">\(\varphi_n(x)\)</span> 对应的 <spanclass="math inline">\(\theta(t,\lambda_n)\)</span> 不可能取到某个 <spanclass="math inline">\(k \pi\)</span>至少两次，否则在相邻零点处导数值异号，但在 <spanclass="math inline">\(\theta=0\)</span> 处导数值应该为 <spanclass="math inline">\(1\)</span>，故这是不可能的.</p><p><strong>定理 9.7</strong>：如果 <spanclass="math inline">\(f(x)\)</span> 在区间 <spanclass="math inline">\(0\leqslant x\leqslant 1\)</span> 上是 Riemann可积的，而且满足 <span class="math display">\[    \int_{0}^{1} f(x)\varphi_n(x) \mathrm{d}x=0 \quad(n=0,1,2,\cdots ),\]</span> 那么 <span class="math inline">\(f(x)\)</span>在连续点处恒等于零.</p><p>证明分几步（下面都是一般形式的 S-L 边值问题）</p><p>可以参考</p><hr /><p>常微分方程边值问题和Sturm比较理论引论https://cadal.edu.cn/cardpage/bookCardPage?ssno=06504331&amp;source=card</p><hr /><p>的P126-P131.</p><p>定理 9.7 说明特征函数系（9.35）在Riemann可积的函数空间 <spanclass="math inline">\(\mathcal{R}\{[0,1];\mathbb{R}^{1}\}\)</span>中是一个完全的正交系. 在区间 <span class="math inline">\([0,1]\)</span>上可以考虑可积函数 <span class="math inline">\(f(x)\)</span>关于特征函数系（9.35）的（广义）Fourier展开 <spanclass="math display">\[    f(x) \sim \sum_{n=0}^{\infty} a_n \varphi_n(x), \tag{9.36}\]</span> 其中 <span class="math display">\[    a_n=\frac{1}{\delta_n} \int_{0}^{1} f(x)\varphi_n(x) \mathrm{d}x\quad(n=0,1,2,\cdots ),\]</span> 而正数 <span class="math display">\[    \delta_n= \int_{0}^{1} \varphi_n^{2}(x) \mathrm{d}x.\]</span> 可以进一步证明下述结论.</p><p><strong>定理 9.8</strong>：设函数 <spanclass="math inline">\(f(x)\)</span> 在区间 <spanclass="math inline">\([0,1]\)</span> 上满足 Dirichlet条件，则它的广义Fourier 级数（9.36）收敛到它自己.</p><p>一些更细致的讨论见</p><hr /><p>常微分方程边值问题和Sturm比较理论引论https://cadal.edu.cn/cardpage/bookCardPage?ssno=06504331&amp;source=card</p><hr /><p>的P132-P143.</p><h3 id="非齐次方程的-s-l-边值问题">非齐次方程的 S-L 边值问题</h3><p><span class="math display">\[    \begin{cases}        y&#39;&#39;+[\lambda+q(x)]y= f(x), \\        y(0)\cos \alpha- y&#39;(0) \sin \alpha=0, \quad y(1)\cos\beta-y&#39;(1)\sin \beta=0.    \end{cases}\]</span></p><p>当 <span class="math inline">\(\lambda\)</span> 不是相应齐次方程的S-L 边值问题的特征值时，它有且只有一个解；而当 <spanclass="math inline">\(\lambda\)</span> 等于某个特征值 <spanclass="math inline">\(\lambda_m\)</span> 时， 它有解的充要条件为 <spanclass="math display">\[    \int_{0}^{1} f(x)\varphi_m(x) \mathrm{d}x=0,\]</span> 其中 <span class="math inline">\(\varphi_m(x)\)</span>为相应于特征值 <span class="math inline">\(\lambda_m\)</span>的特征函数.</p><p><strong>证</strong>：当 <span class="math inline">\(\lambda\)</span>不是特征值时，</p><p><span class="math display">\[    \int_{0}^{1} f\varphi_m = \int_{0}^{1} y&#39;&#39;\varphi_m+[\lambda_m +q]y\varphi_m= \int_{0}^{1}y&#39;&#39;\varphi_m-\varphi_m&#39;&#39;y= (y&#39;\varphi_m-y\varphi_m&#39;)\bigg |_{0}^1=0\]</span></p><h2 id="参考">参考</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>邓宗琦.常微分方程边值问题和Sturm比较理论引论. 湖北：华中师范大学出版社，1987<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>分析学</category>
      
      <category>微分方程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>About Bezier Curve</title>
    <link href="/2022/06/20/About-Bezier-Curve/"/>
    <url>/2022/06/20/About-Bezier-Curve/</url>
    
    <content type="html"><![CDATA[<p>Bezier 曲线是本学期学过相当好用的东西.本学期的Project2中涉及到Bezier曲线的部分大有可为.搜索资料的过程中我找到了一个非常好的相关博客：</p><hr /><p>Fitting cubic Bézier curves | Raph Levien’s bloghttps://raphlinus.github.io/curves/2021/03/11/bezier-fitting.html</p><hr /><p>Project2中，问题的核心是：当曲线两端点处斜率确定，如何取控制点使得Bezier曲线与原曲线最接近.下面读者将主要看到：千方百计凑字数造成的语义冗余、对上述博客的拙劣翻译、LaTeX文档转Markdown后丢失的图片标题（不想搞啦）、不明觉厉其实很naive的公式推导、由于公式过长导致显示出大问题（以后可能会想办法解决这个问题）、第一第三问丢失导致的没见过Project2的读者完全一头雾水的情况。</p><p>由于.png的透明格式，请在日间模式下阅读以获得最佳的阅读体验。</p><p>用三次样条插值得到的导数值控制贝塞尔曲线两个端点的斜率，用贝塞尔曲线画出图一曲线。结果如图四，其中参数<spanclass="math inline">\(h\)</span>的意义下面解释。图四中用不同的颜色画出了Curve1，Curve2和Curve3，并将插值的端点标出。</p><figure><img src="/img/bezier_curve/fig4.jpg" alt="fig4" /><figcaption aria-hidden="true">fig4</figcaption></figure><p>考虑到图一只给出了曲线的端点和端点处切线的斜率，我们引入参数 <spanclass="math inline">\(h_1\)</span>和<spanclass="math inline">\(h_2\)</span>。在 <spanclass="math inline">\(P_1=(x_i,y_i)\)</span>和<spanclass="math inline">\(P_2=(x_{i+1},y_{i+1})\)</span>之间绘制Bezier曲线时，<spanclass="math inline">\(h_1\)</span>定义为第一个控制点 <spanclass="math inline">\(Q_1=(x_3,y_3)\)</span>到<spanclass="math inline">\(P_1\)</span>的距离，<spanclass="math inline">\(h_2\)</span>定义为第二个控制点 <spanclass="math inline">\(Q_2=(x_4,y_4)\)</span>到<spanclass="math inline">\(P_2\)</span>的距离。设曲线在<spanclass="math inline">\(P_1\)</span>处斜率为 <spanclass="math inline">\(p\)</span>，在 <spanclass="math inline">\(P_2\)</span>处斜率为 <spanclass="math inline">\(q\)</span>。则有 <span class="math display">\[    \begin{aligned}        x_3 &amp; =x_1+\frac{h_1}{\sqrt{1+p^{2}}}, &amp; y_3 &amp;=y_1+\frac{ph_1}{\sqrt{1+p^{2}}}, \\        x_4 &amp; =x_2-\frac{h_2}{\sqrt{1+q^{2}}}, &amp; y_4 &amp;=y_2-\frac{qh_2}{\sqrt{1+q^{2}}}  \\    \end{aligned}\]</span> 由Bezier曲线的计算公式 <span class="math display">\[    \begin{aligned}        b_x &amp;=\frac{3h_1}{\sqrt{1+p^{2}}},                                         &amp;b_y &amp;=\frac{3ph_1}{\sqrt{1+p^{2}}},                                         \\        c_x &amp;=3(x_2-x_1)-\frac{6h_1}{\sqrt{1+p^{2}}}-\frac{3h_2}{\sqrt{1+q^{2}}},  &amp;c_y &amp;=3(y_2-y_1)-\frac{6ph_1}{\sqrt{1+p^{2}}}-\frac{3qh_2}{\sqrt{1+q^{2}}}  \\        d_x &amp;=-2(x_2-x_1)+\frac{3h_1}{\sqrt{1+p^{2}}}+\frac{3h_2}{\sqrt{1+q^{2}}},&amp; d_y &amp;=-2(y_2-y_1)+\frac{3ph_1}{\sqrt{1+p^{2}}}+\frac{3qh_2}{\sqrt{1+q^{2}}}\\    \end{aligned}\]</span> 从而Bezier曲线 <span class="math display">\[    \begin{aligned}        x(t) &amp; =x_1+b_xt+c_xt^{2}+d_xt^{3} \\        y(t) &amp; =y_1+b_yt+c_yt^{2}+d_yt^{3} \\    \end{aligned}\]</span> 记该曲线为 <span class="math inline">\(p_f\)</span>。</p><h2 id="h_1h_2与bezier曲线的关系"><spanclass="math inline">\(h_1,h_2\)</span>与Bezier曲线的关系</h2><p>图五给出了当 <span class="math inline">\(h_1=h_2=h\)</span>变动时<span class="math inline">\(p_f\)</span>的变化。注意当 <spanclass="math inline">\(h \to 0\)</span>时 <span class="math display">\[    \frac{y(t)-y_1}{x(t)-x_1}=\frac{p+[\frac{\sqrt{1+p^{2}}(y_2-y_1)}{h}-2p-\frac{\sqrt{1+p^{2}}q}{\sqrt{1+q^{2}}}]t+[-\frac{2\sqrt{1+p^{2}}(y_2-y_1)}{3h}+p+\frac{\sqrt{1+p^{2}}q}{\sqrt{1+q^{2}}}]t^{2}}{1+[\frac{\sqrt{1+p^{2}}(x_2-x_1)}{h}-2-\frac{\sqrt{1+p^{2}}}{\sqrt{1+q^{2}}}]t+[-\frac{2\sqrt{1+p^{2}}(x_2-x_1)}{3h}+1+\frac{\sqrt{1+p^{2}}}{\sqrt{1+q^{2}}}]t^{2}}\top\]</span> 即当 <span class="math inline">\(h\to 0\)</span>时 <spanclass="math inline">\(p_f\)</span>趋向于线性，而端点处仍然满足切线的斜率条件。</p><p>而当 <span class="math inline">\(h\)</span>较大时，相应的 <spanclass="math inline">\(p_f\)</span>会出现尖点，当 <spanclass="math inline">\(h\)</span>继续增大时，相应的 <spanclass="math inline">\(p_f\)</span>会出现自交点。事实上，1989年，Lasser提出了一种曲线曲线算法，通过使用deCasteljau算法对曲线的控制多边形进行细分来计算贝塞尔曲线的自交点。该方法需要在算法开始之前通过控制多边形的旋转角度之和来检查曲线与自交的可能性，即：当Bezier曲线的控制多边形外角和不大于<spanclass="math inline">\(\pi\)</span>时，Bezier曲线一定没有自交点。在图四的情况中，只有当线段<spanclass="math inline">\(P_1Q_1\)</span>和线段<spanclass="math inline">\(P_2Q_2\)</span>相交时， <spanclass="math inline">\(p_f\)</span>才可能有自交点。</p><p>图五展示了使用不同方法绘制Curve2的图像，可以看到，关于h取值过小时过于接近线性的问题可以通过增加插值点避免。</p><figure><img src="/img/bezier_curve/fig5.jpg" alt="fig5" /><figcaption aria-hidden="true">fig5</figcaption></figure><h2 id="h_1h_2的最佳取法"><spanclass="math inline">\(h_1,h_2\)</span>的最佳取法</h2><p>我们认为三次样条插值得到的曲线为标准曲线 <spanclass="math inline">\(s\)</span>，讨论 <spanclass="math inline">\(h_1\)</span>和 <spanclass="math inline">\(h_2\)</span>取何值时 <spanclass="math inline">\(p_f\)</span>与 <spanclass="math inline">\(s\)</span>最为“接近”。其中“接近”的不同含义分类讨论。</p><h3 id="g2几何hermite插值alvin-penner的三种方法">G2几何Hermite插值/AlvinPenner的三种方法</h3><p>一个自然的想法是 <span class="math inline">\(p_f\)</span>与 <spanclass="math inline">\(s\)</span>在端点处的二阶导数值相等。<spanclass="math inline">\(p_f\)</span>使用的是曲线的参数方程，由相关数学知识得<span class="math display">\[    \frac{\mathrm{d}^{2}y}{\mathrm{d}x^{2}}=\frac{y&#39;&#39;(t)x&#39;(t)-x&#39;&#39;(t)y&#39;(t)}{(x&#39;(t))^{3}}\]</span> 计算得到三次样条插值 <span class="math display">\[    s(x)=y_1+p(x-x_1)+\biggl[\frac{3(y_2-y_1)}{(x_2-x_1)^{2}}-\frac{2p+q}{x_2-x_1}\biggr](x-x_1)^{2}+\biggl[\frac{p+q}{(x_2-x_1)^{2}}-\frac{2(y_2-y_1)}{(x_2-x_1)^{3}}\biggr](x-x_1)^{3}\]</span> 那么相应端点二阶导数值相等得方程即为 <spanclass="math display">\[    \begin{aligned}        s&#39;&#39;(x_1) &amp;=\frac{y&#39;&#39;(0)x&#39;(0)-x&#39;&#39;(0)y&#39;(0)}{(x&#39;(0))^{3}}\\        s&#39;&#39;(x_2) &amp;=\frac{y&#39;&#39;(1)x&#39;(1)-x&#39;&#39;(1)y&#39;(1)}{(x&#39;(1))^{3}}\\    \end{aligned}\]</span> 化简得到 <span class="math display">\[    \begin{aligned}        \frac{(1+p^{2})(h_2(p-q)+\sqrt{1+q^{2}}(p(x_1-x_2)-y_1+y_2))}{3h_1^{2}\sqrt{1+q^{2}}}&amp; = \frac{(2p+q)(x_1-x_2)-3(y_1-y_2)}{(x_1-x_2)^{2}}  \\        \frac{(1+q^{2})(h_1(p-q)-\sqrt{1+p^{2}}(q(x_1-x_2)-y_1+y_2))}{3h_2^{2}\sqrt{1+p^{2}}}&amp; = \frac{-(p+2q)(x_1-x_2)+3(y_1-y_2)}{(x_1-x_2)^{2}} \\    \end{aligned}\]</span> 经过消元可以得到关于 <spanclass="math inline">\(h_1\)</span>和 <spanclass="math inline">\(h_2\)</span>的一元四次方程，求该方程在一般情况下的解析解不具有实际意义。而特别的，当<spanclass="math inline">\(p=q\)</span>，即两端点处切线斜率相等时，我们有<span class="math display">\[    h_1=h_2=\frac{\sqrt{1+p^{2}}}{3}(x_2-x_1)\]</span></p><p>对于 <span class="math inline">\(p \neq q\)</span>的情况，<spanclass="math inline">\(h_1\)</span>和 <spanclass="math inline">\(h_2\)</span>可能根本没有解。即使有解，最小的正实数解也很可能较大，从而出现Bezier曲线自交的情况。例如<spanclass="math inline">\((x_1,x_2,y_1,y_2,p,q)=(17,20,4.5,7.0,3,-0.198)\)</span>时，<spanclass="math inline">\(h_1\)</span>的最小正实数解为 <spanclass="math inline">\(3.162\)</span>，<spanclass="math inline">\(h_2\)</span>的最小正实数解为 <spanclass="math inline">\(1.019\)</span>，此时曲线会出现明显的自交点。</p><p>查询相关资料后我们发现这是Carl de Boor etal在1987年的论文中分析的情况。由数据的不同，<spanclass="math inline">\(h_1,h_2\)</span>可以有 <spanclass="math inline">\(0,1,2,3\)</span>个正实数解。这篇论文还证明了 <spanclass="math inline">\(\text{dist}(s,p_f)=O(\lvert P_1P_2\rvert^{6})\)</span>。由于范数的等价性，我们可以任选 <spanclass="math inline">\(\text{dist}(s,p_f)\)</span>的定义。在本文的例子中$P_1P_2 $远大于 <spanclass="math inline">\(1\)</span>，故该估计不具有实际意义。</p><p>最近的一个结果是AlvinPenner于2019年发表的。他在文章中提出了三种方法。这些方法的优缺点总结可以见RahpLevien's Blog。</p><h3id="利用有向面积和图像矩进行曲线拟合">利用有向面积和图像矩进行曲线拟合</h3><p>该方法来源于Raph Levien's Blog。</p><p>首先改述这个问题，作数学上的简化。我们可以排除平移，均匀缩放和旋转，只需考虑从<span class="math inline">\((0,0)\)</span>到 <spanclass="math inline">\((1,0)\)</span>的曲线，给定 <spanclass="math inline">\(\theta_0\)</span>和 <spanclass="math inline">\(\theta_1\)</span>。两条控制臂的长度设为 <spanclass="math inline">\(\delta_0\)</span>和 <spanclass="math inline">\(\delta_1\)</span>。我们仍记原曲线为 <spanclass="math inline">\(s\)</span>，记Bezier曲线为 <spanclass="math inline">\(p_f\)</span></p><figure><img src="/img/bezier_curve/cubic.png" alt="cubic" /><figcaption aria-hidden="true">cubic</figcaption></figure><p>那么两个控制点的坐标为 <span class="math inline">\((\delta_0\cos\theta_0,\delta_0\sin \theta_0)\)</span>和 <spanclass="math inline">\((1-\delta_1\cos \theta_1,\delta_1\sin\theta_1)\)</span>。</p><p>三次样条插值（注意 <span class="math inline">\(q=-\tan\theta_1\)</span>）： <span class="math display">\[    s(x)=\tan \theta_0 x-(2\tan \theta_0-\tan \theta_1)x^{2}+(\tan\theta_0-\tan \theta_1)x^{3}\]</span></p><p>Bezier曲线： <span class="math display">\[    \begin{aligned}        x(t) &amp; =3\delta_0 \cos \theta_0 t+3(1-2\delta_0 \cos\theta_0+\delta_1\cos \theta_1)t^{2}+(-2+3\delta_0 \cos\theta_0-3\delta_1 \cos \theta_1)t^{3} \\        y(t) &amp; =3\delta_0\sin \theta_0t-3(2\delta_0\sin\theta_0+\delta\sin \theta_1)t^2+3(\delta_0\sin \theta_0+\delta_1\sin\theta_1)t_3                  \\    \end{aligned}\]</span></p><p>第一个做法是使得 <span class="math inline">\(p_f\)</span>和 <spanclass="math inline">\(x\)</span>轴围成的有向面积与 <spanclass="math inline">\(s\)</span>和 <spanclass="math inline">\(x\)</span>轴围成的有向面积相同，这是自然的。如图七所示，当围成面积不同时，拟合程度应当不会很高。</p><figure><img src="/img/bezier_curve/area.png" alt="area" /><figcaption aria-hidden="true">area</figcaption></figure><p>由Green公式，有 <span class="math display">\[    \text{area}=\frac{3}{20}(2\delta_0\sin \theta_0+2\delta_1\sin\theta_1-\delta_0\delta_1\sin (\theta_0+\theta_1))\]</span></p><p>对于现在的情况，</p><p><span class="math display">\[    \frac{1}{12}(\tan \theta_0+ \tan\theta_1)=\frac{3}{20}(2\delta_0\sin \theta_0+2\delta_1\sin\theta_1-\delta_0\delta_1\sin (\theta_0+\theta_1))\]</span></p><p>从该式可以从给定的 <spanclass="math inline">\(\delta_0\)</span>中解出唯一的 <spanclass="math inline">\(\delta_1\)</span>：</p><p><span class="math display">\[    \delta_1=\frac{\frac{5}{9}(\tan \theta_0+\tan \theta_1)-2\delta_0\sin \theta_0}{2\sin \theta_1-\delta_0 \sin (\theta_0+\theta_1)}\]</span></p><p>第二个做法是利用图像矩（imagemoment）。在图六化简的情形中，图像矩即为x-矩。图八展示了图像矩对于曲线的影响。所有红色曲线都具有相同的有向面积。</p><figure><img src="/img/bezier_curve/xmoment.png" alt="xmoment" /><figcaption aria-hidden="true">xmoment</figcaption></figure><p>由Green公式也可以化简计算 <span class="math display">\[    \begin{aligned}        \text{moment}_x &amp; = \int_{0}^{1} x p_f(x)\mathrm{d}=\int_{0}^{1} x(t)y(t)x&#39;(t)\mathrm{d}t                                                                         \\                        &amp; = \frac{1}{280}(34\delta_0\sin\theta_0+50\delta_1\sin \theta_1+15\delta_0^{2}\sin \theta_0\cos\theta_0-15\delta_1^{2}\sin \theta_1\cos \theta_1 \\                        &amp; -\delta_0\delta_1(33\sin \theta_0\cos\theta_1+9\cos \theta_0\sin \theta_1)-9\delta_0^{2}\delta_1\sin(\theta_0+\theta_1)\cos \theta_0            \\                        &amp; +9\delta_0\delta_1^{2}\sin(\theta_0+\theta_1)\cos \theta_1)    \end{aligned}\]</span> 同样得到方程 <span class="math display">\[\begin{equation}    \begin{aligned}        \frac{\tan \theta_0}{30}+\frac{\tan \theta_1}{20} &amp; =        \frac{1}{280}(34\delta_0\sin \theta_0+50\delta_1\sin\theta_1+15\delta_0^{2}\sin \theta_0\cos \theta_0-15\delta_1^{2}\sin\theta_1\cos \theta_1                                            \\                                                          &amp;-\delta_0\delta_1(33\sin \theta_0\cos \theta_1+9\cos \theta_0\sin\theta_1)-9\delta_0^{2}\delta_1\sin (\theta_0+\theta_1)\cos \theta_0 \\                                                          &amp;+9\delta_0\delta_1^{2}\sin (\theta_0+\theta_1)\cos \theta_1)    \end{aligned}\end{equation}\]</span> 将（1）与（3）联立可以得到 <spanclass="math inline">\(\delta_0\)</span>和 <spanclass="math inline">\(\delta_1\)</span>的解。消元后得到的是两个一元四次方程，它们不一定有正实数解。在RaphLevien's Blog中提到了还需要考虑 <spanclass="math inline">\(\text{moment}_x\)</span>的极值点（将 <spanclass="math inline">\(\delta_1\)</span>用 <spanclass="math inline">\(\delta_0\)</span>消去）。</p><h3 id="将一般情况转化为简单情形">将一般情况转化为简单情形</h3><p>第二问的最后，我们求出 <span class="math inline">\(h_1,h_2\)</span>与<span class="math inline">\(\delta_0,\delta_1\)</span>的关系。记 <spanclass="math inline">\(\displaystylek=\frac{y_2-y_1}{x_2-x_1}\)</span></p><p>作坐标变换 <span class="math display">\[    x&#39;=\frac{x-x_1}{x_2-x_1}, \quady&#39;=\frac{y-y_1}{x_2-x_1}-\frac{y_2-y_1}{(x_2-x_1)^{2}}(x-x_1)\]</span> 该变换将 <span class="math inline">\((x_1,y_1)\)</span>和<span class="math inline">\((x_2,y_2)\)</span>映到 <spanclass="math inline">\((0,0)\)</span>和 <spanclass="math inline">\((1,0)\)</span>，且保持比例与相切关系，我们有 <spanclass="math display">\[    \tan \theta_0=p-k, \quad \tan \theta_1=-q+k\]</span> 该变换的逆变换是 <span class="math display">\[    x=x_1+(x_2-x_1)x&#39;, \quad y=y_1+(y_2-y_1)x&#39;+(x_2-x_1)y&#39;\]</span> 那么就有 <span class="math display">\[    \begin{aligned}        h_1 &amp; =\delta_0(x_2-x_1) \\        h_2 &amp; =\delta_1(x_2-x_1) \\    \end{aligned}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>分析学</category>
      
      <category>数值分析</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter19</title>
    <link href="/2022/06/11/Chapter19/"/>
    <url>/2022/06/11/Chapter19/</url>
    
    <content type="html"><![CDATA[<h1 id="量子力学的基本原理">量子力学的基本原理</h1><h2 id="物质波">物质波</h2><p><span class="math display">\[    E=h \nu \\    p=\frac{h}{\lambda}\]</span></p><p>（波速不一定是粒子的速度）</p><p>用德布罗意假设推导玻尔量子化条件：电子圆轨道运动，对应于一个环形驻波</p><p>实验验证：1927，戴维逊-革末</p><h3 id="波函数">波函数</h3><p>令 <span class="math display">\[    \hbar=\frac{h}{2\pi}\]</span></p><p>若自由粒子在三维空间运动，其动量 <spanclass="math inline">\(\mathbf{p}\)</span>，能量为 <spanclass="math inline">\(E\)</span>，波函数为 <span class="math display">\[    \Psi(\mathbf{r},t)=\psi_0\exp\left({-\frac{i}{\hbar}(Et-\mathbf{p}\cdot \mathbf{r})} \right)\]</span> 波函数的模的平方（波的强度） <span class="math display">\[    \rho(\mathbf{r},t)=\lvert \Psi(\mathbf{r},t) \rvert ^{2}\]</span> 代表时刻 <span class="math inline">\(t\)</span>，在空间 <spanclass="math inline">\(\mathbf{r}\)</span>点处，单位体积元中微观粒子出现的概率。</p><p><strong>波函数应满足的条件</strong> - 归一化条件 <spanclass="math display">\[    \int_{\Omega}^{} \Psi^{*}(\mathbf{r},t) \Psi(\mathbf{r},t)\mathrm{d}V =1       \]</span></p><p><span class="math display">\[    \rho(\mathbf{r},t)=\lvert \Psi(\mathbf{r},t) \rvert ^{2}\]</span> - 在空间任何有限体积元中找到粒子的概率为有限值</p><ul><li><p>概率密度在任何时刻都是确定的单值</p></li><li><p>波函数连续可导</p></li></ul><p><strong>玻尔的互补原理</strong> omitted</p><h2 id="不确定性原理">不确定性原理</h2><p><span class="math display">\[    \Delta x \Delta p \geqslant \hbar /2    \]</span> 其中 <span class="math display">\[    \Delta x=\sqrt{(x-\bar{x})^{2}}\]</span> 为坐标不确定度（方差），<span class="math inline">\(\Deltap\)</span> 为动量不确定度（方差）</p><p><strong>能量-时间不确定关系</strong> <span class="math display">\[    \Delta E \Delta t \geqslant  \hbar /2\]</span> 其中 <span class="math inline">\(\Delta E\)</span>为激发态能量不确定度，<span class="math inline">\(\Delta t\)</span>为激发态寿命</p><h2 id="薛定谔方程">薛定谔方程</h2><p>对于自由粒子 <span class="math display">\[    \Psi(x,t)=\Psi_0 \exp\left({\frac{i}{\hbar}(p_x x-Et)} \right)\]</span></p><p>得到方程 <span class="math display">\[    i \hbar \frac{\partial }{\partial t} \Psi(x,t)=-\frac{\hbar^{2}}{2m}\frac{\partial ^{2}}{\partial x^{2}} \Psi(x,t)\]</span></p><p>对于势场中的粒子</p><p><span class="math display">\[    i \hbar \frac{\partial }{\partial t}\Psi(x,t)=\left(-\frac{\hbar^{2}}{2m} \frac{\partial ^{2}}{\partialx^{2}}+U\right) \Psi(x,t)\]</span></p><p><strong>定态薛定谔方程</strong> <span class="math display">\[    \hat{H} \Phi (\mathbf{r})=E \Phi(\mathbf{r})        \]</span></p><p>其中 <span class="math display">\[    \hat{H}=-\frac{\hbar ^{2}}{2m} \nabla ^{2}+U(\mathbf{r})\]</span></p><p>解要看势能 <span class="math inline">\(U\)</span> 和边界条件</p><p>除去定态波函数之外，还有一个时间因子 <span class="math display">\[    T(t) \propto \mathrm{e}^{- \frac{i}{\hbar} Et}\]</span> 故 <span class="math display">\[    \Psi(\mathbf{r},t)=\Phi (\mathbf{r}) \mathrm{e}^{- \frac{i}{\hbar}Et}\]</span></p><p>由此见，定态问题归结为求解定态薛定谔方程.</p><h2 id="量子力学的基本假设">量子力学的基本假设</h2><ul><li>波函数公设</li><li>算符公设（看书）</li><li>测量公设</li><li>Schrodinger方程</li><li>全同性原理</li></ul><h2 id="一维定态">一维定态</h2><h3 id="一维无限深势阱">一维无限深势阱</h3><p><span class="math display">\[    U(x)=    \begin{cases}        \infty, \quad x&lt;0 或 x&gt;L \\        0, \quad 0\leqslant x\leqslant L    \end{cases}\]</span></p><p><span class="math display">\[    \Phi(x)=C\sin (kx+\delta)\]</span> <span class="math display">\[    k= \frac{n \pi}{L}\]</span> 由归一化条件得到定态波函数为 <span class="math display">\[    \Phi(x)=    \begin{cases}        \sqrt{\frac{2}{L}} \sin \frac{n \pi}{L}x, \quad 0\leqslantx\leqslant L \\        0, \quad else    \end{cases}\]</span></p><p>动能的可能值 <span class="math display">\[    E=\frac{k^{2} \hbar^{2}}{2m}=n^{2}E_1\]</span> 其中 <span class="math display">\[    E_1= \frac{\pi^{2} \hbar^{2}}{2m L^{2}}\]</span> 为零点能.</p><p>粒子动量的可能值为 <span class="math display">\[    p=\pm n \frac{\pi \hbar}{L}\]</span> 由此得到波长的可能值为 <span class="math display">\[    \lambda_n= \frac{2L}{n}\]</span> 即阱宽为半波长的整数倍</p><p>得到粒子基态波函数 <span class="math display">\[    \Psi_1(x,t)=\Phi_1(x)\exp(-\frac{i}{\hbar}E_1t)=\frac{1}{2i}\sqrt{\frac{2}{L}}(\mathrm{e}^{ik_1x}-\mathrm{e}^{-ik_1x}) \mathrm{e}^{-\frac{i}{\hbar}E_1t}\]</span>可以看成是频率相同、波长相同、传播方向相反的两单色平面波的叠加——形成物质波的驻波</p><h3 id="一维谐振子">一维谐振子</h3><p>方程很难求解</p><p>谐振子能量 <span class="math inline">\(E\)</span> 是量子化的 <spanclass="math display">\[    E_n=(n+\frac{1}{2})\hbar \omega =(n+\frac{1}{2}) h \nu\]</span> 零点能为 <span class="math display">\[    E_0=\frac{1}{2} \hbar \omega\]</span></p><h3 id="散射问题">散射问题</h3><p><strong>台阶势垒</strong></p><p><strong>方势垒</strong></p><p><strong>隧道效应</strong></p><h2 id="氢原子量子理论">氢原子量子理论</h2><h3 id="能量">能量</h3><p><span class="math display">\[    E_n= -\frac{me^{4}}{2\hbar^{2}(4\pi\varepsilon_0)^{2}}\frac{1}{n^{2}}=-13.6 \frac{1}{n^{2}}    \]</span> 其中 $n=1,2,$称为<strong>主量子数</strong></p><h3 id="角动量">角动量</h3><p>角动量的平方 <span class="math display">\[    L^{2}=l(l+1) \hbar^{2}\]</span> <span class="math inline">\(l=0,1,2,\cdots ,n-1\)</span>称为<strong>角量子数</strong></p><p>角动量最小值可取零（与玻尔假设不同）</p><p>角动量 <span class="math inline">\(z\)</span> 分量 <spanclass="math display">\[    L_z= m \hbar\]</span> <span class="math inline">\(m=-l,-l+1,\cdots, l-1,l\)</span>称为<strong>磁量子数</strong></p><p>对于一定的角量子数 <span class="math inline">\(l\)</span>，磁量子数<span class="math inline">\(m\)</span> 可取 <spanclass="math inline">\((2l+1)\)</span> 个值（与玻尔假设不同）</p><h3 id="塞曼效应">塞曼效应</h3><p>强磁场中纳的光谱线分裂</p><p><span class="math display">\[    \Delta E=- \mathbf{\mu}_l \cdot \mathbf{B}=\frac{e}{2m_0} \mathbf{L}\cdot \mathbf{B}= \frac{e}{2m_0} L_z B=m(\frac{e \hbar}{2m_0})B\]</span> <span class="math inline">\(m=-l,-l+1,\cdotsl-1,l\)</span></p><p>塞曼效应从实验上验证了角动量空间取向的量子化.</p><h3 id="波函数-1">波函数</h3><p>电子波函数可划分为角度部分和径向部分的乘积 <spanclass="math display">\[    \Psi_{nlm}(r,\theta,\phi)=R_{nl}(r) Y_{lm}(\theta,\phi)\]</span> 在空间点 <span class="math inline">\((r,\theta,\phi)\)</span>处，小体积元 <span class="math inline">\(\mathrm{d}V\)</span>中电子出现的概率为 <span class="math display">\[    \lvert R_{nl}(r) \rvert ^{2} \lvert Y_{lm}(\theta,\phi) \rvert ^{2}r^{2} \sin \theta \mathrm{d}r \mathrm{d}\theta \mathrm{d} \phi\]</span> 电子出现在 <spanclass="math inline">\(r—r+\mathrm{d}r\)</span>球壳内概率为 <spanclass="math display">\[    W_{nl}(r)\mathrm{d}r= \lvert r R_{nl}(r) \rvert ^{2}= u_{nl}^{2}(r)\mathrm{d}r\]</span> （这里用到 <span class="math inline">\(Y_{lm}\)</span>的归一化条件）<spanclass="math inline">\(u_{nl}^{2}(r)=r^{2}R_{nl}^{2}(r)\)</span>称为<strong>电子的径向概率密度</strong>.电子径向波函数分布反映了电子分布矩原子核的距离.</p><p>电子出现在 <span class="math inline">\((\theta,\phi)\)</span>附近<span class="math inline">\(\mathrm{d}\Omega\)</span> 立体角内概率为<span class="math display">\[    W_{lm}(\theta,\phi) \mathrm{d}\Omega= \lvert Y_{lm}(\theta,\phi)\rvert ^{2} \mathrm{d}\Omega                       \]</span> （这里用到 <span class="math inline">\(R_{nl}(r)\)</span>的归一化条件）<span class="math inline">\(\lvert Y_{lm}(\theta,\phi)\rvert ^{2}\)</span> 称为<strong>电子的角向概率密度</strong>.电子的角向波函数分布反映了电子在空间分布的对称性；判断成键的方向（分子结构理论）.</p><h3 id="电子的自旋">电子的自旋</h3><p>Stern-Gerlach 实验说明电子有自旋</p><p>电子自旋角动量在任意方向的分量只有两个值 <spanclass="math display">\[    S_z= m_s \hbar\]</span> <spanclass="math inline">\(m_s=-\frac{1}{2},\frac{1}{2}\)</span>称为<strong>自旋磁量子数</strong>，自旋量子数只有一个取值 <spanclass="math inline">\(s=1/2\)</span>.</p><p>自旋角动量大小 <span class="math display">\[    S=\sqrt{s(s+1)}\hbar = \frac{\sqrt{3}}{2} \hbar\]</span></p><p>自旋是相对论效应，无经典运动对应.</p><p>氢原子电子运动状态由四个量子数决定 <spanclass="math inline">\((n,l,m_l,m_s)\)</span> - 主量子数 <spanclass="math inline">\(n\)</span> 决定电子的能量 <spanclass="math inline">\(E_n\)</span> - 轨道角量子数 <spanclass="math inline">\(l\)</span> 决定电子轨道角动量 <spanclass="math inline">\(L\)</span>、能量 <spanclass="math inline">\(E_{nl}\)</span> - 轨道磁量子数 <spanclass="math inline">\(m_l\)</span>，决定轨道角动量的方向 - 自旋磁量子数<span class="math inline">\(m_s\)</span>，决定自旋角动量的方向</p><p><strong>泡利不相容原理</strong><strong>费米子</strong>：自旋量子数取半整数，即$s=1/2,3/2,$，如电子、中子、质子 <strong>玻色子</strong>：自旋为整数，即$s=0,1,<span class="math inline">\(，如氘核、光子(\)</span>s=1<spanclass="math inline">\()、\)</span>$介子和 <spanclass="math inline">\(K\)</span>介子(<spanclass="math inline">\(s=0\)</span>)等</p><p>不可能有两个及以上的电子（费米子）处在同一量子状态.玻色子不受限制.</p><p>原子中具有相同主量子数 <span class="math inline">\(n\)</span>的电子属于同一<strong>主壳层</strong>，主壳层中每一个角量子数 <spanclass="math inline">\(l\)</span>对应一个<strong>支壳层</strong>.</p><p>主壳层 <span class="math inline">\(n\)</span> 能容纳的最多电子数<span class="math display">\[    N_n= 2n^{2}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>物理学</category>
      
      <category>普物</category>
      
    </categories>
    
    
    <tags>
      
      <tag>物理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter18</title>
    <link href="/2022/06/11/Chapter18/"/>
    <url>/2022/06/11/Chapter18/</url>
    
    <content type="html"><![CDATA[<h1 id="量子力学的发展">量子力学的发展</h1><h2 id="普朗克的能量子假说">普朗克的能量子假说</h2><h3 id="热辐射概念">热辐射概念</h3><p>任何物体在任何温度下都要发射电磁波，这种与温度有关的辐射称为热辐射.热辐射的电磁波的波长、强度与物体的温度、物体的性质表面形状有关.</p><p><strong>辐出度</strong>：物体从单位面积上发射的所有各种波长的辐射总功率称为物体的辐出度（辐射力）<spanclass="math inline">\(E(T)\)</span>.</p><p><span class="math display">\[    E(T)=\frac{各个方向辐射的总功率}{面积}=\frac{\Delta P}{\Delta S}\]</span></p><p><strong>单色辐出度</strong>：物体单位表面在单位时间内发出的波长在<span class="math inline">\(\lambda\)</span>附近单位波长间隔内的电磁波的能量为单色辐出度（光谱辐射力）<spanclass="math inline">\(E_{\lambda}\)</span>，即 <spanclass="math display">\[    E_{\lambda}(T)= \frac{\mathrm{d}P_{\lambda}}{\mathrm{d}\lambda}\]</span> <span class="math display">\[    M(T)= \int_{0}^{\infty} M_{\lambda}(T) \mathrm{d}\lambda\]</span></p><p><strong>吸收比</strong>： <span class="math display">\[    \alpha = \frac{吸收的能量}{投入的能量（投入辐射）}\]</span></p><p><strong>单色吸收比</strong>： <span class="math display">\[    \alpha(\lambda,T)=\frac{吸收的某一特定波长的能量}{投入到某一特定波长的能量}\]</span></p><p><strong>单色反射比</strong>： <span class="math display">\[    \rho(\lambda,T)=\frac{反射的某一特定波长的能量}{投入到某一特定波长的能量}\]</span></p><p><strong>单色透射比</strong>： <span class="math display">\[    \tau(\lambda,T)\]</span></p><p><strong>黑体</strong>：<spanclass="math inline">\(\alpha(\lambda,T)=1, \alpha=1\)</span><strong>白体</strong>：<spanclass="math inline">\(\alpha=0\)</span>，<spanclass="math inline">\(\rho=1\)</span> <strong>灰体</strong>：<spanclass="math inline">\(0&lt;\alpha&lt;1\)</span></p><h3 id="热辐射的基本规律">热辐射的基本规律</h3><p><strong>Stefan - Boltzmann 定律</strong> <spanclass="math display">\[    M(T)= \sigma T^{4}\]</span> 即总辐出度与温度的四次方成正比</p><p><strong>Wien 位移定律</strong> <span class="math display">\[    T \lambda_m= b\]</span> 即黑体单色辐出度的极值波长 <spanclass="math inline">\(\lambda_m\)</span> 与黑体温度 <spanclass="math inline">\(T\)</span> 之积为常数.</p><p><strong>基尔霍夫定律</strong> <span class="math display">\[    \frac{E(\lambda,T)}{\alpha(\lambda,T)}= M_{b}(\lambda,T)\]</span>在温度一定时，物体单色辐出度与单色吸收比的比值为与材料无关的普适函数<span class="math inline">\(M_b\)</span>，那么 <spanclass="math inline">\(M_b\)</span> 也就是黑体的单色辐出度.</p><p>这说明<strong>一个好的发射体一定也是好的吸收体</strong></p><h3 id="普朗克的能量子假说-1">普朗克的能量子假说</h3><p><strong>维恩公式</strong>：从经典的Maxwell分布律出发 <spanclass="math display">\[    M(T,v)=\alpha v^{3} \mathrm{e}^{-\beta v/T}\]</span></p><p><strong>瑞利-金斯公式</strong>：从能均分定理出发 <spanclass="math display">\[    M(T,v)=\frac{2\pi v^{3}}{c^{2}} kT\]</span></p><p><strong>普朗克公式</strong> <span class="math display">\[    M_b(\nu ,T)=\frac{2\pi \nu^{2}}{c^{2}}\frac{h\nu}{\mathrm{e}^{\frac{h\nu}{kT}} -1}\]</span></p><p>普朗克的能量子假说（1900） <span class="math display">\[    E_n=nh\nu \quad (n=1,2,\cdots )\]</span></p><p><strong>基尔霍夫定律</strong> 对于灰体， <spanclass="math display">\[    \frac{E_{\lambda}(T)}{\alpha_{\lambda}(T)}=M_{\lambda}(T)\]</span> 右侧是一个普适函数，与灰体本身性质无关.</p><h2 id="光电效应">光电效应</h2><p>锌板、铜网</p><h2 id="康普顿散射">康普顿散射</h2><p>频移 <span class="math inline">\(\Delta \lambda=\lambda-\lambda_0\)</span>随散射角的增大而增大，新波长的谱线强度随散射角 <spanclass="math inline">\(\theta\)</span>的增加而增加，但原波长的谱线强度降低.</p><p>X光子（<spanclass="math inline">\(\nu_0\)</span>）与静止自由电子（<spanclass="math inline">\(m_0\)</span>）弹性碰撞，写出（<spanclass="math inline">\(p,E\)</span>）两个守恒方程（考虑相对论效应） <spanclass="math display">\[    \begin{aligned}    h\nu_0+m_0c^{2}&amp;=h \nu+mc^{2} \\    \frac{h}{\lambda_0}\mathbf{n_0}&amp;=\frac{h}{\lambda} \mathbf{n}+m\mathbf{v} \\    m&amp;=\frac{m_0}{\sqrt{1-v^{2}/c^{2}}} \\    \end{aligned}  \]</span></p><p><strong>康普顿红移公式</strong>： <span class="math display">\[    \Delta \lambda=\lambda-\lambda_0=\frac{h}{m_0c}(1-\cos\theta)=2\lambda_c \sin ^{2} \frac{\theta}{2}\]</span> 其中 <span class="math inline">\(\displaystyle\lambda_c=\frac{h}{m_0c}=2.43 \times 10^{-12} \text{m}\)</span>称为康普顿波长. <span class="math inline">\(\theta\)</span>为光子散射前后夹角</p><p><strong>一般来说，自由电子不可能吸收光子，只能散射光子</strong></p><h2 id="氢原子光谱bohr理论">氢原子光谱/Bohr理论</h2><p><strong>巴尔末公式</strong> <span class="math display">\[    \frac{1}{\lambda}=\frac{4}{B}(\frac{1}{2^{2}}-\frac{1}{n^{2}})\]</span></p><p><strong>Rydberg公式</strong> 任意谱线的波数可以表示为 <spanclass="math display">\[    \tilde{\nu}=R(\frac{1}{m^{2}}-\frac{1}{n^{2}})=T(m)-T(n)    \]</span> 其中 <span class="math inline">\(\displaystyle\widetilde{\nu}=\frac{1}{\lambda}\)</span> 为波数，<spanclass="math inline">\(R\)</span> 为Rydberg常量</p><h3 id="bohr氢原子理论">Bohr氢原子理论</h3><p><strong>Bohr假设</strong>： - 定态条件：电子绕核作圆周运动不辐射能量- 角动量量子化条件 <span class="math display">\[      m v_n r_n=n \frac{h}{2\pi}=n \hbar  \]</span> - 频率条件 <span class="math display">\[    h\nu=E_m-E_n\]</span></p><p>氢原子光谱的Bohr理论解释 氢原子的半径 <span class="math display">\[    r_n=n^{2} \frac{\varepsilon_0h^{2}}{\pi m e^{2}} =n^{2} r_1\]</span> 其中 <span class="math inline">\(r_1=a_0=0.053\text{nm}\)</span> 称为Bohr半径</p><p>氢原子的势能 <span class="math inline">\(U_n\)</span> 和 <spanclass="math inline">\(E_{kn}\)</span> 满足 <span class="math display">\[    \begin{aligned}    U_n&amp;=\frac{2E_1}{n^{2}} \\    E_{kn}&amp;=-\frac{E_1}{n^{2}}    \end{aligned}\]</span> 其中 <span class="math inline">\(E_1=-13.6\text{eV}\)</span></p>]]></content>
    
    
    <categories>
      
      <category>物理学</category>
      
      <category>普物</category>
      
    </categories>
    
    
    <tags>
      
      <tag>物理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter17</title>
    <link href="/2022/06/11/Chapter17/"/>
    <url>/2022/06/11/Chapter17/</url>
    
    <content type="html"><![CDATA[<h1 id="光的偏振">光的偏振</h1><h2 id="偏振光">偏振光</h2><p><strong>光矢量（电矢量）</strong></p><ul><li>线偏振光（平面偏振光）:光振动矢量 <spanclass="math inline">\(E\)</span>始终保持在一个确定的平面内</li><li>圆偏振光：迎着光的传播方向，光矢量绕着光的传播方向旋转，端点轨迹是一个圆，分右旋或左旋</li><li>椭圆偏振光：迎着光的传播方向，光矢量绕着光的传播方向旋转，端点轨迹是一个椭圆&gt; 圆偏振光和椭圆偏振光都可看作是由两个线偏振光的叠加， &gt; <spanclass="math display">\[  E_y=E_{y0}\cos [\omega t-kx+\varphi_1] \\  E_x=E_{x0} \cos [\omega t-kx+\varphi_2]  \]</span></li><li>自然光：大量振幅相同（振动方向、相位不确定）的光的叠加，可以看作两个振动方向相互垂直，没有恒定相位关系的独立光矢量的叠加.<strong>它们的振幅相等，光强各占总光强的一半</strong></li><li>部分偏振光——可以认为是相互垂直的、不等幅的线偏振光的非相干叠加.</li></ul><h2 id="偏振片malus定律">偏振片、Malus定律</h2><p>最简单的偏振片：拉长的碘-聚乙烯醇分子，电矢量沿长链方向振动的光驱动材料中的电子在长链方向振动（类似于电流），故被吸收；电矢量垂直于长链方向的光能够透过.</p><p><strong>马吕斯定律</strong> <span class="math display">\[    I_2=I_1\cos ^{2}\alpha\]</span></p><h2 id="反射和折射时光的偏振">反射和折射时光的偏振</h2><p><strong>布儒斯特定律</strong> 当自然光以<strong>布儒斯特角</strong><spanclass="math inline">\(\theta_B\)</span>入射时，折射光是<strong>部分偏振光</strong>，反射光为<strong>振动方向垂直于入射面的线偏振光</strong>，且此时<strong>反射光与折射光之间的夹角恰为<spanclass="math inline">\(\displaystyle\frac{\pi}{2}\)</span></strong></p><p>有 <span class="math display">\[    \tan \theta_B=\frac{n_2}{n_1}\]</span> 从 <span class="math inline">\(n_1\)</span>射向 <spanclass="math inline">\(n_2\)</span>. ## 晶体的双折射现象</p><p>光入射到某些晶体内时，产生两束沿不同方向传播的折射光.其中一束遵守通常的折射定律，称为<strong>寻常光（o光）</strong>，另一束不遵守折射定律，称为<strong>非常光（e光）</strong></p><p>当光沿着<strong>光轴</strong>方向传播时不产生双折射现象.（光轴只是一个方向） 在光轴方向上o光和e光的折射率相等，传播速度也相同.把包含光轴和任一已知光纤所组成的平面称为晶体中该光线的<strong>主平面</strong>.</p><p>由o光和光轴所组成的平面，称为o光的主平面；类似有e光的主平面.</p><p>一般情况下，o光和e光的主平面并不重合，但当光轴位于入射面内时，这两个主平面是重合的.</p><p>双折射现象出现的原因是晶体结构的各向异性. 介电常数 <spanclass="math inline">\(\varepsilon\)</span>与方向有关，导致沿各不同方向传播的光速不同.</p><p>非常光在晶体中传播时其光矢量方向与光轴间的夹角随传播方向而异，因此其速率在各个方向上不同.</p><p>两束光只有在沿光轴方向上传播时，它们的速率才相等.</p><p>若使光轴与晶体表面平行，并以平行光垂直入射，经过厚度为 <spanclass="math inline">\(d\)</span> 的晶体，两束光之间存在光程差 $=n_o-n_ed $ ，相应相位差 <span class="math display">\[    \Delta \varphi=\frac{2\pi}{\lambda}\delta=\frac{2\pi}{\lambda}\lvertn_o-n_e \rvert d\]</span></p><p>波片厚度满足 <span class="math display">\[    \lvert n_e-n_o \rvert d= \frac{\lambda}{4}\]</span> 的波片称为四分之一波片，相应有二分之一波片（或半波片）.</p><p>两束光矢量方向相同，相差相位为 <span class="math inline">\(\Delta\varphi\)</span>光强为 <span class="math inline">\(I_1,I_2\)</span>的线偏振光叠加后合光强为 <span class="math display">\[    I_{out}=I_1+I_2+2\sqrt{I_1I_2}\cos \Delta \varphi\]</span></p><p>线偏振光通过四分之一波片后变为椭圆或圆偏振光.</p><p>线偏振光通过二分之一波片后变为线偏振光，但是振动方向将转过 <spanclass="math inline">\(2\alpha\)</span> 角，<spanclass="math inline">\(\alpha\)</span>是入射线偏振光振动方向与光轴之间的夹角.</p><p>自然光通过波片后仍为自然光.</p><p>利用四分之一波片，入射线偏振光的振动方向与光轴方向成 <spanclass="math inline">\(\frac{\pi}{4}\)</span> 时，可以得到圆偏振光.</p>]]></content>
    
    
    <categories>
      
      <category>物理学</category>
      
      <category>普物</category>
      
    </categories>
    
    
    <tags>
      
      <tag>物理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter16</title>
    <link href="/2022/06/11/Chapter16/"/>
    <url>/2022/06/11/Chapter16/</url>
    
    <content type="html"><![CDATA[<h1 id="光的干涉与衍射">光的干涉与衍射</h1><p>匀速运动的电子不辐射电磁波，加速运动的电子辐射电磁波</p><h2 id="双缝干涉">双缝干涉</h2><p><strong>Q</strong>：双缝 <spanclass="math inline">\(d\)</span>，平行单色相干光 <spanclass="math inline">\((\lambda,)\)</span></p><ol type="1"><li>合振幅：<span class="math inline">\(\displaystyle E(\theta)=2\cos\frac{\Delta \varphi}{2}E_{p}\)</span>，其中 <spanclass="math inline">\(\displaystyle \Delta \varphi=\frac{2\pid}{\lambda}\sin \theta\)</span></li></ol><p><span class="math inline">\(n\)</span>缝干涉：<spanclass="math inline">\(\displaystyle E(\theta)=E_p \frac{\sin (\frac{n\Delta \varphi}{2})}{\sin (\frac{\Delta \varphi}{2})}\)</span></p><blockquote><p>在一般的参数下，光程差 <span class="math inline">\(\Delta=d \sin\theta\)</span></p></blockquote><ol start="2" type="1"><li>光强分布 首先光强 <span class="math display">\[I(\theta)=4I_0\cos ^{2}\frac{\Delta \varphi}{2}=4I \cos^{2}(\frac{\pi}{\lambda}d\sin \theta)\]</span> <span class="math inline">\(\displaystyleI_0=\frac{1}{2}\sqrt{\frac{\varepsilon}{\mu}}E_p^{2}\)</span></li></ol><p>明条纹：<span class="math inline">\(\displaystyle d\sin \theta= \pm n\lambda\)</span>，暗条纹：<span class="math inline">\(\displaystyled\sin \theta=\pm (2n-1)\frac{\lambda}{2}\)</span></p><p>条纹间距 <span class="math display">\[    \Delta x= \frac{L}{d} \lambda\]</span></p><ol start="3" type="1"><li><strong>衬度</strong> <span class="math display">\[V=\frac{I_{\max}-I_{\min}}{I_{\max}+I_{\min}}\]</span> 两个振幅一样时，衬度最大，为 <spanclass="math inline">\(1\)</span>，若分振幅为 <spanclass="math inline">\(E_1,E_2\)</span>，则衬度为 <spanclass="math display">\[V= \frac{2\sqrt{I_1I_2}}{I_1+I_2}\]</span></li></ol><p><strong>Q</strong>：斜入射双缝，干涉条纹要移动</p><p><strong>Q</strong>：杨氏双缝干涉实验中，要加单缝.</p><blockquote><p>判断明暗条纹时注意半波反射</p></blockquote><p>在双缝干涉实验中，当单缝变宽时，干涉条纹衬度变小. 当单缝宽度达到<span class="math inline">\(D\lambda/d\)</span> （<spanclass="math inline">\(D\)</span> 为单缝与双缝间距离，<spanclass="math inline">\(d\)</span> 为双缝间距）时，干涉条纹完全消失.</p><h2 id="光的相干性">光的相干性</h2><p>相干条件：频率相同、振动方向相同、相位差恒定</p><p>相干光的实现： -对普通光源：相干光必须是来自同一光波列，光源不同点发出的光是非相干的，同一点不同时间发出的光是非相干的，不同光源的光是非相干的- 激光光源：不同光源的光可以是相干的</p><h2 id="薄膜干涉">薄膜干涉</h2><p>光程差 <span class="math display">\[    L= \sum_{i}^{} n_i s_i\]</span></p><p>计算时注意半波损失！</p><h3 id="等倾干涉">等倾干涉</h3><p>连续改变厚度时环心不断冒出环纹，中心处明暗交替；膜的厚度 <spanclass="math inline">\(e\)</span> 减小时，条纹内缩，中心处明暗交替.条纹内疏外密.</p><p><strong>高反射膜</strong></p><p><span class="math display">\[    d= \frac{\lambda}{4n}\]</span></p><p><strong>增透膜</strong></p><p>在玻璃板上喷镀透明介质膜 <spanclass="math inline">\(n&lt;n_0\)</span>在两个面上均有半波损失，膜的厚度为 <span class="math display">\[    d= \frac{\lambda}{4n}\]</span> 时是增透膜</p><h3 id="等厚干涉">等厚干涉</h3><p><strong>劈尖干涉</strong></p><p><span class="math display">\[    2ne+ \frac{\lambda}{2} = k \lambda\]</span> 时为明纹 <span class="math display">\[    2ne+ \frac{\lambda}{2} = (2k+1) \frac{\lambda}{2}\]</span> 时为暗纹 <span class="math display">\[    \Delta e= \frac{\lambda}{2n}\]</span> 从而条纹间距 <span class="math display">\[    \Delta x \thickapprox \frac{\lambda}{2n \theta}\]</span></p><p><strong>牛顿环</strong></p><p><span class="math display">\[    r \thickapprox \sqrt{2eR}\]</span></p><p>明环 <span class="math display">\[    r=\sqrt{\frac{(2k-1)R\lambda}{2n}}\]</span> 暗环 <span class="math display">\[    r= \sqrt{\frac{kR\lambda}{n}}\]</span> 环间距 <span class="math display">\[    r_{k+m}^{2}- r_{k}^{2}= mR\lambda\]</span></p><p>条纹间距内疏外密，半径越大干涉级次越大；中央是暗纹（半波损失） ###迈克尔逊干涉仪</p><p>光拍现象</p><h2 id="单缝衍射">单缝衍射</h2><p><span class="math display">\[    E(\theta)=E_0 \frac{\sin \beta}{\beta},\quad \beta=\frac{\pi a \sin\theta}{\lambda}\]</span> 其中 <spanclass="math inline">\(E_0\)</span>为中央亮纹处的振幅</p><h3 id="条纹分析">条纹分析</h3><p>中央是明条纹，暗条纹条件 <span class="math display">\[    a \sin  \theta = \pm k \lambda, \quad k=1,2,3,\cdots\]</span></p><p>明条纹条件 <span class="math display">\[    a\sin \theta= \pm (2m+1) \frac{\lambda}{2}, \quad m=1,2,\cdots\]</span></p><p>明纹宽度，0级明纹角宽度 <span class="math display">\[    \Delta \theta_0 = 2 \theta_1 \thickapprox  2 \frac{\lambda}{a}\]</span></p><p>第 <span class="math inline">\(k\)</span>级明纹角宽度（也是暗纹角宽度） <span class="math display">\[    \Delta \theta_k = \frac{\lambda}{a}\]</span></p><h2 id="双缝衍射">双缝衍射</h2><p>首先单缝衍射中，上下移动单缝，光屏上各条纹不变.</p><p><strong>Q：</strong> 双缝衍射 <span class="math inline">\((\lambda,a, d, E_0)\)</span>， <span class="math inline">\(E(\theta)=\)</span>？考虑到两个单缝之间有光程差 <span class="math inline">\(\delta=d \sin\theta\)</span>，可以认为相位差 <spanclass="math inline">\(\displaystyle \varphi= \frac{2\pi d \sin\theta}{\lambda}\)</span>，认为是两个有相位差的光合成. 因为 <spanclass="math inline">\(\displaystyle E_p=E_0 \frac{\sin\beta}{\beta}\)</span>，故两个振动的叠加 <span class="math display">\[    E=E_p \frac{\sin \varphi}{\sin (\frac{\varphi}{2})}=E_0 \frac{\sin\beta}{\beta}\frac{\sin (\varphi)}{\sin (\frac{\varphi}{2})}\]</span></p><p>那么 <span class="math inline">\(N\)</span>缝衍射合振幅 <spanclass="math display">\[      E=E_0 \frac{\sin \beta}{\beta} \frac{\sin (\frac{N\varphi}{2})}{\sin(\frac{\varphi}{2})}\]</span></p><p>注意 <span class="math inline">\(\displaystyle \beta=\frac{\pi a \sin\theta}{\lambda}, \varphi= \frac{2\pi d \sin\theta}{\lambda}\)</span></p><p><strong>光栅：</strong>大量等宽等间距的平行狭缝（或反射面）构成的光学元件.</p><h2 id="光栅衍射">光栅衍射</h2><p><strong>多缝干涉主极大：</strong> <span class="math display">\[    d\sin \theta = k\lambda, \quad k=0, \pm 1,\cdots\]</span></p><p>此时所有缝的光振动同相，每个缝的振幅为 <spanclass="math inline">\(E_0\)</span> <span class="math display">\[    E=NE_0 \\    I=N^{2} I_0\]</span></p><p><strong>多缝干涉暗纹：</strong></p><p><span class="math display">\[    d \sin \theta = \pm k \frac{\lambda}{N},\quad k=1,2,\cdotsN-1,N+1,\cdots\]</span></p><p><strong>多缝干涉次级大：</strong> 相邻主极大间有 <spanclass="math inline">\(N-1\)</span>个暗纹，一定有 <spanclass="math inline">\(N-2\)</span>条明纹——次级大</p><blockquote><p>次级大的光强仅为主极大的4%</p></blockquote><p><strong>Q:</strong> <spanclass="math inline">\(k,k+1\)</span>级主极大角间距？ <spanclass="math inline">\((N,d,\lambda)\)</span></p><p><span class="math display">\[    \theta_{k+1}-\theta_k=\frac{\lambda}{d}\]</span></p><p>第 <span class="math inline">\(k\)</span>级主极大本身的角间距 <spanclass="math display">\[    \Delta \theta_k=\frac{2\lambda}{N d}\]</span></p><p>第 <span class="math inline">\(k\)</span>级次级大本身的角宽度 <spanclass="math display">\[    \Delta \theta_k&#39;=\frac{\lambda}{Nd}\]</span></p><ul><li><span class="math inline">\(N\)</span>对条纹的影响： <spanclass="math inline">\(N\)</span>增大，主极大条纹位置不变，次级大增多，主极大条纹变细变锐.</li><li><span class="math inline">\(d\)</span>对条纹的影响：<spanclass="math inline">\(d\)</span>变大，主极大间距变小，条纹变密，中央包线内亮纹数目增大.</li><li><span class="math inline">\(a\)</span>对条纹的影响：<spanclass="math inline">\(a\)</span>减小，单缝衍射中央包线宽度变宽，中央包线内亮纹数目增加.</li></ul><p><strong>缺级现象：</strong> 主极大条件： <spanclass="math inline">\(d \sin \theta =k\lambda, (k=0,\pm 1,\pm 2,\cdots)\)</span> 衍射暗条纹：<span class="math inline">\(a \sin \theta=m\lambda,(m=\pm 1,\pm 2,\cdots)\)</span>在某些情况下，主极大条纹与衍射暗条纹重合，主极大条纹没了，这种现象称之为<strong>缺级现象</strong>.</p><p>条件： <span class="math display">\[    \begin{cases}        \sin \theta =\frac{k\lambda}{d} \\        \sin \theta=\frac{m\lambda}{a}    \end{cases}\]</span> 得到 <span class="math display">\[    k=m\frac{d}{a} , (m=\pm 1, \pm 2,\cdots )    \]</span> 注意 <span class="math inline">\(k,m\)</span>为整数</p><p><strong>光栅的色分辨本领：</strong> <span class="math display">\[    R=\frac{\lambda}{\Delta \lambda}\]</span></p><p><strong>Q:</strong> 怎样定义能分辨两条谱线？ 瑞利判据： 得到 <spanclass="math display">\[    R=\frac{\lambda}{\Delta \lambda}=kN\]</span></p><p>（用 <span class="math inline">\(k\)</span> 级主极大的分辨本领）</p><h2 id="夫琅和费圆孔衍射现象">夫琅和费圆孔衍射现象</h2><p><strong>Airy disk（艾里斑）</strong> 84%</p><p>艾里斑的半角宽：</p><p><span class="math display">\[    d \sin \theta_1 \thickapprox 1.22 \lambda\]</span></p><p>透镜的分辨本领</p><p>如果透镜恰好能分辨两个物点，两个物点对透镜中心张角为 <spanclass="math display">\[    \theta_1=  \frac{1.22\lambda}{d}\]</span></p><p>定义最小分辨角的倒数为透镜的分辨本领. <span class="math display">\[    R=\frac{d}{1.22\lambda}\]</span> <span class="math inline">\(d\)</span> 为透光孔径.</p><h2 id="x射线的衍射">X射线的衍射</h2><p>布拉格公式 <span class="math display">\[    2d\sin \theta=k \lambda\]</span></p>]]></content>
    
    
    <categories>
      
      <category>物理学</category>
      
      <category>普物</category>
      
    </categories>
    
    
    <tags>
      
      <tag>物理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter15</title>
    <link href="/2022/06/11/Chapter15/"/>
    <url>/2022/06/11/Chapter15/</url>
    
    <content type="html"><![CDATA[<h1 id="chapter-15-电磁场与电磁波">Chapter 15 电磁场与电磁波</h1><h2 id="maxwell-电磁理论">Maxwell 电磁理论</h2><p>变化的电场如何产生磁场？</p><p>Maxwell's idea 我们定义了电位移矢量 <spanclass="math inline">\(\mathbf{D}=\varepsilon_0\mathbf{E}+\mathbf{P}\)</span>，其中 <spanclass="math inline">\(\mathbf{P}=n q \mathbf{l}\)</span></p><p>可以定义位移电流密度 <span class="math display">\[    \overrightarrow{j}_D=\frac{\partial \overrightarrow{D}}{\partial t}\]</span> 位移电流强度 <span class="math display">\[    I_D= \iint_S \frac{\partial \overrightarrow{D}}{\partial t} \cdot\mathrm{d} \overrightarrow{S}\]</span></p><p>交换积分号与求导符号可得 <span class="math display">\[    I_D=\frac{\mathrm{d}}{\mathrm{d}t} \mathbf{\Phi}_{D}(t)\]</span></p><p><strong>Q</strong>：RC电流充电电流 <spanclass="math inline">\(I_c\)</span>，极板间的 <spanclass="math inline">\(I_d\)</span>是多少？ 首先 <spanclass="math display">\[    j_D=\frac{\partial \mathbf{D}}{\partial t}\]</span> 而 <span class="math display">\[    \displaystyle  I_D= j_D S=\frac{\mathrm{d}D}{\mathrm{d}t}S=\frac{\mathrm{d}(DS)}{\mathrm{d}t}=\frac{\mathrm{d}(\sigma_0(t)S)}{\mathrm{d}t}=\frac{\mathrm{d}q}{\mathrm{d}t}=I_C\]</span></p><p>一般的在介质中主要是位移电流，在导体中主要是传导电流，位移电流可以忽略不计.</p><p><strong>Q</strong>： （<span class="math inline">\(I_c,I_d\)</span>）共同产生磁场 <spanclass="math inline">\(H\)</span>满足的规律？ <spanclass="math display">\[    \oint_L \mathbf{H} \cdot \mathrm{d}\mathbf{l}=I_c+I_d=\iint_S\frac{\partial \mathbf{D}}{\partial t} \mathrm{d} \mathbf{S}\]</span> 磁场强度的环量等于通过界面的全电流.</p><p>“变化电场激发涡旋磁场”与“变化磁场激发涡旋电场”相对称</p><p><strong>Q</strong>：圆板电容器 <span class="math inline">\(d \llR\)</span>，充电电流 <spanclass="math inline">\(I_c\)</span>，求C内磁场分布.</p><p>算出来是 <span class="math display">\[    \mathbf{H}=\frac{r}{2\pi R^{2}}I_c \quad (r&lt;R)\]</span> 而 <span class="math inline">\(\displaystyle \mathbf{B}=\mu_0\mathbf{H}\)</span>.</p><p><strong>全电流的连续性</strong>：在有 <spanclass="math inline">\(I_c\)</span>的地方 <spanclass="math inline">\(I_d\)</span>几乎为零；在有 <spanclass="math inline">\(I_d\)</span>的地方没有 <spanclass="math inline">\(I_c\)</span>. 事实上 <spanclass="math inline">\(I_D=I_c+ I_d\)</span>连续</p><p><strong>证</strong>：任取任意闭合曲面 <spanclass="math inline">\(S\)</span>，只需证明全电流对于该闭合曲面的通量为零.</p><p>一方面由高斯定理 <span class="math display">\[    \oint_S \mathbf{D} \cdot \mathrm{d} \mathbf{S} =q \\    \oint_S \mathbf{j}_c \cdot \mathrm{d}\mathbf{S}=-\frac{\mathrm{d}q}{\mathrm{d}t}       \]</span> 第一式求导与第二式相加，整理即得.</p><p>对于 T96 其实有（对于整个电容器） <spanclass="math inline">\(\displaystyle I_c=-I_d\)</span></p><p><strong>例</strong>：运动电荷得全电流也连续，形成闭合曲线.</p><p><img src="images/2022-04-18-15-15-48.png" /></p><h2 id="maxwell-方程组">Maxwell 方程组</h2><p><span class="math display">\[    \begin{cases}        \oiint_S \mathbf{D} \cdot \mathrm{d}\mathbf{S}=q_0 \\        \oiint_S \mathbf{B} \cdot \mathrm{d}\mathbf{S}=0 \\        \oint_l \mathbf{E} \cdot \mathrm{d}\mathbf{l}=-\iint_S\frac{\partial \mathbf{B}}{\partial t} \cdot \mathrm{d}\mathbf{S} \\        \oint_l \mathbf{H} \cdot \mathrm{d}\mathbf{l} =I +\iint_S\frac{\partial D}{\partial t} \cdot \mathrm{d}\mathbf{S}    \end{cases}\]</span></p><p>微分形式 <span class="math display">\[    \begin{cases}        \nabla \cdot \mathbf{D}= \rho_0 \\        \nabla \cdot \mathbf{B}= 0 \\        \nabla \times \mathbf{E} =-\frac{\partial \mathbf{B}}{\partialt} \\        \nabla \times \mathbf{H}=\mathbf{j}_c+\frac{\partial\mathbf{D}}{\partial t} \\    \end{cases}\]</span></p><p>自由电磁场的微分形式 <span class="math display">\[    \begin{cases}        \nabla \cdot \mathbf{D}= 0 \\        \nabla \cdot \mathbf{B}= 0 \\        \nabla \times \mathbf{E} =-\frac{\partial \mathbf{B}}{\partialt} \\        \nabla \times \mathbf{H}=\frac{\partial \mathbf{D}}{\partial t}\\    \end{cases}\]</span></p><p>还可以加入介质性质 <span class="math display">\[    \begin{cases}        \mathbf{D} = \varepsilon \mathbf{E} \\        \mathbf{B} = \mu \mathbf{H} \\        \mathbf{j} = \gamma \mathbf{E}    \end{cases}\]</span></p><h2 id="电磁波">电磁波</h2><h3 id="电磁波的性质">电磁波的性质</h3><p>（1）电磁波为横波 （<spanclass="math inline">\(E,H,c\)</span>）：RHR</p><p>（2）波动方程 <span class="math display">\[    \begin{cases}        \frac{\partial^{2} B}{\partial t^{2}}=c^{2} \frac{\partial ^{2}E}{\partial z^{2}} \\        \frac{\partial^{2}E}{\partial t^{2}}=c^{2} \frac{\partial ^{2}B}{\partial z^{2}} \\    \end{cases}\]</span> 我们也有 <span class="math inline">\(\varepsilon_0\mu_0=\frac{1}{c^{2}}\)</span> （3）介质中波速 <spanclass="math inline">\(\displaystyle c=\frac{1}{ \sqrt{\varepsilon\mu}}\)</span></p><p>（4） <span class="math inline">\(E=cB\)</span></p><p><strong>证</strong>：（1）由T99知 <spanclass="math inline">\(\displaystyle E_{0x}=0\)</span>，<spanclass="math inline">\(\displaystyle B_{0x}=0\)</span>其中 <spanclass="math inline">\(x\)</span>是传播方向.</p><p><strong>Q</strong>：<spanclass="math inline">\(E,B\)</span>方向关系？ 仍设传播方向为 <spanclass="math inline">\(x\)</span>轴方向，不妨 <spanclass="math inline">\(E\)</span>的方向为 <spanclass="math inline">\(y\)</span>轴方向.</p><p>设出 <spanclass="math inline">\(B\)</span>的方程，然后利用Maxwell方程组的第三个方程得到<span class="math inline">\(B_y\)</span>与 <spanclass="math inline">\(t\)</span>无关. <spanclass="math inline">\(B_y\)</span>有的分量应该是稳恒的，这不是电磁波，我们不关心.</p><p>（2）波动方程 <strong>Q</strong>：<spanclass="math inline">\(\displaystyle E=E(z,t)\mathbf{i}\)</span>, <spanclass="math inline">\(\displaystyle B=B(z,t)\mathbf{j}\)</span></p><p>（3）光速 折射率 <span class="math display">\[    n=\sqrt{\mu_r \varepsilon_r}\]</span> （4）<span class="math inline">\(E,B\)</span>大小关系 有 <spanclass="math display">\[    \frac{1}{2}\varepsilon E^{2}=\frac{1}{2} \mu H^{2}\]</span> 从而在介质中有 <span class="math inline">\(\displaystyleE=uB\)</span></p><h3 id="电磁波的能量">电磁波的能量</h3><ul><li><p>能量密度 <span class="math display">\[  \omega=\varepsilon_0E^{2}\]</span></p></li><li><p>平均能量密度 <span class="math display">\[  \bar{\omega}=\frac{1}{2}\varepsilon_0E_0^{2}=\frac{1}{2}\mu_0H_0^{2}        \]</span> 注意它与频率无关！</p></li><li><p>能流密度 <span class="math display">\[  S= \omega u\]</span> 单位为 <span class="math inline">\(W/m^{2}\)</span>或 <spanclass="math inline">\(J/(s\cdot m^{2})\)</span> 具体来说 <spanclass="math display">\[  \mathbf{S}=\mathbf{E} \times \mathbf{H}\]</span> 称为玻因廷(Poynting)矢量.</p></li><li><p>波强 <span class="math display">\[  I=\bar{S}=\bar{E}\bar{H}=\frac{1}{2}\sqrt{\frac{\varepsilon}{\mu}}E_0^{2}     \]</span> 即波强与振幅的平方成正比，与频率无关.</p></li></ul><h3 id="电磁波的动量">电磁波的动量</h3><p>注意到 <span class="math display">\[    w=\frac{EH}{c}\]</span> 动量密度 <span class="math display">\[    g= \frac{EH}{c^{2}}=\frac{S}{c^{2}}\]</span> <span class="math display">\[    \bar{g}=\frac{\bar{S}}{c^{2}}=\frac{I}{c^{2}}\]</span></p><p>光压 <span class="math display">\[    P=c \bar{g}= \frac{I}{c}\]</span></p><h3 id="电磁波的产生">电磁波的产生</h3><p>除了开放式LC电路以外，加速运动、变速运动电荷的电场也可以辐射电磁波，振荡电偶极子周围的涡旋磁场与涡旋电场垂直套叠.</p><h3 id="电磁波谱">电磁波谱</h3>]]></content>
    
    
    <categories>
      
      <category>物理学</category>
      
      <category>普物</category>
      
    </categories>
    
    
    <tags>
      
      <tag>物理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter14</title>
    <link href="/2022/06/11/Chapter14/"/>
    <url>/2022/06/11/Chapter14/</url>
    
    <content type="html"><![CDATA[<h1 id="电磁感应">电磁感应</h1><h2 id="电磁感应定律">电磁感应定律</h2><h3 id="法拉第电磁感应定律">法拉第电磁感应定律</h3><p><span class="math display">\[    \varepsilon = - \frac{\mathrm{d}\Phi}{\mathrm{d}t}\]</span></p><p>应用：麦克风、电吉他、磁带、电磁继电器······</p><h2 id="动生电动势">动生电动势</h2><p><span class="math display">\[    \mathbf{E}_k= \mathbf{v} \times \mathbf{B}\]</span></p><p><span class="math display">\[    \varepsilon = \int_{l}^{} (\mathbf{v} \times \mathbf{B}) \cdot\mathrm{d}\mathbf{l}\]</span></p><h2 id="感生电动势感生电场">感生电动势/感生电场</h2><h3 id="感生电场">感生电场</h3><h3 id="感生电动势">感生电动势</h3><p><span class="math display">\[    \varepsilon = -\frac{\mathrm{d}\Phi}{\mathrm{d}t} = - \iint_{S}\frac{\partial \mathbf{B}}{\partial t} \cdot \mathrm{d}\mathbf{S}\]</span> 又有 <span class="math display">\[    \varepsilon= \oint_{l} \mathbf{E}_i \cdot \mathrm{d}\mathbf{l}\]</span></p><p>故 <span class="math display">\[    \oint_{l} \mathbf{E}_i \cdot \mathrm{d}\mathbf{l}=-\frac{\mathrm{d}\Phi}{\mathrm{d}t} = - \iint_{S} \frac{\partial\mathbf{B}}{\partial t} \cdot \mathrm{d}\mathbf{S}\]</span></p><p><strong>Q:</strong> 既有动生电动势，又有感生电动势</p><p><span class="math display">\[    \varepsilon = \int_{a}^{b} (\mathbf{v} \times \mathbf{B}) \cdot\mathrm{d}\mathbf{l} + \int_{a}^{b} \mathbf{E}_i \cdot\mathrm{d}\mathbf{l}\]</span></p><h3 id="感生电场的应用">感生电场的应用</h3><p><strong>涡电流</strong></p><p><span class="math display">\[    I \propto E_i \propto \frac{\partial \mathbf{B}}{\partial t} \propto\omega\]</span></p><p><span class="math display">\[    P \propto \omega^{2}\]</span></p><p>减小涡电流：增大电阻减小电流截面——使用硅钢片叠合</p><p><strong>趋肤效应</strong></p><p>考虑自感，电缆通交变电流时表面电流密度大，电缆有效截面积减小，电阻增大.改善方法：表面镀银，分束法，空心</p><p><strong>电子感应加速器</strong></p><p>利用感生电场持续加速电子</p><p>满足条件： <span class="math display">\[    \mathbf{B} = \frac{1}{2} \bar{\mathbf{B}}\]</span></p><p>即任何时刻，电子轨道上的磁感应强度应为轨道内磁场磁感应强度平均值的一半.</p><p>而且只在第一个 <span class="math inline">\(\frac{1}{4}\)</span>周期内加速.</p><h2 id="自感和互感">自感和互感</h2><h3 id="互感">互感</h3><p>一个回路中电流的变化引起另一回路中产生感生电动势——互感电动势</p><p><span class="math display">\[    \Psi_{21} = M_{21} I_{1}\]</span> <span class="math display">\[    \Psi_{12} = M_{12} I_2\]</span> <span class="math display">\[    M_{12}=M_{21}=M\]</span></p><p><span class="math inline">\(M\)</span>称为互感系数，简称<strong>互感</strong></p><p>若 <span class="math inline">\(M\)</span> 保持不变，则 <spanclass="math display">\[    \varepsilon_{21}=-M \frac{\mathrm{d}I_1}{\mathrm{d}t}\]</span> <span class="math display">\[    \varepsilon_{12}=-M \frac{\mathrm{d}I_2}{\mathrm{d}t}\]</span></p><p>单位为亨利：<span class="math inline">\(H=VA ^{-1} s = \Omegas\)</span></p><p>互感系数在数值上对于一个线圈中电流随时间的变化率为一个单位时，在另一个线圈中引起互感电动势的绝对值.<span class="math inline">\(M\)</span>与两回路的几何形状、相对位置、磁介质等有关.</p><p><strong>Q:</strong> 共轴细长螺线管 <span class="math inline">\(S, l,N_1, N_2\)</span>，求互感</p><p><span class="math display">\[    M= \mu_0 \frac{N_1N_2}{l}S\]</span></p><h3 id="自感">自感</h3><p><span class="math display">\[    \Psi = LI\]</span> <span class="math display">\[    \varepsilon_{L} = -L \frac{\mathrm{d}I}{\mathrm{d}t}\]</span></p><p>标定方向和 <span class="math inline">\(I\)</span> 的标定方向相同.<spanclass="math inline">\(L\)</span>：单位电流变化率在回路中产生的电动势</p><p><strong>Q:</strong> 长螺线管 <spanclass="math inline">\(S,l,N\)</span>，求自感</p><p><span class="math display">\[    L= \mu_0 \frac{N^{2}}{l}S= \mu_0 n^{2} V\]</span> 充有介质时， <span class="math display">\[    L= \mu n^{2} V\]</span></p><p><strong>Q:</strong> 共轴长细螺线管 <spanclass="math inline">\(S,l,N_1,N_2\)</span>，求互感和两自感的关系</p><p><span class="math display">\[    M= \sqrt{L_1L_2}\]</span></p><p><strong>耦合系数</strong> <span class="math display">\[    k= \frac{M}{\sqrt{L_1L_2}}\]</span></p><p>当无漏磁时 <span class="math inline">\(k=1\)</span>，否则一般 <spanclass="math inline">\(k&lt;1\)</span></p><p>考虑自感后，电路的端电压超前电流（如趋肤效应、跳环实验中的互感电动势）</p><p><strong>顺接&amp;反接</strong></p><p>两线圈磁通互相加强，称为<strong>顺接</strong>；互相削弱，称为<strong>反接</strong></p><p><strong>Q:</strong> <span class="math inline">\(L_1,L_2,M\)</span>的两线圈顺接串联的等效总自感</p><p><span class="math display">\[    L=L_1+L_2+2M\]</span></p><p><strong>Q:</strong> <span class="math inline">\(L_1,L_2,M\)</span>的两线圈反接串联的等效总自感</p><p><span class="math display">\[    L=L_1+L_2-2M\]</span></p><p>当两线圈无漏磁耦合，且 <spanclass="math inline">\(L_1=L_2=L_0\)</span>，则顺接时 <spanclass="math inline">\(L=4L_0\)</span>，反接时 <spanclass="math inline">\(L=0\)</span>.</p><h2 id="磁场能量">磁场能量</h2><h3 id="自感磁能">自感磁能</h3><p>线圈存储的磁场能量 <span class="math display">\[    W_{m}= \frac{1}{2} LI^{2}\]</span></p><p><strong>Q:</strong> 长螺线管 <spanclass="math inline">\(\mathbf{B},V,\mu\)</span>，求 <spanclass="math inline">\(W_{m}\)</span></p><p><span class="math display">\[    W_{m}= \frac{1}{2} \frac{B^{2}}{\mu}V\]</span></p><h3 id="磁场的能量密度">磁场的能量密度</h3><p><span class="math display">\[    w_{m} = \frac{1}{2} \frac{B^{2}}{\mu} =\frac{1}{2} \muH^{2}=\frac{1}{2} BH\]</span></p><p><span class="math display">\[    W_{m}= \iiint w_m \mathrm{d}V\]</span></p><h3 id="互感磁能">互感磁能</h3><p>两个线圈存储的磁能 <span class="math display">\[    W_m= \frac{1}{2} L_1I_1^{2}+\frac{1}{2} L_2I_2^{2} \pm MI_1I_2\]</span></p><p>其中 <span class="math inline">\(\pm MI_1I_2\)</span>称为互感磁能</p><p>两个电流建立的磁场相互加强，互感磁能为正两个电流建立的磁场相互削弱，互感磁能为负</p>]]></content>
    
    
    <categories>
      
      <category>物理学</category>
      
      <category>普物</category>
      
    </categories>
    
    
    <tags>
      
      <tag>物理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter13</title>
    <link href="/2022/06/11/Chapter13/"/>
    <url>/2022/06/11/Chapter13/</url>
    
    <content type="html"><![CDATA[<h1 id="磁介质">磁介质</h1><h2 id="顺磁性和抗磁性">顺磁性和抗磁性</h2><p>电子绕原子核作圆周运动 <spanclass="math inline">\((r,\mathbf{v})\)</span>，则磁矩 <spanclass="math display">\[    \mathbf{\mu}_l=\frac{1}{2} e \mathbf{v} r\]</span></p><p>它与角动量之间的关系 <span class="math display">\[    \mathbf{\mu}_l=-\frac{e}{2m} \mathbf{L}\]</span></p><p><strong>电子的自旋磁矩</strong></p><p><span class="math display">\[    \mathbf{\mu}_s=-\frac{e}{m} \mathbf{S}\]</span></p><p>其中 <span class="math inline">\(\mathbf{S}\)</span>为自旋角动量，<span class="math inline">\(\mathbf{\mu}_s\)</span>为自旋磁矩</p><p><strong>分子或原子的磁矩</strong></p><p>所有电子轨道和自旋磁矩的和</p><p><span class="math display">\[    \mathbf{\mu}_m= \sum_{i}^{} (\mathbf{\mu}_{li}+\mathbf{\mu}_{si})\]</span></p><p><strong>顺磁质</strong> <span class="math inline">\(\mathbf{\mu}_m\neq 0\)</span>，如铝、铂、铬等磁介质</p><p><strong>抗磁质</strong> <spanclass="math inline">\(\mathbf{\mu}_m=0\)</span>，如铜、银、铋等</p><p>感应电流对应的附加磁矩与所加外磁场方向相反，顺磁质中存在转向磁化，顺磁性远大于抗磁性，整体表现为顺磁性；抗磁介质无转向磁化，整体表现为抗磁性.</p><h2 id="磁化强度和磁化电流">磁化强度和磁化电流</h2><p>磁化强度等于介质单位体积中分子磁矩 <span class="math display">\[    \mathbf{M}= \lim_{\Delta V \to 0} \frac{\sum_{i}^{}\mathbf{\mu}_{mi}}{\Delta V}\]</span></p><p>其单位为A <span class="math inline">\(\cdot\)</span> m <spanclass="math inline">\(^{-1}\)</span></p><p>抗磁质的 <span class="math inline">\(\mathbf{M}\)</span>方向与外磁场方向相反，顺磁质 <spanclass="math inline">\(\mathbf{M}\)</span> 的方向与外磁场方向相同.</p><p><strong>磁化电流线密度</strong></p><p><span class="math display">\[    \mathbf{\alpha}&#39;= \mathbf{M} \times \mathbf{e}_n\]</span></p><p>穿过任意回路 <span class="math inline">\(L\)</span>的磁化电流等于磁化强度沿回路的线积分，即 <span class="math display">\[    \oint_{l} \mathbf{M} \cdot \mathrm{d}\mathbf{l} =I&#39;\]</span></p><h2 id="介质中的安培环路定理">介质中的安培环路定理</h2><h3 id="磁场强度">磁场强度</h3><p><span class="math display">\[    \mathbf{H}= \frac{\mathbf{B}}{\mu_0} -\mathbf{M}\]</span></p><p>单位为A/m</p><p>对于各向同性的磁介质，当外磁场不太强时，磁介质内任意空间点的磁化强度与该点处的磁场强度成正比，即<span class="math display">\[    \mathbf{M}= \chi_{m} \mathbf{H}     \]</span></p><h3 id="磁导率">磁导率</h3><p>其中 <span class="math inline">\(\chi_{m}\)</span> 称为介质的磁化率.顺磁质的磁化率大于零，磁化强度 <spanclass="math inline">\(\mathbf{M}\)</span> 与磁场强度 <spanclass="math inline">\(\mathbf{H}\)</span> 的方向相同；抗磁质反之.</p><p><span class="math display">\[    \mathbf{B}= \mu_0 (1+ \chi_{m})\mathbf{H}=\mu_0 \mu_r \mathbf{H}\]</span></p><p>其中 <span class="math inline">\(\mu_{r}=1+\chi_{m}\)</span>称为介质的相对磁导率. 令磁导率 <span class="math inline">\(\mu=\mu_0\mu_{r}\)</span> 就有 <span class="math display">\[    \mathbf{B}= \mu \mathbf{H}\]</span></p><h3 id="环路定理">环路定理</h3><p>磁介质中的安培环路定理 <span class="math display">\[    \oint_{l} \mathbf{H} \cdot \mathrm{d}\mathbf{l}= I_0\]</span></p><p><span class="math inline">\(I_0\)</span>为穿越电路面的传导电流的代数和</p><p><strong>磁介质中的高斯定理</strong></p><p><span class="math display">\[    \oiint_{S} \mathbf{B}\cdot \mathrm{d}\mathbf{S}=0\]</span></p><h3 id="磁场的边值关系">磁场的边值关系</h3><p>利用高斯定理和安培环路定理，可以证明，在磁介质界面两侧 <spanclass="math display">\[    B_{1n}=B_{2n} \\    H_{1t}=H_{2t}\]</span></p><p><strong>Q:</strong> <span class="math inline">\(\mathbf{B}\)</span>线在边界面上的折射满足 <span class="math display">\[    \frac{\tan \theta_1}{\tan \theta_2}=\frac{\mu_1}{\mu_2}\]</span></p><h2 id="铁磁性">铁磁性</h2><p><strong>铁磁质</strong> 磁性奇异、剩磁性</p><p><strong>铁磁质的磁滞回线</strong> <spanclass="math inline">\(\mathbf{B}\)</span> 滞后 <spanclass="math inline">\(\mathbf{H}\)</span> 的变化</p><p>具体解释看书</p><p><strong>居里点</strong> 加热到居里点时剩磁性完全消除</p>]]></content>
    
    
    <categories>
      
      <category>物理学</category>
      
      <category>普物</category>
      
    </categories>
    
    
    <tags>
      
      <tag>物理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter12</title>
    <link href="/2022/06/11/Chapter12/"/>
    <url>/2022/06/11/Chapter12/</url>
    
    <content type="html"><![CDATA[<h1 id="稳恒磁场">稳恒磁场</h1><h2 id="电流和电源">电流和电源</h2><h3 id="电流密度">电流密度</h3><p><span class="math display">\[    \mathbf{j}=\frac{\mathrm{d}I}{\mathrm{d}S_{\perp}}\mathbf{e}_n\]</span> 方向为正电荷运动的方向</p><p>载流子 <span class="math inline">\(q\)</span>，数密度 <spanclass="math inline">\(n\)</span>，漂移速度 <spanclass="math inline">\(\mathbf{v}_d\)</span>，则 <spanclass="math display">\[    \mathbf{j}=nq \mathbf{v}_d\]</span></p><p><strong>电流强度</strong></p><p><span class="math display">\[    I= \iint_{S} \mathbf{j} \cdot \mathrm{d}\mathbf{S}\]</span></p><p><strong>电荷守恒定律</strong></p><p><span class="math display">\[    \oiint_{S} \mathbf{j} \cdot \mathrm{d}\mathbf{S}=-\frac{\mathrm{d}q_{内}}{\mathrm{d}t}         \]</span></p><p>若空间电荷分布不随时间变化，则 <span class="math display">\[    \oiint_{S} \mathbf{j} \cdot \mathrm{d}\mathbf{S}=0\]</span></p><p>它称为<strong>稳恒电流条件</strong>，恒定电流产生的电场称为<strong>稳恒电场</strong></p><p>它满足 <span class="math display">\[    \oiint_{S} \mathbf{E}\cdot \mathrm{d}\mathbf{S}=\frac{1}{\varepsilon_0} \sum_{i}^{} q_i    \]</span> 即高斯定理</p><p><span class="math display">\[    \oint_{l} \mathbf{E} \cdot\mathrm{d}  \mathbf{l}=0\]</span> 即保守场</p><h3 id="欧姆定律">欧姆定律</h3><p>微观形式</p><p><span class="math display">\[    \mathbf{j} = \frac{1}{\rho} \mathbf{E}= \sigma \mathbf{E}\]</span> 其中 <span class="math inline">\(\sigma\)</span>称为电导率</p><p><strong>电导率公式</strong></p><p><span class="math display">\[    \sigma= \frac{n e^{2} \tau}{m}\]</span> 其中 <span class="math inline">\(n\)</span>是单位体积的电子数，<span class="math inline">\(\tau\)</span>是平均碰撞时间</p><p><strong>焦耳定律的微分形式</strong></p><p><span class="math display">\[    w=\sigma \mathbf{E}^{2}\]</span></p><h3 id="电源">电源</h3><p><strong>非静电力</strong></p><p><span class="math display">\[    \mathbf{E}_k=\frac{\mathbf{F}_k}{q}\]</span></p><p>比如洛伦兹力、旋电场、温差电源扩散作用、化学电池中溶解和沉积过程</p><p><strong>电源电动势</strong></p><p><span class="math display">\[    E=\frac{W}{q}=\int_{-}^{+} \mathbf{E}_k \cdot \mathrm{d}\mathbf{l}  \]</span></p><p><strong>电源内部的欧姆定律</strong></p><p><span class="math display">\[    \mathbf{j}=\sigma(\mathbf{E}+\mathbf{E}_k)\]</span></p><h2 id="磁场">磁场</h2><p>定义1： <span class="math inline">\(\mathbf{F}=q \mathbf{v} \times\mathbf{B}\)</span></p><p>定义2： <span class="math inline">\(\mathbf{F}= I \mathbf{L} \times\mathbf{B}\)</span>（载流导线在磁场中受力）</p><p>定义3： <span class="math inline">\(\mathbf{M}= \mathbf{m} \times\mathbf{B}\)</span>（小磁针在磁场中受力矩）</p><h2 id="毕奥-萨法尔定律">毕奥-萨法尔定律</h2><p><strong>Biot-Savart-Laplace Law</strong> <spanclass="math display">\[    \mathrm{d}\mathbf{B}= \frac{\mu_0}{4\pi} \frac{I\mathrm{d}\mathbf{l} \times \mathbf{r}}{r^{3}}\]</span></p><p><span class="math inline">\(\mu_0=4\pi \times 10^{-7}\)</span>(T<span class="math inline">\(\cdot\)</span> m/A) 为真空磁导率</p><p><span class="math display">\[    \mathbf{B}=\int_{}^{}  \mathrm{d}\mathbf{B}=\frac{\mu_0}{4\pi}\int_{L}\frac{\mathbf{I} \mathrm{d}\mathbf{l} \times \mathbf{r}}{r^{3}}\]</span></p><p><strong>Q:</strong> 离无限长直电流距离为 <spanclass="math inline">\(r\)</span> 的空间点的磁感应强度的大小为 <spanclass="math display">\[    \mathbf{B}=\frac{\mu_0 I}{2\pi r}\]</span></p><p><strong>Q:</strong> 圆电流轴线上的电场分布</p><p><span class="math display">\[    \mathbf{B}=\frac{\mu_0 IR^{2}}{2(R^{2}+z^{2})^{3/2}}\]</span></p><p>圆电流中心处 <span class="math inline">\(z=0\)</span> <spanclass="math display">\[    B_{O}=\frac{\mu_0 I}{2R}\]</span></p><p>远离圆电流中心 <span class="math inline">\(z\gg R\)</span> <spanclass="math display">\[    B_{P} \thickapprox \frac{\mu_0 IR^{2}}{2z^{3}}= \frac{\mu_0}{2\pi}\frac{IS}{z^{3}}\]</span></p><p>如果令 <span class="math inline">\(\mathbf{m}=IS\mathbf{e}_n\)</span>，则 <span class="math display">\[    \mathbf{B}_{P}= \frac{\mu_0}{2\pi} \frac{\mathbf{m}}{z^{3}}\]</span></p><p>把载流圆线圈称为磁偶极子，<spanclass="math inline">\(\mathbf{m}\)</span>称为载流圆线圈的磁偶极矩，简称为<strong>磁矩</strong>. 如果线圈有 <spanclass="math inline">\(N\)</span> 匝，则 <spanclass="math inline">\(m=NIS\)</span></p><p><strong>Q:</strong> 载流无限长直螺线管轴线上的磁场</p><p><span class="math display">\[    \mathbf{B}_{O}=\mu_0 nI\]</span></p><p>实际上不限于轴线，螺线管内空间各点磁感应强度均相同</p><p><strong>Q:</strong> 半无限长螺线管断面中心处，磁感应强度大小为 <spanclass="math inline">\(\mathbf{B}=\frac{1}{2}\mu_0 nI\)</span></p><h2 id="磁高斯定理安培环路定理">磁高斯定理/安培环路定理</h2><p>通过任意曲面 <span class="math inline">\(S\)</span> 的磁通量为 <spanclass="math display">\[    \Phi_{m}= \iint_{S} \mathbf{B} \cdot \mathrm{d}\mathbf{S}   \]</span></p><p><strong>磁场的高斯定理</strong></p><p><span class="math display">\[    \oiint_{S} \mathbf{B}\cdot \mathrm{d}\mathbf{S}=0\]</span></p><p><strong>安培环路定理</strong></p><p><span class="math display">\[    \oint_{l} \mathbf{B}\cdot \mathrm{d}\mathbf{l}= \mu_0 \sum_{}^{} I\]</span> 当 <span class="math inline">\(I\)</span>穿过环路成右手螺旋定则时取正，反之取负.一条电流多次穿过也要分开计算.</p><p><strong>Q:</strong> 无限大均匀载流平面板外的磁场 <spanclass="math display">\[    B(z)=\frac{\mu_0 \alpha}{2}\]</span> 其中 <span class="math inline">\(\alpha\)</span>为电流线密度</p><p>板内是正比例关系</p><p><strong>Q:</strong> 两个圆电流轴线上的磁场分布，分三种情况</p><p>应用：亥姆霍兹线圈——相互平行的一对相同的圆形线圈，它们之间相隔一个半径并进行缠绕，从而使电流沿相同的方向流过这两个线圈。这种绕组在两个线圈之间产生均匀磁场，其主分量与这两个线圈的轴平行。</p><p><strong>Q:</strong> 计算 （<span class="math inline">\(n,\mathrm{d}\mathbf{l}, S, q, \mathbf{v}\)</span>）在 <spanclass="math inline">\(\mathbf{r}\)</span> 的 <spanclass="math inline">\(\mathrm{d}\mathbf{B}\)</span></p><p><span class="math display">\[    \mathrm{d}\mathbf{B}= \frac{\mu_0}{4\pi}\frac{q \mathrm{d}N\mathbf{v} \times \mathbf{r}}{r^{3}}\]</span> 一个载流子产生 <span class="math display">\[    \mathbf{B}= \frac{\mu_0}{4\pi} \frac{q \mathbf{v} \times\mathbf{r}}{r^{3}}\]</span></p><p>但一定要 <span class="math inline">\(v \ll c\)</span></p><p>由安培环路定理和 Kelvin-Stokes Theorem 可以得到以下结论</p><p><span class="math display">\[    \nabla \times \mathbf{B}= \mu_0 \mathbf{j}\]</span></p><p>即有电流的地方会产生磁场的旋转，没有电流的地方没有磁场的旋涡</p><h2 id="磁场对载流导线的作用">磁场对载流导线的作用</h2><p><span class="math display">\[    \mathbf{F}= \int_{L}^{} I \mathrm{d}\mathbf{l} \times \mathbf{B}\]</span></p><p>磁场对闭合载流导线的作用合力为零</p><p><strong>Q:</strong> 任意形状载流线圈在磁场中受到的磁力矩 <spanclass="math display">\[    \mathbf{M}= \mathbf{m} \times \mathbf{B}\]</span> 其中 <span class="math inline">\(\mathbf{m} = NI\mathbf{S}\)</span></p><p><strong>Q:</strong> 一段电流 <spanclass="math inline">\(I\)</span>，在 <spanclass="math inline">\(\mathbf{B}\)</span> 中安培力的功 <spanclass="math display">\[    A=I \Delta \Phi\]</span></p><p>其中当 <span class="math inline">\(I\)</span> 与 <spanclass="math inline">\(\mathbf{B}\)</span> 成右手关系时，<spanclass="math inline">\(\Phi&gt;0\)</span>. 反之则小于零.</p><h2 id="带电粒子的运动">带电粒子的运动</h2><p><strong>Q:</strong> 圆周运动等效磁矩与 <spanclass="math inline">\(\mathbf{B}\)</span> 的方向关系如何：<spanclass="math inline">\(\mathbf{m}\)</span> 与 <spanclass="math inline">\(- \mathbf{B}\)</span> 同向</p><p><strong>霍尔效应</strong></p><p><span class="math display">\[    U_{H}= R_{H} \frac{IB}{d}\]</span></p><p><span class="math inline">\(d\)</span>是 <spanclass="math inline">\(\mathbf{B}\)</span> 方向上的距离，<spanclass="math inline">\(R_H=\frac{1}{qn}\)</span> 称为霍尔系数</p>]]></content>
    
    
    <categories>
      
      <category>物理学</category>
      
      <category>普物</category>
      
    </categories>
    
    
    <tags>
      
      <tag>物理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter11</title>
    <link href="/2022/06/11/Chapter11/"/>
    <url>/2022/06/11/Chapter11/</url>
    
    <content type="html"><![CDATA[<h1 id="导体和电介质">导体和电介质</h1><h2 id="导体">导体</h2><h3 id="静电平衡">静电平衡</h3><p>表面电场 <span class="math display">\[    \mathbf{E}= \frac{\sigma}{\varepsilon_0} \mathbf{n}\]</span> 其中 <span class="math inline">\(\sigma\)</span> 为电荷面密度.用高斯定理即可证明.</p><p>腔内无电荷，<span class="math inline">\(U_{腔}=U_0, E_{腔内}=0,\sigma_{内}=0\)</span>（内表面无净电荷）</p><p>导体接地，导体电势为零，但电荷不一定全跑光</p><h3 id="唯一性定理">唯一性定理</h3><p>空间电荷分布确定，该空间的电场分布由各个导体的电势（或电量）及区域边界上的电势（或电场）唯一确定.空间电荷分布确定，边界条件确定，则场分布唯一确定.</p><h3 id="静电屏蔽">静电屏蔽</h3><p>接地空腔导体可保护腔外空间不受腔内带电体的影响——全静电屏蔽</p><h3 id="孤立导体">孤立导体</h3><h2 id="电容器及电容">电容器及电容</h2><h2 id="电介质">电介质</h2><h3 id="介质的极化">介质的极化</h3><p><strong>有极分子的转向极化</strong> - 极性分子： <spanclass="math inline">\(p\neq 0\)</span> 分子有固有电偶极矩 - 无极分子：<span class="math inline">\(p=0\)</span> 分子无固有电偶极矩</p><p><strong>无极分子的位移极化（感应极化）</strong>无极分子在外场的作用下正、负电荷中心发生偏移而产生的极化现象</p><h3 id="极化强度">极化强度</h3><p>极化强度 <span class="math display">\[    \mathbf{P}= \lim_{\Delta V \to 0} \frac{\sum_{i}^{}\mathbf{p}_i}{\Delta V}\]</span> 要求 <span class="math inline">\(\Delta V\)</span>宏观小，微观大. <span class="math inline">\(\mathbf{p}_i\)</span>为分子电偶极矩，求和为矢量和，单位为 C/m<spanclass="math inline">\(^{2}\)</span></p><p>完全极化的电介质，单位体积内分子数为 <spanclass="math inline">\(n\)</span>，则极化强度为 <spanclass="math display">\[    \mathbf{P}=n \mathbf{p}\]</span></p><h3 id="极化电荷">极化电荷</h3><p>在介质表面出现电荷，称为极化（束缚）电荷；也可能在体内出现极化电荷.</p><p>面元 $S $处的极化电荷面密度 <span class="math display">\[    \sigma&#39;=\mathbf{P} \cdot \mathbf{e}_n\]</span></p><p>任意闭合面内体极化电荷 <span class="math display">\[    q&#39;=-\oiint _{S} \mathbf{P} \cdot \mathrm{d}\mathbf{S}\]</span></p><h3 id="极化率">极化率</h3><p>对于各向同性的电介质，当外加电场不太强时，介质内任意点的极化强度与该点的总电场强度<span class="math inline">\(E=E_0+E&#39;\)</span> 成正比. <spanclass="math inline">\(E&#39;\)</span> 为介质极化所产生的附加电场强度.<span class="math display">\[    \mathbf{P}= \chi_e \varepsilon_0 \mathbf{E}\]</span> 式中 <span class="math inline">\(\chi_e\)</span>为介质的极化率.一般来说极化率与介质中的电场强度无关，且为无量纲常量.</p><h3 id="介质中静电场的环流定理">介质中静电场的环流定理</h3><p><span class="math display">\[    \oint_{l} \mathbf{E}_0 \cdot \mathrm{d}l=0\]</span></p><h3 id="介质中静电场的高斯定理">介质中静电场的高斯定理</h3><p>定义 <span class="math display">\[    \varepsilon_0 \mathbf{E}+\mathbf{P}=\mathbf{D}\]</span> 则 <span class="math display">\[    \oiint_{S} \mathbf{D} \cdot \mathrm{d} \mathbf{S}= \sum_{S内}^{}q_0       \]</span> <span class="math inline">\(\mathbf{D}\)</span>称为电位移矢量，注意 -电位移矢量并不仅仅由空间自由电荷分布决定，它还与外加电场 <spanclass="math inline">\(\mathbf{E}_0\)</span> 和介质的极化电荷有关； -<span class="math inline">\(\mathbf{D}\)</span> 本身没有物理意义 -上面的定义是对于各向同性线性介质而言的. 对于一般介质， <spanclass="math inline">\(\mathbf{P}\)</span> 与 <spanclass="math inline">\(\mathbf{E}\)</span>关系复杂（张量），如铁电体的电滞回线.</p><p>外电场不太强时 <span class="math display">\[    \mathbf{D}=\varepsilon_0 E+\chi_e \varepsilon_0 E=\varepsilon_0(1+\chi_e)\mathbf{E}\]</span></p><p>令 <span class="math inline">\(\varepsilon_r=1+ \chi_e\)</span>为介质的相对介电常数，则 <span class="math display">\[    \mathbf{D}=\varepsilon_0 \varepsilon_r \mathbf{E}=\varepsilon\mathbf{E}\]</span></p><p>注意，对于永久极化的驻极体，上式并不成立</p><p><strong>Q</strong> 均匀介质球发生均匀极化 <spanclass="math inline">\((\mathbf{P},R)\)</span> 则球内电场为 <spanclass="math display">\[    \mathbf{E}&#39;=-\frac{P}{3 \varepsilon_0}\]</span></p><h3 id="介质交界面两侧电场的关系">介质交界面两侧电场的关系</h3><p><strong>电场强度与界面垂直</strong> 价值两侧电场电矢量位移值 <spanclass="math inline">\(\mathbf{D}\)</span> 连续，则电场强度 <spanclass="math inline">\(\mathbf{E}\)</span> 在介质界面不连续</p><p><strong>电场强度与界面斜交</strong>在介质两侧的电场电位移矢量在界面法线方向分量连续，从而电场强度在界面法线方向的分量不连续；介质两侧的电场强度界面切线方向的分量连续，从而电场电位移矢量在界面切线方向的分量不连续.</p><p>满足折射定律 <span class="math display">\[    \frac{\tan \theta_1}{\tan \theta_2} =\frac{\varepsilon_1}{\varepsilon_2}\]</span></p><p>（与法线夹角）</p><p><strong>有介质时的唯一性定理</strong></p><p>空间电荷分布确定， <span class="math inline">\(\varepsilon_i\)</span>分布确定，在各个子区域的边界上满足 <span class="math display">\[    \mathbf{E}_{it}=\mathbf{E}_{jt}, \quad\mathbf{D}_{in}=\mathbf{D}_{jn}\]</span></p><p>该空间的电场分布由各个导体的电势（或电量）及整个区域边界上的电势（或电场）唯一确定.</p><h2 id="静电场的能量">静电场的能量</h2><p><span class="math inline">\(n\)</span> 个点电荷总电势能 <spanclass="math display">\[    W=\frac{1}{2} \sum_{i}^{} q_i U_i\]</span> 令 <span class="math inline">\(U_i\)</span> 为 <spanclass="math inline">\(q_i\)</span> 以外的电荷在 <spanclass="math inline">\(q_i\)</span> 处的电势</p><p>一个带电体的总电势能 <span class="math display">\[    W=\frac{1}{2} \int_{Q}^{} U \mathrm{d}q\]</span></p><p>把各点电荷彼此分散到无限远的过程中电场力做的功，把这些点电荷从无限远离的状态聚合到给定位置时外力做的功</p><p><strong>带电导体球的静电能（<spanclass="math inline">\(Q,R\)</span>）</strong> <spanclass="math display">\[    W=\frac{Q^{2}}{8\pi \varepsilon_0 R}\]</span></p><h3 id="电场能量密度">电场能量密度</h3><p>电场能密度 <span class="math display">\[    w_{e}=\frac{1}{2} \mathbf{D} \cdot \mathbf{E}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>物理学</category>
      
      <category>普物</category>
      
    </categories>
    
    
    <tags>
      
      <tag>物理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chapter10</title>
    <link href="/2022/06/11/Chapter10/"/>
    <url>/2022/06/11/Chapter10/</url>
    
    <content type="html"><![CDATA[<h1 id="chapter-10-静电场">Chapter 10 静电场</h1><h2 id="库仑定律">库仑定律</h2><p><strong>Coulomb's Law</strong> <span class="math display">\[    \mathbf{F}=\frac{q_1q_2}{4\pi \varepsilon_0 r^{2}}\mathbf{e_r}\]</span></p><p>其中真空介电常数 <span class="math display">\[    \varepsilon_0=\frac{1}{4\pi k}=8.854\times 10^{-12}\]</span></p><p><strong>Charge Conservation</strong> The total electric charge in anisolated system never changes</p><h2 id="电场">电场</h2><p>运动电荷不满足作用力等于反作用力，说明存在第三者——场</p><p>电场强度 <span class="math display">\[    \mathbf{E}=\frac{\mathbf{F}}{q_0}=\frac{q}{4\pi \varepsilon_0r^{2}}\mathbf{e_r}\]</span> <spanclass="math inline">\(q_0\)</span>为试验电荷，电量和线度充分小 &gt;电场可以叠加</p><p><strong>电偶极子（Electric Dipole）</strong>等量异号、间距小的两个点电荷，方向从负电荷指向正电荷</p><p><strong>电偶极矩（Electric Dipole Moment）</strong> <spanclass="math display">\[    \mathbf{p}=q \mathbf{l}\]</span></p><h3 id="电场计算">电场计算</h3><p>电偶极子中垂线上场强为 <span class="math display">\[    \mathbf{E_B}=-\frac{\mathbf{p}}{4\pi \varepsilon_0 r^{3}}\]</span></p><p>电偶极子延长线上任一点 <span class="math inline">\(A\)</span>的场强<span class="math display">\[    \mathbf{E_A}=\frac{2\mathbf{p}}{4\pi \varepsilon_0 r^{3}}\]</span> 其中 <span class="math inline">\(r\)</span>为 <spanclass="math inline">\(A\)</span>到电偶极子中点的有向距离</p><p><strong>Q</strong> 计算电偶极子产生的场强 <spanclass="math inline">\((\mathbf{p},\mathbf{r},\theta)\)</span> 一般情况有<span class="math display">\[    \mathbf{E}=\frac{1}{4\pi \varepsilon_0r^{3}}\left[\frac{3(\mathbf{p}\cdot\mathbf{r})\mathbf{r}}{r^{2}}-\mathbf{p}\right]\]</span></p><p><strong>Q</strong> 计算无限长带电直线距离 <spanclass="math inline">\(d\)</span>处场强 <span class="math display">\[    E=\frac{\lambda}{2\pi \varepsilon_0 d}\]</span></p><p><strong>Q</strong> 半无限长端面 <span class="math display">\[    E_x=E_y=\frac{\lambda}{4\pi \varepsilon_0d}\]</span></p><p><strong>Q</strong> 计算均匀带电细圆环轴线上的电场 <spanclass="math inline">\((q,r,z)\)</span> <span class="math display">\[    E_z=\frac{qz}{4\pi \varepsilon_0 (r^{2}+z^{2})^{\frac{3}{2}}}\]</span></p><p><strong>Q</strong> 计算薄圆盘的电场 <span class="math display">\[    \mathrm{d}E=\frac{\mathrm{d}q}{4\pi \varepsilon_0}\frac{z}{(r^{2}+z^{2})^{\frac{3}{2}}}\]</span> 从而 <span class="math display">\[    E=\frac{\sigma}{2\varepsilon_0}\left(1-\frac{z}{\sqrt{R^{2}+z^{2}}}\right)\]</span> 注意 - <span class="math inline">\(\displaystyle z\to \infty\colon E=0\)</span> - <span class="math inline">\(\displaystyle z \gg R\colon E=\frac{Q}{4\pi \varepsilon_0 z^{2}}\)</span> - <spanclass="math inline">\(\displaystyle z \to 0 \colon E=\frac{\sigma}{2\varepsilon_0}\)</span></p><h3 id="电场力和力矩">电场力和力矩</h3><p><span class="math display">\[    \mathbf{F}=q_0 \mathbf{E}\]</span></p><p><strong>Q</strong> 求电偶极子受的合力 <span class="math display">\[    \mathbf{F}=0\]</span></p><p><strong>Q</strong> 求电偶极子在均匀外电场中受到的电场力偶矩 <spanclass="math inline">\((p,E,\theta)\)</span> <spanclass="math display">\[    \mathbf{M}=\mathbf{p} \times \mathbf{E}\]</span></p><p><strong>Q</strong> 非匀强场中电偶极子的受力 <spanclass="math inline">\((\mathbf{p}=p \mathbf{k},\mathbf{E}(\mathbf{r}))\)</span> <span class="math display">\[    \mathbf{F}=p \frac{\partial \mathbf{E}}{\partial z}\]</span></p><p><strong>Q</strong> 求电偶极子在电场中受力. <spanclass="math inline">\(\mathbf{p}=(p_x,p_y,p_z),\mathbf{E}(\mathbf{r})\)</span> <span class="math display">\[    \mathbf{F}=\mathbf{p} \cdot \nabla \mathbf{E}\]</span></p><blockquote><p>上两个式子是令人迷惑的，会有9组不同的坐标出现</p></blockquote><h2 id="高斯定理">高斯定理</h2><h3 id="电场线">电场线</h3><p><span class="math display">\[    \mathrm{d}N=E\mathrm{d}S \cos \theta\]</span> 其中 <span class="math inline">\(\theta\)</span>是 <spanclass="math inline">\(S\)</span>的法向量 <spanclass="math inline">\(\mathbf{n}\)</span>与 <spanclass="math inline">\(E\)</span>的夹角</p><h3 id="电通量flux">电通量（flux）</h3><p><span class="math display">\[    \mathrm{d}\Phi=\mathbf{E}\cdot\mathrm{d}\mathbf{S}=E\mathrm{d}S_{\perp }\]</span> 其中 <spanclass="math inline">\(\mathrm{d}\mathbf{S}=\mathrm{d}S\mathbf{e_n}\)</span> 从定义看出，电通量可以有正负</p><p>通过任意曲面的电通量 <span class="math display">\[    \Phi=\iint_{S} \mathbf{E}\cdot \mathrm{d}\mathbf{S}\]</span> 当曲面为闭合面时，取外法向</p><h3 id="高斯定理-1">高斯定理</h3><p><strong>立体角</strong></p><p><span class="math display">\[    \mathrm{d}\Omega =\frac{\mathbf{e_r}\cdot\mathrm{d}\mathbf{S}}{r^{2}}=\sin \theta\mathrm{d}\theta\mathrm{d}\varphi\]</span></p><p>以观测点为球心，构造一个单位球面；任意物体投影到该单位球面上的投影面积，即为该物体相对于该观测点的立体角。&gt; 立体角可以小于零（有向面积）</p><p><strong>Q</strong> 计算任意闭合曲面对面内一点 o 所张的立体角（<spanclass="math inline">\(4\pi\)</span>）</p><p><strong>Q</strong> 部分曲面 <span class="math inline">\(\Delta\Omega\)</span>，<span class="math inline">\(q\)</span>，电通量？ <spanclass="math display">\[    \Delta \Phi=\frac{q \Delta \Omega}{4\pi \varepsilon_0}\]</span> <strong>Q</strong> 任意闭合曲面，曲面外一点 o，则积分 <spanclass="math display">\[    \oiint_{S} \frac{\mathbf{e_r}\cdot \mathrm{d}\mathbf{S}}{r^{2}}=0\]</span> 从而任意闭合面，不包围 <spanclass="math inline">\(q\)</span>，电通量为0.<strong>点电荷在闭合面外对电通量无贡献，但对闭合面上的电场强度有贡献</strong></p><p>电场线数目 <span class="math display">\[    N=\frac{q}{\varepsilon_0}\]</span></p><p>高斯定理 <span class="math display">\[    \oiint_{S} \mathbf{E}\cdot\mathrm{d}\mathbf{S}=\frac{1}{\varepsilon_0}\sum_{i}^{} q_i\]</span> <span class="math inline">\(S\)</span>称为Gauss面</p><p>连续情况 <span class="math display">\[    \oiint_{S}\mathbf{E}\cdot\mathrm{d}\mathbf{S}=\frac{1}{\varepsilon_0}\iiint_{V} \rho \mathrm{d}V\]</span></p><h3 id="高斯定理应用">高斯定理应用</h3><p>均匀带电球的电场分布 <span class="math display">\[    \begin{cases}        \displaystyle \mathbf{E}=\frac{\rho}{3 \varepsilon_0}\mathbf{r},r &lt; R \\        \displaystyle \mathbf{E}=\frac{q}{4\pi\varepsilon_0r^{3}}\mathbf{r}, r\geqslant R \\    \end{cases}\]</span></p><h3 id="高斯定理的微分形式">高斯定理的微分形式</h3><p>将高斯定理应用在 <span class="math inline">\(\Delta V\)</span>小体积元上 <span class="math display">\[    \frac{\rho \mathrm{d}V}{\varepsilon_0}=\oiint \mathbf{E}\cdot\mathrm{d}\mathbf{S}=(\nabla \cdot \mathbf{E})\mathrm{d}V\]</span> 最后一步用到数学中的高斯定理 故有 <spanclass="math display">\[    \nabla \cdot \mathbf{E}=\frac{\rho}{\varepsilon_0}\]</span> 即<strong>E的散度等于电荷密度除以 <spanclass="math inline">\(\varepsilon_0\)</span></strong></p><h2 id="电势">电势</h2><h3 id="环路定理">环路定理</h3><p>静电场力为保守力，电场强度沿任意闭合路径的环流为零 <spanclass="math display">\[    \oint_{l}\mathbf{E} \cdot \mathrm{d}\mathbf{l}=0\]</span></p><h3 id="电势-1">电势</h3><p><span class="math display">\[    V_1-V_2= \int_{1}^{2} \mathbf{E}\cdot  \mathrm{d}\mathbf{l}\]</span> <span class="math display">\[    -\mathrm{d}V=\mathbf{E}\cdot \mathrm{d}\mathbf{l}\]</span></p><p>点电荷的电势 <span class="math display">\[    V_{P}=\frac{q}{4\pi \varepsilon_0 r}\]</span></p><p><strong>Q</strong> 均匀带电球体的电势分布 omitted</p><p><strong>Q</strong> 均匀带电球面的电势分布注意球内电场强度为零，故电势不变</p><p><strong>Q</strong> 电偶极子的电势分布 <spanclass="math inline">\((p,r,\theta)\)</span> <spanclass="math display">\[    V=\frac{q}{4\pi \varepsilon_0 r_{+}} -\frac{q}{4\pi \varepsilon_0r_{-}}=\frac{q(r_{-}-r_{+})}{4\pi \varepsilon_0 r_{+} r_{-}}=\frac{p\cos\theta}{4\pi \varepsilon_0 r^{2}}=\frac{\mathbf{p}\cdot \mathbf{r}}{4\pi\varepsilon_0 r^{3}}        \]</span></p><p><strong>电偶极层&amp;电偶极层强度</strong> <spanclass="math display">\[    \mathbf{\tau}=\mathbf{l}\sigma_{e}=\frac{\Delta p}{\Delta S}\]</span></p><p>任意形状电偶极层在A点的电势 <span class="math display">\[    V_{A}=\frac{l\sigma_{e}}{4\pi \varepsilon_0}\Omega, (z&gt;0) \\    V_{A}=-\frac{l\sigma_{e}}{4\pi \varepsilon_0}\Omega, (z&lt;0) \\\]</span> 电偶极层两侧电势差为 <span class="math inline">\(\displaystyle\frac{\tau}{\varepsilon_0}\)</span></p><p><strong>Q</strong> 细带电半圆环 <spanclass="math inline">\((R,x)\)</span> 且 <spanclass="math inline">\(\lambda(\theta)=\lambda_0 \sin \theta\)</span>求直径上任一点的电势 <span class="math display">\[    U(x)=\int_{0}^{\pi} \frac{R\lambda_0 \sin \theta\mathrm{d}\theta}{4\pi \varepsilon_0(R^{2}+x^{2}-2Rx \cos\theta)^{\frac{1}{2}}}=\frac{\lambda_0}{2\pi \varepsilon_0}\]</span> 即直径上各点的电势相同 &gt;通过计算可以发现直径上各点电场方向垂直于直径</p><p>类似有均匀带电半球面在底面电势相等</p><h3 id="电势梯度">电势梯度</h3><ul><li>等势面与电场线处处正交</li><li>电荷移动的始点与终点在同一等势面上，电场力不做功</li></ul><p><strong>电势梯度</strong> <span class="math display">\[    \nabla V=\frac{\mathrm{d}V}{\mathrm{d}n}\mathbf{e_n}\]</span> 大小等于沿着等势面法线方向的空间变化率，指向电势增加的方向</p><p>有 <span class="math display">\[    \mathbf{E}=-\nabla V=-(\frac{\partial V}{\partialx}\mathbf{i}+\frac{\partial V}{\partial y}\mathbf{j}+\frac{\partialV}{\partial z}\mathbf{k})\]</span> 即电场强度和电势梯度的大小相等，方向相反</p><h3 id="电势能">电势能</h3><p><span class="math display">\[    W=qV\]</span> <span class="math display">\[    W_1-W_2=\int_{1}^{2} q_0 \mathbf{E} \cdot \mathrm{d}\mathbf{l}\]</span></p><p><strong>Q</strong> 电偶极子 <spanclass="math inline">\(\mathbf{p}\)</span> 在外电场 <spanclass="math inline">\(\mathbf{E}\)</span> 中的电势能 <spanclass="math display">\[    W_{\text{dipole}}=-\mathbf{p}\cdot \mathbf{E}   \]</span></p>]]></content>
    
    
    <categories>
      
      <category>物理学</category>
      
      <category>普物</category>
      
    </categories>
    
    
    <tags>
      
      <tag>物理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>永久记录（二）</title>
    <link href="/2022/05/21/%E6%B0%B8%E4%B9%85%E8%AE%B0%E5%BD%95%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <url>/2022/05/21/%E6%B0%B8%E4%B9%85%E8%AE%B0%E5%BD%95%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>黄金年代的苏联人给未来人写信，问2020年的达瓦里希是不是已经登上月球登上火星了。</p><p>然而2020年的人还有人在质疑美国登月是不是新闻造假。</p><p>俄罗斯媒体Russia Beyond在11月9日发布了其中很有代表性的5则信息。</p><p>新西伯利亚市《开拓月球和火星》“亲爱的后代，今天是苏联建国100周年（苏联的国庆日是“十月革命”纪念日当天）的特别的日子。热烈祝贺这伟大而光荣的纪念日。我们的时代是很有趣的，想必你们的时代也很有趣吧。我们现在还在建设共产主义，你们应该已经生活在共产主义中了吧。我们相信，你们已经漂亮地开发了我们的美丽的蓝色行星，开拓月球，在火星着陆，不断地向着宇宙进发。太空船是不是已经冲出了银河系了呢？是不是已经和其他行星文明的代表们进行了对科学和文化合作的交涉了呢。”</p><p>摩尔曼斯克市《发现了很多自然的新的秘密》“我们才刚刚向着宇宙踏出第一步，但是你们应该已经可以飞往其他行星了吧。很多现在还没发现的自然的新的秘密已经被发现了吧，核能可以控制了吧，元素的量可以根据人类意愿控制了吧，气候已经可以改变了吧，在北极可以开发花园了吧。希望1917年燃起的列宁永恒思想之火，能够永远在你们的心中燃烧。万岁！”</p><p>梁赞市《我很羡慕你，同志》“我现在的生活非常好。（大学里）有各种出色的自主活动、各种不同的部团。很多（梁赞无线电学院的）同学都参加了研究。我也很喜欢运动、戏剧和电影。每个学期都在教室、实验室和图书馆努力学习。放暑假的时候我们离开集体农庄、荒地，去帮助建设俱乐部、住宅等等设施。这叫做劳暑期劳动。2017年我们的大学大概成为更大更优秀的高等教育机构了吧，应该在为新科学发现和成就的世纪——21世纪——培养工程师吧。我很羡慕你，同志。”</p><p>阿尔汉格尔斯克市《在银河系开拓伟业》“阿尔汉格尔斯克市应该已经没有木制地区和木制道路了吧。在我们这个时代，这个城市正在改造之中。阿尔汉格尔斯克全城都在大兴土木，我们也是一直在工地建设。你们大概过着比我们更好的生活，在银河系建功立业，让地球变得更美丽了吧。很羡慕你们能够迎接祖国苏联的100周年，但我知道你们也会羡慕我们这些一直忙碌的年轻一代。我们有着明确的目标、光明的未来，以及无数想要做的事情。我们的技能、智力、思想、能量，都有用武之处。”</p><p>阿迪格共和国迈科普地区《21世纪是共产主义胜利的世纪！》“沙俄时代是一片荒芜、无法地带的阿迪格，现在已经变成了有着先进的社会主义经济、工业，高识字率、高文化程度的地方。在下一个即将到来的50周年，我相信，进步的人类已经面对所有民族的不共戴天之敌——帝国主义——取得了决定性的胜利。在马克思列宁主义的武装下，在科学预见力量的指引下，我们知道共产主义就会建立起来。愿21世纪是共产主义席卷全球、凯旋的世纪！”</p><p>（感谢微博网友@大江户战士OedoSoldier根据日文版翻译，观察者网根据英文版调整）</p><p><strong>不胜唏嘘</strong></p>]]></content>
    
    
    <categories>
      
      <category>杂项</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随想</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计科导复习</title>
    <link href="/2022/05/18/%E8%AE%A1%E7%A7%91%E5%AF%BC%E5%A4%8D%E4%B9%A0/"/>
    <url>/2022/05/18/%E8%AE%A1%E7%A7%91%E5%AF%BC%E5%A4%8D%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="计算机导论">计算机导论</h1><h2 id="计算机起源">计算机起源</h2><ul><li>计算机是一种能够对各种信息进行存储和处理的工具。</li><li>广义上说，计算机是一种不需人的直接干预，自动完成各种算术和逻辑运算的工具。</li><li>早期的计算工具：手指（十进制）、结绳、算筹（祖冲之算圆周率）、算盘（唐朝开始）</li><li>机械计算机：1642，法国，布莱斯·帕斯卡（BlaisePascal），<strong>加法器</strong>（Pascaline）</li><li>1673，德国数学家，戈特弗里德·莱布尼兹（Gottfried Wilhelm vonLeibniz），<strong>步进计算器（乘法器）</strong>（SteppedReckoner）（虽然叫乘法器，但是能够加减乘除），包含阶梯轴（莱布尼茨轮）</li><li>1804，法国，约瑟夫·雅卡尔（JosephJacquard），基于穿孔卡片的<strong>提花织布机</strong>（JacquardLoom），可程式化机器的里程碑</li><li>1822，英国数学家，查尔斯·巴贝奇（CharlesBabbage），<strong>差分机</strong>（Difference Engine）</li><li>1833，英国数学家，查尔斯·巴贝奇，<strong>计算机之父</strong>，<strong>分析机</strong>（AnalyticalEngine）模型，<strong>该模型包括了现代计算机所具有的5个基本组成部分</strong>，没造完</li><li>英国，拜伦之女，爱达·拉芙拉斯（Ada A. LovelaceByron）为分析机贡献一生（翻译巴贝奇的《分析机概论》，建议分析机用二进制，指出分析机可以编程，发现编程的基本要素，如循环、子程序），被誉为<strong>世界上第一位程序员</strong>。为纪念她，1979年美国国防部的一种编程语言命名为<strong>Ada语言</strong>。</li><li>机电计算机：1866，美国统计学家，赫尔曼·霍勒瑞斯（HermanHollerith），<strong>制表机</strong>（TabulatingMachine）；1896创建<strong>制表机公司</strong>（Tabulating MachineCompany,TMC），1911年与另外两家公司合并为CTR，后改名<strong>IBM</strong></li><li>1934<sub>1941，德国，康拉德·祖斯，全部采用继电器的通用计算机Z-3，<strong>世界上第一台完全由程序控制的机电计算机</strong>（Z-1：1934</sub>1938，纯机械；Z-2：因战争搁浅）</li><li>1939~1942，约翰·阿塔诺索夫（John V.Atanasoff），ABC计算机（Atanasoff-BerryComputer），<strong>被遗忘的计算机之父</strong>，最早的电子管计算机（核心部件：控制器）</li><li>1940~1942，阿兰图灵，巨人（Colossus），破译德国恩尼格码机（Enigma）密码</li><li>1944，美国，霍华德·艾肯（Howard H.Aiken），受分析机启发造机电计算机Mark-I(ASCC)；1945~1947造Mark-II，均由IBM赞助（Mark-I部分使用继电器，Mark-II全部使用继电器），1945年bug这个词在Mark-II研制过程中诞生了</li><li>格瑞斯·霍普海军少将，编译语言之母，美国海军计算机化之母，IT界十大最有远见的人才中唯一的女性，发现了计算机程序中的第一个bug，创造了计算机世界最大的Bug——千年虫（Y2K），实现了第一个编译语言和编译器，创造了世界上第一种商业编程语言COBOL</li><li>电子计算机：1946年2月15日，ENIAC (Electronic Numerical IntegratorandComputer)，世界上第一台电子数字计算机，美国宾夕法尼亚大学。<strong>莫奇利</strong>提出总体设计，<strong>埃克特</strong>负责工程技术，<strong>戈尔斯坦</strong>负责组织协调。程序是<strong>外插型</strong>的而不是<strong>存储程序型</strong></li><li>1951年6月14日，埃克特-莫奇利计算机公司生产UNIVAC，交付美国人口统计局使用</li></ul><h2 id="计算机发展">计算机发展</h2><ul><li>第一代计算机（电子管时代1946-1958）：<strong>物理器件</strong>使用电子管，<strong>内存储器</strong>使用汞延迟线，使用穿孔卡片机作为数据和指令的输入设备，用磁鼓纸带或卡片作为<strong>外存储器</strong>；<strong>运算速度</strong>为每秒几千到几万次，使用机器语言和汇编语言编写程序；主要用于科学计算</li><li>第二代计算机（晶体管时代1958-1964）：晶体管代替了电子管；磁芯存储器作主存，磁盘与磁带作辅存；运算速度提高到每秒几十万次基本运算，在软件方面配置了子程序库和批处理管理程序，出现了FORTRAN、COBOL、ALGOL等高级语言及其相应的编译程序；应用于科学计算、数据处理、实时控制</li><li>第三代计算机（中小规模集成电路1964-1971）：<strong>物理器件</strong>使用中、小规模的集成电路；<strong>内存储器</strong>用半导体代替了磁芯体；使用微程序设计技术简化I/O处理机；<strong>外存</strong>使用磁带、磁盘；结构化程序设计语言，多道程序、并行处理、虚拟存储系统以及功能完备的操作系统，大量面向用户的应用程序&gt;摩尔定律：1965，牙膏厂名誉董事长戈登·摩尔，单位面积晶体管数目每18-24个月（1969年修正为18个月）增加一倍，成本下降一半；1995，牙膏厂董事会主席罗伯特·诺伊斯预见摩尔定律受到经济因素——<strong>摩尔第二定律</strong></li><li>第四代计算机（大规模、超大规模集成电路1971-）：微处理器或超大规模集成电路取代了普通集成电路，光学字符识别和条形码等技术输入，互联网广泛应用，形成所谓的地球村······</li><li>第五代计算机：使计算机能够具有像人一样的思维、推理和判断能力，向智能化发展，实现接近人的思维方式；在智能计算机领域完成了大量的基础性研究工作，促进了人工智能和机器人技术的发展；包括神经网络计算机、生物计算机、光子计算机、量子计算机等非电子计算机</li></ul><h2 id="计算机应用">计算机应用</h2><ul><li><p>超算：1976，美国克雷公司，Cray-1，每秒2.5亿次；西摩·克雷主导开发Cray-1至Cray-3，由于技术瓶颈没有Cray-4</p></li><li><p>2009，天河一号；2016，神威·太湖之光；2018，Summit；2021，Fugaku（富岳）</p></li><li><p>发展趋势：巨型化、微型化、网络化、智能化</p></li><li><p>分类：按用途分为通用计算机和专用计算机；按所处理对象的表现形式分为模拟计算机、数字计算机和混合型计算机；按综合性能指标分为巨型机、大型机、小型机、微型机（个人计算机）、工作站和服务器## 计算机学科</p></li><li><p>计算机理论和结构：奠基人为阿兰图灵（1936年发表论文论可计算数及其在判定问题中的应用<strong>奠定了计算机理论基础</strong>，1950年10月发表了论文<strong>计算机和智能</strong>，并提出了<strong>图灵测试</strong>）和冯诺依曼（1944年夏天，戈尔斯坦偶遇冯·诺依曼，后者了解了正在研制中的ENIAC，并提出过建议；冯诺依曼于1945年总结了EDVAC方案）</p></li><li><p>EDVAC奠定了现代计算机基本结构：明确了计算机的5个组成部分（存储器、运算器、控制器、输入设备、输出设备），采用二进制计数和计算，采用存储程序方式</p></li><li><p>中央处理器包括运算器和控制器</p></li><li><p>中国国家最高科学技术奖获得者：吴文俊、王选、金怡濂</p></li><li><p>1956年，中科院计算技术研究所，国营738厂，103机</p></li><li><p>1958年8月1日，103机完成了四条指令的运行，标志着由中国人制造的第一架通用数字电子计算机正式诞生</p></li><li><p>计算机相关协会：美国计算机学会(ACM)，电气和电子工程师协会(IEEE)（目前全球最大的非营利性专业技术学会），计算机系统协会(USENIX)，管理科学与运筹学协会(INFORMS)，美国科学促进会(AAAS)，工业工程和应用数学协会(SIAM)</p></li><li><p>数学物理相关协会：美国数学学会(MAA)，美国统计学会(ASA)，美国物理学会(APS)，美国物理联合会(AIP)</p></li><li><p>船建、材料相关协会：拖拽水池联合会(ITTC)，船舶与海洋结构联合会(ISSC)，美国材料研究学会(MRS)</p></li><li><p>计算机学科中国学会：中国计算机学会(CCF)，中国电子学会(CIE)，中国通信学会(CIC)，中国运筹学会(ORSC)，中国中文信息学会(CIPS)</p></li><li><p>计算机学科扩充后也称<strong>计算学科</strong></p></li><li><p>计算机专业教学：1962年美国普渡大学首开计算机学位课程，1991年IEEE-CS/ACM发布ComputingCurricula(CC1991)，目前到CC2020；1956年哈尔滨工业大学首开计算装置与仪器专业，1998年统一为计算机科学与技术专业，2001年增设软件工程专业，2015年增设网络空间安全专业，2019年增设人工智能专业</p></li></ul><h1 id="程序设计语言">程序设计语言</h1><h2 id="概述">概述</h2><p>计算机指令、符号指令、高级编程语言——本质：不同抽象程度的程序表达</p><h2 id="组成">组成</h2><p>语言要素：字符集、词汇、语法、语义四大基本成分：数据成分（描述程序所涉及的对象——数据）、运算成分（用以描述程序中所包含的运算，算术运算、逻辑运算、字符串运算等）、控制成分（用以控制程序中所含语句的执行顺序）、传输成分（用以描述程序中的数据传输操作）-数据成分：数据是客观事物在计算机内的（格式化）表示，是程序所操作和处理的对象，程序中的数据通常应该<strong>先说明、后使用</strong>- 运算成分：运算符、表达式 -控制成分：提供一种<strong>基本框架</strong>，三种基本控制结构——顺序结构、条件选择结构、重复结构-翻译程序：源程序到目标程序，常见的有——汇编语言通过汇编器到机器语言、高级语言通过编译器到机器语言或汇编语言、高级语言通过解释器边解释边执行不产生目标程序-编译程序：把源程序编译为机器语言目标程序后，再由计算机运行，如C/C++、Java。解释程序：解释器直接解释并且执行源语言程序，不产生目标程序，如BASIC、VB、Python、JavaScript- 编译器与解释器 -国产编译器：华为方舟编译器（一个统一编程平台，包含编译器、工具链、运行时）</p><h2 id="发展">发展</h2><ul><li>1946机器语言（ENIAC）计算机的指令系统，使用二进制编码。现已不直接用机器语言编制程序</li><li>1951 汇编语言 （Grace Hopper,助记符来表示操作码/符和操作数地址）</li><li>1957 高级语言（IBM, John Backus, FORTRAN），目前主流</li><li>1970 结构化语言（Niklaus Wirth, Pascal）</li><li>1995 面向对象语言（Sun, James Gosling, Java）类、对象</li><li>2006 图形化编程</li></ul><h3 id="高级语言范式分类法">高级语言范式（分类法）</h3><p><img src="img/计科导复习/1652886440718.png" /></p><ul><li>过程式语言：基于动作的语言，顺序执行的操作命令序列（FORTRAN、ALGOL、BASIC、Pascal、C）</li><li>面向对象语言：数据封装到对象，问题对象之间的交互（Smalltalk、Ada、C++、Java、C#）</li><li>函数式语言：关注函数计算，而不是计算机状态改变（LISP、ML、Haskell）</li><li>逻辑语言：数理逻辑约束的列举，描述事实与规则，而非具体解决过程（Prolog）</li></ul><h1 id="数据结构与算法">数据结构与算法</h1><h2 id="概述-1">概述</h2><ul><li>数据结构——数据的特性以及数据之间存在的关系</li><li>1968，美国，唐·欧·克努特教授，《计算机程序设计技巧》第一卷《基本算法》，第一本较系统地阐述数据的逻辑结构和存储结构及其操作的著作。</li><li>图书馆的书目检索自动化问题——线性的数据结构</li><li>人机对弈问题——一棵倒置的树</li><li>多岔路口交通灯的管理问题——图</li><li>基本概念：<strong>数据</strong>——所有能输入到计算机中并被计算机程序处理的符号的总称，<strong>数据元素</strong>——组成数据的基本单位（又称为<strong>结点</strong>或<strong>记录</strong>），<strong>数据项</strong>——一个数据元素可由多个数据项组成，是数据不可分割的最小单位（又称为字段或域），<strong>数据结构</strong>——一门研究非数值计算的程序设计问题中计算机操作对象以及它们之间关系和操作的一门学科（三要素：对象、关系、操作），<strong>数据对象</strong>——性质相同的数据元素的集合，是数据的一个子集（如一个班的成绩表），数据结构——数据元素集合（也可称数据对象）中各元素的关系/相互之间存在特定关系的数据元素集合</li><li>数据结构的三方面内容：逻辑结构、存储结构（物理结构）、数据的运算</li><li>算法的设计取决于选定的数据逻辑结构，而算法的实现依赖于采用的存储结构</li><li>顺序存储结构：逻辑上相邻的结点存储在物理位置上相邻的存储单元里，结点间的逻辑关系由存储单元的邻接关系来体现。即只存储结点的值，不存储结点之间的关系。<strong>通常顺序存储结构是借助于语言的数组来描述的</strong></li><li>线性表：<strong>由n个性质相同的数据元素构成的有限序列</strong>。线性表中的数据元素类型多种多样，但同一线性表中的元素必定具有相同特性，即属同一数据对象，相邻数据元素之间的关系是线性的，存在着序偶关系。</li><li>线性表的基本运算：表的初始化、求表长、存取表中的结点、查找结点、插入结点、删除结点等</li><li><strong>顺序表</strong>是线性表的顺序存储表示的简称，它指的是，“用一组<strong>地址连续</strong>的存储单元<strong>依次存放</strong>线性表中的数据元素”，即以“<strong>存储位置相邻</strong>”表示<strong>位序相继的两个数据元素之间的前驱和后继的关系</strong>（有序对<spanclass="math inline">\(&lt;a_{i-1},a_i&gt;\)</span>）””，并以表中第一个元素的存储位置作为线性表的起始地址，称作<strong>线性表的基地址</strong></li><li>顺序表特点：以元素在计算机内物理位置相邻来表示线性表中数据元素之间的逻辑关系；是随机存取的存储结构，只要确定了存储线性表的起始位置，线性表中的任一数据元素可随机存取；数据中的元素间的地址是连续的；数组中所有元素的数据类型是相同的</li><li>顺序表优劣：可以随机定位任意元素，直接实现定位操作；可用数组直接定义顺序存储结构的线性表，便于程序设计/需预先确定数据元素最大个数，即预先分配相应的存储空间，不便于扩充表；插入与删除运算的效率很低</li><li><strong>顺序存储适用于不常进行插入和删除操作、表中元素相对稳定的线性表</strong></li></ul><h2 id="典型数据结构">典型数据结构</h2><p>栈和队列：两种特殊的线性表 -栈（stack）：在表中，允许插入和删除的一端称作<strong>栈顶</strong>（Top），不允许插入和删除的另一端称作<strong>栈底</strong>（Bottom），当栈中无数据元素时，称为<strong>空栈</strong>。- 入栈/进栈（push），出栈/退栈（pop），后进先出（LIFO） -bottom为null表示栈不存在，top=bottom表示空栈，非空栈中的栈顶指针始终在栈顶元素的下一个位置上- 算术表达式求值（操作数、运算符、界限符） - 递归（直接、间接）——汉诺塔-队列（queue）：在表中，允许插入的一端称作<strong>队尾</strong>（Rear），允许删除的另一端称作<strong>队头</strong>（Front）- 进队/入队，出队，FIFO - 顺序队列、循环队列 -使用一维数组来作为队列的顺序存储空间，另外再设立两个指示器：一个为指向队头元素位置的指示器front，另一个为指向队尾的元素位置的指示器rear- 作业排队问题 -树：节点（node），根节点（root），边（edge），父节点，度，叶节点，兄弟节点，子树（除根节点外的其余结点被分为多个不相交的集合，每个集合本身也是树），路径，祖先- 深度（depth）/层次（level），规定根节点深度为0 -高度（height）：由该节点到叶节点的最长路径的长度，所有叶节点高度为0 -树的高度/深度：其根节点的高度 - 树的层次数：树的高度加一 -二叉树，空树，真二叉树（所有节点的子节点数为0或2），满二叉树（最后一层节点的度都为0，其他节点的度都为2），完全二叉树（从根节点至倒数第二层是一颗满二叉树，最后一层叶节点靠左对齐）- 优先队列：键值/优先级，最大/小优先队列——银行服务、网络带宽管理 -优先队列的实现-二叉堆（一颗完全二叉树）：任意节点的键值小于等于其子节点的键值；对于任意子树，其根结点为键值最小的结点；-enqueqe（将新加入的元素插入为最右侧的叶结点，向上调整，将新加入的元素的键值与其父结点的键值比较，调整至合适的位置使二叉树满足二叉堆的性质），deqeueuMin（删除根结点，用最右侧的叶结点替代根结点，将根结点键值与左、右子树的根结点键值相比较，与其中较小者交换；重复上述操作，直到其为叶结点，或键值小于等于左、右子树的根结点键值），getMin</p><h2 id="算法基本概念">算法基本概念</h2><ul><li>波斯，数学家，数学家花拉子密（al-Khwārizmī）</li><li>算法的特征：输入（零个或以上）、输出（一个或以上）、明确性、有限性、有效性</li><li>时间复杂度、空间复杂度</li><li>O-Notation, <span class="math inline">\(\Omega-\)</span> Notation,<span class="math inline">\(\Theta-\)</span> Notation, o-Notation, <spanclass="math inline">\(\omega-\)</span> Notation</li></ul><h1 id="算法图探索">算法·图探索</h1><ul><li>1736，欧拉，戈尼斯堡七桥问题，图论元年妖怪图：每个点关联三条边；边染色需用四种颜色，使公共顶点的边异色；任意切断三条边不会断裂成两个有边的子图（除单点）</li></ul><h2 id="图论基本概念">图论基本概念</h2><ul><li>无向图，顶点，边，顶点集，边集，度，悬点（度为1），悬边，奇点，偶点，正则图（每点度都相同），完全图</li><li>有向图，起点/尾，出边，终点/头，入边，有向完全图，出度，入度</li><li>子图，真子图，生成子图/支撑子图（子图中包含了原图所有的顶点），导出子图（子图中顶点包含了原图中所有的边），平凡子图</li><li>无向树，高（树中最长路的长），树叶，分支点，树枝，有根树</li><li>生成树，连通图生成树不唯一</li></ul><h2 id="广度优先搜索">广度优先搜索</h2><ol type="1"><li>从顶点𝑆开始标记，将𝑆标记为0。</li><li>找出与𝑆相邻的顶点，给它们做标记1(𝑆)（广度优先搜索算法给出的顶点标记指明了该顶点与𝑆之间的距离，以及该顶点的从𝑆到它的一条最短路上的前驱）。</li><li>考虑每个与标记为1的顶点𝑉𝑉相邻的未标记的顶点，把这些顶点加1，标记为2(𝑉)。</li><li>按(3)的方式继续进行，直到𝐺中没有与已标记顶点相邻的未标记顶点为止，算法结束。<img src="img/计科导复习/1652955674112.png" /> 使用队列</li></ol><h2 id="深度优先搜索">深度优先搜索</h2><ol type="1"><li>标记源点𝑠；从𝑠出发搜索𝑠的邻接点𝑢。</li><li>以𝑢作为新的出发点，标记𝑢并记录𝑢的父亲。</li><li>搜索𝑢的邻接点。若𝑢有未标记过的邻接点𝑣，则将𝑣作为新的出发点重复(2),(3)步骤；若𝑢没有未标记过的邻接点，则退回𝑢的父亲结点并重复(3)。</li><li>重复上述步骤直至图中所有顶点均已被访问为止。 <imgsrc="img/计科导复习/1652956389908.png" /> 使用栈</li></ol><h1 id="算法贪心算法">算法·贪心算法</h1><h2 id="简介">简介</h2><ul><li>工厂生产问题：按照商品生产结束时间对商品进行排序，优先选择结束时间早的商品，贪心算法正确性与贪心策略有关</li><li>背包问题：贪心算法不一定对，但在工程中是快速算法（可以优化为1/2-近似算法），可以利用动态规划求最优解</li><li>硬币找零问题，美国邮票问题，贪心算法正确性与问题设定有关</li></ul><h2 id="最短路算法">最短路算法</h2><p>Dijkstra算法：将顶点集合𝑉分为两个集合，𝑆表示已经求得最短路径的顶点集合，构建优先级队列𝑄=𝑉−𝑆。每一步将𝑄中与源点距离最小的顶点𝑢加入集合𝑆，更新𝑢相邻顶点与𝑠的距离。直至𝑆中包含所有顶点。</p><h2 id="应用">应用</h2><p>Dijkstra算法：单源最短路径，带权图，地图寻址、最优路线Prim和Krustal算法：最小生成树，带权图，管道铺设、最小开销哈夫曼树和哈夫曼编码：将字母转化为不等长二进制字符串，降低存储量，信道传输、文件存储</p><h1 id="图灵机">图灵机</h1><h2 id="定义">定义</h2><p>重要性：描述了可计算的极限，计算机从原则上讲是有限制的，用程序我们可以解决什么问题，奠定了现代数字计算机的基础</p><p>基本概念：五个组成部分 - 一组<strong>有限的符号</strong>，<spanclass="math inline">\(\{s_1,\cdots ,s_n\}\)</span> -一条无限长的纸带，纸带被划分为一个接一个的小格子 -一个读写头，可以在纸带上左右移动，可以读写当前所指的格子上的符号 -一组状态寄存器， <span class="math inline">\(\{q_s,q_1,\cdots,q_m,q_h\}\)</span> ，保存图灵机当前所处的状态 -一套控制规则（指令集），根据当前机器所处的状态和当前读写头所指的格子上的符号来确定读写头下一步的动作，并改变状态寄存器的值，令机器进入一个新的状态</p><p>三个主要性质：抽象性（表示计算机的假想装置而非实用的计算技术），简洁性（根据规则表操作磁带上的符号），自动性（自动计算程序）</p><h2 id="基础图灵机">基础图灵机</h2><p>多带图灵机：输入纸带，工作纸带，输出纸带 用 <spanclass="math inline">\((\Gamma, Q, \delta)\)</span> 元组表示，其中：<spanclass="math inline">\(\Gamma\)</span> 为一组有限的符号集，<spanclass="math inline">\(Q\)</span> 为一组有限的状态集，<spanclass="math inline">\(\delta\)</span> 为一个状态转移函数</p><h2 id="图灵机变体">图灵机变体</h2><ul><li><span class="math inline">\(\{0,1, , \Box \}\)</span>和复杂的语言系统：如果函数 <span class="math inline">\(f\)</span>可由复杂符号集计算，那么可用简单符号集计算（ASCII编码）</li><li>多带图灵机可用单带图灵机计算</li><li>双向纸带在 <span class="math inline">\(T(n)\)</span>时间内计算，可用单向纸带在 <span class="math inline">\(4T(n)\)</span>时间内计算</li><li>非确定型图灵机：对于任意一个非确定型图灵机𝑀，存在一个确定型图灵机<span class="math inline">\(\tilde{M}\)</span>，使得它们的语言相等</li></ul><p>邱奇-图灵论题（Church TuringThesis）：逻辑和数学中的各类计算模型均可由图灵机来表示。</p><h1 id="计算机组成">计算机组成</h1><p>“中国计算机之母”——夏培肃</p><p>计算机组成主要研究计算机系统结构的逻辑实现和工作原理，包括机器内部的数据流和控制流的组成及逻辑设计等。</p><h2 id="基础知识">基础知识</h2><p>冯·诺伊曼计算机的特点 - 指令和数据都用二进制代码表示 -指令和数据都以同等地位存放于存储器内，并可按地址寻访 -指令在存储器内顺序存放，且由操作码和地址码组成：操作码用来表示操作的性质；地址码用来表示操作数在存储器中的位置-存储程序：将事先设计好、用以描述计算机解题过程的程序如数据一样，采用二进制形式存储在机器中；计算机在工作时自动高速地从机器中逐条取出指令加以执行</p><h2 id="指令集">指令集</h2><ul><li><p>计算机语言中的基本单词称为<strong>指令</strong>，一台计算机的全部指令称为该计算机的<strong>指令集</strong></p></li><li><p>机器语言：二进制代码，一般由操作码和操作数两部分组成；不同型号的计算机其机器语言是不相通的，按着一种计算机的机器指令编制的程序，不能在另一种计算机上执行</p></li><li><p>汇编语言：用人类容易理解和记忆的字母、单词来代替一个特定的指令；用助记符代替机器指令的操作码，用地址符号或标号代替指令或操作数的地址；在不同设备中，对应着不同的机器语言指令集，通过汇编过程转换成机器指令；特定的汇编语言和特定的机器语言指令集一一对应，不同平台之间不可直接移植</p></li><li><p>复杂指令集（Complex Instruction Set Computer,CISC）：使用大量的指令，包括复杂指令。如x86。</p></li><li><p>精简指令集（Reduced Instruction Set Computer,RISC）：使用少量的指令完成最少的简单操作。如ARM，MIPS，RISC-V。</p></li></ul><p><img src="img/计科导复习/1652970748489.png" /></p><h2 id="cpu">CPU</h2><p>指令的执行过程：通常，执行一条MIPS指令，包含如下5个处理步骤：取指（IF）（从指令存储器中读取指令）；译码（ID）（指令译码的同时读取寄存器）；执行（EXE）（执行操作或计算地址）；访存（MEM）（从数据存储器中读取操作数）；写回（WB）（将结果写回寄存器）</p><ul><li>时钟周期与始终频率：时钟周期（时钟间隔的时间），时钟频率（时钟周期的倒数）</li><li>一个程序需要的CPU执行时间（简称“CPU时间”）：CPU时间=指令数×CPI×时钟周期=指令数×CPI/时钟频率</li><li>指令数：执行该程序所需的总指令数量</li><li>CPI（Clock Cycle per Instruction）：每条指令的时钟周期数</li><li>单周期：在一个时钟周期内完成一条指令的所有的工作。简单容易理解。单周期设计中每条指令的时钟周期必须相同，时钟周期要由执行时间最长的那条指令决定；对于更复杂的指令，比如浮点乘法，问题就更大了；等待一条指令完全执行完成了以后再执行下一条指令，会导致硬件资源的浪费（指令的执行过程将是5个步骤依次执行。当指令执行到某个步骤时，剩下4个步骤的硬件资源将被闲置）</li><li>多周期：一条指令的执行需要经过多个时钟周期，执行不同指令所需要的时钟周期数可能不同。更有效地使用时钟周期——时钟周期由最慢的指令步骤决定，允许更快的时钟频率、不同的指令采取不同数量的时钟周期数、指令可以在不同的时钟周期内多次使用同一个功能单元。CPU的实现更为复杂，需要额外的内部状态寄存器，和更复杂的控制单元</li><li>流水线：在多周期CPU设计的基础上，利用各阶段电路间可并行执行的特点，让各个阶段的执行在时间上重叠起来</li><li>多核：在一枚处理器中集成两个或多个完整的计算引擎；处理器能支持系统总线上的多个处理器，由总线控制器提供所有总线控制信号和命令信号</li></ul><h2 id="存储设备">存储设备</h2><ul><li>局部性原理：在任何时间内，程序访问的只是地址空间相对较小的一部分内容</li><li>时间局部性：如果某个数据项被访问，那么在不久的将来它可能再次被访问（如循环结构中的指令、变量）</li><li>空间局部性：如果某个数据项被访问，那么与它地址相邻的数据项可能很快也将被访问（如访问顺序的指令、遍历数组）</li><li>存储系统：计算机中由存放程序和数据的各种存储设备、控制部件及管理信息调度的设备（硬件）和算法（软件）所组成的系统</li><li>存储层次：在计算机体系结构下存储系统层次结构的排列顺序</li></ul><p><img src="img/计科导复习/1652972012696.png" /></p><ul><li>寄存器（Register）：CPU内部用来存放数据的一些小型存储区域，用来暂时存放参与运算的数据和运算结果，包括通用寄存器、专用寄存器和控制寄存器</li><li>随机存取存储器（Random AccessMemory，RAM）：RAM工作时可以随时从任何一个指定的地址写入（存入）或读出（取出）信息，一旦断电所存储的数据将随之丢失</li><li>静态随机存取存储器（Static Random AccessMemory，SRAM）：所谓的“静态”，是指这种存储器只要保持通电，里面储存的数据就可以恒常保持</li><li>动态随机存取存储器（Dynamic Random AccessMemory，DRAM）：所储存的数据需要周期性地刷新</li><li>闪存（FlashMemory）：一种非易失性存储器，即断电数据也不会丢失；一种电子式可清除程序化只读存储器的形式，允许在操作中被多次擦或写的存储器；支持随机存取，比磁盘快100~1000倍</li><li>磁盘（MagneticDisk）：非易失性存储器，早期计算机使用的磁盘是软磁盘（FloppyDisk，简称软盘），如今常用的磁盘是硬磁盘（Hard disk，简称硬盘）</li></ul><h2 id="操作系统">操作系统</h2><h2 id="概述-2">概述</h2><p>有效管理计算机软硬件资源，合理组织计算机工作流程，控制程序执行，提供多种服务功能及界面，方便用户使用计算机的系统软件，是计算机中最靠近硬件的软件</p><h2 id="发展-1">发展</h2><ul><li>1945~1955：真空管和穿孔卡片</li><li>1955~1965：晶体管和批处理系统</li><li>1965~1980：集成电路和多道程序设计</li><li>1980年至今：个人计算机</li></ul><p><strong>多道程序设计</strong>：是在计算机内存中同时存放几道相互独立的程序，使它们在管理程序控制之下，相互穿插的运行。多道程序设计技术的根本目的是为了提高CPU 的利用率，充分发挥计算机系统部件的并行性。</p><p>Windows：高度兼容：所有程序拥有相同的或相似的基本外观，其高度兼容性保证了它拥有一个丰富的软件资源库；完全封闭：它的源代码均封闭，用户和开发者，被限制在封闭的开发环境中</p><p>MacOS：安全稳定，绘图，Unix风格的内存管理，先占式多任务处理</p><p>Linux：源码公开，稳定安全</p><h2 id="功能">功能</h2><p>处理器管理（进程调度），存储管理，设备管理，文件管理，用户接口</p><p><img src="img/计科导复习/1653049741603.png" /></p><ul><li>处理器管理：在一段时间内，只能有一个进程在CPU中执行。处理器管理的基本功能：作业和进程调度，进程控制，进程通信：同步方式和互斥方式；通信机制&gt;进程（Process）：一个计算机程序关于某数据集的一次执行过程，系统进行资源分配和调度的一个可并发执行的独立单位，在进程看来，自己独占计算机系统，并可以调用OS 提供的服务。 &gt; 同步关系：多个进程为完成同一个任务相互协作 &gt;互斥关系：多个进程为争夺有限的系统资源进入竞争状态</li></ul><p><img src="img/计科导复习/1653049970728.png" /></p><p><img src="img/计科导复习/1653050056563.png" /></p><ul><li>进程的特性：动态性（进程具有生命周期，经历创建、运行、消亡等过程），并发性（多个进程可以并发地执行），独立性（进程既是系统中资源分配和保护的基本单位，也是系统调度的独立单位）</li><li>进程与程序关系：进程是程序的一次动态执行活动，而程序是进程运行的静态描述文本；程序是一个软件，可以长期存在，而进程是一次执行过程；一个程序可以对应多个进程</li><li>进程与线程：线程是程序执行中一个单一的顺序控制流程，是程序执行流的最小单元，是处理器调度和分派的基本单位</li><li>进程的三种典型状态：<strong>运行态</strong>（running），占有处理器正在运行；<strong>就绪态</strong>（ready）具备运行条件，等待系统分配处理器以便运行；<strong>等待/阻塞态</strong>（blocked）不具备运行条件，正在等待某个事件的发生。</li></ul><p><img src="img/计科导复习/1653050401779.png" /></p><ul><li><p>运行态-&gt;阻塞态：等待资源/事件等；如等待外设传输，或人工干预。</p></li><li><p>阻塞态-&gt;就绪态：资源得到满足；如外设传输结束；人工干预完成。</p></li><li><p>运行态-&gt;就绪态：运行时间片到；出现有更高优先权进程。</p></li><li><p>就绪态-&gt;运行态：选择一个就绪进程运行。</p></li><li><p>调度准则：面向用户/面向系统</p></li><li><p>调度方式：不可剥夺式/可剥夺式</p></li><li><p>调度策略：FCFS RR（轮转）SPN（最短进程优先）SRT（最短剩余时间优先）HRRN（最高响应比优先）等。</p></li></ul><p><img src="img/计科导复习/1653050725806.png" /></p><h3 id="round-rubin调度策略">Round-Rubin调度策略</h3><p>各就绪进程轮流运行一小段时间，这一小段时间称为时间片。当一个进程耗费完一个时间片而尚未执行完毕，调度程序就强迫它放弃处理机，使其重新排到就绪队列末尾。</p><h3 id="进程的同步与互斥">进程的同步与互斥</h3><ul><li>两个或两个以上的进程要协作完成一个任务，它们之间就要互相配合，需要在某些动作之间进行<strong>同步</strong></li><li>进程间另一种关系是<strong>互斥</strong>，这种关系一般发生在两个或两个以上的进程竞争某些同时只能被一个进程使用的资源的情况下</li><li>在一段时间内只能允许一个进程访问的资源称为 临界资源，如打印机、磁带机、光盘刻写机、绘图仪等</li><li>进程执行的访问临界资源的程序段称为<strong>临界段</strong>或<strong>互斥段</strong></li></ul><h3 id="进程间互斥控制方法">进程间互斥控制方法</h3><p><strong>锁</strong>可以用于控制临界段的互斥执行。锁有两个状态，一个是打开状态，另一个是关闭状态，故锁可以用布尔变量表示。在C中，锁变量可以定义为char或int类型变量</p><p>问题：锁的关闭操作包含测试和关闭两个操作，是否会被中断？解决方案：原子操作：专用锁操作指令，交换指令</p><p><strong>死锁</strong>：两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去</p><p>产生死锁的四大必要条件： - 资源互斥 (Mutualexclusion)：每个资源同时只能被一个进程使用 - 占有并请求 (Hold andwait)：已经得到资源的进程还能继续请求新的资源 - 资源不可剥夺 (Nopreemption): 已被分配的资源只能由当前占有者主动地释放该资源 - 环路等待(Circular wait):死锁发生时，系统中存在一条环路，环路上的每个进程都在等待下一个进程所占有的资源</p><h3 id="存储器管理">存储器管理</h3><p>基本功能：内存分配，地址映射（逻辑地址、物理地址），内存保护（上界地址值、下界地址值），内存扩充（虚拟存储技术）</p><h3 id="设备管理">设备管理</h3><p>计算机外设种类众多，而且各种设备的传输速度差异很大，很难开发一种通用的、一致的解决方案</p><p>基本功能：缓冲区管理，设备分配，设备驱动，设备无关性</p><h3 id="文件管理">文件管理</h3><p>在大多数应用中，文件是一个核心成分，除了实时应用和一些特殊 应用外应用程序的输入都是通过<strong>文件</strong>来实现的。</p><p>基本功能：文件存储空间的管理，文件操作的一般管理，目录管理，文件的读写管理和存取控制</p><h3 id="用户接口">用户接口</h3><p>操作系统为用户操作提供良好的硬件接口。</p><p>基本接口类型：命令界面（DOS, Unix,windows命令行），程序界面（系统调用界面），图形界面（windows窗口，Linux的X-window）</p><h3 id="附加功能和服务">附加功能和服务</h3><ul><li>网络：TCP/IP</li><li>系统工具：shell: command line interface, ls,find/search,man；系统管理:ps,shutdown,mount,mkdir；软件开发:compilers,debuggers</li><li>OS 库：I/O: data buffering and formatting；Math: common utilities,APIs: (cos,sin,abs,sqrt)</li><li>信息保护与安全：访问控制，只有经过授权的用户才能访问系统，用户只能访问属于自己的信息；信息流：限制系统中的信息流动方向</li></ul><h1 id="软件工程">软件工程</h1><p>软件（Software）是一系列按照特定顺序组织的计算机数据和指令，是计算机中的非有形部分软件特征：软件是逻辑的而不是有形的系统元件；软件具有与硬件完全不同的特征；软件是被开发或设计的，而不是被制造的；虽然软件产品正在向基于构件的组装前进；大多数软件仍然是定制的；软件不会“磨损”</p><p>发展： - 1946：计算机问世 - 1957：高级程序语言 -1958：软件，约翰·图基 - 1966：软件危机 - 1967：软件工程，北约 -1992：面向对象，伊万·雅各布森，Objectory -1997：统一建模语言，克里斯·科布林 - 2001：敏捷软件开发，杰夫·萨瑟兰</p><p><strong>软件工程</strong>是（1）将系统化的、规范的、可度量的方法应用于软件的开发、运行和维护的过程，即将工程化应用于软件中；（2）在（1）中所述方法的研究</p><p><strong>软件工程的关注焦点</strong>是软件质量和软件成本</p><p><img src="img/计科导复习/1653052971746.png" /></p><p>软件成本占比最高的是<strong>维护</strong></p><p><strong>软件生存周期</strong>是指软件产品从考虑其概念开始到产品交付使用，直到最终退役<strong>软件开发模型</strong>是软件开发的全部过程、活动和任务的结构框架。瀑布模型：自上而下，相互衔接</p><p><img src="img/计科导复习/1653053062495.png" /></p><h2 id="软件需求分析">软件需求分析</h2><p>软件需求 ① 用户解决问题或达到目标所需的条件或能力。 ②系统或系统部件要满足合同、标准、规范或其它正式规定文档所需具有的条件或能力。 ③ 一种反映上面① 或 ② 所描述的条件或能力的文档说明。</p><p>软件项目失败的最重要的五个原因：需求不完整，缺少客户的参与，缺少资源，期望值过高，缺少高层的支持</p><p>确定系统必须具有的功能和性能，系统要求的运行环境，并且预测系统发展的前景。即以一种清晰、简洁、一致且无二义性的方式，对一个待开发系统中各个有意义的方面的陈述的一个集合。</p><p><img src="img/计科导复习/1653053551964.png" /></p><p>客户需求说明 <span class="math inline">\(\neq\)</span> 软件需求</p><p>软件需求分析中的七个任务：起始（与利益相关者的初步交流，沟通存在的问题，期望解决方案等），获取（向客户明确软件的目标，明确软件如何满足业务的要求），细化（开发精确的需求模型，由用户场景建模，解析每个用户场景提取最终用户可见的业务实体），协商（协商资源和客户要求之间的矛盾，和客户协商需求的优先级），规格说明（在基于计算机系统的环境下使用术语进行规格说明），确认（检查规格说明保证无歧义的说明了所有的系统需求），需求管理（在整个生存周期中，控制和跟踪需求以及需求变更）</p><p>实例： <img src="img/计科导复习/1653053978724.png" /></p><p><img src="img/计科导复习/1653053995856.png" /></p><h2 id="软件系统设计">软件系统设计</h2><p>在 [IEEE610.12 90] 中，软件设计定义为软件系统或组件的架构、构件、接口和其他特性的定义过程及该过程的结果。软件设计是 软件质量形成的地方。好的设计应该具有如下三个特点 -设计必须实现在分析模型中包含的所有明确要求，必须满足客户所期望的所有隐含要求；- 设计必须对编码人员、测试人员及后续的维护人员是可读可理解的； -设计应提供该软件的完整视图，从实现的角度解决数据、功能及行为等各领域方面的问题</p><ul><li>软件设计主要为分解设计D-design（Decomposition）。即将软件映射为各构件<strong>体系结构设计</strong>描述软件系统如何构成以及其构件如何一起工作<strong>构件级设计</strong>细化描述每个模块定义模块控制对象形成程序代码</li></ul><p><img src="img/计科导复习/1653054460718.png" /></p><h3 id="体系结构设计">体系结构设计</h3><p>数据流体系结构--管道和过滤器模式 -特点：输入数据经过一系列计算构件和操作构件的变换形成输出数据 -应用：媒体播放器，编译器</p><p>调用和返回体系结构--主程序/子程序模式 -特点：目标功能被分解为一个控制层次，主程序调用一组程序构件，这些构件又去调用其他构件- 应用：Safe Home Access 软件</p><p><img src="img/计科导复习/1653054983497.png" /></p><p>构建级设计 -<strong>构件</strong>是系统中模块化的、可部署的和可替换的部件，该部件封装了实现并对外提供一组接口-<strong>类</strong>是现实世界或思维世界中的实体在计算机中的反映，它将数据以及这些数据上的操作封装在一起定义为一种类型-<strong>对象</strong>是具有类类型的变量。类是对象的抽象，而对象是类的具体实例。类是抽象的，不占用内存，而对象是具体的，占用存储空间。类是用于创建对象的蓝图，它是一个定义函数和变量的软件模板。在构件级设计中，每个类都会得到详细阐述，包括所有属性和与其实现相关的操作- <strong>领域模型</strong>（DomainModel）是对领域内的概念类或现实世界中对象的可视化表示</p><h3 id="构建级设计">构建级设计</h3><p>分析类类型 - 实体类：用于对必须存储的信息和相关行为进行建模 -边界类：位于系统与外界的交界处 -控制类：负责协调边界类和实体类的工作</p><p><img src="img/计科导复习/1653055592719.png" /></p><p>对象时序图如下：</p><p><img src="img/计科导复习/1653055620712.png" /></p><p><img src="img/计科导复习/1653055689270.png" /></p><h2 id="软件程序编码">软件程序编码</h2><p>什么是软件编码：软件编码是一个复杂而迭代的过程，包括程序设计和程序实现 软件编码要求 -正确地理解用户需求和软件设计思想 - 正确地根据设计模型进行程序设计 -正确地而高效率地编写和测试源代码软件编码是设计的继续，会影响软件质量和可维护性。</p><p>出错率：每百行源程序的错误数对于少于100个语句的小段程序，源代码行数与出错率是线性相关的。随着程序的增大，出错率以非线性方式增长</p><p>程序设计风格：程序首先是正确，其次是考虑优美和效率；对所有的用户输入，必须进行合法性和有效性检查；不要单独进行浮点数的比较；所有变量在调用前必须被初始化；改一个错误时可能产生新的错误，因此修改前首先考虑其影响；最好使用括号以避免二义性；应保持注释与代码完全一致，处理过程的每个阶段和典型算法前都有相关注释说明，但是不要对每条语句注释</p><p>编码的总体原则：代码可读性优先级高于执行效率（除非有特定质量要求），简化复杂度</p><h2 id="软件测试策略">软件测试策略</h2><p>软件测试是为了发现程序中的错误而执行程序的过程软件开发过程是一个自顶向下，逐步细化的过程；测试过程是依相反的顺序安排的自底向上，逐步集成的过程测试的过程按照四个步骤进行即单元测试集成测试，确认测试和系统测试</p><p><img src="img/计科导复习/1653055984840.png" /></p><h3 id="软件测试技术">软件测试技术</h3><p>白盒测试（结构测试）： -测试应用程序的内部结构或运作，而不是测试应用程序的功能 -以编程语言的角度来设计测试案例。通过输入数据验证数据流在程序中的流动路径，并确定适当的输出</p><p><strong>基于状态图的测试</strong>定义了软件单元可以采用的一组抽象状态，并通过将其实际状态与预期状态进行比较来测试该单元的行为</p><p>黑盒测试（功能测试）： -通过测试来检测每个功能是否都能正常使用。在测试中，程序被看作不能打开的黑盒子，在完全不考虑程序内部结构和内部特性的情况下，在程序接口进行测试-黑盒测试只检查程序功能是否按照需求规格说明书的规定正常使用，程序是否能适当地接收输入数据而产生正确的输出信息</p><p>等价类划分方法 -将所有可能的输入空间划分为等价类，以便期望程序在同一等价类的每个输入上有相同表现- 步骤：(1) 将输入参数的值划分为等价类 (2) 从每个类中选择测试输入值</p><p>边界值划分方法：关注输入参数的边界值。从每个等价类的“边缘”或“异常值”中选择元素例如0，最小最大值，空集，空字符串和null</p><h3 id="软件文档">软件文档</h3><p>文档-确保软件的正确使用和有效维护<strong>用户文档</strong>：告诉用户如何一步步地使用软件包通常包含教程指导用户熟悉软件包的各项特性<strong>系统文档</strong>：定义软件本身，目的是为了让原始开发人员之外的人能够维护和修改软件包；系统文档需要记录软件开发的四个阶段——分析（记录收集到的相关信息并记录信息的来源需求和选用的方法必须用基于它们的推论来清楚表述），设计（记录软件用到的工具，如最终版本结构图），实现（记录代码的每个模块，代码应该使用注释和描述头尽可能详细地形成自文档化），测试（记录每种测试的方法和结果）<strong>技术文档</strong>：技术文档描述软件系统的安装和服务，其中安装文档描述软件如安装在每台计算机上，如服务器和客户端，服务文档描述了如果需要系统应该如何维护和更新</p><h1 id="密码学">密码学</h1><h2 id="概述-3">概述</h2><p>密码学：研究编制密码和破译密码的技术科学。数学和计算机科学的分支，包括信息论、计算复杂性理论、统计学、组合学、抽象代数以及数论。</p><p>密码学保证消息的<strong>保密性</strong>、<strong>完整性</strong>、<strong>真实性</strong></p><p>发展（古典密码学与近代密码学） -公元前5世纪：加密机械（斯巴达，密码棒） - 公元前1世纪：替换加密（凯撒）- 公元9世纪：频度分析（阿拉伯） -1553：多表替换（替换表，意大利密码学家吉奥万·贝拉索） -1919：电子加密系统（恩格玛，德国发明家亚瑟·谢尔比乌斯）</p><p>古典加密：对称加密的雏形，对称（双方共享密钥）。现实：古典密码都已经被破解</p><p>发展（现代密码学） - 1975：数据加密标准（DES），美国国家标准局将DataEncryption Standard颁布为国家标准 - 1976：Diffie-Hellman密钥交换（图灵奖2015），第一个密钥交换算法 -1977：RSA算法（图灵奖2002），麻省理工学院的罗纳德·李维斯特（RonRivest）阿迪·萨莫尔（Adi Shamir）和伦纳德·阿德曼（LeonardAdleman）提出了第一个比较完善的公钥密码体制 -1986：多方安全计算（图灵奖2000），普林斯顿大学的姚期智提出的在无可信第三方情况下，能够安全地进行多方协同计算的理论框架- 2005：基于属性的加密（ABE），最先由Waters提出，是一种公钥加密方案。实现了加密数据的细粒度访问控制，即数据拥有者可以指定谁可以访问加密的数据，数据拥有者对数据具有完全的控制权- 2005：容错学习问题（哥德尔奖2018）由Oded Regev 在 2005 年提出。求解LWE问题的时间开销是指数级的，即使是对量子计算机也没有任何帮助，故LWE 困难性被用作创建公钥密码系统 - 2009：全同态加密（FHE），IBM的 CraigGentry提出的同态加密算法。可以在没有密钥的情况下，把密文任意组合形成新的密文，并且新的密文可以完美的被还原成明文-2013：不可区分混淆（iO）-密码学皇冠上的明珠，是最有可能实现的安全混淆器。混淆后的程与混淆前功能一样，并且无法从混淆代码中提取出任何有效信息（密钥、模型等）。iO可以用来保护机器学习的模型和算法。</p><p>分类： <img src="img/计科导复习/1653125604211.png" /></p><h2 id="古典密码学">古典密码学</h2><h3 id="最早的密码密码棒">最早的密码——密码棒</h3><p>送信人先绕棍子卷一张纸条，然后把要写的信息打纵写在上面，接着打开纸送给收信人。如果不知道棍子的粗细是不能解密里面的内容的</p><h3 id="凯撒密码">凯撒密码</h3><p>平移</p><h3 id="单表替换密码">单表替换密码</h3><p>相比于凯撒密码使用了随机排列</p><h3 id="单表替换密码的破解">单表替换密码的破解</h3><p>频率分析</p><p><img src="img/计科导复习/1653126172410.png" /></p><h3 id="多表替换密码维吉尼亚密码">多表替换密码（维吉尼亚密码）</h3><p>根据密钥来决定用哪一行的替换表</p><p>中国古代密码学 -商朝末年：“阴书”、“阴符”（姜子牙设计，竖向书写，把一段信息分为三段，从不同路线、时间发出；通过纸条长度判别军方信息）-春秋战国：《左传》“暗语”（春秋战国，楚国进攻萧国时候，楚国大夫申叔展与萧国大夫还无社有旧交，通过暗语战争结束还无社获救）-三国时期：“拆字法”（三国时期民谣“千里草，何青青，十日卜，不得生”。其中“千里草”为“董”，“十日卜”为“卓”，暗指董卓作恶多端） -唐朝：“拆字法”（唐朝徐敬业起兵反抗武则天，密信写道“青鹅”，其中“青”为“十二月”，“鹅为“我自与”，意思为“起义在12月发起”）-明朝：戚继光“反切码”（取上字的声母和下字的韵母，“切”出另外一个字的读音，其原理与现代密电码的设计原理完全一样）</p><h2 id="现代密码学">现代密码学</h2><p>基础：ASCII 编码，异或运算</p><h3 id="数字加密与分组密码">数字加密与分组密码</h3><p>数据加密标准 (DES) - 1977年成为美国联邦信息处理标准 - 64位的分组长度和 56 位的秘钥长度 - 初始和末尾有一个置换 - 中间进行 16 轮Feistel 函数</p><p>分组密码 - 以 64bit 的明文为一个单位进行加密 -秘钥通过固定的移位和置换函数产生16个子秘钥</p><p>密码强度： - DES：56位密钥有 <span class="math inline">\(2^{56}=7\times 10^{16}\)</span> -AES：明文长度128位，密钥长度128位，192位，256位 有 <spanclass="math inline">\(2^{128}=3 \times 10^{38}\)</span></p><h3 id="密码设计的变革">密码设计的变革</h3><p>对称密码体制的缺陷 - 加密密钥与相应的解密密钥相同 -消息的发送方和接收方必须在密文传输之前通过安全信道进行密钥传输</p><p>公钥加密体制 -将传统密码的密钥一分为二，分为加密密钥（公钥）和解密密钥（秘钥） -公钥是公开的，任何人都可获得 - 私钥保密，需要妥善保存 -消息的发送方和接收方不再需要通过安全信道进行密钥传输</p><h3 id="公开密钥密码体制">公开密钥密码体制</h3><ul><li>公开密钥密码体制使用 不同的加密密钥与解密密钥，是一种“由已知加密密钥推导出解密密钥在计算上是不可行的”密码体制。</li><li>两个主要用途：一是解决常规密钥密码体制的密钥分配问题，另一是用于数字签名。</li><li>最著名的体制：RSA体制，它基于数论中大数分解问题的体制，由美国三位科学家Rivest,Shamir和Adleman于1976年提出并在1978年正式发表的。</li></ul><p>加密密钥 即公开密钥 )PK、加密算法E和解密算法D都是公开的，只有解密密钥（即秘密密钥）SK是需要保密的虽然秘密密钥SK是由公开密钥PK决定的，但却不能根据PK计算出SK。</p><h3 id="rsa-算法">RSA 算法</h3><p>公钥密码：RSA算法、 - 公钥：e和n - 私钥：d和n - Bob将公钥发给Alice -Alice用公钥加密 - Alice发送密文 - Bob用私钥解密</p><p><img src="img/计科导复习/1653132273563.png" /></p><p>每个用户有两个密钥：加密密钥PK={e,n}和解密密钥SK={d,n}</p><p>RSA流程： - 取两个素数p和q（保密），计算n=pq（公开），<spanclass="math inline">\(\varphi(n)=(p-1)(q-1)\)</span>（保密） -随机选取整数e，满足 <spanclass="math inline">\(\operatorname{gcd}(e,\varphi(n))\)</span>（公开），即e与<span class="math inline">\(\varphi(n)\)</span> 互素且小于 <spanclass="math inline">\(\varphi(n)\)</span> - 计算d，满足 <spanclass="math inline">\(de\equiv 1(\operatorname{mod}\varphi(n))\)</span>（保密） - 利用 RSA加密第一步需将明文数字化，并取长度小于 <span class="math inline">\(\log_{2} n\)</span> 位的数字作明文块。加密算法：<spanclass="math inline">\(c=E(m)\equiv m^{e} (\operatorname{mod} n)\)</span>解密算法：<span class="math inline">\(D(c)\equiv c^{d}(\operatorname{mod} n)\)</span></p><p><img src="img/计科导复习/1653133224606.png" /></p><p>Eve计算秘钥等于求解质因数分解问题，目前算法复杂度为指数级。</p><h3 id="中国现代密码学">中国现代密码学</h3><p>王小云（清华大学教授、中科院院士） -提出了密码哈希函数的碰撞攻击理论，即模差分比特分析法，破解了包括 MD5、SHA-1在内的5个国际通用哈希函数算法的强碰撞性，导致工业界几乎所有软件系统中MD5和SHA-1哈希函数逐步淘汰</p><p>碰撞攻击：找到两个信息块，两者的哈希值相同</p><h1 id="信息安全">信息安全</h1><h2 id="概述-4">概述</h2><p><strong>信息</strong>香农：信息是用来消除不确定性的东西。钟义信：信息是事物运动的状态及其变化方式。</p><p><strong>信息安全</strong>ISO（国际标准化组织，此处转指信息安全管理要求）：在技术上和管理上为数据处理系统建立的安全保护，保护计算机硬件、软件和数据不因偶然和恶意的原因而遭到破坏、更改和泄露。</p><p>维基百科：意为保护信息及信息系统免受未经授权的进入、使用、披露、破坏、修改、检视、记录及销毁。</p><h3 id="信息的生命周期">信息的生命周期</h3><p>创建、使用、存储、传递、更改、销毁</p><h3 id="信息安全理解的两种观点">信息安全理解的两种观点</h3><p>信息安全分层结构，面向应用的信息安全框架（工程角度）（从上到下）：内容安全，数据安全，运行安全，实体安全</p><p>信息安全金三角结构，面向属性的信息安全框架：完整性（确保信息没有遭到篡改和破坏），保密性（确保信息没有非授权的泄漏，不被非授权的个人、组织和计算机程序使用），可用性（确保拥有授权的用户或程序可以及时、正常使用信息）</p><h3 id="传统计算机安全">传统计算机安全</h3><p>定义：信息本身的保密性（Confidentiality）、完整性（Integrity）和可用性（Availability）的保持，即防止未经授权使用信息、防止对信息的非法修改和破坏、确保及时可靠地使用信息。</p><p>其他扩展：可鉴别性（真实性，不可抵赖，可计量性…），可靠性，可控性，隐私…</p><h3id="信息安全和网络安全发展阶段历程">信息安全和网络安全发展阶段历程</h3><p>COMSEC通信安全，COMPUSEC计算机安全，INFOSEC信息系统安全，IA信息安全保障，CS/IA网络空间安全/信息保障安全</p><h2 id="威胁">威胁</h2><p>特洛伊木马，黑客攻击，后门、隐蔽通道，计算机病毒，拒绝服务攻击，内部、外部泄密，蠕虫，逻辑炸弹，信息丢失、篡改、销毁</p><h3 id="安全攻击">安全攻击</h3><p><strong>主动攻击</strong>：更改数据流，或伪造假的数据流。 -伪装（Masquerade）即冒名顶替。一般而言，伪装攻击的同时往往还伴随着其他形式的主动攻击 -重放（Replay）先被动地窃取通信数据，然后再有目的地重新发送 -篡改（Modification）即修改报文内容。或者对截获的报文延迟、重新排序 -拒绝服务（Denial ofService）阻止或占据对通信设施的正常使用或管理。针对特定目标或是某个网络</p><p><strong>被动攻击</strong>：对传输进行偷听与监视，获得传输信息。 -报文分析：窃听和分析所传输的报文内容 -流量分析：分析通信主机的位置、通信的频繁程度、报文长度等信息</p><p><strong>四种常见攻击</strong>：中断，窃听，修改，伪造</p><p><img src="img/计科导复习/1653180176516.png" /></p><h3 id="isoosi安全体系结构内容">ISO/OSI安全体系结构内容</h3><p><strong>安全服务</strong>：在对威胁进行分析的基础上，规定5种标准安全服务：对象认证服务、访问控制安全服务、数据保密性安全服务、数据完整性安全服务、防抵赖性安全服务非否认性安全服务。</p><p><strong>安全机制</strong>： -基本机制：加密机制、数字签名机制、访问控制机智、数据完整性机制、认证交换机制、防业务流量分析机制、路由控制机制、公证机制等；-其它普遍采用的机制：可信功能机制、安全标记机制、事件检测机制、安全审计追踪机制、安全恢复机制等；- OSI 安全体系结构说明了：实现哪些安全服务？应该采用哪些机制？</p><h3 id="安全服务与安全机制的关系">安全服务与安全机制的关系</h3><p>安全服务与安全机制有着密切的关系。安全服务体现了安全系统的功能；而安全机制则是安全服务的实现。</p><p>一个安全服务可以由多个安全机制实现；而一个安全机制也可以用于实现多个安全服务中。</p><p><strong>安全目标统领安全服务</strong></p><h3 id="信息安全保障机制">信息安全保障机制</h3><p><strong>信息安全风险评估</strong> -低级：对于组织（处理能力、功能）、资产、个人造成有限的轻微的影响 -中级：对于组织（处理能力、功能）、资产、个人造成严重的影响 -高级：对于组织（处理能力、功能）、资产、个人造成巨大或灾难性的影响</p><p><img src="img/计科导复习/1653180478854.png" /></p><h2 id="信息安全保障标准介绍">信息安全保障标准介绍</h2><ul><li>可信计算机系统评估准则（TCSEC）（美国国防部评估标准-<strong>美国桔皮书</strong>）</li><li>可信网络解释（TNI, Trusted NetworkInterpretation）（<strong>红皮书</strong>）</li><li>通用准则CC</li><li>信息技术安全测评标准（欧洲评估标准-<strong>欧洲白皮书</strong>）</li><li>信息安全管理标准 BS 7799</li><li>《计算机信息系统安全保护等级划分准则》</li><li>信息系统安全评估方法探讨</li></ul><h3 id="tcsec">TCSEC</h3><p>TCSEC标准是计算机系统安全评估的第一个正式标准，具有划时代的意义。该准则于1970年由美国国防科学委员会提出，并于1985年12月由美国国防部公布。TCSEC最初只是军用标准，后来延至民用领域。</p><p>在TCSEC中，美国国防部按处理信息的等级和应采用的响应措施，将计算机安全从高到低分为：A、B、C、D四类八个级别，共27条评估准则。</p><p>随着安全等级的提高，系统的可信度随之增加，风险逐渐减少。</p><p><img src="img/计科导复习/1653180665988.png" /></p><h3 id="我国网络安全和信息安全发展史">我国网络安全和信息安全发展史</h3><ul><li>1986：计算机安全专业委员会</li><li>1987：国家信息中心信息安全处</li><li>1994：《中华人民共和国计算机信息系统安全保护条例》（这是我国第一个计算机安全方面的法律，较全面地从法规角度阐述了关于计算机信息系统安全相关的概念、内涵、管理、监督、责任。）</li><li>1999：国家计算机网络与信息安全管理协调小组</li><li>2001：国务院信息化工作办公室成立网络与信息安全小组（国家在信息安全的法律、规章、原则、方针上都有对应措施，国家信息安全走向正轨）</li><li>2015：《中华人民共和国网络安全法》（2014年是中国接入国际互联网 20周年。20年来，中国互联网抓住机遇，快速推进，成果斐然。国家对网络安全的重视日益加强。2015年7月6日《中华人民共和国网络安全法》公布，并向社会公开征求意见。）</li></ul><p>国家互联网应急中心：https://www.cert.org.cn/</p><h2 id="软件系统安全">软件系统安全</h2><h3 id="恶意软件分类">恶意软件分类</h3><p>恶意软件（或恶意代码）被定义为<strong>经过存储介质和网络进行传播</strong>，从一台计算机系统到另外一台计算机系统，未经授权认证破坏计算机系统完整性的程序或代码。恶意软件（或恶意代码）两个显著的特点是：<strong>非授权性和破坏性</strong></p><p>恶意软件（或恶意代码）包括：计算机病毒、蠕虫、特洛伊木马、逻辑炸弹、后门、用户级RootKit、核心级RootKit、脚本恶意代码、僵尸、间谍软件、网络钓鱼软件和恶意ActiveX控件等</p><h3 id="恶意软件行为">恶意软件行为</h3><p><strong>恶意行为</strong>：删除敏感信息；作为网络传播的起点；监视键盘；收集你的相关信息；获取屏幕；在系统上执行指令程序；窃取文件，如文档 财务 技术商业机密；开启后门，作为攻击其他计算机的起点/（肉鸡）；隐藏在你主机上的所有活动；诱骗访问恶意网站</p><p><strong>攻击方式</strong>：通过共享目录攻击，通过软盘读写攻击，通过光盘读写攻击，通过邮件攻击，通过FTP方式攻击，通过WEB方式攻击，通过漏洞攻击</p><h3 id="计算机病毒">计算机病毒</h3><ul><li>概念：编制或者在计算机程序中插入的破坏计算机功能或者毁坏数据，影响计算机使用，并能自我复制的一组计算机指令或者程序代码</li><li>特征：复制、感染、隐蔽、破坏</li><li>危害：病毒运行后能够损坏文件、使系统瘫痪，从而造成各种难以预料的后果。网络环境下，计算机病毒种类越来越多、传染速度也越来越快、危害更是越来越大</li><li>防治：以防为主，病原体（病毒库）是病毒防治技术的关键</li><li>结构：病毒起作用的关键是被感染的程序调用时先执行病毒代码，之后再执行原本的程序代码，病毒通用结构如下：</li></ul><p><img src="img/计科导复习/1653181249874.png" /></p><h3 id="计算机病毒恶意软件防治">计算机病毒/恶意软件防治</h3><p>计算机病毒传播方式：通过不可移动的<strong>计算机硬件设备</strong>进行传播；通过<strong>移动存储设备</strong>来传播这些设备包括软盘、磁带等；第三种途径：通过<strong>计算机网络</strong>邮件、网页、局域网、远程攻击、网络下载）进行传播。计算机病毒可以附着在正常文件中通过网络进入一个又一个系统；通过点对点通信系统和<strong>无线通道</strong>传播</p><p>防治技术的6个层次：检测，清除，预防（被动防治），免疫（主动防治），防范策略，数据备份及恢复</p><p><strong>三分技术、七分管理、十二分数据</strong></p><h3 id="黑客hacker与骇客cracker">黑客（Hacker）与骇客（Cracker）</h3><ul><li>入侵 (Intrusion)：指任何企图危及资源的完整性、机密性和可用性的活动。不仅包括发起攻击的人如恶意的黑客取得超出合法范围的系统控制权，也包括收集漏洞信息，造成拒绝服务等对计算机系统产生危害的行为。</li><li>真正黑客具备四种基本素质：“ Free”精神、探索与创新精神、反传统精神和合作精神。</li><li>假冒用户、违法用户、隐秘用户</li></ul><h3 id="网络安全大赛--ctf">网络安全大赛--CTF</h3><p>CTF：Capture The Flag在信息安全领域的CTF是说，通过各种攻击手法，获取服务器后寻找指定的字段，或者文件中某一个固定格式的字段，这个字段叫做flag ，其形式一般为flag{xxxxxxxx}，提交到裁判机就可以得分。最早起源于96年的DEFCON全球黑客大会</p><ul><li>解题模式（Jeopardy）：以解决网络安全技术挑战题目的分值和时间来排名，通常用于在线选拔赛。题目主要包含逆向、漏洞挖掘与利用、Web 渗透、密码、取证、隐写、安全编程等类别。</li><li>攻防模式（Attack-Defense）</li><li>混合模式（Mix）：结合了解题模式与攻防模式的 CTF赛制，iCTF国际CTF竞赛</li></ul><h3 id="相关法律法规">相关法律法规</h3><p>《刑法》第二百八十五条【非法侵入计算机信息系统罪】：违反国家规定，侵入国家事务、国防建设、尖端科学技术领域的计算机信息系统的，处三年以下有期徒刑或者拘役。</p><p>《刑法》第二百八十六条【破坏计算机信息系统罪】：违反国家规定，对计算机信息系统功能进行删除、修改、增加、干扰，造成计算机信息系统不能正常运行，后果严重的，处五年以下有期徒刑或者拘役后果特别严重的，处五年以上有期徒刑。</p><h2 id="网络信息安全">网络信息安全</h2><p>网络空间已经成为继陆、海、空、天之外的第五维国家安全领域，网络空间与现实世界相互渗透，推进人机物三元融合</p><p>网络安全成为国家战略 - 2013 年 11 月 12日，中央国家安全委员会正式成立 - 2014 年 02 月 27 日，中央网络安全和信息化领导小组成立 - 2017 年 6月，我国网络安全领域的基本法《中华人民共和国网络安全法》正式实施</p><p>提升到国家战略 - 高度重视网络空间安全保障工作 -加快构建关键信息基础设施安全保障体系 - 全天候全方位感知网络安全态势 -增强网络安全防御能力和威慑能力 -要有高素质的网络安全和信息化人才队伍</p><p>没有信息化就没有现代化——2018年2月27日，习近平在中央网络安全和信息化领导小组第一次会议上的的讲话没有网络安全就没有国家安全——2018年4月21日，习近平在全国网络安全和信息化工作会议上的讲话</p><p><img src="img/计科导复习/1653183467276.png" /></p><h3 id="社会工程学">社会工程学</h3><p>学会识别社会工程攻击，提高信息安全意识！</p><p>常见黑客社会工程学手段 -恐吓：社会工程学师常常利用人们对安全，漏洞，病毒，木马的敏感性，以权威机构的身份出现，使用危言耸听的伎俩恐吓欺骗计算机用户，以使用户言听计从。（如欺诈短信）-伪装：用来网络钓鱼的银行页面，例如URL里把真实网址的o换成0，让人真假难辨。-恭维：高明的黑客精通心理学，人际关系学等社会工程学方面的知识与技能善于利用人类的本能反应，好奇心、盲目信任等人性弱点，掌握说话的艺术迎合目标对象，投其所好，往往目标人物会友善的对待。-说服：以利益引诱，说服内部员工配合攻击者收集信息，如果内部员工本来就心存不满甚至有了报复的念头，攻击就更容易达成，千防万防，家贼难防。</p><h3 id="社会工程学防范">社会工程学防范</h3><ul><li>及时更新操作系统和应用程序软件</li><li>定期更新系统密码</li><li>学习社工工程案例</li><li>检测发现社交网站的泄露信息</li></ul><h1 id="计算机网络概述">计算机网络概述</h1><h2 id="网络分类与网络示例">网络分类与网络示例</h2><table><thead><tr class="header"><th>互联网（Internet）</th><th>互连网（internet）</th></tr></thead><tbody><tr class="odd"><td>网络的网络</td><td>网络的网络</td></tr><tr class="even"><td>特指遵循TCP/IP标准、利用路由器将各种计算机网络互连起来而形成的、覆盖全球的、特定的互连网</td><td>泛指由多个不同类型计算机网络互连而成的网络</td></tr><tr class="odd"><td>使用TCP/IP</td><td>除TCP/IP外，还可以使用其他协议</td></tr><tr class="even"><td>是一个专用名词</td><td>是一个通用名词</td></tr></tbody></table><h3 id="互连网的构成">互连网的构成</h3><p>网络边缘 - 端系统：位于互联网边缘与互联网相连的计算机和其他设备 -端系统由各类主机（host）构成：桌面计算机、移动计算机、服务器、其他智能终端设备</p><p>网络核心 -由互联端系统的分组交换设备和通信链路构成的网状网络（分组交换：路由器、链路层交换机。通信链路：光纤、铜缆、无线电、激光链路）</p><p><img src="img/计科导复习/1653195788435.png" /></p><h3 id="网络边缘设备">网络边缘设备</h3><p>智能音箱、IP相框、AR眼镜、起搏器和监护仪、安全摄像头、自行车、智能手环、烤面包机、感应床垫、智能汽车、能源监测器、互联网冰箱</p><h3 id="接入网概述">接入网概述</h3><p>接入网目标 - 接入网的目标是将主机连接到边缘路由器上 -<strong>边缘路由器</strong>是端系统Host去往任何其他远程端系统的路径上的第一台路由器</p><p>如何将终端系统连接到边缘路由器？ -有线网络接入技术：光纤到户FTTH，以太网同轴电缆，双绞线的DSL，古老的拨号上网- 无线网络接入技术：WiFi、4G/5G，卫星广域覆盖 -接入场景：住宅（家庭）接入网，机构（学校、公司）接入网，移动接入网络（WiFi、4G/5G）<strong>各种异构网络通过边缘路由器接入</strong></p><p><img src="img/计科导复习/1653196531024.png" /></p><h3 id="接入网-光纤到户ftth">接入网-光纤到户FTTH</h3><p>光纤到户FTTH - FTTH: Fiber To The Home -我国及全球先进地区普遍采用的光纤通信的传输方法 -分为两类：有源光纤网络AON和无源光纤网络PON - 带宽大、线路稳定</p><p>无源光纤网络PON - PON: Passive Optical Network - OLT：局端的光线路终端 - ONU 光网络单元（如光猫ONT） - 光猫 ONT通过一个或多个无源分光器，连接到局端的光线路终端 OLT</p><p><img src="img/计科导复习/1653196544036.png" /></p><h3 id="接入网-数字用户线dsl">接入网-数字用户线DSL</h3><p>数字用户线DSL：Digital Subscriber Line使用电话线连接到数字用户线接入复用器(DSLAM) -DSL电话线上，语音和数据可以同时传输 -数据进入互联网，语音连接到电话网</p><p>上下行速率不对称 - 24-52 Mbps下行速率，3.5-16 Mbps上行速率</p><p>我国已广泛升级为FTTH，国外依然大量使用DSL</p><p><img src="img/计科导复习/1653196771247.png" /></p><h3 id="接入网-同轴电缆">接入网-同轴电缆</h3><p>同轴电缆：Cable -家庭利用传统<strong>有线电视信号线</strong>（同轴电缆）接入头端上网 -多个家庭共享有线电视的头端 - 不对称：高达 40 Mbps 1.2 Gbps下行传输速率，30 100 Mbps 上行传输速率</p><p>混合光纤同轴电缆HFC -先用同轴电缆接入光纤节点，再用光纤连接到头端</p><p>我国已广泛升级为FTTH，美国住宅依然有80% 多使用DSL和同轴电缆接入</p><p><img src="img/计科导复习/1653196989323.png" /></p><h3 id="接入网-无线接入">接入网-无线接入</h3><p>无线接入网 通过基站（“接入点”）将终端系统连接到路由器上</p><p>无线局域网（WLAN） - 通常在建筑物内或周围（10米） -802.11b/g/n（WiFi）：11、54、450 Mbps</p><p>广域蜂窝接入网 - 由移动蜂窝网络运营商提供（10公里） -2G/3G/4G/5G等蜂窝网络 - 0.1-1000Mbps速率</p><h3 id="接入网-企业和家庭网络">接入网-企业和家庭网络</h3><p>实际的接入网 - 往往采用有线、无线等多种技术的混合 - 甚至 WiFi 和 4G等多种无线技术的混合接入</p><p>有线以太网接入 - 100Mbps、1Gbps、10Gbps等接入速率</p><p>无线WiFi接入 - 11、54、450Mbps等</p><p>无线WiFi接入点、有线以太网（1Gbps）、路由器防火墙NAT、电缆或DSL调制解调器经常合并在一个盒子里</p><h3 id="网络实例-物理介质">网络实例-物理介质</h3><p>传输单元：位（bit） - 在发射机接收机之间的物理介质上传播的数据的最小单元 - 用比特表示（bit）</p><p>物理媒体 - 是指发射机和接收机之间的具体链路介质 -<strong>引导型介质</strong>：信号在固体介质中传播，例如铜、光纤、同轴电缆 -<strong>非引导型介质</strong>：信号自由传播，例如无线电（陆地无线电、卫星无线电信道</p><p>存储常用字节Byte，K/M/G层级为 <spanclass="math inline">\(2^{10}\)</span>进制，传输常用比特Bit，K/M/G层级为<span class="math inline">\(10^{3}\)</span> 进制</p><h3 id="物理介质-光纤">物理介质-光纤</h3><p>光纤：玻璃纤维携带光脉冲，每个脉冲一位高速运行：高速点对点传输（10-100 Gbps）低错误率：中继器相距很远，对电磁噪声免疫</p><h3 id="物理介质-双绞线和同轴电缆">物理介质-双绞线和同轴电缆</h3><p>双绞线（Twisted Pair） - 两根绝缘铜线互相缠绕为一对 - 电话线 为 1对双绞线 - 网线 为 4 对双绞线，广泛用于计算机网络（以太网）双向传输 -第5类： 100 Mbps-1 Gbps - 第6类： 10Gbps</p><p>同轴电缆 - 两根同心铜导线，双向传输 - 电缆上的多个频率通道 - 带宽可达100Mbps</p><h3 id="物理介质-非引导型介质">物理介质-非引导型介质</h3><p>无线电 - 电磁频谱中各种“波段”携带的信号 - 没有物理“电线” -不依赖介质的广播 - 半双工（发送方到接收方）</p><p>传播环境影响 - 反射 - 物体阻挡 - 干扰/噪声</p><p>无线链路类型 - 无线局域网（WiFi）（10-100 Mbps；10米） -广域（如3/4/5G蜂窝）（在-10公里范围内） - 蓝牙：短距离，有限速率 -地面微波：点对点；45Mbps -卫星：同步卫星（36000km高空，280ms的往返时延），低轨卫星（近地）</p><h3 id="网络核心">网络核心</h3><p>网络核心 - 目标：将海量的端系统互联起来 -由各类交换机（路由器）和链路，构成的网状网络</p><p>分组交换（也称包交换） - 主机将数据分成分组，发送到网络 -网络将数据分组从一个路由器转发到下一个路由器，通过从源到目标的路径上的链路，<strong>逐跳传输</strong>抵达目的地</p><h3 id="网络核心的两大功能">网络核心的两大功能</h3><p>功能 1 路由 - 全局操作 ：确定数据分组从源到目标所使用的路径 -需要路由协议和路由算法，产生路由表</p><p>功能 2 转发 - 本地操作：路由器或交换机将接收到的数据分组转发出去（即移动到该设备的某个输出接口）- 确定转发出去的接口链路：根据从“入接口”收到分组头中的目的地址查找本地<strong>路由表</strong>，确定“出接口”</p><h3 id="路由器转发模型">路由器转发模型</h3><p><img src="img/计科导复习/1653198823526.png" /></p><h2 id="网络协议与参考模型">网络协议与参考模型</h2><h3 id="协议设计目的">协议设计目的</h3><p>网络协议 -为进行网络中的数据交换而建立的规则、标准或约定，即网络协议（networkprotocol） - 通信双方需要共同遵守，互相理解</p><p>三要素 - 语法：规定传输数据的格式（如何讲） -语义：规定所要完成的功能（讲什么） -时序：规定各种操作的顺序（双方讲话的顺序）</p><p>网络协议设计目的：可靠性、资源分配、拥塞问题、自适应性、安全问题</p><h3 id="协议层次结构">协议层次结构</h3><p>层次栈 -为了降低网络设计的复杂性，大部分网络都组成一个层次栈，每一层都建立在其下一层的基础上</p><p>对等实体 - 不同机器上构成相应层次的实体成为对等实体</p><p>接口 -在每一对相邻层次之间的是接口；接口定义了下层向上层提供哪些原语操作与服务</p><p>网络体系结构 -层和协议的集合为网络体系结构，一个特定的系统所使用的一组协议，即每层的协议，称为协议栈</p><p><img src="img/计科导复习/1653199101892.png" /></p><h3 id="osi参考模型">OSI参考模型</h3><p>OSI 7 层模型由ISO提出，Day,Zimmermann，1983</p><p>物理层 - 定义如何在信道上传输 0、1：Bits on the wire - 机械接口（Mechanical ）：网线接口大小形状、线缆排列等 - 电子信号（ E lectronic）：电压、电流等 - 时序接口（ Timing ）：采样频率、波特率、比特率等 -介质（ M edium ）：各种线缆、无线频谱等</p><p>数据链路层 （Data Link Layer） - <strong>实现相邻（ Neighboring）网络实体间的数据传输</strong> -<strong>成帧</strong>（Framing）：从物理层的比特流中提取出完整的帧 -错误检测与纠正：为提供可靠数据通信提供可能 - 物理地址（ MAC address 48位，理论上唯一网络标识，烧录在网卡，不便更改 - 流量控制，避免淹没（overwhelming）当快速的发送端遇上慢速的接收端，接收端缓存溢出 -共享信道上的访问控制（MAC）：同一个信道，同时传输信号。如同：同一间教室内，多人同时发言，需要纪律来控制。</p><p>网络层（Network Layer） - <strong>将数据包跨越网络从源设备发送到目的设备（host to host）</strong> -<strong>路由</strong>（Routing）：在网络中选取从源端到目的端转发路径，常常会根据网络可达性动态选取最佳路径，也可以使用静态路由-路由协议：路由器之间交互路由信息所遵循的协议规范，使得单个路由器能够获取网络的可达性等信息- 服务质量（QoS）控制：处理网络拥塞、负载均衡、准入控制、保障延迟 -异构网络互联：在异构编址和异构网络中路由寻址和转发</p><p>为何在唯一的MAC地址之外，还需要唯一的IP地址？由于全世界存在着各式各样的网络，他们使用不同的硬件地址。要使这些异构网络能够互相通信就必须进行非常复杂的硬件地址转化工作，因此由用户或用户主机来完成这项工作几乎是不可能的的事。但IP编址就把这个复杂的问题解决了。连接到互联网的主机只需要各自拥有一个唯一的IP地址，他们之间的通信就像连接在同一个网络那么简单方便。因为ARP是由计算机软件自动进行的，对用户来说是看不见这种调用过程的。互联网是由很多异构的物理网络通过路由器联接起来的，不同的物理网络，寻址方式很可能是不同的，可能根本不使用MAC地址。这样，不同的物理网络想要进行通讯就变得十分困难，因为彼此的数据帧相互不兼容。所以，我们想要一个公用的标准去遵循，这个标准就是IP。IP地址的分配是根据网络的拓朴结构，而不是根据谁制造了网络设置。</p><p>传输层（Transport Layer） -将数据从源端口发送到目的端口（进程到进程） - 网络层定位到一台主机（ host），传输层的作用域具体到主机上的某一个进程 -网络层的控制主要面向运营商，传输层为 终端用户 提供端到端的数据传输控制 -两类模式：可靠的传输模式，或不可靠传输模式 -可靠传输：可靠的端到端数据传输，适合于对通信质量有要求的应用场景，如文件传输等-不可靠传输：更快捷、更轻量的端到端数据传输，适合于对通信质量要求不高，对通信响应速度要求高的应用场景，如语音对话、视频会议等</p><p>会话层（Session） -利用传输层提供的服务，在应用程序之间建立和维持会话，并能使会话获得同步</p><p>表示层（Presentation Layer） -关注所传递信息的语法和语义，管理数据的表示方法，传输的数据结构</p><p>应用层（Application Layer） -通过应用层协议，提供应用程序便捷的网络服务调用</p><p><img src="img/计科导复习/1653199198313.png" /></p><h3 id="tcpip参考模型">TCP/IP参考模型</h3><p>TCP/IP 参考模型：ARPANET 所采用 - 以其中最主要的两个协议 TCP/IP 命名- Vint Cerf 和 Bob Kahn 于 1974 年提出</p><p>链路层（Link Layer） -描述了为满足无连接的互联网络层需求，链路必须具备的功能</p><p>互联网层（Internet Layer） -允许主机将数据包注入网络，让这些数据包独立的传输至目的地，并定义了数据包格式和协议（IPv4 协议和 IPv6 协议）</p><p>传输层（Transport Layer） -允许源主机与目标主机上的对等实体，进行端到端的数据传输：TCP UDP</p><p>应用层（Application Layer） -传输层之上的所有高层协议：DNS、HTTP、FTP、SMTP...</p><p><strong>先有 TCP/IP 协议栈，然后有 TCP/IP 参考模型</strong><strong>参考模型只是用来描述协议栈的</strong></p><p><img src="img/计科导复习/1653232383653.png" /></p><p>TCP/IP 参考模型 - 摒弃电话系统中“笨终端 聪明网络”的设计思路 - 采用聪明终端 简单网络，由端系统TCP负责丢失恢复等，简单的网络大大提升了可扩展性 -实现了建立在简单的、不可靠部件上的可靠系统</p><p>IP 分组交换的特点 - 可在各种底层物理网络上运行 IP over everything -可支持各类上层应用 Everything over IP) - 每个 IP分组携带各自的目的地址，网络核心功能简单（通过路由表转发分组），适应爆炸性增长</p><h3 id="osi模型与tcpip模型比较">OSI模型与TCP/IP模型比较</h3><p>7 层模型与 4 层模型 - TCP/IP模型的网络接口层定义主机与传输线路之间的接口，描述了链路为无连接的互联网层必须提供的基本功能- TCP/IP 模型的互联网层、传输层与 OSI 模型的 网络层、传输层 大致对应 -TCP/IP 模型的应用层包含了 OSI 模型的表示层与会话层</p><p>基本设计思想：通用性与实用性 - OSI：先有模型后设计协议，不局限于特定协议<strong>明确了服务、协议、接口等概念</strong>，更具通用性- TCP/IP 模型：仅仅是对已有协议的描述</p><p>无连接与面向连接 - OSI 模型网络层能够支持无连接和面向连接通信 -TCP/IP 模型的网络层仅支持无连接通信（IP）</p><p><img src="img/计科导复习/1653232852570.png" /></p><h3 id="分层模型与网络实例">分层模型与网络实例</h3><p><img src="img/计科导复习/1653232887840.png" /></p><h2 id="互联网发展史与启示">互联网发展史与启示</h2><ul><li>1957年10月：苏联发射成功第一颗人造卫星斯普特尼克1号</li><li>1958年2月：美国成立国防高级研究计划局（ARPA）</li><li>1969年8月：ARPANET诞生，创新地采用了分组交换技术（四个节点UCLA/SRI/UCSB/Utah）</li><li>1972年底：24个站点，美国国防部DARPA、基金委NSF以及UCLA、UCSB等高校</li><li>1983年：ARPANET采用TCP/IP，标志着互联网诞生</li><li>80年代中期：Internet横跨北美、欧洲和澳大利亚，成为全球性网络</li><li>1990年：APRANET退役移交基金委NSF</li></ul><p>商业需求是核心驱动力，技术创新提供重要基础 - 有远见的 企业参与并不断投入： MCI 、 IBM 、 Qwest CISCO…… -有线-&gt;无线-&gt;广覆盖：便捷程度决定了用户使用方式和在线时间 -网络性能决定了应用体验，成了互联网应用发展的必要条件 -网络带宽决定了应用能传什么：文本 图片（静图 动图） 音频 视频（低清高清，短视频 直播） - 网络性能提升、延迟降低：快速响应？可靠性？计算、存储、传输的互换</p><p><img src="img/计科导复习/1653233308149.png" /></p><h3 id="我国的互联网发展情况">我国的互联网发展情况</h3><p>起步阶段 - 1987年，建成第一个电子邮件节点，从北京向德国发送第一封邮件成功（越过长城，走向世界） - 1994 年，我国通过 <strong>64K</strong>专线接入互联网，全功能连接</p><p>我国网络发展规模 - 五大网络： CHINANET 电信， UNINET 联通， CMNET移动，CERNET 教育网， CSTNET 科技网 - 2019 年底，我国国际出口带宽超过8.8T - 4G 基站 551 万个（ 全球 4G 基站总数不超过 900 万） - 2020 年底，5G 基站达 70 万个，占全球比重近 7 成 - IPv4 地址 3.8 亿个； IPv6 地址50903 块 /32 ，居世界第二 - .CN 域名数 2304 万个，全球第一 - 应用 APP在架数量达到 359 万款</p><h3 id="网络设备商华为">网络设备商华为</h3><p>华为现状 - 1987 年成立，全球 20万人，多个产品线全球第一：宽带接入、光传输、移动接入网、移动核心网</p><p>居安思危 - 2000 年全国电子百强首位，任正非：华为的冬天（2000年互联网泡沫来临，设备商巨头朗讯倒下） -长期坚持超大研发投入，从幕后走到前台 -长期受美国打压，难进全球最大市场美国 -美国科技封锁，自研芯片等备胎转正，焉知非福？</p><p>技术领先、标准引领，秉持开放共赢 - 学术：活跃于 Sigcomm等顶级学术会议 - 专利：专利申请全球第一，从数量到质量 - 标准： 3GPP 、ITU T 、 IETF 等标准主要贡献者</p><h2 id="总结">总结</h2><p>网络度量单位：带宽、时延和丢失率</p><h1 id="物联网技术及其应用">物联网技术及其应用</h1><h2 id="基础知识-1">基础知识</h2><h3 id="背景与定义">背景与定义</h3><p>物联网， The Internet Of Things (IOT)IOT)，是把所有物品通过网络连接起来，实现任何物体、任何人、任何时间、任何地点（4A ）的智能化识别、信息交换与管理。</p><p><img src="img/计科导复习/1653263435582.png" /></p><p>Internet 将世界上所有的人联结起来， Internet of People for P2P</p><p>物联网将世界上所有的东西联结起来， Internet of Things for P2T orT2T</p><p>物联网是具有标识、感知和智能处理能力的物体基于通信技术相互连接形成的网络，这些物体可以在无需人工干预的条件下实现协同和互动，目的在于为人们提供智慧和集约的服务。——孙利民</p><h2 id="历史进程">历史进程</h2><ul><li>1999（MIT概念）：麻省理工学院（MIT ）的Kevin Ashton 首次提出：把RFID技术与传感器技术应用于日常物品形成“物联网”。</li><li>2005（智能计算）：国际电信联盟（ITU ）报告：物联网是通过 RFID和智能计算等技术实现全世界设备互联的网络。</li><li>2008（智慧地球）：IBM指出：把物联网设备安装到各种物体中，并普遍连接形成网络，即”物联网“，进而在此基础上形成”智慧地球“。</li><li>2009（物联网行动计划）：欧洲物联网研究所（CERP IoT项目工作组制定 《物联网战略研究路线图 》 ，介绍传感器 /RFID 等前端技术和 20年发展趋势。</li><li>2015（物联网标准联盟）：工业物联网标准联盟的成立证明了物联网有可能改变任何链流程的运行方式。</li></ul><h3 id="物联网的纪元史">物联网的纪元史</h3><ul><li>1999 年 MIT 的 Auto ID Center：EPC(Electronic Product Code电子产品码，在计算机互联网基础上 利用射频识别 、 无线数据通讯等技术构造的一个覆盖世界上万事万物的实物互联网 (Internet of Things) 。</li><li>2005 年 11 月 17 日 国际电信联盟 ITU：发布了《 ITU Internet Reports2005 The Internet of Things 》无所不在的 物联网 通信时代即将来临世界上所有的物体从轮胎到牙刷、从房屋到纸巾都可以通过因特网主动进行交换。</li><li>2008 年 11 月 IBM CEO 彭明盛：《 智慧的地球：下一代领导人议程》</li><li>2009 年 1 月 28 日 美国总统奥巴马：美国工商业领袖 圆桌会议；积极回应智慧地球</li><li>2009 年 2 月 24 日 IBM 大中华 CEO 钱大群：智慧地球 、 赢在中国</li><li>2009 年 8 月 7 日 中国总理温家宝：访问无锡 、 感知中国</li><li>2012 年 2 月 工信部 《 物联网 十二五 发展规划 》</li><li>2013 年 2 月 17 日 国务院 《 关于推进物联网有序健康发展的指导意见》</li></ul><h3 id="物联网的本质">物联网的本质</h3><p>物联网不仅仅是一个网络 更是一个平台 一个应用或者说是一个带有节点和内容的网络 。</p><p>也有将物联网理解为 Intelligent Interconnection Of Things ( 体现出了智慧 和 泛在网络 的含义 。</p><p><strong>物联网不仅仅是网络，更是面向业务的智能应用和服务</strong></p><h3 id="物联网vs互联网">物联网VS互联网</h3><p>网络覆盖范围不同 - 物联网是为物而生，主要是为了管理物，间接为人服务。- 互联网的产生是为了让人通过网络交换信息 其服务的对象是人 。</p><p>终端接入方式不同 -物联网中的传感器结点需要通过无线传感器网络的汇聚结点接入互联网 。 -互联网用户通过端系统的服务器 、 台式机 、 笔记本和移动终端访问互联网资源。</p><p>数据采集方式不同 - 物联网感知的数据是传感器主动感知或者是 RFID读写器自动读出的 。 - 互联网提供的各类服务主要是进行人与人之间的信息交互 。</p><p>网络技术范围不同 -物联网运用的技术包括互联网等等几乎涵盖了信息通信技术的所有领域 。 -互联网只是物联网的一个技术方向 。</p><h2 id="核心技术">核心技术</h2><h3 id="物联网模型物联网层次模型">物联网模型——物联网层次模型</h3><ul><li>综合应用层：物联网应用以“物”或者物理世界为中心，涵盖物品追踪、环境感知、智能物流、智能交通、智能电网等。</li><li>管理服务层：解决数据如何存储 数据库与海量存储技术 、 如何检索搜索引擎 、 如何使用 数据挖掘与机器学习 、如何不被滥用数据安全与隐私保护 等问题 。</li><li>网络构建层：主要负责传递和处理感知层获取的信息分为有线传输和无线传输两大类其中无线传输是物联网的主要应用 。无线传输又可分为短距离传输和广域网通信等几种 。</li><li>感知识别层：（数据采集子层，短距通信技术，协同处理子层），位于物联网四层模型的最底端是所有上层结构的基础 。</li></ul><h3 id="核心技术无线传感器网络wsn">核心技术：无线传感器网络（WSN）</h3><ul><li><strong>传感器</strong>（Sensor）是能感受规定的被测量并按照一定的规律转换成可用信号的器件或装置通常由敏感元件和转换元件组成。</li><li><strong>无线传感器网络</strong> WSN Wireless Sensor Networks是由部署在监测区域内的大量传感器节点通过无线通信方式形成的一个多跳的自组织网络系统，其目的是协作地感知、采集和处理网络覆盖区域中感知对象的信息能够实现数据的采集量化、处理融合和传输应用。</li><li><strong>传感器、感知对象和观察者</strong>构成了无线传感器网络的三个要素。</li><li><strong>无线传感器网络的特点</strong>包括大规模、自组织、动态性、可靠性应用相关和以数据为中心。</li></ul><h3 id="无线传感器网络网络结构">无线传感器网络——网络结构</h3><p><strong>传感器网络结构</strong>：传感器网络系统通常包括传感器节点、汇聚节点和管理节点。</p><p>WSN体系结构：平面拓扑结构 <imgsrc="img/计科导复习/1653264727418.png" /></p><p>WSN体系结构：逻辑分层结构 <imgsrc="img/计科导复习/1653264754966.png" /></p><h3 id="无线传感器网络传感器节点">无线传感器网络——传感器节点</h3><p><strong>传感器节点</strong>是由四个主要部件组成的小微型计算机系统：Sensing，Processing，Communication，Power</p><p>伯克莱Mica Mote节点</p><h3 id="什么是无线传感网络">什么是无线传感网络？</h3><p><strong>无线传感网络</strong>是由部署在监测区域内的大量微型、低成本、低功耗的传感器节点组成的多跳无线网络。</p><p>大规模、自组织、随机部署、环境复杂、传感器节点资源有限、网络拓扑经常变化</p><p>连接物理世界和数字世界</p><h3 id="无线传感器网络关键技术">无线传感器网络——关键技术</h3><p>拓扑控制 -拓扑控制是无线传感器网络研究的核心技术之一，通过拓扑控制自动生成的良好的网络拓扑结构能够提高路由协议和MAC 协议的效率。</p><p>时间同步 -时间同步是需要协同工作的传感器网络系统的一个关键机制。如测量移动车辆速度需要计算不同传感器检测事件的时间差，通过波束阵列确定声源位置节点间时间同步。</p><p>定位技术 -位置信息是传感器节点采集数据中不可缺少的一部分，确定事件发生的位置或采集数据的节点位置是传感器网络最基本的功能之一。</p><p>数据融合 -传感器网络存在能量约束，减少传输的数据量能够有效地节省能量，可利用节点的本地计算和存储能力处理数据的融合去除冗余信息，从而达到节省能量的目的。数据融合技术可以与传感器网络的多个协议层次相结合。</p><h3 id="核心技术射频识别技术rfid">核心技术：射频识别技术（RFID）</h3><ul><li>射频识别技术 RFID (Radio Frequency Identification) 俗称电子标签通过射频信号自动识别目标对象 并对其信息进行标志 、 登记 、 储存和管理。</li><li>射频识别技术具有适用性 、 高效性 、 独一性和简易性 。</li><li>射频识别技术依标签供电方式可分为无源 RFID 、 有源 RFID 和半有源 RFID。</li><li>射频识别技术的发展具有高频化 、 网络化和多能化的趋势 。</li></ul><h3 id="射频识别技术工作流程">射频识别技术——工作流程</h3><ul><li>读写器将要发送的信息，经编码后加载到高频载波信号上再经天线向外发送。</li><li>电子标签接收此信号，卡内芯片对此信号进行处理，然后对命令请求、密码、权限等进行判断。</li><li>若为读命令，则从存储器中读取有关信息，发送给阅读器，收到信号后送至信息系统进行处理。</li><li>若为写命令，电子标签内部电荷泵提升工作电压，擦写 E2PROM。若经判断其对应密码和权限不符，则返回出错信息。</li></ul><h3 id="核心技术wsn和rfid的融合">核心技术：WSN和RFID的融合</h3><p>传感器网络架构下RFID 与 WSN 的融合：智能节点 <imgsrc="img/计科导复习/1653274751691.png" /></p><p>传感器网络架构下RFID 与 WSN 的融合：智能传感标签 <imgsrc="img/计科导复习/1653274771263.png" /></p><h3 id="核心技术5g">核心技术：5G</h3><p>5 G(The 5 th Generation Mobile Networks) 即第五代移动通信技术 是继2 G4 G 通信技术后的新一代数字移动通信技术相比于前代技术实现了提高数据传输速率 、 降低延迟 、 提高系统容量为大规模设备接入提供了可能 这也为物联网的普及提供了万物互联的网络基础。</p><h3id="核心技术移动通信技术发展历史">核心技术：移动通信技术发展历史</h3><ul><li>1G 是模拟式通信系统，代表在无线传输采用模拟式的 FM调制，将 300Hz3400Hz 的语音转换到高频载波频率上。</li><li>2G 是数字通信系统，具有高保密、抗干扰、成本低、多种通信业务等优势，2G 的到来开启了移动上网时代。</li><li>3G 有 4 种标准制式，其中 CDMA（码分多址）是第三代移动通信系统的技术基础。</li><li>4G 包括 TD LTE 和 FDD LTE两种制式，能够快速传输数据、高质量、音频、视频和图像等。</li><li>5G采用统一标准，高速率、低延迟、大容量是主要特征，推动了移动互联网进阶到物联网。</li></ul><p><img src="img/计科导复习/1653274879035.png" /></p><h3 id="核心技术5g和4g对比">核心技术：5G和4G对比</h3><p>总的来说 5 G 相比 4 G 有着很大的优势： - 在容量方面 5 G 通信技术将比4 G 实现单位面积移动数据流量增长 1000 倍； - 在传输速率方面典型用户数据速率提升 10 到 100 倍 峰值传输速率可达10 Gbps 4 G 为 100Mbps 端到端时延缩短 5 倍； - 在可接入性方面 可联网设备的数量增加 10 到100 倍； - 在可靠性方面 低功率 MMC 机器型设备 的电池续航时间增加 10 倍。</p><p>由此可见 5 G 将在方方面面全面超越 4 G 实现真正意义上的融合性网络。</p><h2 id="技术应用">技术应用</h2><p>智能交通，智能城市，智能物流，智能环保，智能家居，M2M应用，精准农业，智能医疗</p><h3 id="智能交通车载自组网">智能交通——车载自组网</h3><p>车载自组网， Vehicle Ad Hoc Networks(VANET)，是物联网应用的典型场景，能够为针对减少交通事故而设计的车辆主动式安全应用提供基础性支持，目标是在道路上构建一个自组织、结构开放的车辆间通信网络，从而实现事故预警、减免交通拥堵、辅助行车驾驶员安全驾驶等应用，最终提高行车效率与安全。</p><p>整个车载自组网分为两部分：车与车V2V Vehicle to Vehicle ）和车与设施（V2I Vehicle to Infrastructure ）。</p><p>车载自组网关系图 <img src="img/计科导复习/1653275175434.png" /></p><h3 id="技术应用智能安防">技术应用：智能安防</h3><p>上海浦东国际机场防入侵系统铺设了 3万多个传感节点，覆盖了地面、栅栏和低空探测，多种传感手段组成一个协同系统后，可以防止人员的翻越、偷渡、恐怖袭击等攻击性入侵。</p><p>系统架构 <img src="img/计科导复习/1653275221375.png" /></p><p>工程概况 <img src="img/计科导复习/1653275229396.png" /></p><h3 id="技术应用智能电网">技术应用——智能电网</h3><ul><li>与现有电网相比，智能电网体现出电力流、信息流和业务流高度融合的显著特点。</li><li>江西电网对分布在全省范围内的 2万台配电变压器安装传感装置，对运行状态进行实时监测，实现用电检查、电能质量监测、负荷管理、线损管理、需求侧管理等高效一体化管理，一年降低电损1.2 亿千瓦时。</li><li>日本计划在 2030年全部普及智能电网，同时官民一体全力推动在海外建设智能电网。</li></ul><p>智能电网架构 <img src="img/计科导复习/1653275278798.png" /></p><h3 id="智能航天航天传感网">智能航天——航天传感网</h3><p>航天传感网借助于航天器布撒的传感器节点实现对星球表面大范围、长时期、近距离的监测和探索，NASA 的 JPL 实验室研制的 Sensor Webs是为将来的火星探测、选定着陆场地等需求进行技术准备的。</p><p>“吉林一号”卫星星座是我国对地观测的重要卫星遥感网络。卫星遥感影像已广泛应用于国土资源监测、土地测绘、矿产资源开发、智慧城市建设等领域。</p><h2 id="深入思考">深入思考</h2><p>物联网面临的问题 - 更安全的防护措施； - 更普及的智能设备； -更加关注人工智能； - 更快速的数据转化； - 应用先行、技术突破、产业同步。</p><p>物联网的发展趋势 - 统筹规划和顶层设计缺乏； -标准规范和核心技术缺失； - 系统管理 、 维护和扩展性差； -规模化应用和成熟商业模式缺乏； - 终端设备多而零散； - 产业链不完善。</p><h1 id="人工智能">人工智能</h1><p>决策——从搜索到学习</p><p>决策：从模拟器到物理现实</p><p>机器人学</p><p>经典案例——鸡尾酒会问题</p><p>计算机视觉：从2D到3D；从IID（独立同分布）到OOD（Out ofDistribution）</p><p>计算机视觉：从判别模型到生成模型</p><p>经典案例：对经典流体物理的模仿和超越，精准预报极端天气，预测蛋白质折叠的三维结构</p><p>我国新一代人工智能发展规划（2030）</p><h2 id="定义-1">定义</h2><p>人工智能就是根据对环境的感知，做出合理的行动，并获得最大收益的计算机程序（《人工智能：一种现代的方法》一书中的定义，既强调人工智能可以根据环境感知做出主动反应，又强调人工智能所做出的反应必须达致目标，同时，不再强调人工智能对人类思维方式或人类总结的思维法则的模仿。）</p><p>作为学科，人工智能是一门集成脑科学、认知科学、心理学、语言学、逻辑学、哲学与计算机科学的交叉学科。它是研究、开发用于模拟、延伸和扩展人的智道能的理论、方法、技术及应用系统的一门新的技术科学</p><p>人工：人为的、人造的</p><p>智能：具有 - 感知能力：感知外部世界 -记忆能力：对感知到的外界信息或由思维产生的内部知识进行存储 -思维能力：对所存储的信息或知识的本质属性、内部规律的认识 -学习与自适应能力：具有特定目的的知识获取 -决策与行为能力：对感知到的外部信息做出动作反应</p><h3 id="强人工智能">强人工智能</h3><p>强人工智能是像人类一样能够对世界进行感知和交互，通过自我学习的方式对所有领域进行记忆、推理和解决问题的人工智能。</p><h3 id="弱人工智能">弱人工智能</h3><p>弱人工智能指的是专注于且只能解决特定领域问题的人工智能，其自身模型只能够解决相关领域的问题。其不具备自我意识，不具备理解、思考、计划解决问题的能力（如人脸识别，淘宝推荐广告）</p><p><img src="img/计科导复习/1653276210906.png" /></p><h2 id="历史">历史</h2><h3 id="问世图灵测试">问世：图灵测试</h3><ul><li>1950年，计算机科学之父阿兰图灵发表论文Computing Machinery andIntelligence（计算机与智能）。</li><li>I propose to consider the question, “Can machinesthink?”（我建议思考这样一个问题：机器能思考吗？）</li><li>第一次提出了智能的概念，并定义了图灵测试（图灵测试至今仍被当做人工智能水平的重要测试标准之一）</li><li>图灵测试是指，人们通过设备和另外一个人进行聊天，可以是文字形式也可以是语音，聊天之后，如果30%的人认为是在和一个真人聊天，而实际对方却是个机器，那么我们就认为这个机器通过了图灵测试，它是具有“智能”的。</li></ul><h3 id="问世达特茅斯会议">问世：达特茅斯会议</h3><ul><li>1956年，一群科学家聚会在美国汉诺思小镇宁静的达特茅斯学院开会，这次会议的主题就是“达特茅斯夏季人工智能研究计划”。</li><li>他们从不同领域（数学，心理学，工程学，经济学和政治学）正式确立了人工智能为研究学科。</li><li>麦卡锡将人工智能定义为：研制智能机器的一门科学与技术</li></ul><h3 id="历史-1">历史</h3><ul><li>1957：贝尔曼方程</li><li>1958：感知机</li><li>1965：专家系统</li><li>1973：莱特希尔辩论</li><li>1982：神经网络受到关注</li><li>1986：多层感知机</li><li>1989：卷积神经网络（CNN）</li><li>1991：DART投入军用</li><li>1995：支持向量机（SVM）</li><li>1997：长短期记忆网络（LSTM）</li><li>2006：深度学习</li><li>2009：图神经网络（GNN）</li><li>2012：AlexNet夺冠</li><li>2014：生成对抗网络（GAN）</li><li>2017:人工智能通过Tensorflow Lite，Caffe2实现从云到端</li></ul><h3 id="形成期1956-1970">形成期（1956-1970）</h3><p>理查德·贝尔曼：动态规划创始人，最短路径Bellman-Ford算法</p><p>1957年，RichardBellman提出贝尔曼方程（也被称作动态规划方程），奠定了强化学习基础</p><p><img src="img/计科导复习/1653277469884.png" /></p><table><thead><tr class="header"><th>名词</th><th>解释</th></tr></thead><tbody><tr class="odd"><td>Agent（智能体）</td><td>学习器与决策者的角色</td></tr><tr class="even"><td>Environment（环境）</td><td>智能体之外一切组成的、与之交互的事物</td></tr><tr class="odd"><td>Action（动作）</td><td>智能体的行为表征</td></tr><tr class="even"><td>State（状态）</td><td>智能体从环境获取的信息</td></tr><tr class="odd"><td>Reward（奖励）</td><td>环境对于动作的反馈</td></tr></tbody></table><p>Frank Rosenblatt 弗兰奇·罗森布拉特（1928-1971）1958年，心理学家Rosenblatt提出感知机模型，是最简单的神经网络，是第一个从算法上描述的神经网络。</p><p>专家系统是一个具有大量的专门知识与经验的程序系统，它应用人工智能技术和计算机技术，根据某领域一个或多个专家提供的知识和经验，进行推理和判断，模拟人类专家的决策过程，以便解决那些需要人类专家处理的复杂问题-1965年在美国国家航空航天局要求下，斯坦福大学成功研制了DENRAL专家系统，该系统具有非常丰富的化学知识，可根据质谱数据帮助化学家推断分子结构。这个系统的完成标志着专家系统的诞生。- 在此之后, 麻省理工学院开始研制MACSYMA系统，现经过不断扩充,它能求解600多种数学问题。、</p><p>专家系统结构示意图 <img src="img/计科导复习/1653278352084.png" /></p><ul><li>Shakey是斯坦福研究院（SRI）的人工智能中心于1966年到1972年研制的世界上第一台真正意义上的移动机器人。</li><li>虽然Shakey只能解决简单的感知、运动规划和控制问题，但它却是当时将AI应用于机器人的最为成功的研究平台，它证实了许多通常属于人工智能领域的严肃的科学结论。</li></ul><h3 id="黯淡期1966-1974">黯淡期（1966-1974）</h3><p>1965年，西蒙提出：20年内，机器将能做人所能做的一切。</p><p>感知机无法解决异或（XOR）问题</p><p>机器翻译遭遇困境 The spirit is willing but the flesh is weak.机器翻译：酒是好的，肉变质了 实际意思：心有余而力不足</p><p>计算机内存有限，运算能力不足，无法解决指数型爆炸的复杂计算问题</p><p>此前对人工智能过于乐观使人们期待过高，当AI研究人员的承诺无法兑现时，公众开始激烈批评AI研究人员，许多机构不断减少对人工智能研究的资助，直至停止拨款。人工智能遭遇发展史上第一次寒冬。</p><p>1973年6月，科学界各方对人工智能看法不一。在英国著名媒体BBC的安排下，对人工智能技术前景持否定态度的詹姆斯·莱特希尔，和对人工智能未来前景强烈认同的美国斯坦福大学教授约翰·麦卡锡，出现在专业的人工智能研讨会上，也出现在BBC的直播镜头之前。</p><p>两位怒气冲冲、针锋相对的科学家，给人工智能历史留下了极其精彩的莱特希尔辩论（LighthillDebate）的故事。</p><p>詹姆斯·莱特希尔爵士在调查研究了美国的AI热之后，在议会发表了著名的批评报告。他在报告中罗列了详尽的证据，认为当时流行的基于逻辑学的符号编程，根本无法解决复杂的现实问题。</p><p>这份报告给了AI领域当头一棒，直接导致欧美国家大幅度削减AI领域的研发资金，直接导致了历史上著名的“AI之冬”的到来。</p><h3 id="应用期1970-1988">应用期（1970-1988）</h3><p>专家系统和知识工程在全世界得到了迅速发展。专家系统为企业等用户赢得了巨大的经济利益。</p><p>DEC公司与卡内基梅隆大学合作开发的XCON-R1专家系统，它每年为DEC公司节省数百万美元。</p><p>专家系统开发过程中，研究者达成共识，即人工智能系统是一个知识处理系统，而<strong>知识获取</strong>、<strong>知识表示</strong>和<strong>知识利用</strong>成为人工智能系统三大基本问题。</p><h3 id="发展期1986-">发展期（1986-）</h3><p>人工智能研究进入稳步发展阶段，技术与方法论发展均保持高速状态，实用化进程也逐渐成熟- 大数据：人工智能发展的助推剂，为人工智能模型提供训练数据 -互联网：使联网计算机都能获得海量数据 -云计算：提供计算资源；可以通过其众包服务来获取手工标记的数据</p><p>支持向量机（supportvectorMachine）是由Cortes和Vapnik于1995年首先提出的。由于它有非常完美的数学理论推导做支撑（统计学与凸优化等），并且非常符合人的直观感受（最大间隔），更重要的是它在线性分类的问题上取得了当时最好的成绩，在深度学习（2012）出现之前，SVM被认为是近十几年来最成功，表现最好的机器学习算法</p><p><img src="img/计科导复习/1653278878360.png" /></p><p>SVM要解决的问题可以用一个经典的二分类问题加以描述。如图所示，红色和蓝色的二维数据点显然是可以被一条直线分开的，在模式识别领域称为线性可分问题。然而将两类数据点分开的直线显然不止一条。图中(b)和(c)分别给出了A、B两种不同的分类方案，其中黑色实线为分界线，术语称为“决策面”。每个决策面对应了一个线性分类器。虽然在目前的数据上看，这两个分类器的分类结果是一样的，但如果考虑潜在的其他数据，则两者的分类性能是有差别的。SVM算法认为图中的分类器A在性能上优于分类器B，其依据是A的分类间隔比B要大。具有“最大间隔”的决策面就是SVM要寻找的最优解。而这个真正的最优解对应的两侧虚线所穿过的样本点，就是SVM中的支持样本点，称为“支持向量”</p><ul><li>1986年，Hinton发明了适用于多层感知器（MLP）的BP算法，并采用Sigmoid进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。</li><li>1989年，LeCun发明了卷积神经网络（CNN）-LeNet，并将其用于数字识别，且取得了较好的成绩</li><li>1997年，LSTM模型被发明，该模型在序列建模上的特性非常突出，缓解循环神经网络（RNN）梯度消失问题</li><li>2006年，深度学习元年，Hinton提出了深层网络训练中梯度消失问题的解决方案：无监督预训练对权值进行初始化+有监督训练微调。第一次提出深度学习的概念</li><li>2011年，ReLU激活函数被提出，该激活函数能够有效的抑制梯度消失问题。</li><li>2012年，Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，其通过构建的CNN网络AlexNet一举夺得冠军，且碾压第二名（SVM方法）的分类性能。</li></ul><h2 id="分类">分类</h2><p><img src="img/计科导复习/1653278927103.png" /></p><p><img src="img/计科导复习/1653278944233.png" /></p><p><img src="img/计科导复习/1653278955230.png" /></p><ul><li><p>从广义上讲，人工智能描述一种机器与周围世界交互的各种方式。</p></li><li><p>通过先进的、像人类一样的智能——软件和硬件结合的结果——一台人工智能机器或设备就可以模仿人类的行为或像人一样执行任务。</p></li><li><p>机器学习是人工智能的一种途径或子集，它强调“学习”而不是计算机程序。</p></li><li><p>一台机器使用复杂的算法来分析大量的数据，识别数据中的模式，并做出一个预测——不需要人在机器的软件中编写特定的指令。</p></li><li><p>深度学习是机器学习的一个子集，推动计算机智能取得长足进步。它用大量的数据和计算能力来模拟深度神经网络。</p></li><li><p>从本质上说，这些网络模仿人类大脑的连通性，对数据集进行分类，并发现它们之间的相关性。</p></li></ul><h2 id="应用-1">应用</h2><p>计算机视觉：对单张图像或一系列图像的有用信息进行自动提取、分析和理解自然语言处理是人工智能和语言学领域的分支学科。此领域探讨如何处理及运用自然语言，特别是如何编程计算机以成功处理大量的自然语言数据。智慧城市 -智慧交通：融合城市多元时空数据，为市民打造智能化出行服务，为政府和交管机构提供管控优化指导-智慧公安：通过大数据和人工智能技术，预测城市各区域犯罪率，设计合理巡检路线-智慧环保：结合大数据和人工智能技术，从数据感知、实时监测、预测未来、历史溯源和调度优化等方向构建智能环保体系-城市画像：应用大数据和人工智能技术，对城市的居民消费、人口流动、环境监测等情况进行分析呈现</p><p>机器人</p><h1 id="机器学习">机器学习</h1><h2 id="基本概念与发展史">基本概念与发展史</h2><p>机器学习（Machine Learning）是一门从数据中研究算法的多领域交叉学科。研究计算机如何模拟或实现人类的学习行为。从以往的经验中得到数据，通过学习构建模型，预测新数据，或对特定问题做出决策。</p><h3 id="基础范式">基础范式</h3><p>给定包含预期结果的示例，机器学习将会发现执行一项数据处理任务的规则。因此，需要以下要素进行机器学习： - 数据：输入数据点与预期输出的示例； -模型：对输入输出关系的假设空间； - 目标：衡量模型效果好坏的方法； -算法：优化模型参数以使模型输出逼近目标。</p><h3 id="机器学习的类型及对应场景">机器学习的类型及对应场景</h3><ul><li>监督学习：数值预测（价格预测等）、类别预测（人脸识别等模式识别任务）</li><li>无监督学习：用户聚类、生物信息对比等等</li><li>半监督学习：图像分类、语音识别等具有海量无标记数据的“监督学习”任务</li><li>强化学习：机器人、游戏等领域中与环境交互但无数据标签的问题</li><li>迁移学习：自动驾驶、自然语言处理等</li></ul><h3 id="监督学习supervised-learningslide16第7页">监督学习（SupervisedLearning）Slide16第7页</h3><ul><li>目的：在监督学习中，已知一些数据集（输入），并且知道它们的答案（输出），学习输入输出的关系</li><li>主要分为分类（Classification）和回归（Regression）</li><li>有监督的分类问题就是用已知类别的样本来学习模型参数</li><li>例子：线性回归问题</li></ul><p><img src="img/计科导复习/1653358669242.png" /></p><table><thead><tr class="header"><th></th><th>分类</th><th>回归</th></tr></thead><tbody><tr class="odd"><td>输入</td><td>离散数据</td><td>连续数据</td></tr><tr class="even"><td>目的</td><td>寻找决策边界</td><td>寻找最优拟合</td></tr></tbody></table><p>假设空间越复杂（模型参数越多），就越能拟合训练数据，训练数据误差就会越低；但过于复杂的模型对导致未知数据的泛化误差可能增大- 解决方法1：取一部分验证数据，确定合适的参数规模（超参数调节） -解决方法2：增加训练数据量（Data augmentation），提升大模型的泛化能力</p><h3id="回归案例芝麻信用分是怎么来的">回归案例：芝麻信用分是怎么来的？</h3><p>机器学习的基本流程： - 步骤1：构建问题，选择模型 -步骤2：收集已知数据，把数据分成几个部分：训练集（Trainingset）、验证集（Validationset）（确保模型没有过拟合）、测试集（Testset）- 步骤3：训练模型得到理想参数 - 步骤4：对新用户进行预测</p><h3 id="无监督学习">无监督学习</h3><p>无监督学习根据<strong>类别未知</strong>的训练样本来学习数据降维表示与恢复是常见的一类无监督学习算法 -传统模型：主成分分析（PCA，Principal components analysis）等 -深度模型：自动编码器（AutoEncoder）等 聚类：K-means等典型应用场景：异常检测、推荐系统、用户细分、无监督预训练</p><h3 id="其他类型的学习">其他类型的学习</h3><p>半监督学习（Semi-supervised Learning） -综合使用大量的未标记数据，以及小部分标记数据，来进行学习工作</p><p>迁移学习（Transfer Learning） - 把为任务A 开发的模型重新应用在为任务B开发模型的过程中</p><p>强化学习（Reinforcement Learning） -通过智能体与环境的交互学习策略以实现回报最大化或实现特定目标 -在决定合适的行动时，需要考虑后续步骤 -三要素：状态、动作、回报（如棋局胜负）</p><p><img src="img/计科导复习/1653359132215.png" /></p><p>状态转移图：其中每个节点是一个状态，每个边是一个可能的转移（通常为概率式的）。边都有奖励，我们的目标是计算出在任何状态下的最佳行为方式，从而最大化奖励。</p><h3id="机器学习和人工智能计算机视觉的关系">机器学习和人工智能、计算机视觉的关系</h3><p>和人工智能的关系 <img src="img/计科导复习/1653359170901.png" /></p><p>和计算机视觉的关系 <img src="img/计科导复习/1653359213291.png" /></p><h3 id="发展历史">发展历史</h3><ul><li>二十世纪五六十年代：推理期，机器学习的起源“逻辑推理家”</li><li>二十世纪七八十年代：知识期，把人总结的知识体系交给计算机专家系统</li><li>二十世纪八九十年代：归纳学习与统计学习，从样例中学习，符号主义、联结主义、统计学习</li><li>二十一世纪：深度学习，深度神经网络，人工智能</li></ul><h3 id="发展历史推理期">发展历史：推理期</h3><p>人们认为只要给机器赋予逻辑推理能力，机器就能具有智能。这一阶段的代表性工作：“逻辑理论家”，西蒙和钮威尔因此获1975年图灵奖。</p><p>逻辑理论家程序：这是第一个刻意模仿人类解决问题技能的程序，被称为“第一个人工智能程序”。模拟人证明符号逻辑定理的思维活动成功地证明了怀特海德和罗素的《数学原理》中前52个定理中的38个。</p><h3 id="发展历史知识期">发展历史：知识期</h3><p>专家系统（Expert System） -大家开始思考如何把人类的知识总结形成“专家系统”，交给计算机。 -知识工程之父费根鲍姆因为该贡献获得了1994 年图灵奖</p><p>代表工作 - 专家系统，费根鲍姆等人的“DENDRAL”系统 -模式识别，包括语音识别、光学字符识别等。让一个计算机程序去做一些看起来很“智能”的事情，例如识别“4”这个手写数字。</p><p>难题 - 把知识成体系总结出来很难，一些领域还有许多特例存在 -不是所有领域的专家都乐意“分享”</p><h3 id="发展历史归纳学习">发展历史：归纳学习</h3><p>由于知识获取难题，八十年代的机器学习主流是从样例中学习（归纳学习），其中的两大主要流派便是符号主义与联结主义。</p><p>符号主义（Symbolism）学习：包括决策树（DecisionTree）和基于逻辑的学习。 -早期人工智能的研究方向，基础是纽威尔和西蒙的物理符号系统假设 -“逻辑理论家”，专家系统，知识工程都是符号主义发展过程中的产物 -八十年代符号主义的经典模型是决策树模型，包括ID3，CART，C4.5三种经典实现（当下广泛应用的GBDT/XGBoost模型也都属于决策树模型）</p><p>联结主义（Connectionism）学习：基于神经网络“黑箱”模型。 -在二十世纪五十年代取得发展，但许多早期人工智能学者更偏爱符号主义 -六七十年代，以感知机为代表的脑模型的研究出现过热潮 -“人工智能之父”明斯基获1969年图灵奖 -八十年代联结主义的代表工作有多层网络中的反向传播(BP)算法，卷积神经网络等至今仍广泛使用的经典模型</p><h3id="发展历史统计学习statistical-learning">发展历史：统计学习（StatisticalLearning）</h3><p>二十世纪九十年代中期统计学习登上历史舞台并迅速取代归纳学习成为主流 -以统计学习理论为直接支撑，主要研究学习的统计性能、算法的收敛性、学习过程的复杂性- 代表性技术是支持向量机(Support Vector Machine，简称SVM)以及更一般的核方法(Kernel Methods) -九十年代中，连接主义学习技术的局限性凸显，而SVM的优越性得到验证</p><h3 id="发展历史深度学习">发展历史：深度学习</h3><p>深度学习的发展历程 -早期联结主义曾多次流行，但在九十年代中期被统计学习压制而进入瓶颈期 -2006年辛顿正式提出了深度学习的概念，随后在学术圈引起巨大反响 -2012年ImageNet大赛中辛顿用AlexNet一举夺魁，掀起深度学习的浪潮</p><p>深度学习领域的经典算法及其应用 - 反向传播（BP，BackPropagation）算法：深度学习基本训练算法，奠定了神经网络的基础 -卷积神经网络（CNN，Convolutional NeuralNetwork）：主要用于处理图像数据，例如图像识别问题 -循环神经网络（RNN，Recurrent NeuralNetwork）：主要用于处理序列数据，例如自然语言处理 -生成对抗网络（GAN，Generative AdversarialNetwork）：无监督学习的生成模型，可用于图像生成 - 图卷积网络（GCN，GraphConvolutional Network）：主要用于图表示学习任务，例如社交网络任务 -深度Q学习模型（DQN，Deep QNetwork）：主要用于强化学习任务，如机器人、导航等领域</p><h2 id="神经网络">神经网络</h2><h3 id="神经网络脑启发">神经网络：脑启发</h3><p><img src="img/计科导复习/1653360068577.png" /></p><h3 id="神经网络感知器">神经网络：感知器</h3><p><img src="img/计科导复习/1653360083405.png" /></p><p>用于布尔函数的感知器 <imgsrc="img/计科导复习/1653360118733.png" /></p><p>罗森布拉特（Rosenblatt）感知器模型（1958） - 输入和权重都是布尔值 -阈值的逻辑：若输入的加权和超过阈值T，则触发神经元（输出为1）</p><h3 id="神经网络多层感知机">神经网络：多层感知机</h3><p>多层感知机（MLP，1969），由包含多层感知器的网络组成 <imgsrc="img/计科导复习/1653360221337.png" /></p><p>用于布尔函数的MLP，第一层是“隐藏”层 <imgsrc="img/计科导复习/1653360247274.png" /></p><p>用于普遍布尔函数的MLP，可以建模任意复杂的布尔函数！ <imgsrc="img/计科导复习/1653360371173.png" /></p><p>用于实数函数的MLP - 输入和权重都是实数 -实数感知器是一个线性分类器</p><p>用于复杂决策边界的MLP - 可以计算任意复杂的分类边界 -将这些线性边界组合起来，第二层感知器用于“AND”</p><p>前向传播 <img src="img/计科导复习/1653360617236.png" /></p><p><img src="img/计科导复习/1653360621352.png" /></p><p>向量化表示 <img src="img/计科导复习/1653360676188.png" /></p><p>激活函数 <img src="img/计科导复习/1653360693193.png" /> <imgsrc="img/计科导复习/1653360727578.png" /> <imgsrc="img/计科导复习/1653360709860.png" />激活函数不一定是阈值函数（thresholdfunctions），可以是非线性的激活函数的用途是向网络中引入非线性成分，在识别实际问题中的复杂数据模式时非常重要</p><p>用于分类问题的MLP - 在高维空间内寻找决策边界 -可以利用一个MLP来表示（也可以选用其他机器学习模型）</p><h3 id="实际问题中的多层感知机">实际问题中的多层感知机</h3><ul><li>单层非线性感知器的权重可视化</li><li>如果权重和输入之间的相关性超过阈值，则输出神经元被触发</li><li>感知器是一个相关性滤波器</li><li>用作级联特征检测器的MLP，较深的网络需要更少的神经元</li></ul><p>底层神经元：对某个局部特征模式（边、角）进行特征检测高层神经元：底层特征模式的复杂组合模版</p><h3 id="神经网络总结">神经网络：总结</h3><p>1.基于阈值激活函数的感知器可以被视为布尔函数或线性分类器2.激活函数向网络中引入了非线性成分 3.多层感知机可以表示任意决策边界4.与较浅的网络相比，较深的网络需要更少的神经元</p><h3id="高级mlp应用三维场景可微渲染-神经辐射场">高级MLP应用：三维场景可微渲染-神经辐射场</h3><h2 id="深度学习">深度学习</h2><h3 id="卷积神经网络如何识别物体">卷积神经网络：如何识别物体？</h3><p>单个感觉神经元的感受野是感觉空间的特定区域。在该区域内，刺激将改变该神经元的激活状态</p><p>如果是让MLP来做，很难适应视角变化、比例变化、光照条件变化等情况。</p><p><img src="img/计科导复习/1653361369080.png" /></p><h3 id="卷积神经网络改进1-局部连接">卷积神经网络：改进1-局部连接</h3><p>将每个神经元只与输入量的局部区域相连其局部区域的空间范围由名为感受野的超参控制</p><h3 id="卷积神经网络改进2-参数共享">卷积神经网络：改进2-参数共享</h3><p><img src="img/计科导复习/1653361431161.png" /></p><h3 id="卷积神经网络举例lenet">卷积神经网络举例：LeNet</h3><p><img src="img/计科导复习/1653361465518.png" /></p><h3 id="卷积神经网络卷积层">卷积神经网络：卷积层</h3><p><img src="img/计科导复习/1653361482409.png" /></p><p>神经元的结果：在卷积核和图像的一小块5<em>5</em>3 之间进行点积(即：5<em>5</em>3 = 75 维点积+ 偏差)</p><h3 id="卷积神经网络步长stride">卷积神经网络：步长（Stride）</h3><p><img src="img/计科导复习/1653361529779.png" /></p><h3 id="卷积神经网络参数共享">卷积神经网络：参数共享</h3><p><img src="img/计科导复习/1653361862893.png" /></p><h3 id="卷积神经网络卷积层-1">卷积神经网络：卷积层</h3><p><img src="img/计科导复习/1653361907372.png" /></p><p>在输出中，第d个大小为28*28的切片是第d个卷积核与输入进行卷积的结果</p><p><img src="img/计科导复习/1653361924377.png" /></p><p>将这些堆叠在一起得到隐藏层特征图，尺寸为28<em>28</em>6</p><h3 id="卷积神经网络池化层pooling">卷积神经网络：池化层（Pooling）</h3><p><img src="img/计科导复习/1653361948002.png" /></p><p><img src="img/计科导复习/1653361951065.png" /></p><p>池化操作在各个28*28的特征切面上是独立的： -池化减少了宽度和高度的尺寸 - 特征图深度在池化前后保持不变</p><h3id="卷积神经网络典型的多层卷积连结方式">卷积神经网络：典型的多层卷积连结方式</h3><p><img src="img/计科导复习/1653361983899.png" /></p><p><img src="img/计科导复习/1653361992883.png" /></p><p>CNN是一组堆叠在一起的卷积层，其中穿插着激活函数和池化层</p><h3 id="循环神经网络序列建模">循环神经网络：序列建模</h3><p>自然语言，医疗频谱，语音波形 机器翻译：编码压缩-解码输出</p><h3id="循环神经网络recurrentneuralnetwork">循环神经网络（RecurrentNeuralNetwork）</h3><p><img src="img/计科导复习/1653362215075.png" /></p><h3 id="循环神经网络rnn沿时间展开">循环神经网络：RNN沿时间展开</h3><p><img src="img/计科导复习/1653362247106.png" /></p><p>使用相同的参数，W和U ht当中包含了所有来自历史时刻的信息</p><h3id="循环神经网络rnn按输入输出类型分类">循环神经网络：RNN按输入输出类型分类</h3><p><img src="img/计科导复习/1653362274806.png" /></p><ol start="2" type="1"><li>图像命名，图像-&gt; 自然语言</li><li>感情分类，自然语言-&gt; 感情</li><li>机器翻译，自然语言-&gt;自然语言</li><li>帧级别的视频分类</li></ol><h3 id="循环神经网络多到多的rnn">循环神经网络：多到多的RNN</h3><p><img src="img/计科导复习/1653362372812.png" /></p><h3id="循环神经网络seq2seq多到一一到多">循环神经网络：Seq2Seq：多到一+一到多</h3><p><img src="img/计科导复习/1653362418434.png" /></p><p>多到一：将输入序列编码为一个单独的向量一到多：通过一个单独的输入向量生成输出序列</p><h3 id="循环神经网络深度rnn">循环神经网络：深度RNN</h3><p><img src="img/计科导复习/1653362669984.png" /></p><h1 id="计算机视觉">计算机视觉</h1><h2 id="概念初探">概念初探</h2><p>计算机视觉的最初目标是建造能够像人类一样看见东西并为机器人执行感知的机器*，随后发展成一门跨学科的科学领域。主要解决如何使计算机对图像或视频产生高阶的认识。从工程学的角度来看，它试图使人类视觉系统可以完成的任务自动化。</p><ul><li>1965：三维机器视觉，Roberts通过计算机程序从数字图像中提取出诸如立方体等多面体的三维结构。</li><li>1973：抽象表示，对于如何识别和表示对象，斯坦福科学家提出“广义圆柱体”和“圆形结构”，即每个对象都是由简单的几何图形单位组成。</li><li>1997：目标分割，Shi &amp; Malik提出若识别太难了，就先做目标分割，就是把一张图片的像素点归类到有意义的区域。</li><li>2004：尺度不变特征变换（SIFT），对关键点提取其局部尺度不变特征的描绘子，采用这个描绘子进行用于对两幅相关的图像进行匹配</li></ul><h2 id="基础知识-2">基础知识</h2><h3 id="多种类型的数据">多种类型的数据</h3><p>图像：图像由一个个像素点组成，每个像素点为RGB像素视频：视频本质上是图像在时间序列上的叠加，组成视频的每一张图像叫做视频帧点云：点云包含了丰富的信息，包括三维坐标X，Y，Z、颜色、分类值、强度值、时间等等深度图像：深度图像也叫距离影像，指将从图像采集器到场景中各点的距离（深度）值作为像素值的图像</p><h3 id="图像滤波">图像滤波</h3><p>图像滤波：计算每个位置处局部邻域的函数值 滤波很重要! -图像增强：去噪、调整大小、对比度增强，等等 -从图像中提取信息：纹理、边缘、特征点，等等 - 检测模式：模板匹配</p><p>空间域图像滤波 - 直接对像素进行操作 - 平滑化、锐化</p><p>频率域图像滤波 - 修改图像的频率 - 去噪、采样、图像压缩</p><p>模板和图像金字塔 - 将模板匹配到图像 - 检测、粗糙到精细</p><p>每个像素的值用其邻域像素的平均值替换 实现平滑效果（去除尖锐特征）<img src="img/计科导复习/1653363075355.png" /></p><p>其他滤波器 <img src="img/计科导复习/1653363135635.png" />垂直边缘（绝对值）</p><p><img src="img/计科导复习/1653363226407.png" /> 水平边缘（绝对值）</p><h3id="基础知识传统图像处理到深度学习">基础知识：传统图像处理到深度学习</h3><p><img src="img/计科导复习/1653363336935.png" /></p><h3id="基础知识卷积神经网络convolutionalneuralnetwork">基础知识：卷积神经网络（ConvolutionalNeuralNetwork）</h3><p><img src="img/计科导复习/1653363363386.png" />卷积神经网络是神经网络家族的一员，最简单的卷积神经网络由一个或多个卷积层和顶端的全连通层组成。复杂的卷积神经网络还包括池化层，激活层等。上图中，每一个彩色小方格便是一个卷积核，通过不同的卷积核可以提取不同特征。</p><h3id="基础知识卷积神经网络的学习过程">基础知识：卷积神经网络的学习过程</h3><p>卷积核是一成不变的吗？我怎么可能一开始就给出有效的卷积核呢？实际上，卷积核不是固定不变的，而是在训练过程中动态更新的。</p><h2 id="基本任务">基本任务</h2><h3 id="图像分类">图像分类</h3><p>将一只猫的照片输入深度卷积神经网络，并设定输出为“猫”。经过训练，得到网络神经网络的参数，神经网络便能从照片中学习猫的特征。</p><p>为了应对图片的复杂性，可以用大量的猫的图片训练模型，提升神经网络识别猫的准确率。</p><p>选取各个类别的大量样本图片训练深度卷积神经网络，可以使得神经网络识别出更多类别的物体。</p><p>ImageNet图像识别大赛 <img src="img/计科导复习/1653363470620.png" />2012年首次使用深度学习模型，2015年起错误率低于人类</p><h3 id="目标检测">目标检测</h3><p>目标检测的任务是找出图像中所有感兴趣的目标，确定它们的类别和位置。</p><p>通过一定的策略选择性搜索候选区域，并将其输入目标分类模块进行图像分类，最终获得目标检测结果。</p><h3 id="语义分割semantic-segmentation">语义分割（SemanticSegmentation）</h3><p>语义分割需要进一步判断图像的轮廓。</p><p>语义分割是对图像中的每个像素都划分出对应的类别，即实现像素级别的分类。如图所示，每一个数字代表一类标签，对图像中的每一个像素进行分类，便可以框出各类物体的边界。</p><h3 id="实例分割instance-segmentation">实例分割（InstanceSegmentation）</h3><p>实例分割不但要进行像素级别的分类，还需在具体的类别基础上区别开不同的实例。目标检测+语义分割。 -先用目标检测方法将图像中的不同实例框出，再用语义分割方法在不同包围盒内进行逐像素标记。</p><h3 id="目标跟踪object-tracking">目标跟踪（Object Tracking）</h3><p>目标跟踪是计算机视觉领域基本问题之一。给出指定物体在视频第一帧中的位置坐标，目标跟踪算法需要在接下来的视频帧中找到该指定物体的位置。-区别于目标检测需要给出图像中所有的物体的类别和位置，目标跟踪只需要给出指定物体的位置。因此，目标跟踪算法可以被看作为对指定物体的目标检测算法。- 目标检测在自动驾驶中有着非常广阔的应用。</p><h3 id="目标检测姿态估计pose-estimation">目标检测：姿态估计（PoseEstimation）</h3><p>对人体姿态的估计常常转化为对人体关键点的预测问题，即首先预测出人体各个关键点的位置坐标，然后根据先验知识确定关键点之间的空间位置关系，从而得到预测的人体骨架。-自顶向下的思路是首先对图片进行目标检测，找出所有的人；然后将人从原图中提取出来，输入到网络中进行姿态估计。-自底向上的思路是首先找出图片中所有关键点，然后对关键点进行分组，从而得到一个个人。</p><h2 id="技术应用-1">技术应用</h2><p><img src="img/计科导复习/1653364299657.png" /></p><h1 id="数据库系统">数据库系统</h1><h2 id="发展历程">发展历程</h2><p>简述：数据处理和管理是计算机应用最重要的领域之一，涉及社会生活的方方面面</p><p><strong>数据库</strong>：存放数据的仓库，用户可以进行新增、截取、更新、删除等操作。</p><ul><li>1960s：集成数据存储（美国Honeywell公司的IDS（Integrated DataStore）系统投入运行，揭开了数据库技术的序幕。）</li><li>1970s：关系模型（数据库蓬勃发展的年代，层次模型、网状模型占据了整个数据库商用市场，首次提出关系模型。）</li><li>1980s：关系数据库（关系系统由于使用简便以及硬件性能的改善，逐步代替网状系统和层次系统占领了市场。）</li><li>2000s：数据仓库（大数据时代到来数据仓库，No逐渐涌现。）</li></ul><p>图灵奖得主： - 1973，查理士·巴赫曼，首创网状模型理论 -1981，埃德加·科德，首创关系模型理论 -1998，詹姆斯·格雷，革新事务处理技术 -2014，迈克尔·斯通布雷克，现代数据库系统底层开发</p><h3 id="数据库系统市场">数据库系统市场</h3><p>关系数据库管理系统的公司： -甲骨文（Oracle）、SAP（Sybase）：最大的数据库软件公司之一 -IBM（DB2）：世界上最大的DBMS供应商之一 -微软的SQL-Server以及Access：精简、相对便宜</p><p>关系数据库公司的挑战：随着互联网web2.0网站的兴起，为了解决大规模数据集合多重数据种类带来的挑战，NoSQL数据库应运而生</p><p>其他数据库产品：Ingres, Paradox, Foxbase, FoxPro, dBase,…</p><p>中国数据库 - Ocean，蚂蚁金服，自研 - Polar DB，阿里云，基于MySQL开发- GaussDB，华为，基于PostgreSQL开发 - TDSQL，腾讯，基于MySQL开发 -TiDB，PingCAP，基于Spanner论文自研 - SequoialDB，巨杉，自研</p><h2 id="设计理论">设计理论</h2><h3 id="关系模型">关系模型</h3><p>1970年首次提出 - 利用简单的数据结构存储数据 -通过更高级的语言访问数据 - 忽略物理层存储的实现细节</p><p>关系是由一个无序的集合组成，属性之间的关系构成实体</p><p>关系中的一行（元组）是一系列属性值（域）的集合 -值可以是整数，字符串，日期等类型 -“NULL”是所有属性里面都存在的特殊值</p><p>一个N维关系=一个N列的表格</p><h3 id="关系模型主键">关系模型——主键</h3><p>一个关系中的主键唯一标识了一个元组</p><p>一些数据库管理系统会自动帮助用户创建一个内部的主键 -SEQUENCE(SQL:2003) - AUTO_INCREMENT(MySQL)</p><p>结构化查询语言（SQL）：Structured Query Language</p><h3 id="关系模型外键">关系模型——外键</h3><p>外键用于与另一张表的关联 - 确定另一张表记录的字段 -用于保持数据的一致性</p><h3 id="关系代数">关系代数</h3><p>关系代数定义了在数据表中查询与操控元组的基本操作每一个操作符输入一个或多个关系，输出一个新的关系 -把多个操作符链接起来可以构成一个更加复杂的操作</p><ul><li><span class="math inline">\(\sigma\)</span> 选择（Select）</li><li><span class="math inline">\(\pi\)</span> 投影（Projection）</li><li>$$ 并（Union）</li><li>$$ 交（Intersection）</li><li><span class="math inline">\(-\)</span> 差（Difference）</li><li>$$ 笛卡尔积（Product）</li><li><span class="math inline">\(\bowtie\)</span> 连接（Join）</li></ul><h3 id="关系代数-选择">关系代数-选择</h3><p>选择又称为限制（Restriction） - SELECT 语句用于从表中选取数据。 -结果被存储在一个结果表中（称为结果集）</p><p>语法：<span class="math inline">\(\sigma_{predicate}(R)\)</span></p><p><img src="img/计科导复习/1653304755316.png" /></p><h3 id="关系代数-投影">关系代数-投影</h3><p>从R中选择出若干属性列组成新的关系 - 可以对属性值进行重新排序 -可以修改属性值</p><p>语法：<span class="math inline">\(\pi_{(A_1,A_2,\cdots,A_n)}(R)\)</span></p><p><img src="img/计科导复习/1653304818871.png" /></p><h3 id="关系代数-并交">关系代数-并/交</h3><p>并：由所有关系中包含的所有元组集合组成一个新的关系 语法：<spanclass="math inline">\((R \cup S)\)</span> <imgsrc="img/计科导复习/1653304905601.png" /></p><p>交：由在所有关系中均出现的元组集合组成一个新的关系 语法：<spanclass="math inline">\((R \cap S)\)</span> <imgsrc="img/计科导复习/1653304920391.png" /></p><h3 id="关系代数-差连接">关系代数-差/连接</h3><p>差：由在第一个关系中出现不在第二个关系中出现的的元组集合组成一个新的关系语法：<span class="math inline">\((R-S)\)</span> <imgsrc="img/计科导复习/1653305001145.png" /></p><p>连接：从两个关系中选取属性间满足一定条件的元组 语法：<spanclass="math inline">\((R \bowtie S)\)</span> <imgsrc="img/计科导复习/1653305015167.png" /></p><h3 id="关系代数-笛卡尔积">关系代数-笛卡尔积</h3><p>由输入的关系中所有可能的元组组合组成一个新的关系 语法：<spanclass="math inline">\((R \times S)\)</span></p><p><img src="img/计科导复习/1653305248604.png" /></p><h3 id="查询实例">查询实例</h3><p><img src="img/计科导复习/1653305290164.png" /></p><h2 id="架构体系">架构体系</h2><p>数据(Data)是数据库中存储的基本对象 定义：描述事物的符号记录种类：文本、图形、图像、音频、视频、学生的档案记录、货物的运输情况等特点：数据与其语义是不可分的</p><p>数据库定义：数据库(Database,简称DB)是长期储存在计算机内、有组织的、可共享的大量数据的集合。组织：数据按一定的数据模型组织，数据相互关联。基本特征：数据按一定的数据模型组织、描述和储存，可为各种用户共享，冗余度较小，数据独立性较高，易扩展</p><p>数据库包含两层意思 -数据库是一个实体，它是能够合理保管数据的“仓库”，数据和库组成了数据库。 -数据库是数据管理的新方法和技术，能更合适的组织数据、更方便的维护数据、更严密的控制数据和更有效的利用数据。</p><h3 id="数据库管理系统dbms">数据库管理系统（DBMS）</h3><p>概念：用于建立、使用和维护数据库的系统软件。位于用户与操作系统之间的数据管理软件。特点：专用于实现对数据进行管理和维护的系统软件；利用DBMS能科学地组织和存储数据、高效地获取和维护数据；如：SQLServer，Oracle，Sybase，Mysql</p><h3 id="数据库应用程序与用户使用">数据库应用程序与用户使用</h3><p>数据库应用程序：运用数据库管理系统所支持的程序设计语言编写，如VisualBasic，Delphi，Java, Python</p><p>功能：创建并处理表单等 - 处理用户查询 - 创建并处理报表 - 执行应用逻辑- 控制应用</p><p><img src="img/计科导复习/1653305529386.png" /></p><h3 id="数据库分类">数据库分类</h3><p>关系型数据库 - 关系型数据库，存储的格式可以直观地反映实体间的关系。 -关系型数据库和常见的表格比较相似，关系型数据库中表与表之间是有很多复杂的关联关系的。常见的关系型数据库有Mysql，SqlServer等。</p><p>非关系型数据库（NoSQL） -随着近些年技术方向的不断拓展，大量的NoSql数据库如MongoDB、Redis、Memcache出于简化数据库结构、避免冗余、影响性能的表连接、摒弃复杂分布式的目的被设计。-NoSQL数据库适合追求速度和可扩展性、业务多变的应用场景。存储方式包括键值对、图结构或者文档等。</p><p>区别 <img src="img/计科导复习/1653305579475.png" /></p><h3 id="数据库领域的学术会议">数据库领域的学术会议</h3><ul><li>SIGMOD：ACM Conference on Management of Data</li><li>SIGKDD：ACM Knowledge Discovery and Data Mining</li><li>VLDB：International Conference on Very Large Data Bases</li><li>ICDE：IEEE International Conference on Data Engineering</li><li>TKDE：IEEE Transactions on Knowledge and Data Engineering</li></ul><h1 id="数据挖掘">数据挖掘</h1><p>什么是数据挖掘？对数据进行收集、清洗、加工和分析并从中获取有用知识的过程。</p><p>为什么需要数据挖掘？ - 进入大数据时代，各类自动化系统生成的数据已达到PB 或 EB 的数量级，如：万维网、财务交往、用户交互、传感器技术和物联网。-数据挖掘旨在使用一系列处理流程，从不同格式的异构数据源中为特定应用目标提取简明扼要的、可操作性强的 洞察及理解。</p><p>经典案例： - 沃尔玛：啤酒与尿布 - 奥巴马：美国总统大选连任 -微软：预测奥斯卡 - 招聘网站：职位信息</p><h3 id="数据挖掘的过程">数据挖掘的过程</h3><p><img src="img/计科导复习/1653305755769.png" /></p><p>数据收集： 硬件、软件、人工 数据预处理 - 特征提取：将大量的原始文件、系统日志、商业交易转换为有意义的数据库特征 -清洗与集成：1.数据清洗： 处理错误或缺失的条目，维护一致性 2.特征选择：将高维特征空间转换为更适合分析的新数据空间 3.数据转换：将一些属性转换为另一种相同或类似数据类型的属性 分析过程与算法：通过数据挖掘过程的“超级问题”或基本模块解决个性化应用</p><h2 id="数据准备阶段">数据准备阶段</h2><p>数据收集的途径 - 传感器网络等专门的硬件 - 手工录入的用户调查 - Web爬虫等软件工具 - 从其他系统批量导入</p><h3 id="基本数据类型-非依赖型数据">基本数据类型-非依赖型数据</h3><p>非依赖型数据 Nondependency Oriented Data) -如多维数据或文本数据，数据项或属性之间没有任何依赖关系。 -多维数据通常包含一组记录（又称数据点、实例、样例、交易记录、实体、元组、对象或特征向量）- 每条记录都包含一组字段（又称属性、维度或特征） -例：人口统计数据集</p><p>非依赖型数据的分类 - 定量型的多维数据（QuantitativeMultidimensional）：连续的、数值的或定量的属性：如年龄；便于分析处理：均值、方差等运算- 类别型和混合型数据（Categorical and MixedAttribute）：类别型数据：离散且无次序的值，如性别、种族和邮政编码；混合型数据：数据项既有定量型的又有类别型的- 二元和集合数据（Binary andSet）：多维类别型数据的特例：表示离散的类别；多维定量型数据的特例：可以给予两个值先后次序；集合归属数据：表示元素是否隶属于某个集合-文本数据（Text）：直接表示为字符串时，单词之间有次序关系，是一种依赖型数据；表示为文档术语矩阵时，是一种多维定量型数据，存在数据稀疏性问题</p><h3 id="基本数据类型-依赖型数据">基本数据类型-依赖型数据</h3><p>依赖型数据 (Dependency Oriented Data) -数据项之间可能有某种隐式的相互依赖关系（时间、空间）或者显示的依赖关系（比如数据项之间有某种网络的链接关系）。-隐式依赖关系：相关领域有“通常”存在的、数据项之间的依赖关系，如传感器连续收集的时序数据- 显式依赖关系：用图或网络数据显式地给出数据项之间的关系</p><p>时间序列数据 ：随着时间的推移通过连续测量所生成的数值 -多元时间序列的定义：一个长度为 𝑛，维度为 𝑑的矩阵在 𝑡1,⋯,𝑡𝑛这𝑛个时间戳的每个时间点上有𝑑个定量型特征（上下文属性：对应于时间维度，如传感器数据中表示测量时间的时间戳；行为属性：对应于测量的数值，如传感器数据的读数）- 典型问题：时序预测（根据单变量 𝑖的历史时间序列𝑧𝑖,𝑡，预测未来时序的行为）</p><p>离散序列和字符串 - 多元离散序列的定义：一个长度为 𝑛，维度为 𝑑的矩阵在𝑡1,⋯,𝑡𝑛这 𝑛个时间戳的每个时间点上有 𝑑个离散值 -事件日志：计算机系统依据用户的活动创建日志（例如，一个金融网站上的用户操作日志如下，可能代表用户试图闯入一个有密码保护的系统）<img src="img/计科导复习/1653306234560.png" /></p><ul><li>生物数据：如核苷酸、氨基酸字符串，提供了有关蛋白功能的特性信息</li></ul><p>空间数据 ：上下文属性用于表示空间位置 -如气象学数据中的经度、纬度为上下文属性</p><p>时空数据 ：包含时间和空间两种属性 -测量时间和空间的动态行为属性：如随着时间的推移测量海面温度变化 -轨迹数据：在时空环境下，对一个或多个移动对象运动过程的采样所获得的数据信息</p><p>时空数据举例 - 出租车订单数据（订单的起终点经纬度、订单类型、出行品类、乘车人数等） -百度迁徙数据（迁入和迁出人口地和规模） - 微信宜出行数据（人流量、经纬度等 - 地铁刷卡数据（进出站流量、乘车时间等）</p><p>网络和图数据 - 一个网络 𝐺=(𝑁,𝐴)包含一组节点 𝑁和一组边𝐴，数据值对应于网络中的节点而数据值之间的关系对应于网络中的边。边(𝑖.𝑗)可以是有向或无向的，取决于当前的应用。 - Web图：节点对应于网页，有向边表示超链接及方向，网页中的文本是相应网页节点的属性。-社交网络：节点对应于社交网络用户，边对应于朋友关系，社交页面的内容是节点上的属性数据，邮件及聊天的通信记录是边上的属性数据。- 化合物数据库：节点对应于元素，边对应于元素之间的化学键。</p><h3 id="数据预处理概述">数据预处理概述</h3><p>为什么要对数据预处理？ - 低质量的数据导致低质量的挖掘结果 -影响数据质量的因素有：准确性、完整性、一致性、时效性、可信性、可解释性</p><p>数据预处理的主要步骤 - 数据清洗 - 数据集成 - 数据归约 - 数据变换</p><h3 id="数据清洗">数据清洗</h3><p><strong>缺失值</strong> 填充方法 -直接删除法：如分类问题中缺少类标号时 - 人工填写缺失值 -使用一个全局常量填充：如 𝑁𝐴𝑁或 −∞等 - 使用属性的均值、众数、中位数填充 -使用最可能的值：如通过回归、贝叶斯推理、决策树等来推断</p><p>填充工具 - Python Pandas 中的 fillna 函数、 interpolate() 插值函数，sklearn 库函数等 - R Hmisc 包、 missMDA包等，包含多种函数，支持简单插补、多重插补和典型变量插补</p><p><strong>噪声（错误或不一致）</strong>：被测量的变量的随机误差或方差光滑法 -分箱（Binning）：通过考察数据的近邻来光滑有序数据值（例：一组数据4,8,15,21,21,24,25,28,34 ，划分到 3 个箱中） <imgsrc="img/计科导复习/1653306654887.png" /> - 回归(Regression)：用一个函数拟合来光滑数据领域知识约束法：限制了属性值的范围和不同属性之间的约束 -如：生日与身份证号之间的关系，国家 城市 区之间的限制 检测法：利用数据的统计行为特征检测异常点</p><h3 id="数据归约">数据归约</h3><p>数据归约的作用 ：更简洁地表示数据，但仍接近于保持数据的完整性数据归约的策略 -维归约：减少考虑的随机变量或属性的个数（小波变换、主成分分析等；特征选择工具：sklearn.feature_extraction 模块、 FeatureSelector 库等） -数量归约：用较小的数据表示形式替换原数据（参数方法：使用模型估计数据，仅存储模型参数；非参数方法：直方图、聚类、抽样、数据立方等）-数据压缩：使用变换压缩数据，如果原数据能从压缩后的数据重构，称为“无损的”</p><h3 id="数据变换">数据变换</h3><p><strong>数据规范化</strong>：把属性数据按比例缩放使之落入一个特定的小区间，如 [1,1] 或 [0,1]</p><p><strong>为什么要规范化？</strong>属性值的度量单位可能影响数据分析，具有较大值域的属性倾向于有较大的影响或较高的权重。</p><p><strong>规范化的方法</strong> - min-max 规范化：设𝑚𝑖𝑛𝐴，𝑚𝑎𝑥𝐴分别为属性 A 的最小值和最大值，min-max 规范化把 A的值映射到新区间 𝑛𝑒𝑤_𝑚𝑖𝑛𝐴，𝑛𝑒𝑤_𝑚𝑎𝑥𝐴中 - z score规范化（标准化）（使用均值和标准差进行规范化；其中 <spanclass="math inline">\(\bar{A}\)</span> 表示均值，<spanclass="math inline">\(s_{A}\)</span>表示标准差）</p><p><img src="img/计科导复习/1653306891268.png" /></p><h3 id="数据挖掘使用的技术">数据挖掘使用的技术</h3><p><img src="img/计科导复习/1653306928667.png" /></p><h3 id="主要任务">主要任务</h3><p>考虑一个具有 n 个记录和 d 个属性的多维数据库 D，每行对应于一条记录，每列对应于一个维度。数据条目之间的关系有两种：</p><p>列之间的关系 - 关联模式挖掘： 确定在一行中各值之间频繁或罕见的关系 -数据分类： 通过其他列与特殊列的关系预测特殊列的缺失值</p><p>行之间的关系 - 聚类：把行分成多个子集，使得属于一个子集中的行在相应列的值上具有相关性 -异常检测： 一行的列值与其他行中相应的列值很不一样（需要用到大量概率统计知识</p><p>这四个问题构成了数据挖掘的主要任务。</p><h3 id="异常检测简介">异常检测简介</h3><p>什么是异常点？与其他观察值偏离很多的值，也称作孤立点、不和谐点、离群点或不规则点</p><p>异常点的类型 - 全局异常点：显著地偏离数据集中的其他对象 -情境（条件）异常点：关于对象的特殊情境，显著地偏离数据集中的其他对象 -集体异常点：数据对象的一个子集显著地偏离数据集中的其他对象</p><p>异常检测模型的输出 ：数据点的异常程度被量化为一个数值，称为异常分（outlier score） -实数值的异常分（定量地表达一个数据点是异常点的倾向；越大（越小）的分数说明数据点越可能是异常点；可以为概率值，量化是异常点的可能性）- 二元标签：输出一个点是否为异常点的二元结果</p><p>异常检测的应用： - 数据清洗：去除噪声 -信用卡欺诈：检测罕见的信用卡活动模式 -网络入侵检测：检测网络流量中罕见的记录或罕见的趋势改变</p><h2 id="关联模式挖掘">关联模式挖掘</h2><p>“购物篮分析”——一个经典问题</p><p>关联模式挖掘的应用： - 超市数据：提供关于目标营销和货架摆放的有用信息- 文本挖掘：发现同时出现的术语和关键词 -推广到时序数据、序列数据、空间数据和图数据：用于互联网日志分析、软件程序- 错误检测、时空事件探测等 -作为子程序，为聚类、分类、异常分析等问题提供解决方案</p><h3 id="基本概念">基本概念</h3><p>以如下超市数据为例</p><p><img src="img/计科导复习/1653353050790.png" /></p><p><img src="img/计科导复习/1653353055945.png" /></p><p><img src="img/计科导复习/1653353059710.png" /></p><ul><li>支持度(Support)：某个商品组合出现的次数与总次数之间的比例</li><li>频繁项集 ：支持度大于等于最小支持度的集合</li><li>如果定义 最小支持度 minsup =0.3 ，则频繁项集为{Bread, Milk}</li><li>支持度的一些性质：支持度越小，频繁项集数越多；支持度单调性：项集的任意子集的支持度 该项集的支持度；<strong>向下闭包性</strong>：频繁项集的每个子集也是频繁的</li><li>最大频繁项集 ：在一个给定的最小支持度下，如果项集 L是频繁的，且它的任何超集都不是频繁的，那么称 L 为最大频繁项集</li><li>通过最大频繁项集可以推导出所有的频繁项集（为最大频繁项集的子集，共11 个）</li><li>置信度 ( Confidence)：指当你购买了商品 A，会有多大的概率购买商品 B<span class="math display">\[  conf(X \Rightarrow Y)=\frac{sup(X \cup Y)}{sup(X)}\]</span></li><li>关联规则 ：令 X 和 Y 为两个项集，若满足以下两个条件，则规则 X Y是最小支持度为 minsup 和最小置信度为 minconf 的强关联规则（项集 X∪Y的支持度&gt;=minsup ，即 X∪Y 是一个频繁项集；规则 X Y的置信度&gt;=minconf）</li></ul><h3 id="关联规则生成框架">关联规则生成框架</h3><p>生成关联规则的总体框架 - 第一阶段：以最小支持度 minsup，生成所有频繁项集 计算密集 - 第二阶段：以最小置信度 minconf，从频繁项集中生成关联规则 相对简单（给定一个频繁项 集的集合𝐹，对于集合中的每个项集 𝐼 把 𝐼分解 成所有可能的集合 𝑋和集合 𝑌=𝐼−𝑋的组合，满足 𝐼=𝑋∪𝑌；计算每条规则 X Y的置信度，保留满足最小置信度要求的规则）</p><p>置信度单调性： 如果项集 𝑋1、𝑋2和 𝐼满足 𝑋1⊂𝑋2⊂𝐼，那么c𝑜𝑛𝑓(𝑋2⇒𝐼−𝑋2)≥𝑐𝑜𝑛𝑓(𝑋1⇒𝐼−𝑋1)</p><p>不同商品的展开可构成 项集格 Lattice，如项空间包含 5 项时，总共有<span class="math inline">\(2^{5}=32\)</span> 个项集节点</p><p>它是频繁模式的<strong>搜索空间</strong></p><h3 id="频繁项集挖掘算法">频繁项集挖掘算法</h3><p><strong>暴力算法</strong> 主要思想：生成所有的可能的频繁项集，在事务数据库上计算其支持率，检查给定的项集是否是每个事务的子集</p><p>穷举的问题：一个项空间 𝑈除去空集共有 <spanclass="math inline">\(2^{\lvert U \rvert }-1\)</span>个不同的子集，当|𝑈|很大时，计算是不现实的</p><p>提高算法效率的方法 - 利用向下闭包性，通过剪除候选项集来缩小搜索空间 -对候选项集计数时，通过剪除已知的与此候选集不相关的事务，使计数更加高效 -使用紧凑的数据结构表示候选项集或事务数据库，实现快速的计数</p><p><strong>Apriori算法</strong> 主要思想：由向下闭包性可知，频繁项集的任意子集也是频繁的，如果一个项集是非频繁的，那么对它的超集进行支持率计数就没有意义</p><p>算法流程： - k=1 开始，对比数据库计算 k 项集的支持度 -筛选掉小于最小支持度的项集 - 当 k 项集不满足条件时， k+1项也不满足条件，即可剪枝，停止该方向的搜索 - 否则k=k+1，重复 1-3步</p><h2 id="聚类">聚类</h2><h3 id="基本概念-1">基本概念</h3><p>聚类的定义：将数据集中的样本划分成若干个通常不相交的子集，使得类内相似度高，类间相似度低。</p><p>聚类的应用 - 数据汇总：从数据中提取摘要信息（或简明的见解） -客户分类：分析相似用户组的共同行为特性如，协同过滤 -社交网络分析：通过连接关系而紧密聚集的节点通常属于相似的组 -与其他数据挖掘问题的关系：聚类常用在分类或异常检测模型的预处理步骤</p><p>什么是聚类中心？ - 关于簇中的数据点的函数（例如均值） -从簇中现有的数据点中选择</p><p>算法思想 - 构造 K 个聚类中心 - 考察数据点之间的相似度 距离 -将数据点分配给距离最近的中心 -调整聚类中心，使其类内的数据点到中心的距离总和最小 - 反复迭代直至收敛 -关键在于：如何衡量数据点的相似度距离，采用不同的距离函数和中心生成策略衍生出不同的算法版本</p><p>欧氏距离：最简单常用的距离，但也正由于将所有维度等同看待，不能满足一些特定需求</p><p><strong>K-Means算法</strong></p><p>距离函数：欧式距离（L2范数） 中心点：平均值 步骤 -初始化：为给定样本集合𝑋 = (𝑥𝑖𝑗 )𝑚×𝑛，随机初始化K个随机聚类中心 -分配样本点：计算每个点到聚类中心的距离，并将之分配到距离它最近的聚类中心- 更新聚类中心：利用每一类点的均值作为新的聚类中心 -重复第2步和第3步，当算法在新的一轮迭代中聚类中心没有更新，或者到达迭代的最大代数，算法终止，输出聚类中心</p><p><img src="img/计科导复习/1653355224607.png" /></p><p>局限性 - K 值需预先给定 -对初始选取的聚类中心点敏感，不同的随机种子点得到的聚类结果不同 -使用贪心思想迭代，算法常常收敛至局部最优解，无法获得全局最优解 -不适合所有数据类型：如不适合处理非球形簇、不同尺寸和不同密度的簇 -找到最优划分是 NP 难问题（Aloise 2009）</p><p><strong>基于代表点的算法</strong> K Medians 算法 -距离函数：曼哈顿距离（ L1 范数） - 中心点：中位数 - 优点：比起 K Means算法鲁棒性更好，使用中位数相较于均值更不容易受到噪声和异常点的干扰</p><p>K Medoids 算法 - 距离函数：曼哈顿距离（ L1 范数） -中心点：数据集中的一个点 -优点：不容易受到噪声和异常点的干扰；一些数据类型的中心点难以计算，如变长时间序列，该方法可以应用于任何一种数据类型，只要有合适的相似度度量函数</p><h3 id="层次聚类算法">层次聚类算法</h3><p>优点：不同层次的聚类粒度给应用提供了不同的视角例：开放式目录网站的分类，这种组织结构使得用户可以方便地手动阅览</p><p><img src="img/计科导复习/1653356559283.png" /></p><p><strong>自底向上（凝聚）法</strong> - 开始每个数据点看作一个簇 -衡量簇之间的相似度，将相似度高的小簇合并成为更大的簇 -终止条件：到达合并簇距离阈值或簇数阈值</p><p><strong>自顶向下（分裂）法</strong> -从数据点集合向下递归地划分成树状结构 -终止条件：树的高度达到特定要求或每个节点包含的数据数目少于预定值 -优点：可以更好地控制树的整体结构，如树的度和分支之间的平衡性 - 二分 KMeans算法（每个结点分裂成两个孩子节点；增长策略——优先分裂所含数据点最多的节点，优先分裂与根节点最近的节点）</p><h3 id="基于网格和密度的算法">基于网格和密度的算法</h3><p>出发点： 可以对任意形状的簇进行聚类 核心思想 -识别数据中细粒度的密集区域，它们是构建任意形状的簇的“构建块” -将这些区域合并为簇</p><p>代表算法 基于网格的算法（需要确定的参数：网格数量、密度阈值） <imgsrc="img/计科导复习/1653356810418.png" /> <imgsrc="img/计科导复习/1653356813864.png" /></p><p>DBSCAN 算法（基于密度的算法） 记半径为 Eps ，密度为 τ，将数据点分类为 - 核心点 ：至少包含 τ 个数据点 - 边界点 ：包含小于 τ个数据点且半径内至少包含 1 个核心点 - 噪点 ：既不是核心点也不是 边界点 -步骤：从某个核心点出发，不断向密度可达的区域扩张，从而得到一个包含核心点和边界点的最大化区域，区域中任意两点密度相连<img src="img/计科导复习/1653356912924.png" /></p><h2 id="分类-1">分类</h2><p>分类的定义：给定一个训练数据集，其中每个数据点带有一个类标签，确定一个或多个之前未见过的测试实例的类标签。</p><p>分类的应用 | 应用 | 特征 | 标签 | | -------------- |---------------------- | ---------------------- | | 消费者目标营销 |客户历史购买行为 | 客户是否对某商品感兴趣 | | 医疗疾病管理 |病人医疗检测和治疗方案 | 治疗效果 | | 文档分类与过滤 | 文档中的单词 |文档的主题 | | 多媒体数据分析 | 照片、视频、音频特征 | 用户的特定行为|</p><h3 id="基本概念-2">基本概念</h3><p>分类算法的两个阶段 - 训练阶段：通过训练 样例构建 一个训练模型 -测试阶段：通过训练模型确定之前没有见过的测试实例的类标签</p><p>输入： 数据集 𝐷，其中含有 𝑛个数据点和 𝑑个特征 输出 -标签预测（手写数字识别） -数值分数（适合某一个类别稀少的情况）（给每一个实例标签组合打一个分数，衡量该实例属于某个特定类别的倾向大小） -数值分数取最大值 取在不同类别上加权最大值  标签预测</p><h3 id="决策树">决策树</h3><p>决策树学习的本质：从训练集中归纳出一组划分准则，使得树的每个分支的类变量的“混杂”程度逐层减小且尽可能小</p><p>步骤： ①将所有的数据看成是一个节点； ②从所有的特征（属性）中挑选一个属性 a 对划分该节点 ③生成若干孩子节点，依次判断每个孩子节点是否满足停止分裂条件： (1)若是，跳转 ④ (2)否则，跳转 ②④设置该节点是叶子节点，对应的类别定义为该节点在训练数据上数量占比最大的类别。</p><p>核心问题：如何选择分裂特征？何时停止节点分裂？</p><p>什么是好的划分准则？使决策树的分支节点所包含的样本尽可能属于同一类别。</p><p>量化划分准则质量的指标 - 错误率（Error）：令 𝑝为数据集𝑆中属于主导类别的实例比例，则错误率为 1−𝑝；选择错误率最低的划分 -基尼系数（GiniIndex）：反映了从数据集中随机抽取两个样本，其类别标记不一致的概率；具体计算方法见Slide19第54-55页</p><p>停止准则 -过拟合：在训练集上表现很好，在测试集上表现不佳（当决策树生长到最末端，直到每一个叶节点只包含某个特定的类时，得到的决策树能100%正确分类训练数据中的实例，但此时决策树已经过拟合于训练样例中的随机特性）- 剪枝：修建决策树过拟合的部分，把内节点转换为叶节点（保留一小部分（如20%20%）的训练数据，在剩下的数据上构建决策树；在保留数据集上测试剪除掉一个节点对分类准确率的影响，若提高，则剪掉这个节点；反复对叶节点进行剪枝，直到通过剪枝不能再提高准确率为止）</p><h1 id="推荐系统">推荐系统</h1><h2 id="简介-1">简介</h2><p>推荐系统是一种 信息过滤系统 ，用于预测用户对物品的评分或偏好。</p><p>把那些最终会在节点之间产生的连接找出来。（人与人之间、人与商品之间、人与咨询之间）</p><p>设计要诀：已经存在的连接-&gt;未来的连接</p><h3 id="发展史">发展史</h3><ul><li>1994：明尼苏达大学，第一个自动化推荐系统</li><li>1997：Resnick等人首次提出“推荐系统”（Recommender System）一词</li><li>2003：亚马逊Linden 等人，公布基于物品的协同过滤算法</li><li>2006：Netflix举办电影推荐算法竞赛，引起了学术界和工业界的极大关注</li><li>2007：第一届ACM 推荐系统大会在美国举行</li><li>2016：YouTube发表论文，将深度神经网络应用推荐系统中</li><li>2019：强化学习用于推荐系统，蚂蚁金服提出生成对抗用户模型</li></ul><h2 id="算法">算法</h2><h3 id="算法分类">算法分类</h3><p>推荐系统可以利用的信息：物品特征（关键词、类别、品牌等），用户特征（性别、年龄、地域、画像等），用户物品交互（评分、购买数、喜欢、转发等）</p><p>推荐系统算法的分类： - 基于内容：主要使用用户和特征的特征信息； -协同过滤：主要基于用户 物品交互； - 结合新技术的新兴算法：图嵌入、神经网络、深度学习</p><h3 id="基于内容的推荐系统">基于内容的推荐系统</h3><p>基本思想：给用户推荐与其曾经喜爱的物品的相似物品例如：电影：相同的演员，导演，电影风格新闻：相似的主题，事件，人物，地点 书籍：同一作者，出版社，类型</p><h3 id="基于内容的推荐步骤">基于内容的推荐步骤</h3><ul><li>物品表示：为每个物品抽取出一些特征来表示此物品。并将这些特征通过向量的形式组合在一起。</li><li>用户偏好学习：利用一个用户过去喜欢（及不喜欢）的物品的特征数据，来学习出此用户的喜好特征（Profile）。</li><li>生成推荐：通过比较上一步得到的用户偏好与候选物品的特征，为此用户推荐一组相关性最大的物品。</li></ul><h3 id="物品表示">物品表示</h3><p>物品(Item)有一些可以描述的属性，包括<strong>结构化属性</strong>和<strong>非结构化属性</strong>，表示物品即需要对其不同的属性进行编码。</p><p>例如在电影网站上，物品(Item)即不同的电影 -结构化属性：类别、国家、主角等 - 非结构化属性：电影简介，用户评论等 -例如编码：成龙主演的某动作片在中国大陆上映</p><h3 id="相似度计算">相似度计算</h3><p>将用户画像和物品画像表示成向量，计算相似度，来度量用户偏好</p><p>余弦相似度：计算两个向量夹角的余弦值，从方向上区分差异，而对绝对数值不敏感，更多的用于使用用户对内容评分来区分用户兴趣的相似度和差异，</p><p>欧式距离：从维度的 数值大小中体现差异的分析，如使用用户行为指标分析用户价值的相似度或差异</p><h3 id="常用相似度">常用相似度</h3><p>余弦、杰卡德、欧几里得、曼哈顿、切比雪夫、闵可夫斯基、皮尔逊相关系数、斯皮尔曼相关系数</p><h3 id="用户偏好学习">用户偏好学习</h3><p>用户<strong>历史</strong>上包含很多类型的<strong>物品交互记录</strong>- 显示记录： 评分，评论，投票等，直接表达用户偏好信息 - 隐式记录：点击，购买，关注，点赞等，相比显示记录有时更能反映用户偏好</p><p>如何对隐式记录进行建模以学习用户偏好特征是推荐系统的重点及难点，常用方法：时间序列模型（长短期记忆网络（LSTM）、循环门控单元（GRU）），注意力机制（Self-Attention模型、Transformer模型）</p><h3 id="生成推荐">生成推荐</h3><p>用户偏好学习(Profile Learning)模型： <imgsrc="img/计科导复习/1653282691306.png" /></p><h3 id="基于内容的推荐系统-1">基于内容的推荐系统</h3><p>优点 - 为某一用户做推荐时不需要其他用户的数据 -可以为具有特殊口味用户做预测 - 可解释性好，物品的特征决定了推荐值</p><p>缺点 -某些物品的特征提取比较难，例：图像，音乐，电影，如果对应的人没有元数据（例如风格，演员，导演，作者等），自动提取特征较难-推荐物品过于单一，永远不会推荐和用户曾经喜欢的物品不相关的物品，完全没有利用其他用户的喜好提高对此用户的推荐质量- 对于新用户有冷启动（ Cold Start）问题，新用户的用户画像为空，无法做出推荐</p><h3 id="协同过滤">协同过滤</h3><p>利用用户的交互来过滤感兴趣物品。可以将交互的集合可视化为一个矩阵，其中每一项(𝑖,𝑗)代表用户 𝑖和物品 𝑗的交互。</p><p>利用矩阵分解，补全评分矩阵R 中的未知用户 物品交互，可得到两个新的矩阵U和 I ，满足 U × I 和 R 的所有已知项相等。那么， U × I 也提供了 R中未知项的值，这些值可以用来生成推荐。</p><p><img src="img/计科导复习/1653282954819.png" /></p><p>基于用户的协同过滤算法（UserCF） -推荐系统中最古老的算法，基于用户的协同过滤方法的诞生标志了推荐系统的诞生-主要包括两个步骤：1.找到和目标用户兴趣相似的用户集合2.找到这个集合中的用户喜欢的，且目标用户没有听说过的物品推荐给用户</p><p>基于物品的协同过滤算法（ItemCF） - 目前业界应用最多的算法，亚马逊、Netflix 、 Hulu 、 Youtube 等的基础推荐算法 -主要包括两个步骤：1.计算物品之间的相似度2.根据物品的相似度和用户的历史行为给用户生成推荐列表</p><p>UserCF和ItemCF优缺点对比</p><table><thead><tr class="header"><th></th><th>UserCF</th><th>ItemCF</th></tr></thead><tbody><tr class="odd"><td>性能</td><td>适用于用户较少的场景，如果用户数量很多，计算用户相似度矩阵代价很大</td><td>适用于物品数目明显小于用户数的场合，如果物品很多（网页），计算物品相似度矩阵代价很大</td></tr><tr class="even"><td>领域</td><td>实时性较强，用户个性化兴趣不太明显的领域</td><td>长尾物品丰富，用户个性化需求强烈的领域</td></tr><tr class="odd"><td>实时性</td><td>用户有新行为，不一定造成推荐结果立刻变化</td><td>用户有新行为，一定会导致推荐结果的实时变化</td></tr><tr class="even"><td>推荐理由</td><td>很难提供令用户信服的推荐解释</td><td>利用用户的历史行为给用户做推荐解释，可以令用户比较信服</td></tr></tbody></table><h3 id="知识图谱">知识图谱</h3><ul><li>知识图谱是一个有向异构图，包含节点和节点间的关系</li><li>一个知识图谱由很多三元组组成（ head relation tail</li><li>推荐系统中的项目也是知识图谱中的节点</li></ul><p><img src="img/计科导复习/1653283243706.png" /> <imgsrc="img/计科导复习/1653283247924.png" /></p><h3 id="图嵌入">图嵌入</h3><p>淘宝（阿里公司）基于图嵌入的推荐算法：获得用户行为序列-&gt;构建有向图-&gt;获得随机游走序列-&gt;神经网络训练</p><p><img src="img/计科导复习/1653283296327.png" /></p><h2 id="应用-2">应用</h2><h3 id="应用流程">应用流程</h3><p>推荐系统应用的一般流程：建立用户与产品数据库-&gt;提取用户与产品特征-&gt;应用推荐算法获得推荐结果-&gt;向用户展示推荐结果</p><p><img src="img/计科导复习/1653283353587.png" /></p><h3 id="国内现状">国内现状</h3><p>推荐系统已经被应用于诸多与生活息息相关的领域中，国内涌现了一大批互联网企业，引领了推荐系统的研究。</p><p><img src="img/计科导复习/1653283382250.png" /></p><h1 id="数据可视化">数据可视化</h1><h2 id="概述-5">概述</h2><p>疫情下的数据表达 - 患病年龄分布——直方图 - 病患总数排名——柱状图 -治愈率-死亡率分布——气泡图 - 每日新增情况——折线图 - 患病类型比例——饼图 -COVID-19疫情地图——地图</p><h3 id="定义-2">定义</h3><p>维基百科定义：数据可视化旨在借助于图形化手段，清晰有效地传达与沟通信息</p><p>为什么需要可视化？ -直观：重要的见解往往隐藏在数据之中，而单纯查看数据往往无法获知 -生动：数据可视化让数据生动起来，快速有效得到问题的关键性见解 -有效：可视化是分享和交流信息的有效工具（展示模型结果，传达疫情趋势，了解政策影响，显示数据关系）</p><h3 id="分类-2">分类</h3><p>数据对比图：折线图、柱状图、杆图、堆叠图数据统计图：饼图、环图、雷达图、南丁尔格玫瑰图、旭日图数据分布图：箱型图、直方图、小提琴图、蜜蜂图离散数据图：散点图、气泡图、热力图、矩阵视图文本可视化：词云、文档卡片、新闻地图地理可视化：散点地图、区域地图、热力地图、气泡地图、统计地图信息关联图：桑葚图、主题河流、弦图、弧图、圆装箱图层次网络图：树图、网络图、金字塔图函数解析图：一元/多元函数、极坐标图形、三维曲线/平面示意图：流程图、思维导图</p><h2 id="类型">类型</h2><h3 id="描绘数据的变化折线图plot-line">描绘数据的变化–折线图（PlotLine）</h3><ul><li>显示随时间（根据常用比例设置）而变化的连续数据，非常适用于显示在相等时间间隔下数据的趋势、</li><li>类别数据沿水平轴均匀分布，所有值数据沿垂直轴均匀分布</li></ul><h3 id="突出数据的排序条形图bar-chart">突出数据的排序–条形图（BarChart）</h3><ul><li>用宽度相同的条形的高度或长短来表示数据多少的图形，可以横置、竖置</li><li>条形图是统计图资料分析中最常用的图形（能够一眼看出各个数据的大小，易于比较数据之间的差别）</li></ul><h3 id="突出数据的排序柱状图bar-chart">突出数据的排序–柱状图（BarChart）</h3><h3 id="突出数据的排序堆叠图stack-diagram">突出数据的排序–堆叠图（StackDiagram）</h3><ul><li>堆叠有两种形式，普通的堆叠和按百分比堆叠；普通堆叠是按照数值大小依次堆叠，百分比堆叠是按照数值所占百分比进行堆叠</li><li>默认情况下图形是根据数据列的属性依次从上到下堆叠的，可以设置堆叠反转（ReversedStacks）属性可以将这个顺序调换，即从下到上堆叠</li></ul><h3 id="突出数据的排序正反对比图">突出数据的排序–正反对比图</h3><h3id="描述数据的分布直方图histogram">描述数据的分布–直方图（Histogram）</h3><ul><li>用来显示在连续间隔或特定时间段内的数据分布。</li><li>当中每个条形表示每个间隔/时间段中的频率。直方图的总面积也相等于数据总量。直方图有助于估计数值集中位置、上下限值以及确定是否存在差距或异常值；也可粗略显示概率分布</li></ul><h3 id="描述数据的分布饼图pie-chart">描述数据的分布–饼图(Pie Chart)</h3><ul><li>显示一个数据系列：在图表中绘制的相关数据点，这些数据源自数据表的行或列</li><li>使用要求：1.仅有一个要绘制的数据系列；2.要绘制的数值没有负值；3.要绘制的数值几乎没有零值；4.类别数目无限制；5.各类别分别代表整个饼图的一部分；6.各个部分需要标注百分比</li></ul><h3 id="描述数据的分布雷达图radar-chart">描述数据的分布–雷达图(RadarChart)</h3><ul><li>从同一点开始的轴上表示的三个或更多个定量变量的二维图表的形式显示多变量数据的图形方法</li><li>主要用于查看哪些变量具有相似的值、变量之间是否有异常值；也可用于查看哪些变量在数据集内得分较高或较低</li></ul><h3id="描述数据的分布南丁格尔玫瑰图rose-chart">描述数据的分布–南丁格尔玫瑰图(RoseChart)</h3><ul><li>最早由弗罗伦斯·南丁格尔发明，又名为极区图。</li><li>是一种圆形的直方图</li></ul><h3 id="描述数据的分布玫瑰图变种">描述数据的分布–玫瑰图变种</h3><h3 id="描述数据的分布灵活变种">描述数据的分布–灵活变种</h3><h3id="描述数据之间的相关性散点图scatter-plot">描述数据之间的相关性–散点图（ScatterPlot）</h3><ul><li>表示因变量随自变量而变化的大致趋势，据此可以选择合适的函数对数据点进行拟合</li><li>通常用于显示和比较数值，例如科学数据、统计数据和工程数据</li></ul><h3 id="描述数据的概率分布箱式图box-plot">描述数据的概率分布–箱式图(BoxPlot)</h3><ul><li>箱式图又称盒须图、盒式图或箱线图，是一种用作显示一组数据分散情况资料的统计图。</li><li>能显示出一组数据的最大值、最小值、中位数、及上下四分位数。</li></ul><h3id="描绘数据的概率分布小提琴图violin-chart">描绘数据的概率分布–小提琴图(ViolinChart)</h3><ul><li>小提琴图(ViolinChart)用于显示数据分布及其概率密度</li><li>结合箱形图和密度图的特征，主要用来显示数据的分布形状。上下两条线段代表95%置信区间；中间的线段为中位数；左右的浅蓝色区域描绘了数据的概率分布</li></ul><h3id="描述数据之间的相关性气泡图bubble-chart">描述数据之间的相关性–气泡图（BubbleChart）</h3><ul><li>一种包含多个变量的图表，结合了散布图和比例面积图。使用笛卡尔双轴座标来绘制数值点</li><li>每一点都会获分配一个标签或类别（在旁边或图例中显示）。每个数值点再以其圆形面积表示第三个变量</li></ul><h3id="描述数据之间的相关性相关图热力图heat-map">描述数据之间的相关性–相关图/热力图（HeatMap）</h3><ul><li>将一个表结构一样的数据，可以是一个矩阵，以表的结构呈现出来。每一个位置的颜色代表了该元素的数值大小</li><li>如果每一行（每一列）代表数据中的一个维度或一个属性，则此时的图表称为相关图</li></ul><h3 id="表格透视数据透视表pivot-table">表格透视–数据透视表（PivotTable）</h3><ul><li>数据透视表（PivotTable）是一种交互式的表，可以进行某些计算，如求和与计数等。所进行的计算与数据跟数据透视表中的排列有关</li><li>创建数据透视表后，可对其进行自定义以集中在所需信息上。自定义的方面包括更改布局、更改格式或深化以显示更详细的数据</li><li>表格透视（TableLens）：不直接列出数据在每个维度上的值，而是将这些数据用水平横条或者点表示，占用的空间较少，可以在有限的屏幕空间中表示大量的数据和属性</li></ul><h3 id="矩阵视图matrix-view">矩阵视图（Matrix View）</h3><ul><li>矩阵的行代表承载用户观念的载体，列代表用于的评价属性，颜色用于传达用户的观念倾向程度，而矩阵中的小格子则表示参与评价的用户人数</li><li>例如：新闻数据中蕴藏的情感信息可视化效果。其中不同性质的图元代表不同的评价对象，水平轴指示时间属性，纵轴代表用户观念分数</li></ul><h3 id="文本可视化词云word-cloud">文本可视化–词云（Word Cloud）</h3><ul><li>“词云”就是通过形成“关键词云层”或“关键词渲染”，对网络文本中出现频率较高的“关键词”的视觉上的突出</li><li>词云图过滤掉大量的文本信息，使浏览网页者只要一眼扫过文本就可以领略文本的主旨</li></ul><h3 id="文本可视化主题河流theme-river">文本可视化–主题河流（ThemeRiver）</h3><ul><li>主题河流图是一种特殊的流图,它主要用来表示事件或主题等在一段时间内的变化</li><li>主题河流中不同颜色的条带状河流分支编码了不同的事件或主题，河流分支的宽度编码了原数据集中的value值</li></ul><h3 id="地理信息可视化地图map">地理信息可视化–地图（Map）</h3><ul><li>按照一定的法则，有选择地以二维或多维形式与手段在平面或球面上表示地球（或其它星球）若干现象的图形或图像</li><li>它具有严格的数学基础、符号系统、文字注记，并能用地图概括原则，科学地反映出自然和社会经济现象的分布特征及其相互关系</li><li>由使用地图语言表示事物所产生的直观性。地图上表示各种复杂的自然和人文信息都是通过地图语言来实现的</li></ul><h3 id="地理信息可视化点示地图dot-map">地理信息可视化–点示地图（DotMap）</h3><ul><li>点示地图(DotMap)在地理区域上放置圆点，旨在检测该地域上的空间布局或数据分布</li></ul><h3 id="地理信息可视化线数据">地理信息可视化–线数据</h3><ul><li>流向地图(FlowMap)在地图上显示信息或物体从一个位置到另一个位置的移动及其数量，通常用来显示人物、动物和产品的迁移数据。</li><li>单一流向线所代表的移动规模或数量由其粗幼度表示，有助显示迁移活动的地理分布</li></ul><h3 id="地理信息可视化区域数据">地理信息可视化–区域数据</h3><h3 id="地理信息可视化poi网格视图">地理信息可视化–POI网格视图</h3><h3 id="流数据可视化桑基图sankey-diagram">流数据可视化–桑基图（SankeyDiagram）</h3><ul><li>用来显示流向和数量（彼此之间的比例）。箭头或线的宽度用于显示大小，因此箭头越大，流量也越大</li><li>桑基图非常适合描述数据流（Flow），我们可用不同颜色来区分图表中的不同类别</li></ul><h3id="描述数据随时间的演变弧图arc-diagram">描述数据随时间的演变–弧图(ArcDiagram)</h3><ul><li>弧图(ArcDiagram)是二维双轴图表以外另一种数据表达方式</li><li>在弧线图中，节点(Nodes)将沿着X轴（一维轴）放置，再利用弧线表示节点与节点之间的连接关系</li><li>每条弧线的粗幼度表示源节点和目标节点之间的频率。弧线图适合用来查找数据共同出现的情况</li></ul><h3 id="基于时间线发展势态图">基于时间线发展势态图</h3><p>态势可视化(Generative Graph) -该看板以团队发明的通用可视化图表‘跌落态势图’为核心，主要用于可视化呈现部分省市COVID-19患者每个个体从发病到治愈，以及从接触到发病等不同维度的历程与时间分布。 -试图通过这种可视化方法观察患者病程与病毒传代，毒力，及潜伏期之间可能存在的关系。</p><p><img src="img/计科导复习/1653301416444.png" /></p><h3 id="网络可视化树形结构图tree-map">网络可视化–树形结构图（TreeMap）</h3><ul><li>树状结构图(TreeMap)是一种利用嵌套式矩形显示层次结构的方法，同时通过面积大小显示每个类别的数量</li></ul><h3id="网络可视化基本网络图network">网络可视化–基本网络图（Network）</h3><ul><li>网络图(Networkplanning)是一种图解模型，形状如同网络，故称为网络图。网络图是由作业（箭线）、事件（又称节点）和路线三个因素组成的</li><li>选择基本网络图后，可以快捷修改数据。网络图中不能出现循环路线</li></ul><h3id="描述一个工作过程的具体步骤流程图flow-diagram">描述一个工作过程的具体步骤–流程图（FlowDiagram）</h3><ul><li>流程图用图形化的符号记录整个系统和系统各模块的结构，描述了系统各子系统、相关文件和数据之间的关系。</li><li>形象直观，各种操作一目了然，不会产生“歧义性”，便于理解</li><li>缺点是所占篇幅较大，由于允许使用流程线，过于灵活，不受约束，使用者可使流程任意转向，从而造成阅读和修改上的困难</li></ul><h2 id="可视化技术">可视化技术</h2><p>可视化与工具–Web 工具</p><p>基于前端套件：HTML、CSS、JavaScript工具繁多：HighCharts、Echarts、D3.js…特点：效果美观，套用模版，简单易学，便于实现可视化交互</p><h3 id="可视化与工具编程语言">可视化与工具–编程语言</h3><p>Python：Matplotlib，功能全面，风格简洁、朴素而正式；不提供交互式界面（对比Matlab），只能自己查阅文档，修改参数。入门易，进阶难；适用于快速可视化实验结果Seaborn：基于Matplotlib，提供更为高层的APIPlotly：提供了更加现代化、更美观的图形输出，提供交互式绘图，可以制作3D图像Matlab/Mathematica：优势是与进行计算的程序可无缝对接，缺点是作图不如专门的作图工具美观，Matlab提供交互式作图，大大提升了作图效率，降低了对文档的依赖和上手难度R+ggplot2：核心理念是将绘图与数据分离，可以快速地将可视化结果转化成好看的、现代化主题TikZ：基于LaTeX，方便与正文字体保持高度一致性，易于在图中插入公式，学习曲线略微陡峭</p><h3 id="可视化与工具可视化软件">可视化与工具–可视化软件</h3><p>Gnuplot：基于命令行的交互式图工具，优点是速度快、画风清爽，软件开源且免费，图片质量相当专业Gephi：开源作图软件，专长于制作网络图；内置了许多先进的网络图可视化算法供用户选择；作为桌面软件，可以交互式操作，上手快；作为桌面软件失去一定的灵活性，不具备作复杂网络图的能力，数据量大时会有卡顿</p><h3 id="可视化与美学">可视化与美学</h3><p>聚焦 - 将用户注意力吸引到重要的区域上 -通过形状、大小、朝向、权重、位置、颜色等形成对比 平衡 - 有效利用空间 -使重要元素位于中间 - 使各元素在空间中平衡分布 简单 -避免包含过多造成混乱的图形元素 - 平衡美学特征与信息量 生动 -使用形象的图像、符号以及视觉隐喻 - 通过交互拉近传者与受者的距离</p><p>可视化，为有精度地观察疫情而设计 疫情可视化包含95%以上的图表类型 -常规条形图、折线图、气泡图等基本图形 -网络关系图、日历图、南丁格尔玫瑰图涵盖病例数据、人口流动、知识科普、应对措施、疫情影响等</p><p>可视化的作用 - 通过数据可视化，辨别出更广阔的疫情背景和更详尽的情景 -借助具体疫情数据和呈现，可以辅助检测政策是否合理 -跟踪进度，发现趋势，给出明智的战略性决定</p>]]></content>
    
    
    <categories>
      
      <category>计算机科学与技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SJTU</tag>
      
      <tag>计算机</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数值分析笔记</title>
    <link href="/2022/05/17/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/05/17/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="求解方程">求解方程</h1><h2 id="二分法">二分法</h2><p>时间复杂度：<span class="math inline">\(O(\log n)\)</span>n步二分法操作后，得到 <span class="math display">\[ \lvert x_c-r \rvert&lt; \frac{b-a}{2^{n+1}}\]</span> 如果误差小于<spanclass="math inline">\(0.5 \times10^{-p}\)</span>，精确到小数点后p位。</p><p><strong>定义1.5</strong> 令<spanclass="math inline">\(e_i\)</span>表示迭代过程中第i步时的误差，如果<span class="math display">\[ \lim_{i \to\infty}\frac{e_{i+1}}{e_i}=S&lt;1\]</span>该方法被称为满足<strong>线性收敛</strong>，收敛速度为<spanclass="math inline">\(S\)</span>。</p><p><strong>定理1.6</strong> 假设函数<spanclass="math inline">\(g\)</span>是连续可微函数，<spanclass="math inline">\(g(r)=r\)</span>,<spanclass="math inline">\(S=\lvert g&#39;(r)\rvert&lt;1\)</span>，则不动点迭代对于一个足够接近<spanclass="math inline">\(r\)</span>的初始估计，以速度<spanclass="math inline">\(S\)</span>线性收敛到不动点<spanclass="math inline">\(r\)</span>。</p><p>根据定理1.6，近似误差之间的关系 <span class="math display">\[ e_{i+1}\approx Se_i\]</span> 在收敛过程中得到保持。</p><p><strong>定义1.7</strong> 如果迭代方法对于一个足够接近<spanclass="math inline">\(r\)</span>的初值能收敛到<spanclass="math inline">\(r\)</span>，该迭代方法被称为<strong>局部收敛</strong>到<spanclass="math inline">\(r\)</span>。</p><p>定理1.6的结论是当<span class="math inline">\(\lvert g&#39;(r)\rvert&lt;1\)</span>时不动点迭代局部收敛。</p><blockquote><p>如果误差小于 <span class="math inline">\(0.5 \times10^{-p}\)</span>，解精确到小数点后 <spanclass="math inline">\(p\)</span>位.</p></blockquote><h2 id="牛顿法">牛顿法</h2><p>用 <span class="math inline">\(x_i\)</span>点上切线解作为迭代的下一步 <span class="math inline">\(x_{i+1}\)</span><span class="math display">\[    x_{i+1}=x_i- \frac{f(x_i)}{f&#39;(x_i)}\]</span></p><h3 id="二次收敛性">二次收敛性</h3><p><strong>定义</strong> 令 <span class="math inline">\(e_i\)</span>表示第 <span class="math inline">\(i\)</span> 步的误差，若满足条件 <spanclass="math display">\[    M= \lim_{i \to \infty} \frac{e_{i+1}}{e_i^{2}} &lt; \infty\]</span> 则称迭代<strong>二次收敛</strong></p><p><strong>定理</strong> 令 <span class="math inline">\(f\)</span>为二阶连续可导函数，满足 <span class="math inline">\(f(r)=0\)</span> 和<span class="math inline">\(f&#39;(r)\neq 0\)</span>，则Newton方法局部二次收敛，误差满足 <span class="math display">\[    \lim_{i \to \infty}=\frac{e_{i+1}}{e_i^{2}}=M=\frac{f&#39;&#39;(r)}{2f&#39;(r)}\]</span></p><h3 id="重根问题和改进的newton方法">重根问题和改进的Newton方法</h3><p>Newton方法不能二次收敛到重根，如 <span class="math display">\[    f(x)=x^{m}\]</span></p><p><strong>定理</strong> <span class="math inline">\(f \inC^{m+1}[a,b]\)</span>，<span class="math inline">\(f\)</span> 在 <spanclass="math inline">\(r\)</span> 点有 <spanclass="math inline">\(m\)</span> 阶多重根，则Newton方法局部线性收敛到<span class="math inline">\(r\)</span>，常数 <spanclass="math inline">\(S=(m-1)/m\)</span></p><p><strong>定理</strong> 若已知 <spanclass="math inline">\(m&gt;1\)</span> 重根，则改进的Newton方法 <spanclass="math display">\[    x_{i+1}=x_i-\frac{mf(x_i)}{f&#39;(x_i)}         \]</span> 二次收敛到 <span class="math inline">\(r\)</span>. 设 <spanclass="math inline">\(f(x)=(x-r)^{m}g(x)\)</span>，则 <spanclass="math display">\[    \lim_{i \to \infty}\frac{e_{i+1}}{e_{i}^{2}}=\frac{g&#39;(r)}{mg(r)}\]</span></p><p>或者可以通过计算 <span class="math inline">\(\displaystyle\mu(x)=\frac{f(x)}{f&#39;(x)}\)</span> 的根（注意它没有重根）来得到<span class="math inline">\(f(x)\)</span> 的根.</p><h3 id="割线法">割线法</h3><p>思想是用差商代替Newton法中的导数. 取 <spanclass="math inline">\(x_0\)</span>为初始估计 <spanclass="math display">\[    x_{i+1}=x_i-\frac{f(x_i)(x_i-x_{i-1})}{f(x_i)-f(x_{i-1})}\]</span></p><p>它满足<strong>超线性收敛</strong> <span class="math display">\[    e_{i+1} \thickapprox \lvert \frac{f&#39;&#39;(r)}{2f&#39;(r)} \rvert^{\alpha-1}e_i^{\alpha}\]</span> 其中 <spanclass="math inline">\(\alpha=(1+\sqrt{5})/2\)</span></p><p>一般当 <span class="math inline">\(1&lt;\alpha&lt;2\)</span>时都称为超线性收敛.</p><p>事实上对于割线法的收敛阶有（当 <spanclass="math inline">\(f&#39;(r),f&#39;&#39;(r)\neq 0\)</span>） <spanclass="math display">\[    e_{i+1}= \lvert f&#39;&#39;(r)/2f&#39;(r) \rvert e_i e_{i-1}\]</span> 成立. 这可以推出上面的 <spanclass="math inline">\(\alpha\)</span>.</p><h1 id="插值">插值</h1><h3 id="数据和插值函数">数据和插值函数</h3><p>对 <span class="math inline">\(n\)</span>个不同点，存在唯一一个不高于 <span class="math inline">\(n-1\)</span>次的插值多项式</p><p>方便的做法是利用Newton差商. <span class="math inline">\(f[x_1\cdotsx_n]\)</span> 表示（唯一）多项式的 <spanclass="math inline">\(x^{n-1}\)</span> 项的系数，有递归式 <spanclass="math display">\[    略\]</span> 或者 <span class="math display">\[    f[x_1\cdots x_k]= \sum_{j=1}^{k} \frac{f(x_j)}{(x_j-1)\cdots(x_j-x_{j-1})(x_j-x_{j+1})\cdots (x_j-x_k)}\]</span> 从而 <span class="math display">\[    f[x_1\cdots x_n]=f[\sigma(x_1)\cdots \sigma(x_n)]\]</span> 如果 <span class="math inline">\(f(x)\)</span> 在区间内有<span class="math inline">\(n-1\)</span> 阶导数，则在区间内存在一点<span class="math inline">\(c\)</span>使得 <span class="math display">\[    f[x_1\cdots x_n]=\frac{f^{(n-1)}(c)}{(n-1)!}\]</span></p><p>Newton插值多项式 <span class="math display">\[    P(x)=f[x_1]+f[x_1 x_2](x-x_1)+\cdots +f[x_1\cdots x_n](x-x_1)\cdots(x-x_{n-1})\]</span></p><h3 id="插值误差">插值误差</h3><p><span class="math display">\[    f(x)-P_{n-1}(x)=f[x_1\cdots x_nx](x-x_1)\cdots (x-x_n)=\frac{f^{(n)}(c)}{n!}(x-x_1)\cdots (x-x_n)\]</span></p><h3 id="龙格现象">龙格现象</h3><p>略，大一上写过文章.</p><h3 id="hermite插值">Hermite插值</h3><p><span class="math inline">\(f(x) \in C^{1}[a,b]\)</span>，<spanclass="math inline">\(a\leqslant x_0&lt;x_1&lt;\cdots &lt;x_n\leqslantb\)</span>. 则 <span class="math display">\[    H_{2n+1}(x)= \sum_{i=0}^{n} f(x_i)A_i(x)+ \sum_{i=0}^{n}f&#39;(x_i)B_i(x)\]</span> <span class="math display">\[    A_i(x)=[1-2(x-x_i)l_i&#39;(x_i)]l_i^{2}(x)\]</span> <span class="math display">\[    B_i(x)=(x-x_i)l_i^{2}(x)\]</span> 其中 <span class="math inline">\(l_i\)</span>是Lagrange插值基.</p><h3 id="chebyshev插值">Chebyshev插值</h3><p>希望找到最佳的一致逼近</p><p>取 <span class="math display">\[    x_i= \cos \frac{(2i-1)\pi}{2n}, i=1,\cdots,n\]</span> 则此时 <span class="math display">\[    \max_{-1\leqslant x\leqslant 1} \lvert (x-x_1)\cdots (x-x_n) \rvert= \frac{1}{2^{n-1}} T_n(x)\leqslant \frac{1}{2^{n-1}}\]</span> 取到最小值</p><p><strong>Chebyshev多项式</strong> 满足 <spanclass="math inline">\(T_n(x)=\cos (n \arccos x)\)</span>，且有递推关系<span class="math display">\[    T_0(x)=1, \quad T_1(x)=x\]</span> <span class="math display">\[    T_{n+1}(x)=2x T_n(x)- T_{n-1}(x)\]</span> - <span class="math inline">\(T_n(x)\)</span> 为 <spanclass="math inline">\(n\)</span> 次多项式. - 最高次项系数为 <spanclass="math inline">\(2^{n-1}\)</span>. <spanclass="math inline">\(T_n(1)=1, T_n(-1)=(-1)^{n}\)</span>. -最大绝对值为 <span class="math inline">\(1\)</span>. - <spanclass="math inline">\(T_n(x)=\cos (n \arccos x)\)</span> 的极值点为<span class="math display">\[    x_i= \cos \frac{i\pi}{n}, i=0,\cdots ,n        \]</span></p><p>令 <span class="math inline">\(\displaystyley=\frac{b-a}{2}x+\frac{b+a}{2}, x\in [-1,1]\)</span>，则 <spanclass="math display">\[    \lvert (y-y_1)\cdots (y-y_n) \rvert =(\frac{b-a}{2})^{n}\lvert(x-x_1)\cdots (x-x_n) \rvert \leqslant (\frac{b-a}{2})^{n}\frac{1}{2^{n-1}}\]</span></p><h3 id="三次样条插值">三次样条插值</h3><p><span class="math inline">\(n\)</span> 个点 <spanclass="math inline">\((x_i,y_i)\)</span> 一共 <spanclass="math inline">\(3n-5\)</span> 个方程，<spanclass="math inline">\(3n-3\)</span> 个未知数. 需要补充两个条件.具体计算见p152-153.</p><ul><li>自然三次样条：<spanclass="math inline">\(S_1&#39;&#39;(x_1)=S_{n-1}&#39;&#39;(x_n)=0\)</span>.</li><li>曲率调整样条：<spanclass="math inline">\(S_1&#39;&#39;(x_1)=2c_1=v_1,S_{n-1}&#39;&#39;(x_n)=2c_n=v_n\)</span>.</li><li>钳制边界条件：<span class="math inline">\(S_1&#39;(x_1)=v_1,S_{n-1}&#39;(x_n)=v_n\)</span>.</li><li>抛物线端点的三次样条：<spanclass="math inline">\(d_1=0=d_{n-1}\)</span>.</li><li>非纽结三次样条：<spanclass="math inline">\(S_1=S_2,S_{n-2}=S_{n-1}\)</span></li></ul><p><strong>误差</strong>：设 <span class="math inline">\(f(x) \inC^{4}[a,b]\)</span>，对于曲率调整样条或钳制样条 <spanclass="math inline">\(S(x)\)</span>，令 <spanclass="math inline">\(\delta = \max_{i} \delta_i\)</span>，有估计式<span class="math display">\[    \max_{a\leqslant x\leqslant b} \lvert f(x)- S(x) \rvert \leqslant\frac{5}{384} \max_{a\leqslant x\leqslant b} \lvert f^{(4)}(x) \rvert\delta^{4}\]</span></p><h3 id="bezier曲线">Bezier曲线</h3><h1 id="最小二乘">最小二乘</h1><h3 id="离散">离散</h3><p>给定函数 <span class="math inline">\(f\)</span>，求 <spanclass="math inline">\(P_n(x)\)</span> 使误差 <spanclass="math display">\[    E=\frac{1}{m} \sum_{i=1}^{m} (y_i-P_{n-1}(x_i))^{2}\]</span> 最小 设 <span class="math display">\[    A=    \begin{bmatrix}    1 &amp; x_1 &amp; \cdots &amp; x_1^{n-1} \\    \vdots &amp; &amp; &amp; \vdots \\    1 &amp; x_{m} &amp; \cdots &amp; x_m^{n-1} \\    \end{bmatrix}\]</span> 则法线方程（<span class="math inline">\(a=(a_0,a_1,\cdots,a_{n-1})^{\mathsf{T}},y=(y_1,\cdots ,y_m)^{\mathsf{T}}\)</span>） <spanclass="math display">\[    A^{\mathsf{T}}A a= A^{\mathsf{T}}y\]</span></p><p>给定 <span class="math inline">\((t_1,y_1),\cdots,(t_n,y_n)\)</span>，则平方误差最小逼近的直线 <spanclass="math inline">\(y=a_0+a_1t\)</span>满足 <spanclass="math display">\[    \begin{bmatrix}    \sum_{i=1}^{m} 1 &amp; \sum_{i=1}^{m} t_i \\    \sum_{i=1}^{m} t_i &amp; \sum_{i=1}^{m} t_i^{2} \\    \end{bmatrix}    \begin{bmatrix}    a_0 \\    a_1    \end{bmatrix}    =    \begin{bmatrix}    \sum_{i=1}^{m} y_i \\    \sum_{i=1}^{m} t_i y_i \\    \end{bmatrix}\]</span></p><p>有时 <span class="math inline">\(A^{\mathsf{T}}A\)</span>的条件数过高，可以考虑QR分解. 设 <spanclass="math inline">\(A=QR\)</span>， <span class="math display">\[    \left\| QRx-b \right\|_{2}= \left\| Rx-Q^{\mathsf{T}}b \right\|_{2}\]</span> 考虑到 <span class="math display">\[    R=\begin{bmatrix}    R_1 \\    0 \\    \end{bmatrix}\]</span> 再令 <span class="math display">\[    Q^{\mathsf{T}}b=\begin{bmatrix}    b_1 \\    b_2 \\    \end{bmatrix}\]</span></p><p>而 <span class="math inline">\(R_1\)</span>是上三角方阵，所以法线方程即为 <span class="math display">\[    R_1 x=b_1\]</span></p><p><strong>数据线性化</strong> <span class="math display">\[    y=c_1 \mathrm{e}^{c_2t} \Rightarrow \ln y= \ln c_1+ c_2t\]</span></p><h3 id="连续">连续</h3><p>在一般的Euclid空间中，给定函数 <spanclass="math inline">\(f\)</span>，求 <spanclass="math inline">\(P_n(x)\)</span> 使误差 <spanclass="math display">\[    E= \int_{a}^{b} [f(x)-P_n(x)]^{2} \mathrm{d}x\]</span> 最小，称 <span class="math inline">\(P_n(x)\)</span>为该函数的<strong>最小二乘逼近多项式</strong>，假设 <spanclass="math inline">\(P_n(x)=a_nx^{n}+\cdots+a_1x+a_0\)</span>，那么法线方程 <span class="math display">\[    \frac{\partial E}{\partial a_j}=0   \]</span> 即为 <span class="math display">\[    \sum_{k=0}^{n} a_k \int_{a}^{b} x^{k+j} \mathrm{d}x= \int_{a}^{b}f(x) x^{j} \mathrm{d}x\]</span> 如果我们令 <spanclass="math inline">\(\varphi_k=x^{k}\)</span>为基函数，并赋予Euclid空间的典范内积则法线方程变为 <spanclass="math display">\[    \begin{bmatrix}    (\varphi_0,\varphi_0) &amp; \cdots &amp; (\varphi_0,\varphi_n) \\    \vdots &amp; &amp; \vdots \\    (\varphi_n,\varphi_0) &amp; \cdots &amp; (\varphi_n,\varphi_n) \\    \end{bmatrix}    \begin{bmatrix}    a_0 \\    \vdots \\    a_n \\    \end{bmatrix}    =    \begin{bmatrix}    (f,\varphi_0) \\    \vdots \\    (f,\varphi_n) \\    \end{bmatrix}\]</span> 比如如果我们用 <span class="math inline">\(\varphi_k\)</span>求 <span class="math inline">\(f(x)=\sin \pi x\)</span>的最小二乘逼近多项式，那么得到的系数矩阵是Hilbert矩阵，条件数很大，求解困难.通常需要采用<strong>正交基</strong>.</p><h3 id="权函数和正交基">权函数和正交基</h3><p><strong>权函数</strong> <span class="math inline">\(w(x)\)</span>满足 <span class="math inline">\(\forall x \in I=[a,b],(w(x)\geqslant0)\)</span>，且对于 <span class="math inline">\(I\)</span>上的任意子区间不恒为零. 从而可以定义内积 <span class="math display">\[    (f,g)_{w}= \int_{a}^{b} w(x)f(x)g(x) \mathrm{d}x\]</span> 如果 <span class="math inline">\(\{\varphi_0,\cdots,\varphi_n\}\)</span> 满足 <span class="math display">\[    \int_{a}^{b} w(x)\varphi_j(x)\varphi_k(x) \mathrm{d}x=    \begin{cases}        0,\quad j\neq k \\        \alpha_k&gt;0,\quad j=k    \end{cases}\]</span> 那么它们就是一组正交基，如果 <spanclass="math inline">\(\alpha_k=1\)</span>，则标准正交.</p><h3 id="最小二乘问题">最小二乘问题</h3><p>给定函数 <span class="math inline">\(f\)</span>，<spanclass="math inline">\(w(x)\)</span> 为权函数，求 <spanclass="math inline">\(P_n(x)=\sum_{k=0}^{n} a_k\varphi_k(x)\)</span>使误差 <span class="math display">\[    E(a_0,\cdots ,a_n)=\int_{a}^{b} w(x)[f(x)-P_n(x)]^{2} \mathrm{d}x\]</span> 极小，称 <span class="math inline">\(P_n(x)\)</span>为该函数以<span class="math inline">\(w(x)\)</span>为权函数的<strong>最小二乘函数逼近</strong>.</p><p>如果 <span class="math inline">\(\{\varphi_0,\cdots,\varphi_n\}\)</span> 为一组正交函数系，则 <span class="math display">\[    a_k=\frac{(\varphi_k,f)_{w}}{(\varphi_k,\varphi_k)_{w}}\]</span></p><h3 id="gram-schmidt-正交化过程">Gram-Schmidt 正交化过程</h3><p>是想把 <span class="math inline">\(\{1,x,\cdots ,x^{n}\}\rightarrow\{\varphi_0,\cdots ,\varphi _n\}\)</span> 成为正交基.</p><p>设 <span class="math inline">\(w(x)\)</span>为 <spanclass="math inline">\([a,b]\)</span>上的权函数. 令 <spanclass="math inline">\(\varphi_0(x)=1\)</span>，<spanclass="math inline">\(\varphi_1(x)=x-B_1\)</span>，其中 <spanclass="math display">\[    B_1=\frac{(x\varphi_0,\varphi_0)_{w}}{(\varphi_0,\varphi_0)_{w}}\]</span> 对于权函数 <span class="math inline">\(w(x)\equiv1\)</span>的情况 <span class="math display">\[    B_1=\frac{\int_{-1}^{1} x \mathrm{d}x}{\int_{-1}^{1} 1\mathrm{d}x}=0\]</span></p><p>当 <span class="math inline">\(k\geqslant 2\)</span>时，有 <spanclass="math display">\[    \varphi_k(x)=(x-B_k)\varphi_{k-1}-C_k\varphi_{k-2}\]</span> 其中 <span class="math display">\[    B_k=\frac{(x\varphi_{k-1},\varphi_{k-1})_{w}}{(\varphi_{k-1},\varphi_{k-1})_{w}},\qquadC_k=\frac{(x\varphi_{k-2},\varphi_{k-1})_{w}}{(\varphi_{k-2},\varphi_{k-2})_{w}}\]</span> 则所得到的集合 <span class="math inline">\(\{\varphi_0,\cdots,\varphi_n\}\)</span> 是正交的.按这种办法求出来的多项是首一的，可能与一般定义的Legendre多项式或Chebyshev多项式不同.（标准定义的Legendre多项式要求<span class="math inline">\(P_n(1)=1\)</span>）</p><p>首一性还可以得到以下性质 <span class="math display">\[    (x\varphi_{k-2},\varphi_{k-1})_{w}=(\varphi_{k-1},\varphi_{k-1})_{w}\]</span></p><h2 id="qr分解">QR分解</h2><p><strong>完全QR分解</strong> 对于 <span class="math inline">\(A\in\mathbb{F}^{m \times n}, m &gt; n\)</span>, <spanclass="math inline">\(A=QR\)</span>. 其中 <span class="math inline">\(Q\in O(m)\)</span> <span class="math display">\[    R=    \begin{bmatrix}        \tilde{R} \\        0 \\    \end{bmatrix}\]</span></p><p>计算的复杂度为 <span class="math inline">\(O(n^{3})\)</span></p><h3 id="gram-schimidt-正交">Gram-Schimidt 正交</h3><p>朴素的和改进的版本p194</p><p>过程需要大约 <span class="math inline">\(m^{3}\)</span> 次乘法和<span class="math inline">\(m^{3}\)</span> 次加法：</p><p>计算 <span class="math inline">\(r_{ij}=q_{i}^{\mathsf{T}}A_j\)</span> 时需要进行 <spanclass="math inline">\(m\)</span>次乘法和 <spanclass="math inline">\(m-1\)</span>次加法，计算 <spanclass="math inline">\(y-y-r_{ij}q_i\)</span>时进行 <spanclass="math inline">\(m\)</span>次乘法和 <spanclass="math inline">\(m\)</span>次加减法；舍去计算 <spanclass="math inline">\(r_{jj}\)</span>和 <spanclass="math inline">\(q_j\)</span>的次数. 则加减法次数和乘除法次数都约为<span class="math display">\[    \sum_{j=1}^{n} (j-1)\cdot 2m=m^{3}-m\]</span> 即需要大约 <span class="math inline">\(m^{3}\)</span>次乘法和<span class="math inline">\(m^{3}\)</span>次加法.</p><h3 id="householder-反射">Householder 反射</h3><p>令 <span class="math inline">\(w \in \mathbb{R}^{n}\)</span>，满足<span class="math inline">\(w^{\mathsf{T}}w=1\)</span>，则 <spanclass="math display">\[    H=I-2ww^{\mathsf{T}}\]</span> 为一个 Householder 反射，它<strong>对称正交</strong></p><p>设 <span class="math inline">\(\left\| x \right\|_{2}= \left\| y\right\|_{2}\)</span>，取 <span class="math inline">\(\displaystylew=\frac{x-y}{\left\| x-y \right\|_{2}}\)</span>，则 <spanclass="math inline">\(H=I-2ww^{\mathsf{T}}\)</span> 满足 <spanclass="math inline">\(Hx=y\)</span></p><p>其实每步选模长的正负号时应该与原位置的正负号相反，书上忽略了这一点.</p><p><strong>Householder变换做QR分解的复杂度</strong></p><p>只考虑向量与向量相乘，矩阵与向量相乘，矩阵与矩阵相乘的计算 对 <spanclass="math inline">\(m \times n\)</span> 的矩阵 <spanclass="math inline">\(A\)</span> 进行QR分解， - 若只需要R，则大约需要<span class="math display">\[    n^{2}(m-\frac{n}{3})\]</span> 次乘法和加减法.</p><ul><li>若还需要Q，则大约需要 <span class="math display">\[  m^{2}n-mn^{2}+\frac{n^{3}}{3}\]</span> 次乘法和加减法.</li></ul><hr /><p>QR decompositon - Wikipediahttps://en.wikipedia.org/wiki/QR_decomposition</p><hr /><h1 id="数值微分和积分">数值微分和积分</h1><h2 id="数值微分">数值微分</h2><h3 id="有限差分公式">有限差分公式</h3><p><strong>二点前向差分公式</strong> <span class="math display">\[    f&#39;(x) =\frac{f(x+h)-f(x)}{h} - \frac{h}{2} f&#39;&#39;(c)\]</span> 其中 <span class="math inline">\(c\)</span> 在 <spanclass="math inline">\(x\)</span> 和 <spanclass="math inline">\(x+h\)</span> 之间</p><p>如果误差是 <span class="math inline">\(O(h^{n})\)</span>，称公式为<span class="math inline">\(n\)</span> 阶近似</p><p><strong>三点中心差分公式（二阶中心差分公式）</strong> <spanclass="math display">\[    f&#39;(x)= \frac{f(x+h)-f(x-h)}{2h}-\frac{h^{2}}{6}f&#39;&#39;&#39;(c)\]</span> 其中 <span class="math inline">\(x-h&lt;c&lt;x+h\)</span></p><p><strong>五点中心差分公式</strong> <span class="math display">\[    f&#39;(x)\thickapprox  \frac{f(x-2h)-8f(x-h)+8f(x+h)-f(x+2h)}{12h}\]</span> 误差 <span class="math inline">\(O(h^{4})\)</span></p><p><strong>二阶导数的三点中心差分公式</strong> <spanclass="math display">\[    f&#39;&#39;(x)= \frac{f(x+h)-2f(x)+f(x-h)}{h^{2}}-\frac{h^{2}}{12}f^{(4)}(c)\]</span> 其中 <span class="math inline">\(x-h&lt;c&lt;x+h\)</span></p><h3 id="舍入误差">舍入误差</h3><p>以三点中心差分公式为例，加上机器误差，总误差的绝对值上界是 <spanclass="math display">\[    \frac{h^{2}}{6} f&#39;&#39;&#39;(c)+ \frac{\varepsilon_{mach}}{h}\]</span></p><h3 id="外推">外推</h3><p>如果有 <span class="math display">\[    Q=F_n(h) + Kh^{n} +O(h^{n+1})\]</span> 那么用 <span class="math inline">\(\frac{h}{2}\)</span> 代替<span class="math inline">\(h\)</span>，重新解出得 <spanclass="math display">\[    Q\thickapprox \frac{2^{n}F(h/2)-F(h)}{2^{n}-1}\]</span> <span class="math inline">\(F_{n+1}(h)\)</span> 至少是 <spanclass="math inline">\(n+1\)</span> 阶近似 <spanclass="math inline">\(Q\)</span> 的公式</p><p>一个例子是用二阶中心差分公式进行外推，得到的其实是四阶公式.且误差项仅和 <span class="math inline">\(h\)</span> 的偶数阶有关.</p><h2 id="数值积分">数值积分</h2><p>基本上都是用插值公式和误差导出积分的公式和误差.</p><h3 id="梯形法则">梯形法则</h3><p><span class="math display">\[    \int_{x_0}^{x_1} P(x) \mathrm{d}x= \frac{h}{2}(f(x_0)+f(x_1))\]</span> <span class="math display">\[    \int_{x_0}^{x_1} E(x) \mathrm{d}x= -\frac{h^{3}}{12}f&#39;&#39;(c)\]</span></p><h3 id="simpson法则">Simpson法则</h3><p><span class="math display">\[    \int_{x_0}^{x_2} f(x)\mathrm{d}x=\frac{h}{3}(y_0+4y_1+y_2)-\frac{h^{5}}{90}f^{(4)}(c)\]</span> 其中 <spanclass="math inline">\(h=x_2-x_1=x_1-x_0\)</span>，<spanclass="math inline">\(c\)</span>在 <spanclass="math inline">\(x_0\)</span>和 <spanclass="math inline">\(x_2\)</span>之间.</p><p><strong>误差的证明</strong></p><p>首先误差 <span class="math display">\[    E_s= \int_{x_0}^{x_2}\frac{(x-x_0)(x-x_1)(x-x_2)}{3!}f&#39;&#39;&#39;(c(x)) \mathrm{d}x\]</span> 因为会变号，所以不能使用积分中值定理，尝试改写 <spanclass="math inline">\(f&#39;&#39;&#39;(c(x))\)</span> 得到</p><p><span class="math display">\[    f&#39;&#39;&#39;(c(x))=f&#39;&#39;&#39;(c(x_1))+(x-x_1)f^{(4)}(c(\bar{x}))c&#39;(\bar{x})\]</span> 代入 <span class="math inline">\(E_s\)</span> 后 <spanclass="math inline">\(f&#39;&#39;&#39;(c(x_1))\)</span>对应项积分为零，这可以解释为什么误差是跟 <spanclass="math inline">\(f^{(4)}(x)\)</span> 有关.</p><p>另一方面，用Newton插值，可以认为再在 <spanclass="math inline">\(x_1\)</span> 点插值 <span class="math display">\[    P_3(x)=P_2(x)+D(x-x_0)(x-x_1)(x-x_2)\]</span> 那么 <span class="math display">\[    \int_{x_0}^{x_2} P_2(x)  \mathrm{d}x=\int_{x_0}^{x_2} P_3(x)\mathrm{d}x\]</span> 而由插值误差即知 <span class="math display">\[    E_s=\int_{x_0}^{x_2} \frac{(x-x_0)(x-x_1)^{2}(x-x_2)}{4!}f^{(4)}(c)\mathrm{d}x\]</span></p><p>这下可以使用积分中值定理了，计算得到 <span class="math display">\[    E_s=-\frac{h^{5}}{90} f^{(4)}(c)\]</span></p><p>从该表达式知Simpson法则的代数精度为3.</p><h3id="阶newton-cotes公式simpson38公式">3阶Newton-Cotes公式（Simpson3/8公式）</h3><p><span class="math display">\[    \int_{x_0}^{x_3} f(x) \mathrm{d}x \thickapprox\frac{3h}{8}(f_0+3f_1+3f_2+f_3)\]</span> 其误差也为 <spanclass="math inline">\(O(h^{5})\)</span>，而因计算更复杂而通常不使用.</p><h3 id="复合梯形法则">复合梯形法则</h3><p>对于 <span class="math inline">\(\displaystyle x_j=x_0+jh, x_0=a,x_n=b, h=(b-a)/n\)</span>，如果 <spanclass="math inline">\(f&#39;&#39;\)</span> 在区间 <spanclass="math inline">\([a,b]\)</span> 连续，有 <spanclass="math display">\[    \int_{a}^{b} f(x) \mathrm{d}x=\frac{h}{2}(f_0+2f_1+\cdots+2f_{n-1}+f_n)-\frac{(b-a)h^{2}}{12}f&#39;&#39;(c)\]</span></p><p>数值精度是2阶，代数精度是1阶.</p><h3 id="复合simpson公式">复合Simpson公式</h3><p>如果 <span class="math inline">\(f^{(4)}\)</span> 在 <spanclass="math inline">\([a,b]\)</span> 上连续，有 <spanclass="math display">\[    \int_{a}^{b} f(x) \mathrm{d}x=\frac{h}{3}(y_0+y_{2m}+4\sum_{i=1}^{m}y_{2i-1}+2\sum_{i=1}^{m-1} y_{2i})-\frac{(b-a)h^{4}}{180}f^{(4)}(c)\]</span></p><p>注意使用 <span class="math inline">\(m\)</span> 次Simpson公式需要<span class="math inline">\(2m\)</span> 等分. <spanclass="math inline">\(h=(b-a)/2m\)</span></p><h3 id="舍入误差对复合积分公式的影响">舍入误差对复合积分公式的影响</h3><p><span class="math display">\[    \int_{a}^{b} f(x) \mathrm{d}x\thickapprox\frac{h}{2}(f_0+2f_1+\cdots +2f_{n-1}+f_n)+\varepsilon_R\]</span> 有 <span class="math display">\[    \varepsilon_R \thickapprox (b-a)\varepsilon_h\]</span></p><h3 id="中点法则">中点法则</h3><p>当 <span class="math inline">\(f \in C^{2}[a,b]\)</span> 时， <spanclass="math display">\[    \int_{x_0}^{x_1} f(x)\mathrm{d}x=hf(w)+\frac{h^{3}}{24}f&#39;&#39;(c)\]</span> 其中 <span class="math inline">\(h=x_1-x_0\)</span>，<spanclass="math inline">\(w\)</span> 是中点 <spanclass="math inline">\(x_0+h/2\)</span>，<spanclass="math inline">\(x_0\leqslant c\leqslant x_1\)</span></p><p>Taylor展开至二阶即可得到误差.与梯形法则相比，中点法则的误差只有一半.</p><h3 id="复合中点法则">复合中点法则</h3><p><span class="math display">\[    \int_{a}^{b} f(x) \mathrm{d}x= h\sum_{i=1}^{m}f(w_i)+\frac{(b-a)h^{2}}{24}f&#39;&#39;(c)\]</span> 其中 <span class="math inline">\(h=(b-a)/m\)</span>，<spanclass="math inline">\(w\)</span> 是 <spanclass="math inline">\([a,b]\)</span> 中 <spanclass="math inline">\(m\)</span> 个相等子区间的中点</p><h3 id="另一个开newton-cotes法则">另一个开Newton-Cotes法则</h3><p><span class="math display">\[    \int_{x_0}^{x_4} f(x)\mathrm{d}x=\frac{4h}{3}[2f(x_1)-f(x_2)+2f(x_3)]+\frac{14h^{5}}{45}f^{(4)}(c)\]</span></p><h2 id="romberg积分">Romberg积分</h2><p>一个表格 <span class="math display">\[    \begin{aligned}    &amp;R_{11} \\    &amp;R_{21} \quad R_{22} \\    &amp;R_{31} \quad R_{32} \quad R_{33} \\    &amp;\vdots    \end{aligned}\]</span> 定义 <span class="math display">\[    h_j=\frac{1}{2^{j-1}}(b-a)\]</span></p><p>其中 <span class="math display">\[    R_{j1}=\frac{1}{2}R_{j-1,1}+h_j\sum_{i=1}^{2^{j-2}} f(a+(2i-1)h_j)\]</span></p><p><span class="math display">\[    R_{jk}=\frac{4^{k-1}R_{j,k-1}-R_{j-1,k-1}}{4^{k-1}-1}\]</span></p><p>这里 <span class="math inline">\(R_{jj}\)</span>是 <spanclass="math inline">\(2j\)</span>阶的近似.</p><p>Romberg积分第二列最后一项和复合Simpson法则的结果一致.事实上，Romberg积分的第一列定义为连续的复合梯形法则，第二列是复合Simpson项，即复合梯形法则的外推是复合Simpson法则.</p><p>Romberg积分的常用停止条件是计算新的一行直到相邻的对角线元素 <spanclass="math inline">\(R_{jj}\)</span>差异小于当前的容差.</p><h2 id="自适应积分">自适应积分</h2><p>为什么要自适应： - 高阶导数不易计算 - 有些区域变化缓慢</p><h2 id="高斯积分">高斯积分</h2><p>高斯积分具有 <spanclass="math inline">\(2n+1\)</span>阶的代数精度（使用 <spanclass="math inline">\(n+1\)</span>个点），该精度是Newton-Cotes公式的两倍.</p><p>三点Gauss积分 <span class="math display">\[    x_1=-\sqrt{\frac{3}{5}} \quad x_2=0 \quad x_3=\sqrt{\frac{3}{5}} \\    c_1=\frac{5}{9} \quad c_2=\frac{8}{9} \quad c_3=\frac{5}{9}\]</span></p><p>关于正交多项式系，有这样的定理：</p><p>如果 <spanclass="math inline">\(\{p_0,p_1,p_2,\cdots,p_n\}\)</span>在 <spanclass="math inline">\([a,b]\)</span>区间上是多项式的正交集，并且如果deg<span class="math inline">\(p_i=i\)</span> ，则 <spanclass="math inline">\(p_i\)</span>在区间 <spanclass="math inline">\((a,b)\)</span>上有 <spanclass="math inline">\(i\)</span>个不同的根</p><p>证明可以参考</p><hr /><p>多项式以及相关内容 https://zhuanlan.zhihu.com/p/270620896</p><table style="width:6%;"><tbody><tr class="odd"><td>### 正交多项式</td></tr><tr class="even"><td>Legendre多项式：在 <spanclass="math inline">\([-1,1]\)</span>上以权函数 <spanclass="math inline">\(w(x)=1\)</span>的正交多项式. 它们满足递推关系<span class="math display">\[(n+1)P_{n+1}(x)=(2n+1)x P_n(x)-nP_{n-1}(x)\]</span> 或者 <span class="math display">\[p_i(x)=\frac{1}{2^{i}i!}\frac{\mathrm{d}^{i}}{\mathrm{d}x^{i}}[(x^{2}-1)^{i}]\]</span> 其中 <span class="math inline">\(P_0(x)=1\)</span>，<spanclass="math inline">\(P_1(x)=x\)</span></td></tr><tr class="odd"><td>这里Legendre多项式非首一，而是满足 <spanclass="math inline">\(P_n(1)=1\)</span>.</td></tr><tr class="even"><td>正交性： <span class="math display">\[(P_n,P_m)=\int_{-1}^{1} P_n(x)P_m(x) \mathrm{d}x=\begin{cases}0, \quad m \neq n \\\frac{2}{2n+1}, m = n\end{cases}\]</span></td></tr><tr class="odd"><td>奇偶性：<span class="math inline">\(P_{2k}(x)\)</span>只含有偶次幂， <span class="math inline">\(P_{2k+1}\)</span>只含有奇次幂</td></tr><tr class="even"><td>若令 <span class="math inline">\(\tilde{P}_n(x)=\frac{2^{n}(n!)^{2}}{(2n)!}P_n(x)\)</span>，则称 <spanclass="math inline">\(\tilde{P}_n(x)\)</span>为首一的Legendre多项式.</td></tr><tr class="odd"><td>更多关于正交多项式还可以参考</td></tr></tbody></table><p>逆问题解析 https://zhuanlan.zhihu.com/p/352724194</p><hr /><p>还有一类正交多项式Laguerre多项式，其对应的权函数 $w(x)=^{-x} $</p><p>接下去我们要证明插值节点恰为Legendre多项式的根，这跟首项系数无关.</p><h3 id="高斯积分-1">高斯积分</h3><p>我们想用 <span class="math inline">\(n\)</span>阶插值多项式尽可能拟合高阶多项式的积分.</p><p><span class="math display">\[    \int_{-1}^{1} x^{k} \mathrm{d}x=\sum_{i=1}^{n} c_i x_i^{k}\]</span> 一共有 <spanclass="math inline">\(x_i,c_i,i=1,2,\cdots,n\)</span> 这 <spanclass="math inline">\(2n\)</span> 个未知数，因此我们希望 <spanclass="math inline">\(k=0,1,\cdots ,2n-1\)</span>，即 Gauss积分的代数精度为 <span class="math inline">\(2n-1\)</span>. 事实上有</p><p><span class="math display">\[    \int_{-1}^{1} 1 \cdot f(x) \mathrm{d}x \thickapprox \sum_{i=1}^{n}c_if(x_i)\]</span> 其中 <span class="math display">\[    c_i=\int_{-1}^{1} 1 \cdot L_i(x) \mathrm{d}x, \quad i=1,\cdots ,n\]</span> <span class="math inline">\(L_i\)</span>是Lagrange插值基.<span class="math inline">\(x_i\)</span>是Legendre多项式的根.写出1是为了强调权函数.</p><p><strong>定理</strong>：Gauss积分方法，在 <spanclass="math inline">\([-1,1]\)</span>上使用 <spanclass="math inline">\(n\)</span>阶Legendre多项式，具有 <spanclass="math inline">\(2n-1\)</span>阶代数精度.</p><blockquote><p>并不需要解那个很复杂的非线性方程组，只要说明Gauss积分方法能够具有<spanclass="math inline">\(2n-1\)</span>阶代数精度，就可由唯一性得知它就是最好的插值法.然而事实上Lengdre多项式的根确实是那个很复杂的非线性方程组的根，这一点很难直接证明.</p></blockquote><p>在一般区间 <spanclass="math inline">\([a,b]\)</span>上近似积分，作变换 <spanclass="math inline">\(t=(2x-a-b)/(b-a)\)</span>，容易检验 <spanclass="math display">\[    \int_{a}^{b} f(x) \mathrm{d}x=\int_{-1}^{1}f(\frac{(b-a)t+b+a}{2})\frac{b-a}{2} \mathrm{d}t        \]</span></p><p><strong>Gauss积分的误差</strong>： <span class="math display">\[    R=\int_{-1}^{1} f(x) \mathrm{d}x-\sum_{i=1}^{n}c_if(x_i)=\frac{f^{(2n)}(\xi)}{(2n)!}\int_{-1}^{1} q(x) \mathrm{d}x\]</span> 其中 <span class="math inline">\(q(x)=[(x-x_1)\cdots(x-x_n)]^{2}\)</span></p><p>可以考虑 <spanclass="math inline">\(x_1,x_2,\cdots,x_n\)</span>的Hermite插值.</p><h3 id="gauss-chebyshev-积分">Gauss-Chebyshev 积分</h3><p>权函数 <span class="math inline">\(\displaystylew(x)=\frac{1}{\sqrt{1-x^{2}}}\)</span>，实际上是换了一个Hilbert空间.</p><p>此时希望 <span class="math display">\[    \int_{-1}^{1} f(x) \mathrm{d}x= \int_{-1}^{1} w(x)g(x) \mathrm{d}x\thickapprox  \sum_{i=1}^{n} c_i g(x_i)\]</span> 其中 <span class="math inline">\(g(x)=\sqrt{1-x^{2}}f(x)\)</span>. 这时我们得到的正交多项式为Chebyshev多项式<span class="math inline">\(p_i(x)\)</span>，满足 <spanclass="math display">\[    \int_{-1}^{1} \frac{1}{\sqrt{1-x^{2}}}p_m(x)p_n(x) \mathrm{d}x=0,m\neq n\]</span> 诸零点为 <span class="math inline">\(\displaystyle x_i=\cos\frac{2i-1}{2n}\pi,i=1,2,\cdots ,n\)</span>，而 <spanclass="math inline">\(\displaystyle c_i=\int_{-1}^{1}\frac{L_i(x)}{\sqrt{1-x^{2}}} \mathrm{d}x= \frac{\pi}{n}\)</span></p><h3 id="反常积分的计算">反常积分的计算</h3><p>尝试利用数值方法计算 <span class="math display">\[    \int_{a}^{b} \frac{g(x)}{(x-a)^{p}} \mathrm{d}x\]</span> 其中 <span class="math inline">\(0&lt;p&lt;1\)</span></p><p>该积分有一个奇点 <span class="math inline">\(x=a\)</span>. -梯形和Simpson方法发散 - 开Newton-Cotes公式精度很低</p><p>最简单的想法 <span class="math display">\[    \int_{a}^{b} \frac{g(x)-g(a)}{(x-a)^{p}} \mathrm{d}x+ \int_{a}^{b}\frac{g(a)}{(x-a)^{p}} \mathrm{d}x\]</span> 这样第一项的被积函数是连续函数. 可以使用之前说到的积分方法.当然我们可以做得更加精细：Simpson 将 <spanclass="math inline">\(g(x)\)</span> Taylor 展开至4阶，这样 <spanclass="math inline">\(\displaystyleG(x)=\frac{g(x)-Q_4(x)}{(x-a)^{p}}\in C^{4}[a,b]\)</span>就可以利用之前说过的公式计算. （ <spanclass="math inline">\(Q_4(x)\)</span> 为 <spanclass="math inline">\(g(x)\)</span> 在 <spanclass="math inline">\(x=a\)</span> 处Taylor展开的前5项. 而 <spanclass="math display">\[    \int_{a}^{b} \frac{Q_4(x)}{(x-a)^{p}} \mathrm{d}x= \sum_{k=0}^{4}\frac{g^{(k)}(a)}{k!}\frac{(b-a)^{k+1-p}}{k+1-p}\]</span> 令 <span class="math display">\[    G(x)= \frac{g(x)-Q_4(x)}{(x-a)^{p}}\]</span> 则相应误差 <span class="math display">\[    E=-\frac{h^{4}}{180}(b-a) G^{(4)}(\xi)\]</span></p><h3 id="高维积分的计算">高维积分的计算</h3><p>这会牵扯到许多极限换序和收敛性的问题.</p><h1 id="特征值与奇异值">特征值与奇异值</h1><p><strong>Gerschgorin 圆盘定理</strong> 略</p><h2 id="幂迭代法">幂迭代法</h2><p>幂迭代法线性收敛</p><p><strong>Rayleigh商</strong> 如果有近似特征向量 <spanclass="math inline">\(x\)</span> 那么对于特征值的最优近似是 <spanclass="math display">\[    \lambda= \frac{x ^{\mathsf{T}}Ax}{x ^{\mathsf{T}}x}\]</span></p><p><strong>定理</strong>：设 <span class="math inline">\(A\)</span>是特征值为 $_1 &gt; _2 _m $ 的 <span class="math inline">\(m \timesm\)</span> 矩阵. <span class="math inline">\(A\)</span>有完全特征向量系. 则对于几乎所有的初始向量，幂迭代线性收敛到和 <spanclass="math inline">\(\lambda_1\)</span> 相关的特征向量，收敛常数为$S=_2/_1 $.</p><p>先取逆再进行幂迭代可以得到绝对值最小的特征值（注意特征向量不变）.为了避免对矩阵 <span class="math inline">\(A\)</span>的逆矩阵的显式求解，将 <span class="math inline">\(x_{k+1}= A ^{-1}x_k\)</span> 等价为 <span class="math inline">\(Ax_{k+1}=x_k\)</span>并使用Gauss消去法求解.</p><h3 id="逆向幂迭代">逆向幂迭代</h3><p>p469</p><h3 id="rayleigh商迭代rqi">Rayleigh商迭代（RQI）</h3><p>可利用Rayleigh商估计最大特征值，Rayleigh商迭代对称矩阵三次收敛，一般则二次收敛.</p><h3 id="qr算法">QR算法</h3><ul><li>利用Householder变换 <span class="math inline">\(B=HAH\)</span>作相似约化：对称阵约化为三对角，非对称阵约化为上Hessenberg矩阵</li><li>利用QR分解构造迭代序列. 分解 <spanclass="math inline">\(A_j=Q_jR_j\)</span>，计算 <spanclass="math inline">\(A_{j+1}=R_{j}Q_{j}\)</span></li></ul><p>约化到上Hessenberg矩阵大约 <span class="math inline">\(\displaystyle\frac{5}{3} n^{3}\)</span> 次乘法. 对于 <spanclass="math inline">\(n\)</span> 阶矩阵 <spanclass="math inline">\(A\)</span>需要作 <spanclass="math inline">\(n-2\)</span> 次Householder变换将 <spanclass="math inline">\(A\)</span> 变为上Hessenberg形式.</p><h2 id="svd">SVD</h2><p><span class="math inline">\(A\)</span> 为 <spanclass="math inline">\(m \times n\)</span> 矩阵，<spanclass="math inline">\(\{u_1,\cdots ,u_m\}\)</span> 和 <spanclass="math inline">\(\{v_1,\cdots ,v_n\}\)</span>是单位正交集合，若存在 <span class="math inline">\(s_1\geqslants_2\geqslant \cdots \geqslant s_n\geqslant 0\)</span>，满足 <spanclass="math display">\[    Av_1=s_1u_1 \\    \vdots \\    A v_n= s_n u_n\]</span> 称 <span class="math inline">\(v_i\)</span> 为 <spanclass="math inline">\(A\)</span> 的右奇异向量， <spanclass="math inline">\(u_i\)</span> 为 <spanclass="math inline">\(A\)</span> 的左奇异向量， <spanclass="math inline">\(s_i\)</span> 为 <spanclass="math inline">\(A\)</span> 的奇异值.</p><p>对称矩阵的SVD简化为 <span class="math inline">\(A= U ^{\mathsf{T}}SV\)</span>，其中 <span class="math inline">\(U\)</span> 的每一列与<span class="math inline">\(V\)</span> 的对应列相同或相反.</p><h3 id="低秩逼近">低秩逼近</h3><p><span class="math inline">\(A\)</span> 的最优最小二乘逼近为保留前面<span class="math inline">\(p\)</span> 项，<spanclass="math inline">\(p&lt;r\)</span>.</p><h3 id="另一种svd的方法">另一种SVD的方法</h3><p>考虑 <span class="math display">\[    B=\begin{bmatrix}    0 &amp; A ^{\mathsf{T}} \\    A &amp; 0 \\    \end{bmatrix}\]</span> 设有 <span class="math inline">\(B\)</span> 的特征向量 <spanclass="math display">\[    \begin{bmatrix}    A ^{\mathsf{T}}w \\    Av \\    \end{bmatrix}    =    \begin{bmatrix}    0 &amp; A^{\mathsf{T}} \\    A &amp; 0 \\    \end{bmatrix}    \begin{bmatrix}    v \\    w \\    \end{bmatrix}    =\lambda    \begin{bmatrix}    v \\    w \\    \end{bmatrix}\]</span> 注意 <span class="math inline">\(w\)</span> 是 <spanclass="math inline">\(A ^{\mathsf{T}}A\)</span>的特征向量，对应的特征值为 <spanclass="math inline">\(\lambda^{2}\)</span>.</p><h3 id="其他svd相关性质">其他SVD相关性质</h3><p><strong>Frobenius范数</strong>：<span class="math inline">\(\left\| A\right\|_{F}= (\sum_{i}^{} \sum_{j}^{} a_{ij})^{1/2}\)</span>，则 <spanclass="math display">\[    \left\| A \right\|_{F}^{2}= s_1^{2}+\cdots +s_{r}^{2}\]</span> <span class="math display">\[    \left\| A \right\|_{2}=s_1\]</span></p><p>设 <span class="math inline">\(U^{\mathsf{T}}AV= diag\{s_1,\cdots,s_r\}\)</span>，若 <span class="math inline">\(k&lt;r\)</span>，令<span class="math inline">\(A_k=\sum_{i=1}^{k}s_iu_iv_i^{\mathsf{T}}\)</span>，则 <span class="math display">\[    \min_{\operatorname{rank}(B)=k}\left\| A-B \right\|_{2}= \left\|A-A_k \right\|_{2}= s_{k+1}\]</span></p><h1 id="数值代数">数值代数</h1><h2id="方程组的求解高斯消去法和lu分解">方程组的求解——高斯消去法和LU分解</h2><h3 id="朴素的gauss消去法">朴素的Gauss消去法</h3><p><span class="math inline">\(n\)</span>个方程 <spanclass="math inline">\(n\)</span>个未知数的消去计算，可以在 <spanclass="math inline">\(\displaystyle\frac{2}{3}n^{3}+\frac{1}{2}n^{2}-\frac{7}{6}n\)</span>次操作后完成.<span class="math inline">\(\displaystyle \frac{1}{3}n^{3}\)</span>次操作是加减， <span class="math inline">\(\displaystyle\frac{1}{3}n^{3}\)</span> 次操作是乘除.所有操作完成后，变为上三角矩阵.</p><p>再经历回代的操作，回代计算复杂度是 <spanclass="math inline">\(O(n^{2})\)</span>（实际上就是 <spanclass="math inline">\(n^{2}\)</span> 次），加减与乘除也是一半一半.</p><blockquote><p>如果某时主元为0，会导致Gauss消去法终止.</p></blockquote><h3 id="lu分解">LU分解</h3><p><span class="math inline">\(A=LU\)</span> <spanclass="math display">\[    Ly=b \Rightarrow Ux=y\]</span></p><p>如果 <span class="math inline">\(L\)</span> 和 <spanclass="math inline">\(U\)</span>的对角线元素满足不同条件，也是不同的分解. - <spanclass="math inline">\(l_{ii}=1\)</span> 为 Doolittle 分解 - <spanclass="math inline">\(u_{ii}=1\)</span> 为 Crount 分解 - <spanclass="math inline">\(l_{ii}=u_{ii}\)</span> 为 Choleski 分解</p><p>一般 LU 分解的复杂度是 <spanclass="math inline">\(O(n^{3})\)</span>，而有限带宽矩阵的 LU分解复杂度为 <span class="math inline">\(O(n)\)</span>.</p><p><strong>LU分解相比经典的 Gauss 消去法，更适合求解 <spanclass="math inline">\(b\)</span> 变化时的问题</strong></p><h3 id="矩阵范数">矩阵范数</h3><ul><li>算子范数</li><li>无穷范数 <span class="math display">\[  \left\| A \right\|_{\infty}= \max_{\left\| x \right\|_{\infty}=1}\left\| Ax \right\|_{\infty}\]</span> 有 <span class="math inline">\(\left\| A \right\|_{\infty}=\max_{1\leqslant i\leqslant n} \sum_{j=1}^{n} \lvert a_{ij}\rvert\)</span></li><li><span class="math inline">\(L_2\)</span> 范数 <spanclass="math display">\[  \left\| A \right\|_{2}= \max_{\left\| x \right\|_{2}=1} \left\| Ax\right\|_{2}\]</span> 有 <span class="math inline">\(\left\| A \right\|_{2}=[\rho(A^{\mathsf{T}}A)]^{1/2}\)</span>；<spanclass="math inline">\(\rho(A)\leqslant \left\| A \right\|_{}\)</span>对任意满足相容性的范数成立</li></ul><h3 id="误差放大和条件数">误差放大和条件数</h3><p>令 <span class="math inline">\(x_{a}\)</span> 为 <spanclass="math inline">\(Ax=b\)</span>的近似解，<strong>后向误差</strong>为余项的范数 <spanclass="math inline">\(\left\| b-Ax_{a}\right\|_{\infty}\)</span>，前向误差为 <spanclass="math inline">\(\left\| x-x_{a} \right\|_{\infty}\)</span>.</p><p><strong>定义</strong> <span class="math display">\[    误差放大因子Q=\frac{相对前向误差}{相对后向误差}=\frac{\left\|x-x_{a} \right\|_{\infty}/ \left\| x \right\|_{\infty}}{\left\| r\right\|_{\infty}/ \left\| b \right\|_{\infty}}\]</span></p><ul><li><p>方阵 <span class="math inline">\(A\)</span> 的条件 cond(<spanclass="math inline">\(A\)</span>) 为最大的误差放大因子.</p></li><li><p><span class="math inline">\(n\)</span> 阶矩阵 <spanclass="math inline">\(A\)</span> 的条件数为 <spanclass="math display">\[  \text{cond}(A)= \left\| A \right\|_{} \left\| A^{-1} \right\|_{}    \]</span> 一般可以取无穷范数</p></li></ul><hr /><p>线性方程组-迭代法 0.2：条件数https://zhuanlan.zhihu.com/p/388053510</p><hr /><p><strong>Q:</strong> 证明矩阵 <spanclass="math inline">\(A\)</span>的条件数为 <spanclass="math inline">\(\text{cond}(A)=\left\| A \right\|_{}\left\| A^{-1} \right\|_{}\)</span></p><p><strong>证:</strong> 注意误差放大因子 <span class="math display">\[    Q=\frac{\left\| x-x_a \right\|_{}/\left\| x \right\|_{}}{\left\| b-Ax_a \right\|_{}/\left\| b \right\|_{}}\leqslant\]</span></p><p><span class="math display">\[    \exists b,x,r \quad \text{s.t.} \quad \left\| A\right\|_{}=\frac{\left\| Ax \right\|_{}}{\left\| x\right\|_{}}=\frac{\left\| b \right\|_{}}{\left\| x \right\|_{}}, \quad\left\| A ^{-1} \right\|_{}=\frac{\left\| A ^{-1} r \right\|_{}}{\left\|r \right\|_{}}\]</span></p><h3 id="palu-分解">PA=LU 分解</h3><p><strong>淹没问题</strong> p80</p><p><strong>部分主元</strong> 第一步选择第 <spanclass="math inline">\(p\)</span> 行，其中 <span class="math display">\[    \lvert a_{p1} \rvert \geqslant \lvert a_{i1} \rvert ,1\leqslanti\leqslant n\]</span> 交换第 <span class="math inline">\(1\)</span> 行和第 <spanclass="math inline">\(p\)</span> 行，然后再作消去.以后每一步都要比较.</p><p>然后 <span class="math display">\[    Lc=Pb \Rightarrow Ux=c\]</span></p><h3 id="迭代方法">迭代方法</h3><p><strong>不动点迭代</strong>：<spanclass="math inline">\(Ax=b\)</span> 等价于 <spanclass="math inline">\(x= Tx+b\)</span></p><p>设 <span class="math inline">\(A=D+L+U\)</span>，则有 - Jacobi迭代中的 <span class="math inline">\(T\)</span>: <spanclass="math display">\[    x=D ^{-1} [b-(L+U)x]\]</span> - Gauss-Seidel迭代中的 <spanclass="math inline">\(T\)</span>： <span class="math display">\[    x=(L+D) ^{-1}(b-Ux)\]</span></p><p><strong>连续松弛迭代（SOR）</strong> - 引入参数 <spanclass="math inline">\(\omega\)</span>，对上面两种迭代方法加权组合 <spanclass="math display">\[    [\omega(D+L)+(1-\omega)D]x=\omega(-Ux+b)+(1-\omega)Dx\]</span> 当 <span class="math inline">\(0&lt;\omega&lt;1\)</span>时称为欠松弛，<span class="math inline">\(\omega&gt;1\)</span>称为过松弛</p><p><strong>迭代基本定理</strong> 设 <span class="math inline">\(x=Bx+f\)</span>，对任意的 <span class="math inline">\(x^{(0)}\)</span>和<span class="math inline">\(f\)</span>，迭代法 <spanclass="math inline">\(x^{(k+1)}=Bx^{(k)}+f\)</span> 收敛 <spanclass="math inline">\(\Leftrightarrow\)</span> <spanclass="math inline">\(\rho(B)&lt;1\)</span>.</p><p><strong>收敛速度：</strong> <spanclass="math inline">\(\varepsilon^{(k)}=B^{k}\varepsilon^{(0)}\)</span>设 <span class="math inline">\(B\)</span>有 <spanclass="math inline">\(n\)</span>个线性无关的特征向量 <spanclass="math inline">\(u_1,\cdots ,u_n\)</span>，对应特征值为 <spanclass="math inline">\(\lambda_1,\cdots ,\lambda_n\)</span>，则由 <spanclass="math inline">\(\varepsilon^{(0)}=\sum_{i=1}^{n}a_iu_i\)</span>,</p><p><span class="math display">\[    \varepsilon^{(k)}=\sum_{i=1}^{n} a_i\lambda_i^{k}u_i\]</span> 可以看到 <span class="math inline">\(\rho(B)&lt;1\)</span>越小，收敛越快.</p><p><strong>定义</strong>： <span class="math display">\[    R(B)=-\ln \rho(B)\]</span> 称为迭代法的收敛速度</p><p><strong>定理：</strong> <spanclass="math inline">\(x^{(k+1)}=Bx^{(k)}+f\)</span>为迭代公式，存在某一范数（需要满足相容性条件）使得<span class="math inline">\(\left\| B \right\|_{}=q&lt;1\)</span>，则 -迭代法收敛 - <span class="math inline">\(\displaystyle \left\|x^{(k)}-x^{*} \right\|_{}\leqslant \frac{q}{1-q}\left\|x^{(k)}-x^{(k-1)} \right\|_{}\)</span> - <spanclass="math inline">\(\displaystyle \left\| x^{(k)}-x^{*}\right\|_{}\leqslant \frac{q^{k}}{1-q}\left\| x^{(1)}-x^{(0)}\right\|_{}\)</span></p><h3 id="稀疏矩阵计算">稀疏矩阵计算</h3><p>稀疏矩阵：非零元密度远小于1的矩阵. 对于很大的 <spanclass="math inline">\(N\)</span>，直接法存储 <spanclass="math inline">\(N\)</span>阶矩阵数据需要 <spanclass="math inline">\(O(N^{2})\)</span>大小；稀疏矩阵只存储非零元位置和数据，需要<span class="math inline">\(2N\)</span>（或者 <spanclass="math inline">\(3N\)</span>） 大小.</p><p><strong>应用例</strong>：有限差分离散泊松方程 <spanclass="math display">\[    AU=F\]</span> <span class="math inline">\(U\)</span> 为 <spanclass="math inline">\((n-1)^{2}\)</span> 维向量，<spanclass="math inline">\(A\)</span> 的带宽为 <spanclass="math inline">\(2(n-1)+1\)</span></p><h3 id="cholesky-分解">Cholesky 分解</h3><p>如果 <span class="math inline">\(A\)</span> 是 <spanclass="math inline">\(n \times n\)</span> 对称正定矩阵，则存在上三角<span class="math inline">\(n \times n\)</span> 矩阵 <spanclass="math inline">\(R\)</span> 满足 <span class="math inline">\(A= R^{\mathsf{T}} R\)</span></p><p><strong>证</strong> 归纳 <span class="math display">\[    A=    \begin{bmatrix}    a &amp; b^{\mathsf{T}} \\    b &amp; C \\    \end{bmatrix}\]</span> 设 <span class="math inline">\(\displaystyleu=\frac{b}{\sqrt{a}}\)</span>，令 <span class="math inline">\(A_1=C-uu^{\mathsf{T}}\)</span> 定义可逆矩阵 <span class="math display">\[    S=    \begin{bmatrix}    \sqrt{a} &amp; u^{\mathsf{T}} \\    0 &amp; I           \end{bmatrix}\]</span> 得到 <span class="math display">\[    S ^{\mathsf{T}}    \begin{bmatrix}    1 &amp; 0 \\    0 &amp; A_1 \\    \end{bmatrix}    S=A     \]</span> <span class="math inline">\(A_1\)</span> 也是对称正定的，故<span class="math inline">\(A_1=V ^{\mathsf{T}}V\)</span>，最后定义<span class="math display">\[    R=    \begin{bmatrix}    \sqrt{a} &amp; u ^{\mathsf{T}} \\    0 &amp; V \\    \end{bmatrix}\]</span> 在寻找 <span class="math inline">\(R\)</span> 时，继续对 <spanclass="math inline">\(A_1\)</span> 作操作即可.</p><h3 id="共轭梯度法">共轭梯度法</h3><p><strong>等价极值问题</strong></p><p><strong>定理</strong> <span class="math inline">\(A\)</span> 是 <spanclass="math inline">\(n\)</span> 阶对称正定矩阵，<spanclass="math inline">\(x\)</span> 是方程组 <spanclass="math inline">\(Ax=b\)</span>的解 <spanclass="math inline">\(\Leftrightarrow\)</span> <spanclass="math inline">\(x\)</span> 是二次函数 <spanclass="math inline">\(f(x)=\frac{1}{2}x^{\mathsf{T}}Ax-b^{\mathsf{T}}x\)</span>的极小点. 即 <span class="math display">\[    Ax^{*}=b \Leftrightarrow f(x^{*})= \min_{x\in \mathbb{R}^{n}}f(x)\]</span></p><p><strong>证</strong> 注意 <span class="math inline">\(\nabla f(x)=Ax-b\)</span> 且 <span class="math inline">\(f(x+\alpha y)=f(x)+\alpha(Ax-b,y)+\frac{1}{2}\alpha^{2}(Ay,y)\)</span></p><p>那么我们就把一个求方程根的问题转化为了一个求函数极小值的问题</p><h3 id="最速下降法">最速下降法</h3><ul><li><p>每次找一个方向 <span class="math inline">\(d_k\)</span>，使得<span class="math display">\[  x_{k+1}=x_k+\alpha_k d_k\]</span> 其中系数 <spanclass="math inline">\(\alpha_k\)</span>通过求一维问题的极小获得.</p></li><li><p>若 <span class="math inline">\(d_k=-\nablaf(x_k)=b-Ax_k\)</span>，则该方向为<strong>最速下降方向</strong>. 有<span class="math inline">\(d_k=r_k\)</span>，且 <spanclass="math inline">\(d_k\)</span>和 <spanclass="math inline">\(d_{k+1}\)</span>正交.</p></li></ul><p>计算可以得到 <span class="math display">\[    \alpha_k=\frac{(r_k,r_k)}{(Ar_k,r_k)}\]</span> &gt; 可以直接考虑为什么 <spanclass="math inline">\(d_k\)</span> 和 <spanclass="math inline">\(d_{k+1}\)</span>正交，注意并<strong>不</strong>一定有 <spanclass="math inline">\((d_i,d_j)=0, i\neq j\)</span></p><p>误差估计 <span class="math display">\[    \left\| x_k-x^{*} \right\|_{A}\leqslant\left(\frac{\lambda_1-\lambda_n}{\lambda_1+\lambda_n}\right)^{k}\left\|x_0-x^{*} \right\|_{A}\]</span></p><p><strong>证</strong> 用归纳法，然后用Kantorovich不等式，具体推导略.Kantorovich不等式可以见</p><hr /><p>康托洛维奇(Kantorovich)不等式的一种初等证明https://zhuanlan.zhihu.com/p/271983329</p><hr /><blockquote><p>最速下降法当矩阵条件数过大，即最大与最小特征值相差极大时收敛非常慢，不具有实际应用意义### 共轭梯度法</p></blockquote><blockquote><p><span class="math inline">\(d_k\)</span> 是方向， <spanclass="math inline">\(r_k\)</span> 是残差（标量），<spanclass="math inline">\(x_k\)</span> 是经过 <spanclass="math inline">\(k\)</span> 步得到的估计</p></blockquote><ul><li><p><span class="math inline">\(f(x)\)</span>极小问题可迭代计算，每次找方向 <spanclass="math inline">\(d_k\)</span>，使 <span class="math display">\[  x_{k+1}=x_{k}+\alpha_k d_k\]</span> 其中系数 <span class="math inline">\(\alpha\)</span>通过求一维问题的极小获得</p></li><li><p>搜索方向两两共轭，且 <span class="math display">\[  d_{k+1}=r_k+\beta_k d_k\]</span> 其中的系数由共轭性质确定.</p></li></ul><p><strong>共轭向量</strong> 定义 <span class="math inline">\(\left\| x\right\|_{A}=(Ax,x)^{\frac{1}{2}}\)</span> 定义 如果两个方向 <spanclass="math inline">\(d_i\)</span> 和 <spanclass="math inline">\(d_j\)</span> 满足 <span class="math display">\[    (d_i,Ad_j)=0\]</span> 则称这两个向量关于矩阵 <span class="math inline">\(A\)</span>共轭. 类似可定义矩阵 <span class="math inline">\(A\)</span>的共轭向量组.</p><p>共轭梯度法有误差估计，收敛速度为（K为矩阵条件数） <spanclass="math display">\[    \left\| x_k-x^{*} \right\|_{A}\leqslant2\left(\frac{\sqrt{K}-1}{\sqrt{K}+1}\right)^{k} \left\| x_0-x^{*}\right\|_{A}\]</span></p><p><strong>有限步收敛性定理</strong> 设 <spanclass="math inline">\({d_1,\cdots ,d_n}\)</span> 为对称正定矩阵 <spanclass="math inline">\(A\)</span> 的共轭向量组，令 <spanclass="math display">\[    \alpha_k=\frac{(d_k,r_{k-1})}{(d_k,Ad_k)},  x_k=x_{k-1}+\alpha_kd_k,  k=1,\cdots ,n\]</span> 该方向为<strong>共轭方向法</strong>（比共轭梯度更广泛）.对于该迭代法， <span class="math inline">\(n\)</span> 步迭代可得准确解<span class="math display">\[    A x_n=b\]</span></p><p><strong>证</strong> 首先 <spanclass="math inline">\(Ax_n=Ax_0+\alpha_1 Ad_1+\cdots +\alpha_n Ad_n\)</span></p><p>由共轭性质，<spanclass="math inline">\((d_k,r_{k-1})=(d_k,b-Ax_0)\)</span>.</p><p>而 <spanclass="math inline">\((b-Ax_n,d_k)=(b-Ax_0,d_k)-\alpha_k(Ad_k,d_k)=0\)</span>（代入<span class="math inline">\(\alpha_k\)</span> 的定义即知）</p><p>故 <span class="math inline">\(r_n\)</span> 与所有的 <spanclass="math inline">\(d_k\)</span>正交， 故 <spanclass="math inline">\(Ax_n=b\)</span>.</p><p><strong>共轭梯度法</strong>选择的搜索方向为负梯度方向和前一个搜索方向的组合，使得所得到的余项与前面的余项两两正交.负梯度方向即为残差 <span class="math inline">\(r_k\)</span>的方向（<spanclass="math inline">\(\beta_{k-1}\)</span>是为了使方向向量两两共轭，<spanclass="math inline">\(\alpha_k\)</span> 是为了使残差向量两两共轭） <spanclass="math display">\[    (r_i,r_j)=0, i \neq j\]</span></p><p>令 <span class="math display">\[    r_{k-1}=b-Ax_{k-1}, \quad(r_{k-1},d_i)=0, \quad i=1,\cdots ,k-1\]</span> 则 <span class="math display">\[    d_k=r_{k-1}+\beta _{k-1}d_{k-1}\]</span> 选择合适的 <span class="math inline">\(\beta_{k-1}\)</span>使得与前面的搜索方向共轭 <span class="math display">\[    (d_{k-1},Ad_k)=0\]</span> 可以得到 <span class="math display">\[    \beta_{k-1}=-\frac{(d_{k-1},Ar_{k-1})}{(d_{k-1},Ad_{k-1})}\]</span></p><p>证明实际上 <span class="math inline">\(d_k\)</span>与 <spanclass="math inline">\(d_j\)</span>都共轭（<spanclass="math inline">\(j&lt;k\)</span>），归纳法 <spanclass="math display">\[    d_{j-1} ^{\mathsf{T}}A d_k=d_{j-1} ^{\mathsf{T}} Ar_{k-1}+\beta_{k-1} d_{j-1} ^{\mathsf{T}}A d_{k-1}\]</span> 那么由归纳法 <span class="math inline">\(d_{j-1} ^{\mathsf{T}}A d_{k-1}=0\)</span> 且 <span class="math display">\[    A d_{j-1}=\frac{1}{\alpha_{j-1}}(r_{j-2}-r_{j-1}) \Rightarrowd_{j-1} ^{\mathsf{T}} Ar_{k-1}=\frac{1}{\alpha_{j-1}}(r_{j-2}-r_{j-1})^{\mathsf{T}} r_{k-1}=0\]</span></p><p>共轭梯度法复杂度还是 <span class="math inline">\(O(n^{3})\)</span>甚至比直接法还要慢，但是在处理一些特殊问题（大型稀疏矩阵、良态即条件数很小）时有非常高的效率.</p><blockquote><p>总结算法如下 初始：<span class="math inline">\(r_0=b-Ax_0, \quadd_1=r_0\)</span> For <span class="math inline">\(k=1,\cdots ,n\)</span><span class="math inline">\(\displaystyle\alpha_k=\frac{(r_{k-1},r_{k-1})}{(d_k,Ad_k)}\)</span> <spanclass="math inline">\(x_k=x_{k-1}+\alpha_k d_k\)</span> <spanclass="math inline">\(r_k=r_{k-1}-\alpha_k A d_k\)</span> （新残差）<span class="math inline">\(\displaystyle\beta_k=\frac{(r_k,r_k)}{(r_{k-1},r_{k-1})}=\frac{-(d_k, A r_k)}{(d_k,Ad_k)}\)</span> <span class="math inline">\(d _{k+1}=r_k+\beta_kd_k\)</span> End For</p></blockquote><p>（下标可能与书本有所不同）</p><p><strong>预条件</strong> 若 <spanclass="math inline">\(A\)</span>为病态矩阵，过程易受舍入误差的影响，使用预条件可以在<span class="math inline">\(\sqrt{n}\)</span> 步得到收敛解.</p><p>目标是选择合适的矩阵 <span class="math inline">\(M\)</span>，使得方程<span class="math display">\[    M ^{-1} Ax= M ^{-1}b\]</span> 更“好”. <span class="math inline">\(M\)</span>当然可逆，称为<strong>预条件子</strong>. 我们希望 - <spanclass="math inline">\(M\)</span>和 <spanclass="math inline">\(A\)</span> 足够接近 - <spanclass="math inline">\(M\)</span> 容易求逆</p><p><strong>雅可比预条件子</strong> <spanclass="math inline">\(M=D\)</span> ，其中 <spanclass="math inline">\(D\)</span> 是 <spanclass="math inline">\(A\)</span> 的对角线矩阵. 用 <spanclass="math inline">\((v,w)_{M}\)</span> 替换欧几里得内积. 则 <spanclass="math inline">\(M ^{-1}A\)</span>相对于新的内积仍然是对称正定矩阵.</p><p><strong>高斯-赛德尔预条件子</strong>对称连续过松弛（SSOR），中预条件子定义为 <span class="math display">\[    M=(D+\omega L)D ^{-1}(D+\omega U)\]</span> 其中 <span class="math inline">\(A=L+D+U\)</span>被分为下三角部分、对角线以及上三角部分. <spanclass="math inline">\(\omega \in [0,2]\)</span>，当 <spanclass="math inline">\(\omega=1\)</span> 时称为高斯-赛德尔预条件子</p><blockquote><p>经过一些化简可以得到有预条件的算法 <spanclass="math inline">\(x_0=\)</span> 初始条件 初始：<spanclass="math inline">\(r_0=b-Ax_0, \quad d_1=z_0=M ^{-1}r_0\)</span> For<span class="math inline">\(k=1,\cdots ,n\)</span> <spanclass="math inline">\(\displaystyle\alpha_k=\frac{(r_{k-1},z_{k-1})}{(d_k,Ad_k)}\)</span> <spanclass="math inline">\(x_k=x_{k-1}+\alpha_k d_k\)</span> <spanclass="math inline">\(r_k=r_{k-1}-\alpha_k A d_k\)</span> （新残差）<span class="math inline">\(z_k=M ^{-1}r_k\)</span> <spanclass="math inline">\(\displaystyle\beta_k=\frac{(r_k,z_k)}{(r_{k-1},z_{k-1})}\)</span> <spanclass="math inline">\(d _{k+1}=z_k+\beta_k d_k\)</span> End For</p></blockquote><p>在SSOR下，不需要求 <span class="math inline">\(M^{-1}\)</span>，在计算 <span class="math inline">\(z_{k}= M ^{-1}r_k\)</span> 时，由于 <span class="math inline">\(M=(I+\omega LD^{-1})(D+\omega U)\)</span>是下上三角矩阵的乘积，故该方程可以通过两次回代求解. <spanclass="math display">\[    (I+\omega LD ^{-1})c=v \\    (D+ \omega U)z=c \\\]</span></p><p>可以将具备SSOR的共轭梯度法与一般的迭代法作比较.</p>]]></content>
    
    
    <categories>
      
      <category>分析学</category>
      
      <category>数值分析</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>永久记录（一）</title>
    <link href="/2022/05/06/%E6%B0%B8%E4%B9%85%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <url>/2022/05/06/%E6%B0%B8%E4%B9%85%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<div class="note note-info">            <p>文章内容可能包含虚构创作</p>          </div><p>曾经我幻想过自己晚年写回忆录的样子，大概是像《魔戒》和《霍比特人》中的比尔博巴金斯，坐在书桌前叙写自己的前半生，阳光透射过窗扉洒在桌椅。这条路或许终究只是泡影，上了大学后越来越感觉自己在遗忘曾经想永远记住的东西，对将来自己能记住多少前半生，我深表怀疑。所以，不妨现在开始吧。</p><p>本系列标题《永久记录》取自爱德华·斯诺登的同名自传，它记录了棱镜门的真相。值得一提的是，该书在亚马逊热卖售罄的当天即遭美国政府起诉。</p><p>从哪开始写呢？思来想去，还是决定从俄乌战争写起。</p><p>2022年2月24日，俄罗斯以去纳粹化的名义，宣布对乌克兰进行“特别军事行动”。也许互联网确实没有记忆，但互联网的参与者绝对有。从开战当天的“1小时22分拿下基辅”，到2022年5月6日为止西方世界斥资上千亿美元军援乌克兰，战场形势迷离。</p><p>马克思在《〈黑格尔法哲学批判〉导言》写道：“批判的武器当然不能代替武器的批判。战场上得不到的，谈判桌上也得不到。不知道迎接泽连斯基和普京的会是什么下场呢？</p><div class="note note-info">            <p>留待后来者更新</p>          </div><p>我们共同见证俄乌开战后简中互联网（其实我很讨厌这个称呼）前所未有的撕裂，双方的骂战甚于清零派和共存派的对喷。我知道互联网放大了生活的戾气，但当扣帽子成了某种“政治正确”，交换观点真的成了一种奢望。也许在互联网寻找安宁就是一种错误，某些话题本就是要刀戈相见，交换意见（物理）。</p><p>我们也不会忘记，俄乌开战后前几天，台媒渲染着“台湾不会是下一个乌克兰”；而在5月的某一天，美国派出了迄今规格最高的代表团访问台湾，翘着二郎腿听台上那条姓蔡的母狗汇报所谓的工作。5月的另一天，母狗宣布要延长南海岛礁的机场跑道，以便使得更大的军用机能够降落。</p><p>有时候我会想到底是谁将“美”字赋予这么一个罪行罄竹难书却将自己美化成圣贤的国家。以抢劫起家的昂撒白皮，已经挑拨起斯拉夫人的对立，并在可预见的将来挑拨起中国人间的对立。可是如今谁能够戳破这普世价值观的谎言呢。</p><p>我们共同见证上海疫情爆发后山呼海啸般的负面消息。一位主播提到，在现在的时间点，你想看道歉，就去看上海每天的发布会。他们会先辟谣，再道歉，然后接着辟谣。上海如今真的还是人民的上海吗？</p><p>2022年是世界信心崩塌的一年，我国尤甚。没有人知道世界会去往何方，没有人知道我们会怎么样。任何一个时代，信心都是比金子还要珍贵的东西，谁还敢在没有丝毫确定性的2022年作出任何担保呢。在朋友圈里见过许多人怀念2019年。的确，我们先是怀念涨价去库存前的2015年，然后怀念疫情发生前的2019年，我们甚至能怀念社会尚显正常的2021年。前一个十年最坏的一年，却是下一个十年最好的一年，这句话真不是说说而已。</p><p>用加缪的一句话作为结尾吧：</p><p>活着，带着世界赐予我们的裂痕去生活，用残缺的手掌抚平彼此的创伤，固执地迎向幸福。</p>]]></content>
    
    
    <categories>
      
      <category>杂项</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随想</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>常微分方程（8）</title>
    <link href="/2022/05/01/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%888%EF%BC%89/"/>
    <url>/2022/05/01/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%888%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="定性理论与分支理论初步">定性理论与分支理论初步</h1><p>The following videos are helpful to understand this chapter. Thanksto <span class="citation" data-cites="inversioner">@inversioner</span>at Zhihu.</p><hr /><p>【维度】数学漫步第一季（熟肉）[2008]https://www.bilibili.com/video/av13923799?from=search&amp;seid=12319283859508785318</p><hr /><hr /><p>维度：数学漫步2-混沌https://www.bilibili.com/video/av4194600?from=search&amp;seid=12319283859508785318</p><hr /><h2 id="动力系统相空间与轨线">动力系统，相空间与轨线</h2><p>Recall <strong>autonomous</strong> differential equations:differential equations whose independent variables are not explicitlyincluded in the equation.</p><p>Considering a moving particle in a n-dimensional space. We know itsspeed at <span class="math inline">\(\mathbf{x}\)</span> is <spanclass="math inline">\(\mathbf{v}(\mathbf{x})=(v_1(\mathbf{x}),\cdots,v_n(\mathbf{x}))\)</span>. The equation of motion is <spanclass="math display">\[    \frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t}=\mathbf{v}(\mathbf{x})\tag{1}\]</span> If <span class="math inline">\(\mathbf{v}(\mathbf{x})\)</span>satisfies the condition of existence and uniqueness theorem, then forall initial condition <span class="math display">\[    \mathbf{x}(t_0)=\mathbf{x}_0\]</span> (1) has unique solution satisfying (2) <spanclass="math display">\[    \mathbf{x}=\mathbf{\phi}(t,t_0,\mathbf{x}_0) \tag{3}\]</span></p><p>We call the space for <span class="math inline">\(\mathbf{x}\)</span>(namely <span class="math inline">\(\mathbb{R}^{n}\)</span> )<strong>phase space</strong>, the space for <spanclass="math inline">\((t,\mathbf{x})\)</span> (namely <spanclass="math inline">\(\mathbb{R}^{1} \times \mathbb{R}^{n}\)</span> )<strong>augmented phase space</strong>.</p><ol type="1"><li>defines a <strong>vector field</strong> in the phase space. <spanclass="math display">\[\mathbf{v}(\mathbf{x})=(v_1(\mathbf{x}),\cdots ,v_n(\mathbf{x})) \tag{4}\]</span></li></ol><p>The solution (3) gives a <strong>trajectory</strong> in the phasespace.</p><div class="note note-info">            <p>Trajectory is the projection of integral curve on the phasespace.</p>          </div><p>If <span class="math inline">\(\mathbf{x}_0\)</span> is a zero pointof (4), namely <span class="math inline">\(v(\mathbf{x}_0)=0\)</span>,then (1) has a constant solution <spanclass="math inline">\(\mathbf{x}=\mathbf{x}_0\)</span>. We call <spanclass="math inline">\(\mathbf{x}_0\)</span> is a <strong>balancepoint</strong> to (1). The balance points to (1) is called<strong>singularities</strong>.</p><p>If (3) is an unsteady periodic motion, namely <spanclass="math inline">\(\exists T&gt;0\)</span>, s.t. <spanclass="math display">\[    \mathbf{\varphi}(t+T,t_0,\mathbf{x}_0)\equiv\mathbf{\varphi}(t,t_0,\mathbf{x}_0)\]</span> Then its trajectory in the phase space is a closed curve,denoted as <strong>close orbit</strong>.</p><p>Any autonomous differential equation has the form of (1). Thisintroduces the concept of <strong>dynamic system</strong>.</p><p><strong>Basic properties of dynamic system</strong>: - translationinvariance of integral curves: for all constant <spanclass="math inline">\(C\)</span>, <spanclass="math inline">\(\mathbf{x}=\mathbf{\varphi}(t+C)\)</span> is asolution to (1). - uniqueness of trajectories passing through each pointin phase space. We can only consider the solution corresponding to theinitial time <span class="math inline">\(t_0=0\)</span>. Define <spanclass="math display">\[    \mathbf{\varphi}(t,\mathbf{x}_0)=\mathbf{\varphi}(t,0,\mathbf{x}_0)\]</span> - properties of group: <spanclass="math inline">\(\mathbf{\varphi}(t,\mathbf{x}_0)\)</span>satisfies <span class="math display">\[    \mathbf{\varphi}(t_2,\mathbf{\varphi}(t_1,\mathbf{x}_0))=\mathbf{\varphi}(t_2+t_1,\mathbf{x}_0)\tag{5}\]</span></p><h2 id="解的稳定性">解的稳定性</h2><p>翻不动了，上中文.</p><p>对于一般方程 <span class="math display">\[    \frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t}= \mathbf{f}(t,\mathbf{x})\tag{6}\]</span> 其中函数 <spanclass="math inline">\(\mathbf{f}(t,\mathbf{x})\)</span> 对 <spanclass="math inline">\(\mathbf{x} \in G \subset \mathbb{R}^{n}\)</span>和 <span class="math inline">\(t \in (-\infty,\infty)\)</span>连续，并对 <span class="math inline">\(\mathbf{x}\)</span> 满足Lipschitz 条件. 又设上方程有一个解 <spanclass="math inline">\(\mathbf{x}= \mathbf{\varphi}(t)\)</span> 在 <spanclass="math inline">\(t_0\leqslant t&lt; \infty\)</span> 有定义.如果对任意给定的 <spanclass="math inline">\(\varepsilon&gt;0\)</span>，都存在 <spanclass="math inline">\(\delta=\delta(\varepsilon)&gt;0\)</span>，使得只要 <spanclass="math display">\[    \lvert \mathbf{x}_0- \mathbf{\varphi}(t_0) \rvert &lt; \delta\]</span> 就有以 <spanclass="math inline">\(\mathbf{x}(t_0)=\mathbf{x}_0\)</span> 为初值的解<span class="math inline">\(\mathbf{x}(t,t_0,\mathbf{x}_0)\)</span>就也在 <span class="math inline">\(t\geqslant t_0\)</span>有定义，并且满足 <span class="math display">\[    \lvert \mathbf{x}(t,t_0,\mathbf{x}_0)-\mathbf{\varphi}(t) \rvert&lt;\varepsilon, \forall t\geqslant t_0\]</span> 则称方程的解 <spanclass="math inline">\(\mathbf{x}=\mathbf{\varphi}(t)\)</span>是（在Lyapunov意义下）<strong>稳定的</strong>. 假设 <spanclass="math inline">\(\mathbf{x}= \mathbf{\varphi}(t)\)</span>是稳定的，而且存在 <spanclass="math inline">\(\delta_1(0&lt;\delta_1\leqslant\delta)\)</span>，使得只要 <span class="math display">\[    \lvert \mathbf{x}_0-\mathbf{\varphi}(t_0) \rvert &lt;\delta_1\]</span> 就有 <span class="math display">\[    \lim_{t \to\infty}(\mathbf{x}(t,t_0,\mathbf{x}_0)-\mathbf{\varphi}(t))=0\]</span> 则称解 <span class="math inline">\(\mathbf{x}=\mathbf{\varphi}(t)\)</span>是（在Lyapunov意义下）<strong>渐近稳定的</strong>. 如果解 <spanclass="math inline">\(\mathbf{x}=\mathbf{\varphi}(t)\)</span>不是稳定的，则称它是<strong>不稳定的</strong>.</p><p>还有<strong>渐近稳定域</strong>（<strong>吸引域</strong>）的概念.如果吸引域是全空间，则称解 <spanclass="math inline">\(\mathbf{x}=\mathbf{\varphi}(t)\)</span>是<strong>全局渐近稳定的</strong>.</p><p>通过作代换 <span class="math inline">\(\mathbf{y}= \mathbf{x}-\mathbf{\varphi}(t)\)</span>，可以得到一个常解 <spanclass="math inline">\(\mathbf{f}(t,\mathbf{0})=\mathbf{0}\)</span>，以下讨论的都是零解的情况.</p><h3 id="线性近似">线性近似</h3><p>把（6）右端的函数 <spanclass="math inline">\(\mathbf{f}(t,\mathbf{x})\)</span>（注意 <spanclass="math inline">\(\mathbf{f}(t,\mathbf{0})=\mathbf{0}\)</span>）展开成 <spanclass="math inline">\(\mathbf{x}\)</span> 的线性部分和非线性部分之和<span class="math display">\[    \frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t}= A(t) \mathbf{x}+N(t,\mathbf{x}) \tag{7}\]</span> 其中 <span class="math inline">\(A(t)\)</span> 是一个 <spanclass="math inline">\(n\)</span> 阶的矩阵函数，对 <spanclass="math inline">\(t\geqslant t_0\)</span> 连续；而函数 <spanclass="math inline">\(N(t,\mathbf{x})\)</span> 对 <spanclass="math inline">\(t\)</span> 和 <spanclass="math inline">\(\mathbf{x}\)</span> 在区域 <spanclass="math display">\[    G: \quad t\geqslant t_0, \lvert x \rvert \leqslant M\]</span> 上连续，对 <span class="math inline">\(\mathbf{x}\)</span>满足 Lipschitz 条件，且满足 <spanclass="math inline">\(N(t,\mathbf{0})=\mathbf{0}(t\geqslantt_0)\)</span> 和 <span class="math display">\[    \lim_{\lvert \mathbf{x} \rvert  \to 0} \frac{\lvert N(t,\mathbf{x})\rvert }{\lvert \mathbf{x} \rvert }=0 \quad(对 t\geqslant t_0 一致成立)\]</span></p><p><strong>定理8.1</strong>：考虑线性方程 <span class="math display">\[    \frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t}= A(t) \mathbf{x} \tag{8}\]</span> 假设其中的 <span class="math inline">\(A(t)\)</span>是常矩阵，则 - 零解是渐近稳定的 <spanclass="math inline">\(\Leftrightarrow\)</span> 矩阵 <spanclass="math inline">\(\mathbf{A}\)</span> 的全部特征根都有负的实部； -零解是稳定的 <span class="math inline">\(\Leftrightarrow\)</span> 矩阵<span class="math inline">\(\mathbf{A}\)</span>的全部特征根的实部是非正的，且实部为零的特征根所对应的 Jordan块都是一阶的； - 零解是不稳定的 <spanclass="math inline">\(\Leftrightarrow\)</span> 矩阵 <spanclass="math inline">\(\mathbf{A}\)</span>的特征根中至少有一个实部为正；或者至少有一个实部为零，且它对应的 Jordan块是高于一阶的.</p><p><strong>定理8.2</strong>：设方程（7）中的 <spanclass="math inline">\(\mathbf{A}(t)=\mathbf{A}\)</span> 为常矩阵，而且<span class="math inline">\(\mathbf{A}\)</span>的全部特征根都具有负的实部，则（7）的零解是渐近稳定的.</p><p><strong>定理8.3</strong>：设方程（7）中的 <spanclass="math inline">\(\mathbf{A}(t)=\mathbf{A}\)</span> 为常矩阵，且<span class="math inline">\(\mathbf{A}\)</span>的特征根中至少有一个具有正的实部，则（7）的零解是不稳定的</p><p>下面这个推广的Gronwall不等式我不会证，来源是知乎用户@inversioner的常微分方程学习笔记（9），我也查不到这样形式的推广.该推广与Gronwall不等式的最大区别在于函数具有类似迭代的形式，以下再给出一个足够证明上面定理的推广Gronwall不等式.</p><p><strong>引理（推广的Gronwall不等式）</strong>：设非负连续函数 <spanclass="math inline">\(u_{n}(t),v(t)\)</span> 满足不等式</p><p><span class="math display">\[    u_0(t)\leqslant c_0,u_n(t)\leqslant c_n+ \int_{t_0}^{t}v(s)u_{n-1}(s) \mathrm{d}s     \]</span></p><p>其中 <span class="math inline">\(c_n\)</span> 为常数，则有估计式</p><p><span class="math display">\[    u_{n}(t)\leqslant \frac{c_0&#39;+\cdots +c_n&#39;}{n+1} \mathrm{e}^{\int_{t_0}^{t} v(s) \mathrm{d}s }\]</span></p><p>其中 <span class="math inline">\(c_k&#39;:= \sup_{n\geqslantk}\{c_n\}\)</span>.</p><p>可以证明的版本</p><p><strong>引理</strong>：非负连续函数 <spanclass="math inline">\(u_n(t),v(t)\)</span> 满足不等式 <spanclass="math display">\[    u_n(t)\leqslant u_0(t)+ \int_{t_0}^{t} v(s)u_{n-1}(s) \mathrm{d}s\]</span></p><p>则有估计式 <span class="math display">\[    u_n(t)\leqslant u_0(t) \mathrm{e}^{\int_{t_0}^{t} v(s) \mathrm{d}s}\]</span></p><p><strong>证</strong>：直接依次代入，然后用 $^{x} $的Taylor展开即可.</p><p>回到上面两个定理</p><p><strong>证</strong>：首先原方程（7）等价于积分方程 <spanclass="math display">\[    \mathbf{x}(t)= \mathrm{e}^{(t-t_0)\mathbf{A}} \mathbf{x}_0+\mathrm{e}^{t \mathbf{A}} \int_{t_0}^{t} \mathrm{e}^{-s \mathbf{A}}\mathbf{N}(s,\mathbf{x}(s)) \mathrm{d}s\]</span> 构造函数序列 <spanclass="math inline">\(\mathbf{x}_n(t)\)</span> 满足 <spanclass="math inline">\(\mathbf{x}_0(t)=\mathrm{e}^{(t-t_0)\mathbf{A}}\mathbf{x}_0\)</span>，且 <span class="math display">\[    \mathbf{x}_n(t)= \mathrm{e}^{(t-t_0)\mathbf{A}} \mathbf{x}_0+\mathrm{e}^{t \mathbf{A}} \int_{t_0}^{t} \mathrm{e}^{-s \mathbf{A}}\mathbf{N}(s, \mathbf{x}_{n-1}(s)) \mathrm{d}s\]</span></p><p>如果 <span class="math inline">\(\mathbf{A}\)</span>的全部特征根都具有负的实部，那么 <span class="math inline">\(\lim_{t \to\infty}\mathrm{e}^{t \mathbf{A}} = 0\)</span>. 注意到 <spanclass="math inline">\(\mathbf{N}(s,\mathbf{x})\)</span>满足一致Lipschitz条件，可设 <span class="math display">\[    \lvert \mathbf{N}(s,\mathbf{x})- \mathbf{N}(s,\mathbf{y})\rvert\leqslant L \lvert \mathbf{x}- \mathbf{y} \rvert\]</span> 并且有 <spanclass="math inline">\(\mathbf{N}(s,\mathbf{0})=\mathbf{0}\)</span>. 故<span class="math display">\[    \lvert \mathbf{x}_n(t) \rvert\leqslant\mathrm{e}^{(t-t_0)\mathbf{A}} \mathbf{x}_0+ \int_{t_0}^{t}L\mathrm{e}^{(t-s) \mathbf{A}} \lvert \mathbf{x}_{n-1}(s) \rvert\mathrm{d}s\]</span></p><p>（这里 <span class="math inline">\(\leqslant\)</span>的意思是每个坐标都 <spanclass="math inline">\(\leqslant\)</span>）那么</p><p><span class="math display">\[    \lvert \mathbf{x}_n(t)- \mathbf{x}_{n-1}(t) \rvert \leqslant\int_{t_0}^{t} L\mathrm{e}^{(t-s) \mathbf{A}} \lvert\mathbf{x}_{n-1}(t)-\mathbf{x}_{n-2}(t) \rvert  \mathrm{d}s\]</span></p><p>依次递推可得 <span class="math display">\[    \lvert \mathbf{x}_{n}(t)-\mathbf{x}_{n-1}(t) \rvert \leqslant\frac{(L(t-t_0))^{n}\mathrm{e}^{(t-t_0)\mathbf{A}}\mathbf{x}_0 }{n!}\]</span></p><p>这就说明如果 $_0 $ 足够小，那么 <spanclass="math inline">\(\mathbf{x}_n(t)\)</span> 在 <spanclass="math inline">\(t\geqslant t_0\)</span> 上一致收敛，设极限为 <spanclass="math inline">\(\mathbf{x}(t)\)</span>，那么有 <spanclass="math display">\[    \lvert \mathbf{x}(t) \rvert \leqslant \mathrm{e}^{L(t-t_0)}\mathrm{e}^{(t-t_0)\mathbf{A}} \mathbf{x}_0\]</span></p><div class="note note-info">            <p>这里的证明是很微妙的. 先取 $_0 $ 足够小，那么可以证明 $_1(t) $也足够小，进而可以先验地得到 $_n(t) $ 都足够小，然后再有每一步的 <spanclass="math inline">\(L\)</span>都足够小，可以认为这里用到了Gronwall不等式，并且有一种归纳的思想在.</p>          </div><p>证明零解的渐近稳定性，故考虑 $_0 $ 充分小，此时因为 <spanclass="math inline">\(L=o(\mathbf{x}_0)\)</span>，故对于 $_0 $ 足够小，<span class="math display">\[    \lim_{t \to \infty}\mathrm{e}^{L(t-t_0)}\mathrm{e}^{(t-t_0)\mathbf{A}} \mathbf{x}_0= \mathbf{0}\]</span></p><div class="note note-info">            <p>这个证明很大的缺陷是只有 $_0 $ 足够小时构造的序列才一致收敛.希望之后能找到更好的证法</p>          </div><p>定理8.3的证明只要构造一个 <spanclass="math inline">\(\mathbf{x}_0\)</span>任意小，但是对应的解不稳定即可.</p><p>还有一个相关的结论：</p><p><strong>结论</strong>：线性方程零解的渐近稳定性等价于它的全局渐近稳定性.</p><h3 id="lyapunov-第二方法">Lyapunov 第二方法</h3><p>只考虑自治系统 <span class="math display">\[    \frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t}=\mathbf{f}(\mathbf{x})\tag{9}\]</span></p><p>其中 <span class="math inline">\(\mathbf{x} \in\mathbb{R}^{n}\)</span>，而函数 <spanclass="math inline">\(\mathbf{f}(\mathbf{x})=(f_1(\mathbf{x}),\cdots,f_{n}(\mathbf{x}))\)</span> 满足初值问题解的存在和唯一性条件.</p><p>假设存在标量函数 <spanclass="math inline">\(V(\mathbf{x})\)</span>，它在区域 <spanclass="math inline">\(\lvert \mathbf{x} \rvert \leqslant M\)</span>上有定义，并且有连续的偏导数. 有如下几组条件：</p><p><strong>条件 I</strong> <span class="math display">\[    V(\mathbf{0})=0; V(\mathbf{x})&gt;0, 当 \mathbf{x} \neq \mathbf{0}\]</span> 称 <span class="math inline">\(V\)</span>为<strong>定正函数</strong>（丁真函数？）</p><p><strong>条件 II</strong> <span class="math display">\[    \frac{\mathrm{d}V}{\mathrm{d}t}\bigg |_{(9)}=\frac{\partialV}{\partial x_1} f_1+\cdots +\frac{\partial V}{\partial x_n}f_n&lt;0,当\mathbf{x} \neq \mathbf{0}\]</span> 即 <spanclass="math inline">\(\frac{\mathrm{d}V}{\mathrm{d}t}|_{(9)}\)</span>为<strong>定负函数</strong></p><p><strong>条件 II</strong>* <span class="math display">\[    \frac{\mathrm{d}V}{\mathrm{d}t} \bigg|_{(9)}\leqslant 0\]</span> 称 <spanclass="math inline">\(\frac{\mathrm{d}V}{\mathrm{d}t}|_{(9)}\)</span>为<strong>常负函数</strong></p><p><strong>条件 III</strong> <span class="math display">\[    \frac{\mathrm{d}V}{\mathrm{d}t} \bigg|_{(9)}&gt; 0,当 \mathbf{x}\neq\mathbf{0}\]</span> 即 <spanclass="math inline">\(\frac{\mathrm{d}V}{\mathrm{d}t}|_{(9)}\)</span>为定正</p><p><strong>定理8.4</strong> Lyapunov 稳定性判据： -若I和II成立，则方程（9）的零解是渐近稳定的； -若I和II*成立，则方程（9）的零解是稳定的； -若I和III成立，则方程（9）的零解是不稳定的.</p><div class="note note-info">            <p>当条件I到III的不等号全部反置时，定理8.4仍然成立.</p>          </div><div class="note note-info">            <p>当条件I和III成立时，方程（9）的零解负向渐近稳定，因此它用于判断零解的不稳定性十分苛刻；一般的不稳定性判据可以提较弱条件.</p>          </div><p><strong>证</strong>：</p><hr /><p>如何用李雅普诺夫第二法分析非线性系统在每个平衡点处的稳定性？https://www.zhihu.com/question/38156489</p><hr /><p>李雅普诺夫函数的构造是个玄学问题. 给几个例子</p><p>（1）常系数线性微分方程组 <spanclass="math inline">\(\frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t}=\mathbf{A} \mathbf{x}\)</span></p><p>令 <span class="math inline">\(V(\mathbf{x})= \mathbf{x}^{\mathsf{T}} \mathbf{B} \mathbf{x}\)</span>，<spanclass="math inline">\(\mathbf{B}\)</span> 是待定实对称矩阵，则其全导数为<span class="math inline">\(2\mathbf{x} ^{\mathsf{T}}\mathbf{BAx}=\mathbf{x}^{\mathsf{T}}(\mathbf{A}^{\mathsf{T}}\mathbf{B}+\mathbf{B}\mathbf{A})\mathbf{x}\)</span>.这时候问题变成找一个正定矩阵 <spanclass="math inline">\(\mathbf{B}\)</span> 使得矩阵 <spanclass="math inline">\(\mathbf{A}^{\mathsf{T}}\mathbf{B}+\mathbf{BA}\)</span>为负定/半负定/正定. 可以预先找一个负定/半负定/正定矩阵 <spanclass="math inline">\(\mathbf{C}\)</span>，问题变成矩阵方程 <spanclass="math inline">\(\mathbf{A}^{\mathsf{T}}\mathbf{B}+\mathbf{BA}=\mathbf{C}\)</span></p><p>（2） <span class="math inline">\(\frac{\mathrm{d}x}{\mathrm{d}t}=y-x f(x, y), \frac{\mathrm{d}y}{\mathrm{d}t}= -x-y f(x, y)\)</span>.（<span class="math inline">\(f\)</span> 在原点附近可微）</p><p>令 <span class="math inline">\(V (x, y)= x^{2}+y^{2}\)</span>，此时<span class="math display">\[    \frac{1}{2}\frac{\mathrm{d}(x^{2}+y^{2})}{\mathrm{d}t}=-(x^{2}+y^{2}) f(x, y)\]</span> 这就说明方程的稳定性与 <span class="math inline">\(f(x,y)\)</span> 的性质有关.</p><p>（3） <span class="math inline">\(x&#39;&#39;+g(x)=0\)</span>，其中<span class="math inline">\(xg(x)&gt;0, \forall x \neq 0\)</span></p><p>方程可以化为 <spanclass="math inline">\(\frac{\mathrm{d}x}{\mathrm{d}t}=y,\frac{\mathrm{d}y}{\mathrm{d}t}=-g(x)\)</span>.将Lyapunov函数看成一种能量，系统的能量为 <spanclass="math inline">\(E=\frac{1}{2}y^{2}+\int_{0}^{x} g(s)\mathrm{d}s\)</span>. 显然 <span class="math inline">\(E\)</span>是正定的，<span class="math inline">\(E\)</span>的全导数是常负的，故零解是稳定但不渐近稳定的.</p><hr /><p>构造李亚普诺夫函数的规则化方法https://zhuanlan.zhihu.com/p/410896233</p><hr /><hr /><p>如何构建李雅普诺夫方程（或者说有什么构建方程的技巧吗）？https://www.zhihu.com/question/38006572</p><hr /><h2id="平面上的动力系统奇点与极限环">平面上的动力系统，奇点与极限环</h2><p>考虑平面上的动力系统 <span class="math display">\[    \frac{\mathrm{d}x}{\mathrm{d}t}=X(x,y), \quad\frac{\mathrm{d}y}{\mathrm{d}t}= Y(x,y) \tag{10}\]</span></p><p>其中 <span class="math inline">\(X (x, y)\)</span> 和 <spanclass="math inline">\(Y (x, y)\)</span> 在 <spanclass="math inline">\((x, y)\)</span>平面上连续，且保证初值问题的解唯一.</p><p>消去 <span class="math inline">\(t\)</span> 得到 <spanclass="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}= \frac{Y(x,y)}{X(x,y)} \tag{11}\]</span></p><h3 id="初等奇点">初等奇点</h3><p>以 <span class="math inline">\((0,0)\)</span> 为奇点的线性系统 <spanclass="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t} \begin{pmatrix}    x \\    y \\    \end{pmatrix}    =    \mathbf{A}    \begin{pmatrix}    x \\    y \\    \end{pmatrix} \tag{12}\]</span> 其中 <span class="math inline">\(\mathbf{A}\)</span>为常矩阵.</p><ul><li><span class="math inline">\(\mathbf{A}\)</span> 非退化，称 <spanclass="math inline">\((0,0)\)</span> 为<strong>初等奇点</strong>.</li><li><span class="math inline">\(\mathbf{A}\)</span> 退化，称 <spanclass="math inline">\((0,0)\)</span> 为<strong>高阶奇点</strong>.</li></ul><p>初等奇点都是孤立奇点，线性高阶奇点都是非孤立的.高阶奇点常可视为两个或多个初等奇点的复合</p><p>不妨 <span class="math inline">\(\mathbf{A}\)</span>已是Jordan标准型. 具体的定性结构看书吧，太多了.</p><p><strong>定理8.5</strong>（初等奇点类型的判定）对于系统（12），记<span class="math display">\[    p=-\text{tr}(\mathbf{A})=-(a+d) 和 q= \det(\mathbf{A})=ad-bc\]</span> 则有 - 当 <span class="math inline">\(q&lt;0\)</span>时，<span class="math inline">\((0,0)\)</span> 为鞍点； - 当 <spanclass="math inline">\(q&gt;0\)</span> 且 <spanclass="math inline">\(p^{2}&gt;4q\)</span> 时， <spanclass="math inline">\((0,0)\)</span> 为两向结点； - 当 <spanclass="math inline">\(q&gt;0\)</span> 且 <spanclass="math inline">\(p^{2}=4q\)</span> 时， <spanclass="math inline">\((0,0)\)</span> 为单向结点或星形结点； - 当 <spanclass="math inline">\(q&gt;0\)</span> 且 <spanclass="math inline">\(0&lt;p^{2}&lt;4q\)</span> 时， <spanclass="math inline">\((0,0)\)</span> 为焦点； - 当 <spanclass="math inline">\(q&gt;0\)</span> 且 <spanclass="math inline">\(p=0\)</span> 时，<spanclass="math inline">\((0,0)\)</span> 为中心点；</p><p>此外，在第2到第4种情形中，当 <spanclass="math inline">\(p&gt;0\)</span> 时奇点 <spanclass="math inline">\((0,0)\)</span> 是稳定的，而当 <spanclass="math inline">\(p&lt;0\)</span> 时是不稳定的.</p><p>书上还介绍了当矩阵 <span class="math inline">\(\mathbf{A}\)</span>不是 jordan 标准型时判断定性结构的方法，参见第二版p266.</p><p>对于非线性系统（10）. 假设 <span class="math inline">\((0,0)\)</span>是孤立奇点，将右端分解成线性部分与高次项之和 <spanclass="math display">\[    \begin{cases}        \frac{\mathrm{d}x}{\mathrm{d}t}= ax+by+\varphi(x,y) \\        \frac{\mathrm{d}y}{\mathrm{d}t}= cx+dy+ \psi (x, y) \\    \end{cases} \tag{13}\]</span></p><p>提出三组条件（其中 <spanclass="math inline">\(r=\sqrt{x^{2}+y^{2}}\)</span>：<strong>条件A</strong> <span class="math inline">\(\varphi (x, y), \psi(x, y)=o(r), r\to 0\)</span>. <strong>条件A</strong>* <spanclass="math inline">\(\varphi (x, y),\psi (x, y)=o(r^{1+\varepsilon}),r\to 0\)</span>，其中 <spanclass="math inline">\(\varepsilon\)</span> 是一个任意小的正数.<strong>条件B</strong> <span class="math inline">\(\varphi (x,y)\)</span> 和 <span class="math inline">\(\psi (x, y)\)</span>在原点的一个小领域内对 <span class="math inline">\(x\)</span> 和 <spanclass="math inline">\(y\)</span> 连续可微</p><p><strong>定理8.6</strong> 太长了，看第二版书p268</p><p>总的就是说当系统满足一些条件时，系统（10）和（12）有相同的<strong>定性结构</strong>.</p><p>比保持定性结构更弱的要求：保持拓扑结构. 概念太多，不抄了，开摆！</p><p><strong>定理8.7</strong>如果系统（13）的线性部分矩阵的特征根实部都不为零（此时称 <spanclass="math inline">\((0,0)\)</span>为它的<strong>双曲奇点</strong>），则它在奇点 <spanclass="math inline">\((0,0)\)</span>附近是（局部）结构稳定的，并轨道拓扑等价于它的线性化系统.</p><div class="note note-info">            <p>定理8.7推广到 <span class="math inline">\(\mathbb{R}^{n}\)</span>中称为Hartman-Grobman定理</p>          </div><h3 id="极限环">极限环</h3><p>动力系统（10）在闭轨 <span class="math inline">\(\Gamma\)</span>的某个（环形）领域内不再有别的闭轨，即 <spanclass="math inline">\(\Gamma\)</span>为孤立闭轨，则称它为（10）的<strong>极限环</strong>. 极限环 <spanclass="math inline">\(\Gamma\)</span>有一个外侧和内侧领域，使得在这个领域内出发的所有轨线当 <spanclass="math inline">\(t \to +\infty\)</span> 或 <spanclass="math inline">\(-\infty\)</span> 时都盘旋趋向 <spanclass="math inline">\(\Gamma\)</span>. 如果 <spanclass="math inline">\(\Gamma\)</span> 内外两侧轨线都在 <spanclass="math inline">\(t \to +\infty\)</span> （或 <spanclass="math inline">\(-\infty\)</span> ）时盘旋趋于 <spanclass="math inline">\(\Gamma\)</span>，则称 <spanclass="math inline">\(\Gamma\)</span>为<strong>稳定</strong>（或<strong>不稳定</strong>）<strong>极限环</strong>；若一侧是当<span class="math inline">\(t \to +\infty\)</span> 时盘旋趋于 <spanclass="math inline">\(\Gamma\)</span>，另一侧是 <spanclass="math inline">\(-\infty\)</span>，则称 <spanclass="math inline">\(\Gamma\)</span>为<strong>半稳定极限环</strong>.</p><p>这种闭轨 <span class="math inline">\(\Gamma\)</span>的稳定性称为<strong>轨道稳定性</strong>.</p><p><strong>Poincaré-Bendixson 环域定理</strong>：设区域 <spanclass="math inline">\(D\)</span> 是由两条简单闭曲线 <spanclass="math inline">\(L_1\)</span> 和 <spanclass="math inline">\(L_2\)</span> 所围成的环域，并且在 <spanclass="math inline">\(\bar{D}=L_1 \cup D \cup L_2\)</span>上动力系统（10）无奇点；从 <span class="math inline">\(L_1\)</span> 和<span class="math inline">\(L_2\)</span>上出发的轨线都不能离开（或都不能进入） <spanclass="math inline">\(\bar{D}\)</span>. 设 <spanclass="math inline">\(L_1\)</span> 和 <spanclass="math inline">\(L_2\)</span> 均不是闭轨线，则系统（10）在 <spanclass="math inline">\(D\)</span> 内至少存在一条闭轨线 <spanclass="math inline">\(\Gamma\)</span>，即 <spanclass="math inline">\(\Gamma\)</span> 在 <spanclass="math inline">\(D\)</span> 内不能收缩到一点.</p><p>把动力系统（10）看成一平面流体的运动方程，上述环域定理表明：如果流体从环域<span class="math inline">\(D\)</span> 的边界流入 <spanclass="math inline">\(D\)</span>，而在 <spanclass="math inline">\(D\)</span> 内又没有渊和源，那么流体在 <spanclass="math inline">\(D\)</span> 内有环流存在. <spanclass="math inline">\(\Gamma\)</span>不一定是孤立的闭轨；但如果是解析向量场，那环域中的闭轨都是孤立的，因而它们都是极限环.</p><p><strong>Liénard 方程</strong> <span class="math display">\[    x&#39;&#39;+f(x)x&#39;+g(x)=0 \tag{14}\]</span></p><p>其中 <span class="math inline">\(f(x)\)</span> 和 <spanclass="math inline">\(g(x)\)</span> 连续，且 <spanclass="math inline">\(xg(x)&gt;0\)</span>，当 <spanclass="math inline">\(x \neq 0\)</span>. 它等价于 <spanclass="math display">\[    \frac{\mathrm{d}x}{\mathrm{d}t}= y-F(x), \quad\frac{\mathrm{d}y}{\mathrm{d}t}= -g(x) \tag{15}\]</span></p><p>其中 <span class="math inline">\(F(x)= \int_{0}^{x} f(x)\mathrm{d}x\)</span>. 当 <span class="math inline">\(g(x)=-x\)</span>时，可以画出（15）在相平面上的向量场 <spanclass="math inline">\((y-F(x),-x)\)</span> 在任一点 <spanclass="math inline">\(P(x,y)\)</span> 处的方向.</p><h3 id="liénard-作图法">Liénard 作图法</h3><h3 id="poincaré-映射与后继函数法">Poincaré 映射与后继函数法</h3><p><strong>定理8.9</strong>：系统（10）的单重极限环 <spanclass="math inline">\(\Gamma\)</span> 是结构稳定的，亦即：存在 <spanclass="math inline">\(\varepsilon&gt;0\)</span> 和 <spanclass="math inline">\(\Gamma\)</span> 的环形领域 <spanclass="math inline">\(\mathcal{U}\)</span>，使得（10）的任何 <spanclass="math inline">\(\varepsilon-\)</span> 邻近系统在 <spanclass="math inline">\(\mathcal{U}\)</span> 内仍有唯一闭轨，而且它与<span class="math inline">\(\Gamma\)</span> 有相同的稳定性.</p><h2 id="结构稳定与分支现象">结构稳定与分支现象</h2><h3 id="一个大范围的结构稳定性定理">一个大范围的结构稳定性定理</h3><h3 id="高阶奇点的分支">高阶奇点的分支</h3><h3 id="hopf-分支">Hopf 分支</h3><h3 id="poincaré-分支">Poincaré 分支</h3><h3 id="多重闭轨的分支">多重闭轨的分支</h3><h3 id="同宿轨线的分支">同宿轨线的分支</h3><h3 id="奇异向量场的普适开折">奇异向量场的普适开折</h3>]]></content>
    
    
    <categories>
      
      <category>分析学</category>
      
      <category>微分方程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (2)</title>
    <link href="/2022/05/01/Neuronal-Dynamics-2/"/>
    <url>/2022/05/01/Neuronal-Dynamics-2/</url>
    
    <content type="html"><![CDATA[<h1 id="ion-channels-and-the-hodgkin-huxley-model">Ion Channels and theHodgkin-Huxley Model</h1><p>The online version of this chapter:</p><hr /><p>Chapter 2 Ion Channels and the Hodgkin-Huxley Modelhttps://neuronaldynamics.epfl.ch/online/Ch2.html</p><hr /><p>In the previous description, spikes are formal events that aregenerated at the moment of threshold crossing.</p><p>Now we will talk about how a spike can be generated based on thebiophysics of cell membranes.</p><h2 id="equilibrium-potential">2.1 Equilibrium potential</h2><h3 id="nernst-potential">2.1.1 Nernst potential</h3><p>The probability of a molecule to take a state of energy <spanclass="math inline">\(E\)</span> is proportional to the Boltzman factor<span class="math inline">\(p(t) \propto \exp (-E/kT)\)</span> where<span class="math inline">\(k\)</span> is the Boltzman constant and<span class="math inline">\(T\)</span> the temperature.</p><p>The probality to find an ion in the region around location <spanclass="math inline">\(x\)</span> proportional to <spanclass="math inline">\(\exp [-qu(x)/kT]\)</span>.</p><p>Write <span class="math inline">\(n(x)\)</span> for the ion densityat point <span class="math inline">\(x\)</span>. <spanclass="math display">\[    \frac{n(x_1)}{n(x_2)}=\exp [-\frac{qu(x_1)-qu(x_2)}{kT}] \tag{2.1}\]</span></p><blockquote><p><strong>Q:</strong> What's the meaning of "Since this is a statementabout an equilibrium state, the reverse must also be true"?</p></blockquote><p><strong>Nernst potential</strong> (能斯特势) <spanclass="math display">\[    \Delta u=\frac{kT}{q}\ln \frac{n_2}{n_1} \tag{2.2}\]</span></p><h3 id="reversal-potential">2.1.2 Reversal Potential</h3><p>The cell membrane bilayer lipids - a nearly perfect electricalinsulator specific proteins as ion gates - ion pumps: actively transportions from one side to the other. - ion channels: passively</p><p><strong>Resting Potential</strong> The value <spanclass="math inline">\(u_{rest}\)</span> is determined by the dynamicequilibrium between the ion flow through the channels (permeability ofthe membrane) and active ion transport (efficiency of the ion pump inmaintaining the concentration difference).</p><h2 id="hodgkin-huxley-model">2.2 Hodgkin-Huxley Model</h2><p>Four equations <span class="math display">\[    \begin{aligned}        C &amp;\frac{\mathrm{d}u}{\mathrm{d}t}=-g_Kn^{4}(u-E_K)-g_{Na}m^{3}h(u-E_{Na})-g_l(u-E_l)+I(t) \\        &amp;\frac{\mathrm{d}}{\mathrm{d}t}n=-\frac{n-n_0(u)}{\tau_n(u)} \\        &amp;\frac{\mathrm{d}}{\mathrm{d}t}m=-\frac{m-m_0(u)}{\tau_m(u)} \\        &amp;\frac{\mathrm{d}}{\mathrm{d}t}h=-\frac{h-h_0(u)}{\tau_h(u)}    \end{aligned}\]</span></p><p>Gating variables <span class="math display">\[    I_{ion}=-g_{ion}r^{n_1}s^{n_2}\]</span> <span class="math inline">\(r\)</span> represents activationvariables, <span class="math inline">\(r_0(u)\to 1, u\to\infty\)</span>. <span class="math inline">\(s\)</span> representsinactivation variables, <span class="math inline">\(s_0(u)\to 0, u\to\infty\)</span> &gt; There could be 2 gating variables for some ions.&gt; It turns out to be useful to distinguish between a deactivatedchannel (<span class="math inline">\(m\)</span> close to zero and <spanclass="math inline">\(h\)</span> close to one) and an inactivatedchannel ( <span class="math inline">\(h\)</span> close to zero)</p><h3 id="threshold-in-the-hodgkin-huxley-model">Threshold in the HodgkinHuxley Model</h3><p><strong>Here, threshold relates to the paradigm of thecurrent.</strong></p><p><strong>threshold for repetitive firing</strong> current threshold(constant current)</p><p>threshold current: <span class="math inline">\(\theta_i\)</span>. Ifwe are above this minimal current, the model generates regular firing.If not, then the model shows constant firing. <strong>threshold foraction potential</strong> current threshold (step current)(uninformative)</p><p><strong>pulse current</strong> Consider <spanclass="math inline">\(I(t)=q\delta(t-t_0)\)</span>. It causes a jump ofthe membrane potential.</p><p>Fixing duration of the current, we have a current threshhold. If thecurrent is above the threshhold, then the neuron gives an actionpotential.</p><p>SAP: spike after potential</p><p><strong>step current input</strong> <spanclass="math inline">\((I_1,\Delta I,I_2)\)</span></p><p><img src="img/neu_dyn/2022-05-18-15-19-28.png" /></p><p>The final current as well as the step size that come intoaccount.</p><p><strong>ramp input</strong></p><p>Type II/Class II Behavior(Hodgking-Huxley model with standardparameters, giant axon of squid): Using a very slow ramp input on aHodgkin-Huxley model, we find an f-I curve with a jump</p><p>Type I/Class I Behavior(Hodgking-Huxley model with other parameters,e.g. for cortical pyramidal neuron): Using the frame variable ofHodgking-Huxley models but change parameters so it's more adapted tocortical neurons, then you make it a smooth response with very lowfrequencies.</p><h3 id="stochastic-channel-opening">Stochastic Channel Opening</h3><p>Channels open stochastically.</p><blockquote><center>Example: Time Constants, Transition Rates, and Channel Kinetics</center><p>The activation and inactivation dynamics of each channel type can bedescribed in terms of voltage-dependent transition rates <spanclass="math inline">\(\alpha\)</span> and <spanclass="math inline">\(\beta\)</span>, <span class="math display">\[\frac{\mathrm{d}m}{\mathrm{d}t}=\alpha_m(u)(1-m)-\beta_m(u)m\]</span> <span class="math display">\[\frac{\mathrm{d}n}{\mathrm{d}t}=\alpha_n(u)(1-n)-\beta_n(u)n\]</span> <span class="math display">\[\frac{\mathrm{d}h}{\mathrm{d}t}=\alpha_h(u)(1-h)-\beta_h(u)h\]</span> The asymptotic value <spanclass="math inline">\(x_0(u)\)</span> and the time constant <spanclass="math inline">\(\tau_x(u)\)</span> are given by <spanclass="math inline">\(x_0(u)=\alpha_x(u)/[\alpha_x(u)+\beta_x(u)]\)</span>and <spanclass="math inline">\(\tau_x(u)=[\alpha_x(u)+\beta_x(u)]^{-1}\)</span>.</p><p><span class="math inline">\(n\)</span> can be intepreted as theprobability of finding a single potassium channel open. In a patch with<span class="math inline">\(K\)</span> channels, approximately <spanclass="math inline">\(k \thickapprox (1-n)K\)</span> channels areexpected to be closed. We may interpret <spanclass="math inline">\(\alpha_n(u)\Delta t\)</span> as the probabilitythat in a short time interval <span class="math inline">\(\Deltat\)</span> one of the momentarily closed channels switches to the openstate.</p></blockquote><h2 id="the-zoo-of-ion-channels">2.3 The Zoo of Ion Channels</h2><h3 id="framework-for-biophysical-neuron-models">Framework forbiophysical neuron models</h3><h3 id="sodium-ion-channels-and-the-type-i-regime">Sodium Ion Channelsand the Type-I Regime</h3><h3 id="adaptation-and-refractoriness">Adaptation andRefractoriness</h3><p>I have a constant input current, an adaptation means that in thespike intervals, get longer and longer.</p><p>Ex: <span class="math display">\[    I_{M}= g_{M} m(u-E_{k})\]</span> - Potassium current - Kv7 subunits - slow time constant</p><p>A current such as <span class="math inline">\(I_{M}\)</span> is oneof the potentially many sources of adaptation. It works by lowering themembrane potential, by lowering the spike after potential(SAP).</p><p>Another way of generating adaptation: not by changing SAP, but byincreasing the firing threshold.</p><p><span class="math display">\[    I_{NaP} = g_{NaP} mh(u-E_{Na})      \]</span> - persistent sodium current - fast activation time constant -slow inactivation (~1s)</p><h3 id="subthreshold-effects">Subthreshold Effects</h3><h3 id="calcium-spikes-and-postinhibitory-rebound">Calcium spikes andpostinhibitory rebound</h3><h2 id="summary">2.4 Summary</h2>]]></content>
    
    
    <categories>
      
      <category>神经动力学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>神经科学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ascoli-Arzela Theorem</title>
    <link href="/2022/04/29/Ascoli-Arzela-Theorem/"/>
    <url>/2022/04/29/Ascoli-Arzela-Theorem/</url>
    
    <content type="html"><![CDATA[<h1 id="ascoli-arzela-theorem">Ascoli-Arzela Theorem</h1><p>Ascoli-Arzela定理在分析的多门课中都有用到：数分中作为一个函数项级数中的重要定理给出；ODE中用它来证明解的存在和唯一性定理；复分析中用它来证明Montel定理，从而证明RiemannMapping Theorem；调和分析中用它来证明Peter-WeylTheorem；在泛函分析中......</p><p>趁着脑子清醒，把以前记的相关结果记录一下，主要参考北京某高校数分三的笔记。这一节的标题为“紧集上的连续函数空间”主要是想证明如下定理：</p><p><strong>Theorem (Ascoli-Arzela)</strong>: 设 <spanclass="math inline">\(D \subset \mathbb{R}^{d}\)</span>是一个紧集，<spanclass="math inline">\(\mathcal{F} \subset C(D)\)</span>， 则 <spanclass="math inline">\(\mathcal{F}\)</span>完全有界 <spanclass="math inline">\(\iff\)</span> <spanclass="math inline">\(\mathcal{F}\)</span> 有界且等度连续.</p><p>它回答了这样一个问题：<spanclass="math inline">\(C(D)\)</span>中什么样的集合是紧的？</p><p>解释一下概念： <strong>$C(D) $</strong> <spanclass="math inline">\(D\)</span>上连续函数构成的空间，其中 <spanclass="math inline">\(D\)</span>作为紧集可以是有限子集、有界闭区域或者紧子流形.</p><p><strong>（逐点）等度连续</strong>：<spanclass="math inline">\(\forall x_0 \in D, \forall \varepsilon&gt;0,\exists \delta&gt;0, \forall f \in \mathcal{F}, \forall \lvertx-x_0 \rvert &lt;\delta,(\lvert f(x)-f(x_0) \rvert&lt;\varepsilon)\)</span></p><p>一致等度连续就是上面定义中的 <spanclass="math inline">\(\delta\)</span>不依赖于 <spanclass="math inline">\(x_0\)</span>的选取</p><p>规定 <span class="math inline">\(D\)</span>上的范数：<spanclass="math inline">\(\lVert \cdot \rVert \colon C(D) \to \mathbb{R}, f\mapsto \max_{x \in D} \lvert f(x) \rvert\)</span>，易见 <spanclass="math inline">\(C(D)\)</span>中函数列按由该范数诱导的度量定义的收敛性即为一致收敛性.由此得到</p><p><strong>定理</strong>：<spanclass="math inline">\(C(D)\)</span>是一个Banach空间.</p><p><strong>证</strong>：<spanclass="math inline">\(C(D)\)</span>中的Cauchy列 <spanclass="math inline">\(\{f_n\}\)</span>按上度量（一致）收敛到某个 <spanclass="math inline">\(f\)</span>，易知 <spanclass="math inline">\(f\)</span>连续，故属于 <spanclass="math inline">\(C(D)\)</span>.</p><p>在一般的完备度量空间上，紧与列紧等价，但是有界闭的条件太弱了，可以从<span class="math inline">\(f_n(x)=x^{n}, x \in[0,1]\)</span>得出，有界闭并不与列紧等价.</p><p>下面总假设 <spanclass="math inline">\((X,d_{X})\)</span>是一个完备度量空间，<spanclass="math inline">\(E\)</span>是 <spanclass="math inline">\(X\)</span>的一个子集.</p><p><strong>Lemma 1</strong>：<span class="math inline">\(E\)</span>紧<span class="math inline">\(\Rightarrow\)</span> <spanclass="math inline">\(E\)</span>列紧. <strong>证</strong>：设 <spanclass="math inline">\(\{x_k\}\)</span>是 <spanclass="math inline">\(E\)</span>中的点列，反证法：考虑 <spanclass="math inline">\(E\)</span>的像 <spanclass="math inline">\(F\)</span>. <spanclass="math inline">\(F\)</span>为有限集的情况是易证的；对于 <spanclass="math inline">\(F\)</span>为无限集的情况，考虑 <spanclass="math inline">\(E\)</span>中每个点与 <spanclass="math inline">\(F\)</span>交只为有限集的球领域，所有这些球领域构成<spanclass="math inline">\(E\)</span>的开覆盖，则它的任意有限子覆盖只能覆盖<span class="math inline">\(F\)</span>的有限个点，总有 <spanclass="math inline">\(F\)</span>中的点不被盖到，这与 <spanclass="math inline">\(E\)</span>的紧性矛盾！</p><p><strong>Lemma 2</strong>： <span class="math inline">\(E\)</span>列紧<span class="math inline">\(\Rightarrow\)</span> <spanclass="math inline">\(E\)</span>紧 在给出证明前先做一些准备工作</p><p><strong>定义</strong>：若对任意的 <spanclass="math inline">\(\varepsilon &gt; 0\)</span>，存在 <spanclass="math inline">\(E\)</span>中的有限点集 <spanclass="math inline">\(x_1,\cdots ,x_m\)</span>使得 $E B(x_1,)B(x_m,)$则称 <spanclass="math inline">\(E\)</span>是<strong>完全有界</strong>的，有限点集<span class="math inline">\(x_1,\cdots ,x_m\)</span>称为一个<strong><span class="math inline">\(\varepsilon\)</span>-网</strong></p><p><strong>Lemma3</strong>：<span class="math inline">\(E\)</span>列紧<span class="math inline">\(\Rightarrow\)</span> <spanclass="math inline">\(E\)</span> 完全有界<strong>证</strong>：反证法，归纳得到一个两两之间距离大于 <spanclass="math inline">\(\varepsilon_{0}\)</span>的点列，它不是Cauchy列，从而<span class="math inline">\(E\)</span>不列紧.</p><p>接下来讨论 <span class="math inline">\(X\)</span>中完备和闭的关系</p><p><strong>Lemma4</strong>：<span class="math inline">\(E\)</span>完备<span class="math inline">\(\Leftrightarrow\)</span> <spanclass="math inline">\(E\)</span>闭<strong>证</strong>：完备推闭时，假设不闭，则某个不在 <spanclass="math inline">\(E\)</span>中的点会有一个 <spanclass="math inline">\(E\)</span>中趋向于该点的序列，由完备性，它在 <spanclass="math inline">\(E\)</span>中，得到矛盾.闭推完备时，假设不完备，则某个Cauchy列的极限不在 <spanclass="math inline">\(E\)</span>中，由聚点原理易得矛盾.</p><blockquote><p>总体上说这是一个比较显然但是很重要的引理</p></blockquote><p><strong>Lemma5</strong>：<span class="math inline">\(E\)</span>列紧<span class="math inline">\(\Rightarrow\)</span> <spanclass="math inline">\(E\)</span>完备 <spanclass="math inline">\(\Rightarrow\)</span> <spanclass="math inline">\(E\)</span>闭<strong>证</strong>：Cauchy列子列的极限也一定是Cauchy列的极限.</p><p><strong>Lemma6</strong>：<spanclass="math inline">\(E\)</span>完全有界（且闭） <spanclass="math inline">\(\Rightarrow\)</span> <spanclass="math inline">\(E\)</span>（自）列紧<strong>证</strong>：任取一个点列 <spanclass="math inline">\(\{x_k\}\)</span>，利用完全有界的性质，可以归纳地构造一系列以某个子列为圆心的开球族，每个开球都包含<spanclass="math inline">\(E\)</span>中的无穷多个点（类似于闭区间套定理的证明），证明该子列是Cauchy列，由闭（故完备）知极限也在<span class="math inline">\(E\)</span>中，从而 <spanclass="math inline">\(E\)</span>列紧.</p><p><strong>Lemma7</strong>：<span class="math inline">\(E\)</span>列紧<span class="math inline">\(\Rightarrow\)</span> <spanclass="math inline">\(E\)</span>具有Lebesgue数性质，即对于 <spanclass="math inline">\(E\)</span>的任一开覆盖 <spanclass="math inline">\(\{U_{\alpha}\}\)</span>，存在一个 <spanclass="math inline">\(\varepsilon&gt;0\)</span>，使得对于任意的 <spanclass="math inline">\(x \in E\)</span>，存在一个 <spanclass="math inline">\(U_{\alpha}\)</span>，使得 $B(x,) U_{} $.</p><p><strong>证</strong>：也是利用反证法，依次取 <spanclass="math inline">\(\displaystyle\varepsilon_k=\frac{1}{k}\)</span>，得到不满足上性质的一个序列，它有一个收敛子列，但是这样就会与<span class="math inline">\(U_{\alpha}\)</span> 是开集矛盾了.</p><p><strong>Lemma8</strong>：<spanclass="math inline">\(E\)</span>完全有界且具有Lebesgue数性质 <spanclass="math inline">\(\Rightarrow\)</span> <spanclass="math inline">\(E\)</span>紧<strong>证</strong>：倒一倒，完全有界得到的 <spanclass="math inline">\(\varepsilon-\)</span>网和Lebesgue数性质基本上是一个东西.这样可以从任意一个开覆盖得到一个有限子覆盖.</p><p>这样我们就可以证明Lemma2了：列紧可以得到完全有界（Lemma3）且具有Lebesgue数性质（Lemma7），由Lemma8得其紧.</p><p>至此证明了如下定理 <strong>Theorem1</strong>：设 <spanclass="math inline">\((X,d_{X})\)</span>是一个完备度量空间，<spanclass="math inline">\(E\)</span>是 <spanclass="math inline">\(X\)</span>的子集，则以下性质等价 - <spanclass="math inline">\(E\)</span>紧 - <spanclass="math inline">\(E\)</span>列紧 - <spanclass="math inline">\(E\)</span>完全有界闭</p><p>关于完全有界性，还有如下结果</p><p><strong>Theorem2</strong>：<span class="math inline">\(X\)</span>和<span class="math inline">\(E\)</span>如前，则以下性质等价 - <spanclass="math inline">\(E\)</span>完全有界 - <spanclass="math inline">\(\bar{E}\)</span>紧（这在拓扑中叫<strong>预紧</strong>或<strong>相对紧</strong>）- <span class="math inline">\(E\)</span>是紧集的子集 - <spanclass="math inline">\(E\)</span>中任意序列有收敛子列（其极限不必在 <spanclass="math inline">\(E\)</span>中）</p><p><strong>证</strong>：1证2：只要证明 <spanclass="math inline">\(\bar{E}\)</span>完全有界，这在 <spanclass="math inline">\(E\)</span>完全有界的 <spanclass="math inline">\(\varepsilon\)</span>取小一点即可. 2证3：显然.3证4：设 <span class="math inline">\(E\)</span>是紧集 <spanclass="math inline">\(F\)</span>的子集，则 <spanclass="math inline">\(E\)</span>中的点列都有收敛于 <spanclass="math inline">\(F\)</span>的子列. 4证1：同Lemma3.</p><blockquote><p>实际应用中Theorem2的最后一个条件比较重要.一般的完备度量空间上能得到的最好的结果就是上面的两个定理.</p></blockquote><p>在泛函分析中，对 <span class="math inline">\(\forall\varepsilon&gt;0\)</span> 存在有限 <spanclass="math inline">\(\varepsilon\)</span>-网和 <spanclass="math inline">\(\forall \varepsilon&gt;0\)</span> 存在列紧的 <spanclass="math inline">\(\varepsilon\)</span>-网是等价的。注意到 <spanclass="math inline">\(\varepsilon\)</span>的任意性其实是很强的一个条件。</p><p>下面回到 <spanclass="math inline">\(C(D)\)</span>，在该空间上紧性的刻画.</p><p><strong>Theorem (Ascoli-Arzela)</strong>: 设 <spanclass="math inline">\(D \subset \mathbb{R}^{d}\)</span>是一个紧集，<spanclass="math inline">\(\mathcal{F} \subset C(D)\)</span>， 则 <spanclass="math inline">\(\mathcal{F}\)</span>完全有界 <spanclass="math inline">\(\iff\)</span> <spanclass="math inline">\(\mathcal{F}\)</span> 有界且等度连续.</p><blockquote><p>此处的有界可以是逐点有界，也可以是一致有界.等度连续可以是逐点等度连续，也可以是一致等度连续，它们在紧集上基本是等价的.</p></blockquote><p><strong>Lemma1</strong>：设 <spanclass="math inline">\(D\)</span>紧，则 <spanclass="math inline">\(\mathcal{F}\)</span>逐点等度连续 <spanclass="math inline">\(\Leftrightarrow\)</span> <spanclass="math inline">\(\mathcal{F}\)</span>一致等度连续.</p><p><strong>证</strong>：同紧集上逐点连续函数必一致连续的证明.</p><p><strong>Lemma2</strong>：设 <spanclass="math inline">\(D\)</span>紧，$ C(D) $等度连续，则 <spanclass="math inline">\(\mathcal{F}\)</span>逐点有界 <spanclass="math inline">\(\Leftrightarrow\)</span> <spanclass="math inline">\(\mathcal{F}\)</span>一致有界.</p><p><strong>证</strong>：只需要证明正方向.由等度连续和紧（从而完全有界）的性质，可以从 <spanclass="math inline">\(x_1,\cdots ,x_m\)</span>的 <spanclass="math inline">\(\delta-\)</span>网得到 <spanclass="math inline">\(f(x_1),\cdots ,f(x_m)\)</span>的 <spanclass="math inline">\(\varepsilon-\)</span>网，取其中最大的，再加上一个<span class="math inline">\(\varepsilon\)</span>即为一致的界.</p><p><strong>Ascoli-Arzela定理的证明</strong>： - 完全有界 <spanclass="math inline">\(\Rightarrow\)</span>有界： 取 <spanclass="math inline">\(\mathcal{F}\)</span>的一个 <spanclass="math inline">\(1-\)</span>网，再放大一下即可.</p><ul><li>完全有界 <span class="math inline">\(\Rightarrow\)</span>等度连续先证一个引理</li></ul><p><strong>Lemma3</strong>：若 <span class="math inline">\(f_k\rightrightarrows f\)</span>，则 <spanclass="math inline">\(\{f_k\}\)</span>完全有界且等度连续.</p><p><strong>证</strong>：“<spanclass="math inline">\(\Rightarrow\)</span>”：在一开始说过的度量的意义下，<spanclass="math inline">\(\{f_k\}\)</span>是 <spanclass="math inline">\(C(D)\)</span>中的Cauchy列，所以任一子列都收敛，因此完全有界（注意这里谈论的是点列的完全有界性）.</p><p>下证等度连续，采用三分法. 一致收敛得到 <spanclass="math inline">\(d(f_n(x),f(x))\)</span>的估计，<spanclass="math inline">\(f\)</span>的一致连续性得到 <spanclass="math inline">\(d(f(x),f(y))\)</span>的估计，于是可以得到 <spanclass="math inline">\(n&gt;N\)</span>时 <spanclass="math inline">\(d(f_n(x),f_n(y))\)</span>的估计. 由 <spanclass="math inline">\(f_k\)</span>的一致连续性可以补充 <spanclass="math inline">\(k\)</span>较小的时候的情况. Lemma3证毕.</p><p>回到Ascoli-Arzela定理的证明. 假设 <spanclass="math inline">\(\mathcal{F}\)</span>不等度连续，则存在 <spanclass="math inline">\(\varepsilon_0&gt;0\)</span>使得对于任意的 <spanclass="math inline">\(\delta&gt;0\)</span> 都存在 <spanclass="math inline">\(x_{\delta},y_{\delta} \in D, f_{\delta} \in\mathcal{F}\)</span>，满足 <spanclass="math inline">\(d(x_{\delta},y_{\delta})&lt;\delta\)</span>，且<spanclass="math inline">\(d(f_{\delta}(x_{\delta}),f_{\delta}(y_{\delta}))&gt;\varepsilon_0\)</span>.</p><p>依次取 <span class="math inline">\(\displaystyle\delta_k=\frac{1}{k}, k=1,2,..\)</span>可得 <spanclass="math inline">\(x_k,y_k,f_k\)</span>满足上述条件. 因为 <spanclass="math inline">\(\mathcal{F}\)</span>完全有界，所以 <spanclass="math inline">\(\{f_k\}\)</span>有收敛子列 <spanclass="math inline">\(\{f_{k_j}\}\)</span>. 由刚刚证明的Lemma3，<spanclass="math inline">\(\{f_{k_j}\}\)</span>等度连续，于是对上面的 <spanclass="math inline">\(\varepsilon_0&gt;0\)</span>，存在 <spanclass="math inline">\(\delta&gt;0\)</span>，使得当 <spanclass="math inline">\(x,y \in D\)</span>满足 <spanclass="math inline">\(d(x,y)&lt;\delta\)</span>时，有 <spanclass="math inline">\(d(f_{k_j}(x),f_{k_j}(y))&lt;\varepsilon_0, \forallj \in \mathbb{N}\)</span>.</p><p>另一方面，取充分大的 <span class="math inline">\(j\)</span>使得 <spanclass="math inline">\(\frac{1}{k_j}&lt;\delta\)</span>，则 <spanclass="math inline">\(d(x_{k_j},y_{k_j})&lt;\frac{1}{k_j}&lt;\delta\)</span>，并且有<spanclass="math inline">\(d(f_{k_j}(x_{k_j}),f_{k_j}(y_{k_j}))\geqslant\varepsilon_0\)</span>，矛盾！</p><p>“<span class="math inline">\(\Leftarrow\)</span>”：若 <spanclass="math inline">\(D\)</span>是有限集，记 <spanclass="math inline">\(n\)</span>为 <spanclass="math inline">\(D\)</span>的势，则 <spanclass="math inline">\(C(D)=\mathbb{R}^{n}\)</span>，此时有界 <spanclass="math inline">\(\Rightarrow\)</span> 完全有界.</p><p>若 <span class="math inline">\(D\)</span>是无限集，下证有界和等度连续<spanclass="math inline">\(\Rightarrow\)</span>任一序列有收敛子列（极限不必在<span class="math inline">\(\mathcal{F}\)</span>中）. 从而由Theorem2知<span class="math inline">\(\mathcal{F}\)</span>完全有界.</p><p>设 <span class="math inline">\(\{f_n\}\)</span>是 <spanclass="math inline">\(\mathcal{F}\)</span>中一个点列，<spanclass="math inline">\(D\)</span>紧从而完全有界. 依次取 $_k=,k=1,2,$有相应的 <span class="math inline">\(\varepsilon_k-\)</span>网<span class="math inline">\(\{x_{k,1},\cdots ,x_{k,m_k}\}\)</span>. 记<span class="math inline">\(I\)</span>是这些 <spanclass="math inline">\(\varepsilon_k-\)</span>网的并，并将 <spanclass="math inline">\(I\)</span>中元素重新编号为 <spanclass="math inline">\(I=\{x_1,x_2,\cdots \}\)</span>.</p><p>对于每个 <span class="math inline">\(x_k \in I\)</span>，因为 <spanclass="math inline">\(\mathcal{F}\)</span>逐点有界，所以 <spanclass="math inline">\(f_n(x_k)\)</span>有界，于是有收敛子列 <spanclass="math inline">\(\{f_n^{(k)}(x_k)\}\)</span>，再考虑 <spanclass="math inline">\(\{f_n^{(k)}(x_{k+1})\}\)</span>，它也有收敛子列<spanclass="math inline">\(\{f_n^{(k+1)}(x_{k+1})\}\)</span>，继续下去得到一系列子列<span class="math inline">\(\{f_{n}^{(k)}\}\)</span>. 取 <spanclass="math inline">\(g_n=f_n^{(n)}\)</span>，对于每个 <spanclass="math inline">\(x_k \in I\)</span>，<spanclass="math inline">\(\{g_n(x_k)\}=\{f_n^{(n)}(x_k)\}\)</span>. 推得<span class="math inline">\(\{g_n\}\)</span>在 <spanclass="math inline">\(I\)</span>上逐点收敛.</p><p>三分法，取一个足够大的 <spanclass="math inline">\(n_0\)</span>对应的一个 <spanclass="math inline">\(\varepsilon_{n_0}-\)</span>网 <spanclass="math inline">\(\{\xi_1,\cdots ,\xi_k\}\)</span>，则 <spanclass="math inline">\(\{g_n(\xi_i)\}\)</span>收敛. 应用等度连续性即知<span class="math inline">\(\{g_n\}\)</span>在 <spanclass="math inline">\(D\)</span>上一致收敛，于是 <spanclass="math inline">\(\mathcal{F}\)</span>完全有界.</p><p>最后看一个例子 <strong>例</strong>：<spanclass="math inline">\(D=[a,b], X=C[a,b]\)</span>，$ X $一致有界，设<span class="math inline">\(\lvert f(x) \rvert \leqslant M, \forall x,\forall f\)</span>. 定义 $X X, f _{a}^{x} f(t) t $，则 <spanclass="math inline">\(\lvert \varphi(f)(x) \rvert \leqslantM(b-a)\)</span>，即 <spanclass="math inline">\(\varphi(\mathcal{F})\)</span>一致有界.$(f)(x)-(f)(y) M x-y $ 这说明 <spanclass="math inline">\(\varphi(\mathcal{F})\)</span>等度连续.于是由Ascoli-Arzela定理知，<spanclass="math inline">\(\varphi(\mathcal{F})\)</span>完全有界，即任意序列有收敛子列，像<spanclass="math inline">\(\varphi\)</span>这样将有界集映成完全有界集的映射在泛函分析中叫<strong>紧算子</strong>，证明一个算子是紧的通常就要用Ascoli-Arzela定理.</p>]]></content>
    
    
    <categories>
      
      <category>分析学</category>
      
      <category>杂项</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neuronal Dynamics (1)</title>
    <link href="/2022/04/28/Neuronal-Dynamics-1/"/>
    <url>/2022/04/28/Neuronal-Dynamics-1/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction-neurons-and-mathematics">Introduction: Neurons andMathematics</h1><p>The online version of the book:</p><hr /><p>Neuronal Dynamics - a neuroscience textbook by Wulfram Gerstner,Werner M. Kistler, Richard Naud and Liam Paninskihttps://neuronaldynamics.epfl.ch/index.html</p><hr /><h2 id="elements-of-neuronal-systems">Elements of Neuronal Systems</h2><h3 id="the-ideal-spiking-neuron">The Ideal Spiking Neuron</h3><p>dendrites(树突), soma(), axon(轴突) synapse(突触) &gt; synapseslocate in dendrites.</p><h3 id="spike-trains">Spike Trains</h3><p>action potentials or spikes, have an amplitude of about 100mV andtypically a durationn of 1-2 ms.</p><p>Spike train: a chain of action potentials emitted by a single neuron,a sequence of stereotyped events which occur at regular or irregularintervals.</p><p>The number and the timing of spikes matters.</p><p>Action potentials in a spike train are ususally well seperated.</p><h3 id="synapses">Synapses</h3><p>synaptic cleft</p><p>chemical synapses, electrical synapses(gap juntions),</p><h3 id="neurons-are-part-of-a-big-system">Neurons are part of a bigsystem</h3><p>neuron's receptive field(感受野): the limited zone where a neuron issensitive to stimuli.(out of brain)</p><h2 id="elements-of-neuronal-dynamics">Elements of NeuronalDynamics</h2><p>membrane potential synapse: excitatory(the change is positive),inhibitory(the change is negative)</p><p>At rest, the cell membrane has already a strongly negativepolarization of about -65mV.</p><p>An input at an excitatory synapse reduces the negative polarizationof the membrane and is therefore called depolarizing. An input theincrease the negative polarization of the membrane even further iscalled hyperpolarizing.</p><h3 id="postsynaptic-potentials">Postsynaptic Potentials</h3><p>postsynaptic potential(PSP), excitatory postsynaptic potential(EPSP),inhibitory postsynaptic potential(IPSP)</p><p>The membrane potential responds linearly to input spikes. Linearitybreaks dwon if too many input spikes arrive during a short interval.</p><p><strong>spike-afterpotential</strong>: afer the pulse the membranepotential does not directly return to the resting potential, but passes,for many neuron types, through a phase of hyperpolarization below theresting value.</p><p>About 20-50 presynaptic spikes have to arrive within a short timewindow to trigger a postsynaptic action potential.</p><blockquote><p>when the voltage hits the threshold, the neuron can enter a state ofrefractoriness.</p></blockquote><h2 id="integrate-and-fire-models">Integrate-And-Fire Models</h2><p>The moment of threshold crossing defines the firing time <spanclass="math inline">\(t_i^{(f)}\)</span>.</p><p>'Leaky-Integrate-and-Fire' Model</p><h3 id="integration-of-inputs">Integration of Inputs</h3><p>The equation of a passive membrane: <span class="math display">\[    \tau_m \frac{\mathrm{d}u}{\mathrm{d}t}=-[u(t)-u_{rest}]+RI(t)\tag{1.5}\]</span> We refer to <span class="math inline">\(u\)</span> as themembrane potential and to <span class="math inline">\(\tau_m\)</span> asthe <strong>membrane time constant</strong> of the neuron. The aboveequation is called the equation of a passive membrane.</p><p>The solution of the differential equation with initial condition<span class="math inline">\(u(t_0)=u_{rest}+\Delta u\)</span> and $I(t)=0 $ is <span class="math display">\[    u(t)-u_{rest}=\Delta u \exp(-\frac{t-t_0}{\tau_m})\quad \text{for}\quad t&gt;t_0\]</span></p><p>This indicates that, in the absence fo input, the membrane potentialdecays exponentiallly to its resting value. The membrane time constant<span class="math inline">\(\tau_m =RC\)</span> is the characteristictime of the decay. For a typical neuron it is in the range of 10ms, andhence rather long compared to the duration of a spike which is on theorder of 1ms.</p><h3 id="pulse-input">Pulse Input</h3><p>Suppose that the passive membrane is stimulated by a constant inputcurrent <span class="math inline">\(I(t)=I_0\)</span> which starts at<span class="math inline">\(t=0\)</span> and ends at time <spanclass="math inline">\(t=\Delta\)</span>. For the sake of simplicity weassume that the membrane potential at time <spanclass="math inline">\(t=0\)</span> is at its resting value <spanclass="math inline">\(u(0)=u_{rest}\)</span>.</p><p>Now, the solution to the differential equation for <spanclass="math inline">\(0&lt;t&lt;\Delta\)</span> is <spanclass="math display">\[u(t)=u_{rest}+RI_0[1-\exp(-\frac{t}{\tau_m})]\]</span></p><p>If the input current never stopped, the mambrane potential wouldapproach for <span class="math inline">\(t\to \infty\)</span> theasymptotic value <spanclass="math inline">\(u(\infty)=u_{rest}+RI_0\)</span>. Once a steadystate is reached, the charge on the capacitor no longer changes. Allinput current must then flow thorugh the resistor. The steady-statevoltage at the resistor is therefore <spanclass="math inline">\(RI_0\)</span> so that the total membrane voltageis <span class="math inline">\(u_{rest}+RI_0\)</span>.</p><h4 id="example-short-pulses-and-the-dirac-function">Example: Shortpulses and the Dirac $ $ function</h4><p>For pulse duration <span class="math inline">\(\Delta \ll\tau_m\)</span>, expand the exponential term into a Taylor series, wefind:</p><p><span class="math display">\[u(\Delta)=u_{rest}+RI_0\frac{\Delta}{\tau_m}\]</span></p><p>Namely, the voltage deflection depends linearly on the amplitude andthe duration of the pulse. As long as <spanclass="math inline">\(q=I_0\Delta\)</span> stay the same, the voltagechange induced by a short current pulse is always the same, whenever theduration of the pulse <span class="math inline">\(\Delta\)</span> ismuch shorter than the time constant <spanclass="math inline">\(\tau_m\)</span>. Thus, the exact duration of thepulse is irrelevant, as long as it is short enough.</p><p>We no longer have to worry about the time course of the membranepotential during the application of the current pulse: the membranepotential simply jumps at time <span class="math inline">\(t=0\)</span>by an amount <span class="math inline">\(q/C\)</span>.</p><p>In conclusion, the solution of <span class="math display">\[        \tau_{m}\frac{\mathrm{d}u}{\mathrm{d}t}=-[u(t)-u_{rest}]+Rq\delta(t)    \]</span> is <span class="math inline">\(u(t)=u_{rest}\)</span> for<span class="math inline">\(t\leqslant 0\)</span> and given by <spanclass="math display">\[        u(t)-u_{rest}=q \frac{R}{\tau_m}\exp (-\frac{t}{\tau_m}) \quad\text{for} \quad t&gt;0    \]</span></p><p>We call it <strong>the impulse-response function or Green's functionof the linear differential equation</strong>.</p><h3 id="nonlinear-integrate-and-fire-model">Nonlinear Integrate-and-fireModel</h3><p><span class="math display">\[    \tau \frac{\mathrm{d}u}{\mathrm{d}t}=F(u)+RI(t)\]</span></p><p>where <span class="math inline">\(F(u)\)</span> can be a non-linearfunction. For example, <spanclass="math inline">\(F(u)=c_2(u-c_1)^{2}+c_0\)</span> (the quadraticintegrate-and-fire models) or <spanclass="math inline">\(F(u)=-(u-u_{rest})+c_0\exp (u-\theta)\)</span>(the exponential integrate-and-fire model)</p><h3 id="the-threshold-for-spike-firing">The Threshold for SpikeFiring</h3><p><strong>firing time</strong>: the moment when a given neuron emits anaction potential <span class="math inline">\(t ^{(f)}\)</span>. In theleaky-and-fire model is defined as <span class="math display">\[    t^{(f)}: u(t^{(f)})=\theta\]</span> immediately after <span class="math inline">\(t^{(f)}\)</span>the potential is reset to a new value <spanclass="math inline">\(u_r&lt;\theta\)</span> <spanclass="math display">\[    \lim_{\delta \to 0;\delta&gt;0} u(t^{(f)}+\delta)=u_r\]</span> <span class="math inline">\(u_r\)</span>refers to thespike-afterpotential</p><p>For <span class="math inline">\(t&gt; t^{(f)}\)</span>, the dynamicsis as usual until the next threshold crossing occurs.</p><p>The combination of leaky integration and reset defines the leakyintegrate-and-fire model.</p><p><strong>Q</strong>: What's the meaning of the notes under Fig. 1.9"Units of input current are chosen so that <spanclass="math inline">\(I_0=1\)</span> corresponds to a trajectory thatreaches the threshold for <span class="math inline">\(t \in\infty\)</span>"</p><h3 id="time-dependent-input">Time-dependent Input</h3><p>This subsection study a leaky integrate-and-fire model which isdriven by an arbitrary time-dependent input current <spanclass="math inline">\(I(t)\)</span>;.</p><p><strong>spike train of a neuron <spanclass="math inline">\(i\)</span></strong> <span class="math display">\[    S_i(t)=\sum_{f}^{} \delta(t-t_i^{(f)})  \tag{1.14}\]</span></p><p>In the absence of a threshold, the linear differential equation (1.5)has a solution <span class="math display">\[    u(t)=u_{rest}+\frac{R}{\tau_m}\int_{0}^{\infty} \exp(-\frac{s}{\tau_m})I(t-s) \mathrm{d}s \tag{1.15}\]</span> (Check ODE by Ding Tongren p33) where <spanclass="math inline">\(I(t)\)</span> is an arbitrary input current and<span class="math inline">\(\tau_m=RC\)</span> is the membrane timeconstant. Assume here that the input curreent is defined for a long timeback into the past: <span class="math inline">\(t \to -\infty\)</span>to avoid initial condition.</p><p>Consider adding a threshold <spanclass="math inline">\(\theta\)</span> to (1.15). The reset of thepotential corresponds to removing a charge <spanclass="math inline">\(q_r=C(\theta-u_r)\)</span> from the capacitor/adding a negative charge <span class="math inline">\(-q_r\)</span> ontothe capacitor. Therefore, the reset corresponds to a short current pulse<span class="math inline">\(I_r=-q_r\delta(t-t^{(f)})\)</span> at themoment of the firing <span class="math inline">\(t^{(f)}\)</span>. Thereset current is <span class="math display">\[    I_r=-q_r \sum_{f}^{} \delta(t-t^{(f)})=-C(\theta-u_r)S(t)\]</span> where <span class="math inline">\(S(t)\)</span> denotes thespike train.</p><p>The total current <span class="math inline">\(I(t)+I_r(t)\)</span>,consisting of the stimulating current and the reset current. The finalresult <span class="math display">\[    u(t)=u_{rest}+\sum_{f}^{} (u_r-\theta)\exp(-\frac{t-t^{(f)}}{\tau_m})+\frac{R}{\tau_m}\int_{0}^{\infty} \exp(-\frac{s}{\tau_m})I(t-s) \mathrm{d}s\]</span> (1.17) where the firing times <spanclass="math inline">\(t^{(f)}\)</span> are defined by the thresholdcondition <span class="math display">\[    t^{(f)}=\{t|u(t)=\theta\} \tag{1.18}\]</span> &gt; The second term of (1.17) describes the ' discharging' ofneurons at the moment of the firing/ the effect of the dischargingcurrent pulses at the moment of the reset. &gt; The last term decribesthe sum of the impulse response functions since the electricity existsat ant moment.</p><h3id="linear-differential-equation-vs.-linear-filter-two-equivalent-pictures">LinearDifferential Equation vs. Linear Filter: Two Equivalent Pictures</h3><p>Rewrite the solution (1.17) in the form <span class="math display">\[    u(t)=\int_{0}^{\infty} \eta(s)S(t-s) \mathrm{d}s+\int_{0}^{\infty}\kappa(s)I(t-s) \mathrm{d}s  \tag{1.22}\]</span> where the filter <span class="math inline">\(\displaystyle\eta(s)=(u_r-\theta)\exp (-\frac{s}{\tau_m})\)</span> , <spanclass="math inline">\(\displaystyle \kappa(s)=\frac{1}{C}\exp(-\frac{s}{\tau_m})\)</span> and <span class="math display">\[    S_i(t)=\sum_{f}^{} \delta(t-t_i^{(f)})\]</span></p><blockquote><p>(1.22) is much more general than the leaky integrate-and-fire model,because the filters do not need to be expoenntial but could have anyarbitrary shape.</p></blockquote><blockquote><p>The <strong>filter</strong> <span class="math inline">\(\eta\)</span>describes the reset of the membrane potential and, more generally,accounts for <strong>neuronal refractoriness</strong>. The<strong>filter</strong> <span class="math inline">\(\kappa\)</span>summarizes the linear electrical properties of the membrane.</p></blockquote><blockquote><p>(1.22) also tells us how to write a sum to the form of convolution,which relys on the propertiy of <spanclass="math inline">\(\delta\)</span> function.</p></blockquote><p><strong>Q</strong>: What is a filter?</p><h3 id="periodic-drive-and-fourier-transform">Periodic drive and Fouriertransform</h3><p>recall <span class="math display">\[    \hat{f}(\omega)=\int_{-\infty}^{\infty} f(t)\mathrm{e}^{-i\omegat}  \mathrm{d}t=\left\vert \hat{f}(\omega) \right\vert \mathrm{e}^{i\phi_f(\omega)}\]</span> where <span class="math inline">\(\left\vert \hat{f}(\omega)\right\vert\)</span> and <spanclass="math inline">\(\phi_f(\omega)\)</span> are called<strong>amplitude</strong> and <strong>phase</strong> of the Fouriertransform at frequency <span class="math inline">\(\omega\)</span>.</p><p>Consider a system <span class="math display">\[    u(t)=\int_{-\infty}^{\infty} \kappa(s)I(t-s) \mathrm{d}s\]</span> then we have <spanclass="math inline">\(\hat{u}(\omega)=\hat{\kappa}(\omega)\hat{I}(\omega)\)</span></p><p>In the above system, let <span class="math display">\[    I(t)=I_0\mathrm{e}^{i \omega t}\]</span> we focus on the real part. If the input is periodic atfrequency <span class="math inline">\(\omega\)</span> the output <spanclass="math display">\[    u(t)= \biggl[ \int_{-\infty}^{\infty} \kappa(s)\mathrm{e}^{-i\omegas}  \mathrm{d}s \biggr] I_0 \mathrm{e}^{i\omega t} \tag{1.27}\]</span> write <span class="math inline">\(u(t)=u_0 \mathrm{e}^{i\phi_k(\omega)+i\omega t}\)</span>. We have <span class="math display">\[    \frac{u_0}{I_0}=\left\vert \hat{\kappa}(\omega) \right\vert\tag{1.28}\]</span></p><p>calculate $() $ <span class="math display">\[    \left\vert \hat{\kappa}(\omega) \right\vert=\frac{1}{C}\left\vert\frac{\tau_m}{1+i\omega \tau_m} \right\vert  \]</span> Therefore the amplitude of the response to a periodic inputdecreases at high frequencies.</p><h2 id="limitations-of-the-leaky-integrate-and-fire-model">Limitationsof the Leaky Integrate-and-Fire Model</h2><h3 id="adaption-bursting-and-inhibitory-rebound">Adaption, Bursting,and Inhibitory Rebound</h3><p><strong>Adaption</strong>: Most neurons will respond to the currentstep with a spike train where intervals between spikes increasesuccessively until a steady state of periodic firing is reached.</p><p><strong>Fast-spiking neurons</strong>: neurons showing no adaption ——can be well approximated by non-adapting integrate-and-fire models. &gt;Many inhibitory neurons are fast-spiking neurons.</p><p><strong>Bursting and stuttering neurons</strong>: respond to constantstimulation by a sequence of spikes that is periodically (bursting) oraperiodically (stuttering) interrupted by rather long interals.</p><blockquote><p>changing 'filters' in (1.22), we can describe bursting.</p></blockquote><p><strong>Post-inhibitory rebound</strong>: when an inhibitory input isswitched off, many neurons respond with one or more 'rebound spikes',even the release of inhibition can trigger action potentials.</p><h3 id="shunting-inhibition-and-reversal-potential">Shunting Inhibitionand Reversal Potential</h3><p>postsynaptic current, PSC <span class="math display">\[    \text{PSC} \quad \propto \quad [u_0-E_{syn}]\]</span> where <span class="math inline">\(u_0\)</span> is the membranepotential and <span class="math inline">\(E_{syn}\)</span> is the'reversal potential' of the synapse.</p><blockquote><center>Ex: Shunting Inhibition</center></blockquote><p><strong>Q</strong>: I'm not clear about shunting inhibition</p><h3 id="conductance-changes-after-a-spike">Conductance Changes after aSpike</h3><p>The shape of the postsynaptic potentials does not only depend on thelevel of depolarization but also on the internal state of the neuron,e.g., on the timing relative to previous action potentials.</p><h3 id="spatial-structure">Spatial Structure</h3><p>The form of postsynaptic potentials also depends on the location ofthe synapse on the dendritic tree.</p><p><strong>Fig.1.12</strong> shows that a presynaptic spike that arrivesat time <span class="math inline">\(t_j^{(f)}\)</span> shortly after thespike of the postsynaptic neuron has a smaller effect than a spike thatarrives much later.</p><p>In the situation of successive inputs, the first input will causelocal changes of the membrane potential, changing the response of spikesthat arrive later.</p><p><strong>hot spots</strong>: small regions on the dendrite where astrong nonlinear boosting of synaptic currents occurs. The boosting canlead to dendritic spikes which last longer (tens of milliseconds).</p><h2 id="what-can-we-expect-from-integrate-and-fire-models">What Can WeExpect from Integrate-And-Fire Models?</h2><p>By adding adaptation and refractoriness to the neuron model, theprediction of leaky integrate-and-fire model works surprisinglywell.</p><p>The first way to add adaptation is: after each spike the threshold<span class="math inline">\(\theta\)</span> is increased by an amount<span class="math inline">\(\theta\)</span>, while during a quiescentperiod the threshold approaches its stationary value <spanclass="math inline">\(\theta_0\)</span>. Use the Dirac <spanclass="math inline">\(\delta\)</span>-function to express this idea<span class="math display">\[    \tau_{\text{adapt}}\frac{\mathrm{d}}{\mathrm{d}t}\theta(t)=-[\theta(t)-\theta_0]+\theta\sum_{f}^{} \delta(t-t^{(f)}) \tag{1.34}\]</span> where <span class="math inline">\(\tau_{\text{adapt}}\)</span>is the time constant of adaptation (a few hundred milliseconds) and$t<sup>{(f)}=t</sup>{(1)},t^{(2)}$ are the firing times of theneuron.</p><h2 id="summary">Summary</h2><p>The whole paragraph is valuable. For me, the most important sentenceis: The simple leaky integrate-and-fire model does not account for<strong>long-lasting refractoriness</strong> or<strong>adaptation</strong>.</p><h3 id="exercise">Exercise</h3><p><strong>Synaptic current pulse</strong> Synaptic inputs can beapproximated by an exponential current <spanclass="math inline">\(\displaystyle I(t)=\frac{q}{\tau_s}\exp[-\frac{t-t^{(f)}}{\tau_s}]\)</span> for <spanclass="math inline">\(t&gt;t^{(f)}\)</span> where <spanclass="math inline">\(t^{(f)}\)</span> is the moment when the spikearrives at the synapse.</p><ol type="a"><li><p>Calculate the response of a passive membrane with time constant<span class="math inline">\(\tau_m\)</span> to an input spike arrivingat time <span class="math inline">\(t^{(f)}\)</span>.</p></li><li><p>In the solution resulting from (a), take the limit <spanclass="math inline">\(\tau_s \to \tau_m\)</span> and show that in thislimit the response is proportional to <spanclass="math inline">\(\propto [t-t^{(f)}] \exp[-\frac{t-t^{(f)}}{\tau_s}]\)</span>. A function of this form issometimes called an <spanclass="math inline">\(\alpha\)</span>-function.</p></li><li><p>In the solution resulting from (a), take the limit <spanclass="math inline">\(\tau_s \to 0\)</span>. Can you relate your resultto the discussion of the Dirac- <spanclass="math inline">\(\delta\)</span> function?</p></li></ol><p><strong>Solution</strong> (a) <span class="math display">\[    \begin{aligned}        u(t)&amp;=u_{rest}+\exp{[-\frac{t}{\tau_m}]} \int_{t^{(f)}}^{t}\frac{qR}{\tau_m \tau_s} \exp[\frac{t^{(f)}}{\tau_s}+\frac{t}{\tau_m}-\frac{t}{\tau_s}]\mathrm{d}t \\        &amp;= u_{rest}+\frac{qR}{\tau_s-\tau_m}[\exp(\frac{t^{(f)}-t}{\tau_s})-\exp (\frac{t^{(f)}-t}{\tau_m}) ]\quad(t&gt;t^{(f)})   \\    \end{aligned}\]</span></p><ol start="2" type="a"><li>Verifying the equicontinuity, we can exchange the order oflimitation and integration. <span class="math display">\[\lim_{\tau_s \to \tau_m}u(t)=u_{rest}+\frac{qR}{\tau_m^2}[t-t^{(f)}]\exp[-\frac{t-t^{(f)}}{\tau_m}]\]</span></li></ol><p><img src="img/neu_dyn/2022-05-08-10-50-17.png" /></p><ol start="3" type="a"><li><span class="math display">\[\lim_{\tau_s \to 0^{+}}u(t)=u_{rest}+\frac{qR}{\tau_m}\exp(-\frac{t-t^{(f)}}{\tau_m}) \quad (t&gt;t^{(f)})\]</span></li></ol><p>It's exact the solution of the Dirac-<spanclass="math inline">\(\delta\)</span> function. That's because <spanclass="math display">\[    \int_{t^{(f)}}^{\infty} I(t) \mathrm{d}t=q\]</span> and <span class="math display">\[    \lim_{\tau_s \to 0^{+}} \frac{I(t)}{q} =\delta(t-t^{(f)})\]</span></p><p><strong>Time-dependent solution</strong> Show that (1.15) is asolution of (1.5) for time-dependent input <spanclass="math inline">\(I(t)\)</span>.</p><p><strong>Chain of linear equations</strong> Suppose that arrival of aspike at time <span class="math inline">\(t^{(f)}\)</span> releasesneurotransmitter into the synaptic cleft. The amount of availableneurotransmitter at time <span class="math inline">\(t\)</span> is <spanclass="math inline">\(\displaystyle \tau_x\frac{\mathrm{d}x}{\mathrm{d}t}=-x+\delta(t-t^{(f)})\)</span>. Theneurotransmitter binds to the postsynaptic membrane and opens channelsthat enable a synaptic current <span class="math inline">\(\displaystyle\tau_s \frac{\mathrm{d}I}{\mathrm{d}t}=-I+I_0 x(t)\)</span>. Finally,the current charges the postsynaptic membrane according to <spanclass="math inline">\(\displaystyle \tau_m\frac{\mathrm{d}u}{\mathrm{d}t}=-u+RI(t)\)</span>. Write the voltageresponse to a single current pulse as an integral.</p><p><strong>solution</strong> I'm not very sure. I omited some constantsin calculating.</p><p><span class="math display">\[    u=\frac{\tau_x^{2}}{(\tau_x-\tau_s)(\tau_x-\tau_m)}I_0R \exp(-\frac{t-t^{(f)}}{\tau_x})\]</span></p>]]></content>
    
    
    <categories>
      
      <category>神经动力学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>神经科学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>常微分方程（7）</title>
    <link href="/2022/04/19/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%887%EF%BC%89/"/>
    <url>/2022/04/19/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%887%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="幂级数解法">幂级数解法</h1><h2 id="柯西定理">柯西定理</h2><p>这部分的主要结果是建立初值问题收敛的幂级数解的存在和唯一性定理.只讨论一阶的微分方程 <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y)\]</span> 其中 <span class="math inline">\(f(x, y)\)</span>在区域 <spanclass="math inline">\(G \in\mathbb{R}^{2}\)</span>上<strong>解析</strong>（能够局部展开成收敛幂级数）</p><p>对于初值问题 <span class="math display">\[    (E): \quad \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y),\quad y(x_0)=y_0\]</span></p><p>下证明该解 <span class="math inline">\(y=y(x)\)</span>在 <spanclass="math inline">\(x_0\)</span>附近是<strong>解析</strong>的</p><p>我们有<strong>优级数</strong>（或<strong>强级数</strong>），<strong>优函数</strong>（或<strong>强函数</strong>）的概念.</p><p><strong>引理7.1</strong>：设函数 <span class="math inline">\(f(x,y)\)</span>在矩形区域 <span class="math display">\[    R: \quad \left\vert x-x_0 \right\vert &lt;\alpha,\quad \left\verty-y_0 \right\vert &lt; \beta\]</span> 上可以展成 <span class="math inline">\((x-x_0)\)</span>和<spanclass="math inline">\((y-y_0)\)</span>的一个收敛的幂级数，则存在常数<span class="math inline">\(M&gt;0\)</span>，使得函数 <spanclass="math display">\[    F(x, y)=\frac{M}{(1-\displaystyle \frac{x-x_0}{a})(1-\displaystyle\frac{y-y_0}{b})} \tag{1}\]</span> 在矩形区域 <span class="math display">\[    R_0: \quad \left\vert x-x_0 \right\vert &lt;a, \quad \left\verty-y_0 \right\vert &lt;b\]</span> 内是 <span class="math inline">\(f(x,y)\)</span>的一个优函数，其中 <span class="math inline">\(a&lt;\alpha\)</span>和 <span class="math inline">\(b&lt;\beta\)</span>.</p><p><strong>证</strong>：把 <span class="math inline">\(F (x,y)\)</span>展开即可找 <span class="math inline">\(M\)</span>.</p><p><strong>引理7.2</strong>：设在 <spanclass="math inline">\(R_0\)</span>上由（1）给定 <spanclass="math inline">\(F(x, y)\)</span>，则初值问题 <spanclass="math display">\[    (\hat{E}): \quad \frac{\mathrm{d}y}{\mathrm{d}x}=F(x, y), \quady(x_0)=y_0      \]</span> 在区间 <span class="math inline">\(\left\vert x-x_0\right\vert &lt;\rho\)</span>内存在一个解析的解 <spanclass="math inline">\(y=\hat{y}(x)\)</span>，其中常数 <spanclass="math inline">\(\rho=a(1-\mathrm{e}^{-b/2aM} )\)</span>，而正数<span class="math inline">\(a,b\)</span>和 <spanclass="math inline">\(M\)</span>的意义同上.</p><p><strong>证</strong>：利用分离变量法求解 <spanclass="math inline">\((\hat{E})\)</span>，解出 <spanclass="math inline">\(y=\hat{y}(x)\)</span>，分析 <spanclass="math inline">\(\hat{y}(x)\)</span>中 <spanclass="math inline">\(\displaystyle \ln(1-\frac{x-x_0}{a})\)</span>的收敛半径和 <spanclass="math inline">\(\sqrt{1+\cdot}\)</span>的收敛半径；可以得到 <spanclass="math inline">\(\hat{y}(x)\)</span>是 <spanclass="math inline">\((\hat{E})\)</span>的解析解.</p><p><strong>定理7.1</strong>（Cauchy）：如果函数 <spanclass="math inline">\(f(x, y)\)</span>在矩形区域 <spanclass="math inline">\(R\)</span>上可以展开成 <spanclass="math inline">\((x-x_0)\)</span>和 <spanclass="math inline">\((y-y_0)\)</span>的一个收敛的幂级数，则初值问题<span class="math inline">\((E)\)</span>在 <spanclass="math inline">\(x_0\)</span>点的领域 <spanclass="math inline">\(\left\vert x-x_0 \right\vert&lt;\rho\)</span>内有一个解析解 <spanclass="math inline">\(y=y(x)\)</span>，而且它是唯一的. 此处区域 <spanclass="math inline">\(R\)</span>和常数 <spanclass="math inline">\(\rho\)</span>的意义同上.</p><p><strong>证</strong>：首先展开 <spanclass="math inline">\(f(x,y)\)</span>： <span class="math display">\[    f(x, y)=\sum_{i,j=0}^{\infty} a_{ij}(x-x_0)^{i}(y-y_0)^{j} \tag{2}\]</span> 作形式解 <span class="math display">\[    y=y_0+\sum_{n=1}^{\infty} C_n(x-x_0)^{n} \tag{3}\]</span> 可以直接求 <span class="math inline">\(C_n\)</span>，它是诸<span class="math inline">\(a_{ij}\)</span>的多项式 <spanclass="math inline">\(P_n\)</span>，<spanclass="math inline">\(P_n\)</span>的系数<strong>全为正</strong>且 <spanclass="math inline">\(P_n\)</span>与 <span class="math inline">\(f(x,y)\)</span>无关.这就说明了解析解的唯一性，我们还要说明（3）是一致收敛的.</p><p>考虑初值问题 <span class="math display">\[    (\hat{E}): \quad \frac{\mathrm{d}y}{\mathrm{d}x}=F(x, y), \quady(x_0)=y_0      \]</span> 其中 <span class="math inline">\(F(x, y)\)</span>和 <spanclass="math inline">\(a,b,M\)</span>如之前所定义. 它在 <spanclass="math inline">\(R_0\)</span>内是 <span class="math inline">\(f(x,y)\)</span>的优函数. 作 <spanclass="math inline">\((\hat{E})\)</span>的形式解 <spanclass="math display">\[    y=\hat{y}(x)=y_0+\sum_{n=1}^{\infty} \hat{C_n}(x-x_0)^{n}\]</span> 则有 $C_n $，由优级数判别法，定理证毕.</p><div class="note note-success">            <p>非解析的微分方程可能没有形式的幂级数解.如 <spanclass="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=\frac{y-x}{x}, y(0)=0\]</span></p>          </div><div class="note note-success">            <p>非解析微分方程的形式解可能不收敛.如 <span class="math display">\[    x^{2}\frac{\mathrm{d}y}{\mathrm{d}x}=y-x, \quad y(0)=0\]</span> 它的形式解是 <span class="math display">\[    y=x+x^{2}+2!x^{3}+\cdots +n! x^{n+1}+\cdots\]</span> 但是它对任何 <span class="math inline">\(x \neq0\)</span>不收敛.</p>          </div><p>优级数方法可以应用于微分方程组，具体见书本.</p><p>叙述解析微分方程组的Cauchy定理 对于微分方程组的初值问题 <spanclass="math display">\[    \frac{\mathrm{d}y^{(k)}}{\mathrm{d}x}=f_k(x,y^{(1)},\cdots,y^{(n)}), \quad y^{(k)}(x_0)=y_k\]</span> <span class="math inline">\((k=1,2,\cdots ,n)\)</span>. 如果<span class="math inline">\(f_k\)</span>在区域 <spanclass="math display">\[    \left\vert x-x_0 \right\vert \leqslant \alpha, \quad \left\verty^{(1)}-y_1 \right\vert \leqslant \beta,\quad \cdots ,\quad \left\verty^{(n)}-y_n \right\vert \leqslant \beta      \]</span> 上能展开成 <span class="math inline">\(x-x_0\)</span>和诸<spanclass="math inline">\(y^{(k)}-y_k\)</span>的收敛的幂级数，则上初值问题在<span class="math inline">\(x_0\)</span>点的领域 <spanclass="math inline">\(\left\vert x-x_0 \right\vert&lt;\rho\)</span>内有一个解析解组 <spanclass="math inline">\(y^{(k)}=y^{(k)}(x)\)</span>，而且它是唯一的. 这里<span class="math display">\[    \rho=a(1-\mathrm{e}^{-b/2aM} )\]</span> 其中 <span class="math inline">\(M\)</span>类似定义.</p><h2 id="幂级数解法-1">幂级数解法</h2><p>考虑二阶齐次线性微分方程 <span class="math display">\[    A(x)y&#39;&#39;+B(x)y&#39;+C(x)y=0 \tag{4}\]</span> 其中 <span class="math inline">\(A(x),B(x)\)</span>和 <spanclass="math inline">\(C(x)\)</span>在区间 $x-x_0 &lt;r $内解析，不妨它们没有公因子 <span class="math inline">\((x-x_0)\)</span>.- <span class="math inline">\(A(x_0) \neq 0\)</span>. 称这样的 <spanclass="math inline">\(x_0\)</span>为（4）的<strong>常点</strong>.将方程写成 <span class="math display">\[    y&#39;&#39;+p(x)y&#39;+q(x)y=0 \tag{5}\]</span></p><ul><li><span class="math inline">\(A(x_0)=0\)</span>，称这样的 <spanclass="math inline">\(x_0\)</span>为（4）的<strong>奇点</strong>.</li></ul><p><strong>定理7.2</strong>：设微分方程（5）系数函数 <spanclass="math inline">\(p(x)\)</span>和 <spanclass="math inline">\(q(x)\)</span>在区间 $x-x_0 &lt;r $可以展成 <spanclass="math inline">\((x-x_0)\)</span>的收敛的幂级数. 则（5）在区间<span class="math inline">\(\left\vert x-x_0 \right\vert&lt;r\)</span>内有收敛的幂级数解 <span class="math display">\[    y=\sum_{n=0}^{\infty} C_n(x-x_0)^{n}\]</span> 其中 <span class="math inline">\(C_0=y_0\)</span>和 <spanclass="math inline">\(C_1=y_0&#39;\)</span>.</p><p><strong>证</strong>：由微分方程组的Cauchy定理即可.</p><h2 id="legendre-多项式">Legendre 多项式</h2><p>Legendre方程 <span class="math display">\[    (1-x^{2})y&#39;&#39;-2xy&#39;+n(n+1)y=0 \tag{6}\]</span> 有幂级数解 <span class="math display">\[    y=C_0y_1(x)+C_1y_2(x), \quad (-1&lt;x&lt;1) \tag{7}\]</span> 其中 <span class="math inline">\(C_0\)</span>和 <spanclass="math inline">\(C_1\)</span>是任意常数，而 <spanclass="math display">\[    y_1(x)=1-\frac{n(n+1)}{2!}x^{2}+\frac{(n-2)n(n+1)(n+3)}{4!}x^{4}-\cdots\]</span> <span class="math display">\[    y_2(x)=x-\frac{(n-1)(n+2)}{3!}x^{3}+\frac{(n+3)(n-1)(n+2)(n+4)}{5!}x^{5}-\cdots\]</span> 显然 <span class="math inline">\(y_1(x)\)</span>和 <spanclass="math inline">\(y_2(x)\)</span>线性无关. 当 <spanclass="math inline">\(n\)</span>是正偶数时, <spanclass="math inline">\(y_1(x)=P_n(x)\)</span>是一个 <spanclass="math inline">\(n\)</span>次多项式， <spanclass="math inline">\(y_2(x)\)</span>是无穷级数；当 <spanclass="math inline">\(n\)</span>是正奇数时， <spanclass="math inline">\(y_2(x)=P_n(x)\)</span>是一个 <spanclass="math inline">\(n\)</span>次多项式. <spanclass="math inline">\(P_n(x)\)</span>相差一个常数因子的情况下可以写成<span class="math display">\[    P_n(x)=\frac{1}{2^{n}}\sum_{k=0}^{[\frac{n}{2}]}\frac{(-1)^{k}(2n-2k)!}{k!(n-k)!(n-2k)!}x^{n-2k} \tag{8}       \]</span> 对任意的非负整数 <spanclass="math inline">\(n\)</span>，由（8）表达的 <spanclass="math inline">\(n\)</span>次多项式 <spanclass="math inline">\(P_n(x)\)</span>是相应的Legendre方程的解.它们叫作<strong>Legendre多项式</strong>.</p><p>有 <span class="math display">\[    P_n(-x)=(-1)^{n}P_n(x)\]</span> Legendre多项式系 <span class="math display">\[    P_0(x),P_1(x), P_2(x),\cdots  \tag{9}\]</span> 有如下性质 <strong>性质1</strong>： <spanclass="math inline">\(P_n(x)\)</span>满足Rodrigues公式 <spanclass="math display">\[    P_n(x)=\frac{1}{2^{n}n!}\frac{\mathrm{d}^{n}}{\mathrm{d}x^{n}}(x^{2}-1)^{n}\tag{10}\]</span> 从而容易得出 <span class="math display">\[    P_n(1)=1, \quad P_n(-1)=(-1)^{n} \tag{11}\]</span> <strong>证</strong>：用二项式定理即可.</p><p><strong>性质2</strong>：Legendre函数系是正交的，但结果与一般的结论有所不同<span class="math display">\[    \int_{-1}^{1} P_n(x)P_m(x) \mathrm{d}x =    \begin{cases}        0,\quad m \neq n \\        \sigma_n&gt;0, \quad m=n    \end{cases}\]</span> 并且有 <span class="math inline">\(\sigma_n=\displaystyle\frac{2}{2n+1}\)</span> <strong>证</strong>：当 <spanclass="math inline">\(n \neqm\)</span>时，用分部积分法和（11）的结果得到 <spanclass="math inline">\((P_n,P_m)=0\)</span>. 当 <spanclass="math inline">\(n=m\)</span>时进一步计算就有结果成立，这里用到<span class="math inline">\(\displaystyle \int_{0}^{\frac{\pi}{2}} \cos^{2n+1}\theta \mathrm{d}\theta\)</span> 的结果.</p><p>设 <span class="math inline">\(f(x)\)</span>在区间 <spanclass="math inline">\([-1,1]\)</span>上可积，则可作 <spanclass="math inline">\(f(x)\)</span>关于 <spanclass="math inline">\(P_n(x)\)</span>的<strong>广义Fourier级数</strong><span class="math display">\[    f(x) \approx \sum_{n=0}^{\infty} a_nP_n(x) \tag{12}\]</span> 其中 <span class="math display">\[    a_n=\frac{2n+1}{2} \int_{-1}^{1} f(x)P_n(x) \mathrm{d}x\]</span> 这里定义的级数的收敛性与一般Fourier级数中有类似的结果.</p><p>如果函数 <spanclass="math inline">\((1-x^{2})^{-1/4}f(x)\)</span>在区间 <spanclass="math inline">\(-1\leqslant x\leqslant1\)</span>上绝对可积，并且下列条件之一成立： -<strong>Dirichlet条件</strong>：<spanclass="math inline">\(f(x)\)</span>在 <spanclass="math inline">\(x_0\)</span>附近的一个区间上分段单调，并且在这区间上不连续点的个数至多是一个有限数.- <strong>Dini条件</strong>：对于某一常数 <spanclass="math inline">\(h&gt;0\)</span>，积分 <spanclass="math display">\[    \int_{0}^{h} \frac{\left\vert f(x_0+t)-f(x_0+0)+f(x_0-t)-f(x_0-0)\right\vert }{t} \mathrm{d}t\]</span> 存在 - <strong>Holder条件</strong>：<spanclass="math inline">\(f(x)\)</span>在 <spanclass="math inline">\(x_0\)</span>点连续，并且对于充分小的 <spanclass="math inline">\(t&gt;0\)</span>，不等式 <spanclass="math display">\[    \left\vert f(x_0 \pm t)-f(x_0) \right\vert \leqslant Lt ^{\alpha}\]</span> 成立，其中 <span class="math inline">\(L\)</span>和 <spanclass="math inline">\(\alpha\)</span>都是正的常数，且 <spanclass="math inline">\(\alpha\leqslant 1\)</span>；</p><p>那么（12）定义的级数在 $x=(-1&lt;&lt;1) $收敛到 <spanclass="math display">\[    \frac{1}{2}[f(\xi +0)+f(\xi -0)]\]</span> 特别，如果 <span class="math inline">\(f(x)\)</span>在 <spanclass="math inline">\(x=\xi\)</span>连续，则该级数在 <spanclass="math inline">\(x=\xi\)</span>处收敛到 <spanclass="math inline">\(f(\xi)\)</span>.</p><h2 id="广义幂级数解法">广义幂级数解法</h2><p>考虑（4）有奇点的情况.</p><p>首先由方程 <spanclass="math inline">\(x^{2}y&#39;&#39;-2y=0\)</span>在 <spanclass="math inline">\(x=0\)</span>附近的性态知方程（4）在奇点附近不会再有幂级数形式的通解且在奇点<span class="math inline">\(x_0\)</span>的初值问题可能是无解的.</p><p>求解 $x<sup>{2}y''+xy'+(x</sup>{2}-y)=0$知，解为两个线性无关的<strong>广义幂级数</strong>.</p><p><span class="math display">\[    \sum_{n=0}^{\infty} C_n(x-x_0)^{n+\rho}\quad(C_0 \neq 0)\]</span> 其中常数 <spanclass="math inline">\(\rho\)</span>叫作<strong>指标</strong>.</p><p><strong>正则奇点</strong>：设（4）可改写成如下形式： <spanclass="math display">\[    (x-x_0)^{2}P(x)y&#39;&#39;+(x-x_0)Q(x)y&#39;+R(x)y=0 \tag{13}\]</span> 其中 <span class="math inline">\(P(x)\)</span>，<spanclass="math inline">\(Q(x)\)</span> 和 <spanclass="math inline">\(R(x)\)</span>是多项式（或它们在 <spanclass="math inline">\(x_0\)</span>点附近可展成 <spanclass="math inline">\((x-x_0)\)</span> 的幂级数）；并且 <spanclass="math inline">\(P(x)\neq 0\)</span> ，而 <spanclass="math inline">\(Q(x_0)\)</span>和 <spanclass="math inline">\(R(x_0)\)</span>不同时等于零，则称 <spanclass="math inline">\(x_0\)</span>为微分方程（4）的<strong>正则奇点</strong>.</p><p><strong>定理7.3</strong>：微分方程（4）在正则奇点 <spanclass="math inline">\(x_0\)</span>的领域内有收敛的广义幂级数解 <spanclass="math display">\[    y=\sum_{k=0}^{\infty} C_k(x-x_0)^{k+\rho} \quad (C_0\neq 0) \tag{14}\]</span> 其中 <span class="math inline">\(\rho\)</span>和 <spanclass="math inline">\(C_k\)</span>由代入决定.</p><p><strong>证</strong>：在 <spanclass="math inline">\(x_0\)</span>的领域 <spanclass="math inline">\(\left\vert x-x_0 \right\vert &lt;r\)</span> 内将<span class="math inline">\(Q(x)\)</span>和 <spanclass="math inline">\(R(x)\)</span>展开为幂级数 <spanclass="math display">\[    Q(x)=\sum_{k=0}^{\infty} a_k(x-x_0)^{k} \quadR(x)=\sum_{k=0}^{\infty} b_k(x-x_0)^{k}       \]</span> 代入，也将（14）的形式解代入，可以整理得到递推公式.</p><p>其中第一式得<strong>指标方程</strong> <span class="math display">\[    \rho(\rho-1)+a_0\rho+b_0=0\]</span></p><p>当 <span class="math inline">\(\rho_1\)</span>和 <spanclass="math inline">\(\rho_2\)</span>这两个<strong>指标根</strong>为实数时，取<span class="math inline">\(\rho_1\geqslant \rho_2\)</span>下的 <spanclass="math inline">\(\rho_1\)</span>，否则任取一根.</p><p>这样就可以由递推公式得到 $C_1,C_2,,C_k,$.</p><p>需要证明（14）在 <spanclass="math inline">\(x_0\)</span>附近收敛（不一定包括 <spanclass="math inline">\(x_0\)</span>点）.</p><p>首先由数项级数的性质知对于取定的 <spanclass="math inline">\(0&lt;r_1&lt;r\)</span>存在 <spanclass="math inline">\(M&gt;0\)</span>（不妨 <spanclass="math inline">\(M\geqslant 1\)</span>）使得 $a_k $， $b_k $， $_1a_k+b_k $均不大于 <span class="math inline">\(\displaystyle\frac{M}{r_1^{k}}\)</span>.</p><p>由归纳法就可以证明 $C_k ()^{k} $.</p><p>于是（14）在 <span class="math inline">\(\displaystyle0&lt;\left\vert x-x_0 \right\vert \leqslant\frac{r_2}{M}\)</span>上收敛.</p><div class="note note-success">            <p>当 <spanclass="math inline">\(\rho_1\)</span>是复数时，可以从（14）分离出两个实的级数解</p>          </div><div class="note note-success">            <p>当 <span class="math inline">\(\rho_1\)</span>和 <spanclass="math inline">\(\rho_2\)</span>都是实数且 <spanclass="math inline">\(\rho_1-\rho_2=m \in\mathbb{N}\)</span>时，一般不能从 <spanclass="math inline">\(\rho_2\)</span>出发得到一个与（14）不同的广义幂级数解.但可以利用Liouville公式，从与 <spanclass="math inline">\(\rho_1\)</span>相应的（14）出发，得到另一个与其线性无关的解.</p>          </div><p><strong>Bessel方程</strong>： <span class="math display">\[    x^{2}y&#39;&#39;+xy&#39;+(x^{2}-n^{2})y=0 \tag{15}\]</span> 其中常数 <span class="math inline">\(n\geqslant 0\)</span>它的两个指标根为 <span class="math inline">\(\pm n\)</span>，对于 <spanclass="math inline">\(\rho_1=n\)</span>，取一个比较好的 <spanclass="math inline">\(C_0\)</span> <span class="math display">\[    C_0=\frac{1}{2^{n}\Gamma(n+1)}\]</span> 则 <span class="math display">\[    y=J_n(x)=\sum_{k=0}^{\infty}\frac{(-1)^{k}}{\Gamma(n+k+1)\Gamma(k+1)}(\frac{x}{2})^{2k+n} \tag{16}\]</span>它的收敛半径是无穷大，它叫<strong>第一类Bessel函数</strong>.</p><p>对于 <span class="math inline">\(\rho_2=-n\)</span> - 若 <spanclass="math inline">\(2n\)</span>不等于任何整数 类似有 <spanclass="math display">\[    y=J_{-n}(x)=\sum_{k=0}^{\infty}\frac{(-1)^{k}}{\Gamma(-n+k+1)\Gamma(k+1)}(\frac{x}{2})^{2k-n} \tag{17}\]</span> 它叫做<strong>第二类Bessel函数</strong>. - <spanclass="math inline">\(2n=N\in \mathbb{Z}\)</span> 1）若 <spanclass="math inline">\(2n=2s+1\)</span>，此时令 <spanclass="math inline">\(C_{2s+1}=0\)</span>则得到一个同（17）的解 2）若<span class="math inline">\(n \in \mathbb{Z}\)</span> 此时不可能从 <spanclass="math inline">\(\rho_2=-n\)</span>和递推公式求出方程的广义幂级数解.为求解，取参数 <span class="math inline">\(\alpha \neq n\)</span>但<span class="math inline">\(\alpha \rightarrow n\)</span>，则 <spanclass="math inline">\(J_{\alpha}(x)\)</span>和 <spanclass="math inline">\(J_{-\alpha}(x)\)</span>都有意义，且函数 <spanclass="math display">\[    y_{\alpha}(x)=\frac{J_{\alpha}\cos \alpha\pi-J_{-\alpha}(x)}{\sin\alpha\pi} \quad (\sin \alpha\pi\neq 0)\]</span> 是Bessel方程当 <spanclass="math inline">\(n=\alpha\)</span>时的一个解，且它在 <spanclass="math inline">\(x=0\)</span>的领域内是无界的. 令 <spanclass="math display">\[    Y_{n}(x)=\lim_{\alpha \to n}y_{\alpha}(x)\]</span> 则 <spanclass="math inline">\(y=Y_{n}(x)\)</span>是Bessel方程的解，且与第一类Bessel函数<spanclass="math inline">\(J_{n}(x)\)</span>线性无关，称为<strong>Neumann函数</strong>.</p><p>解释一下其中的一些性质 - 当 <span class="math inline">\(n \in\mathbb{Z}\)</span>时，有 <spanclass="math inline">\(J_{-n}(x)=(-1)^{n}J_n(x)\)</span>. 事实上由于<span class="math inline">\(\Gamma(-n+k+1)\to \infty\)</span>，故 <spanclass="math display">\[    \begin{aligned}        J_{-n}(x)&amp;= \sum_{k=n}^{\infty}\frac{(-1)^{k}}{\Gamma(-n+k+1)\Gamma(k+1)}(\frac{x}{2})^{2k-n} \\        &amp;= \sum_{k=0}^{\infty}\frac{(-1)^{k+n}}{\Gamma(n+k+1)\Gamma(k+1)}(\frac{x}{2})^{2k+n} \\        &amp;= (-1)^{n}J_n(x)    \end{aligned}\]</span> - 一个中间结论： <span class="math display">\[    J_{v}J_{-v}&#39;-J_{-v}J_{v}&#39;=-\frac{2\sin v\pi}{\pi x}\]</span> <strong>证</strong>： - 关于 <spanclass="math inline">\(Y_n(x)\)</span>的构造，这部分可以看《特殊函数概论》中关于第二类Bessel函数的相关章节.证明极限存在和 <spanclass="math inline">\(Y_n(x)\)</span>确实是方程解，可以参考Watson(1944), pp. 58~59.</p><h2 id="bessel-函数">Bessel 函数</h2><p>下面都假设 <span class="math inline">\(n\)</span>为非负整数. 称 <spanclass="math inline">\(J_n(x)\)</span>为 <spanclass="math inline">\(n\)</span><strong>阶Bessel函数</strong>；称 <spanclass="math inline">\(Y_n(x)\)</span>为 <spanclass="math inline">\(n\)</span><strong>阶Neumann函数</strong>.</p><p><strong>性质1</strong>：当 <span class="math inline">\(x \to\infty\)</span>时， <span class="math inline">\(J_n(x)\)</span>和 <spanclass="math inline">\(Y_n(x)\)</span>有如下<strong>渐进式</strong>：<span class="math display">\[    J_n(x)=\frac{A_n}{\sqrt{x}}[\sin (x+\alpha_n)+o(1)] \tag{18}        \]</span> 和 <span class="math display">\[    Y_n(x)=\frac{B_n}{\sqrt{x}}[\cos (x+\beta_n)+o(1)] \tag{19}\]</span></p><p>其中 <span class="math inline">\(o(1)\)</span>表示一个无穷小量，而<span class="math inline">\(A_n,B_n,\alpha_n,\beta_n\)</span>都是只与<span class="math inline">\(n\)</span>有关的常数，且 <spanclass="math inline">\(A_n&gt;0,B_n&gt;0\)</span>.</p><p><strong>证</strong>：作代换 <span class="math display">\[    J_n(x)=\frac{u(x)}{\sqrt{x}} \quad (x&gt;0)\]</span> 代入Bessel方程后得到一个二阶线性微分方程的非零解 <spanclass="math display">\[    u&#39;&#39;(x)+  \biggl( 1-\frac{n^{2}-\frac{1}{4}}{x^{2}}\biggr)u(x)=0\]</span> 由解的唯一性知 <span class="math inline">\(u(x)\)</span>和<span class="math inline">\(u&#39;(x)\)</span>不同时为零，则 <spanclass="math display">\[    r(x)=\sqrt{[u(x)]^{2}+[u&#39;(x)]^{2}}&gt;0\]</span> 把 <span class="math inline">\(u(x)\)</span>和 <spanclass="math inline">\(u&#39;(x)\)</span>表达成极坐标的形式 <spanclass="math display">\[    \begin{cases}        u(x)=r(x)\sin \theta(x) \\        u&#39;(x)=r(x)\cos \theta(x)        \end{cases}\]</span> 再代回得到关于 <span class="math inline">\(r(x)\)</span>和<span class="math inline">\(\theta(x)\)</span>的微分方程.</p><p><span class="math display">\[    \begin{cases}        r&#39;(x) =\frac{n^{2}-\frac{1}{4}}{x^{2}}r(x)\sin \theta(x)\cos \theta(x) \\        \theta&#39;(x)=1-\frac{n^{2}-\frac{1}{4}}{x^{2}}\sin^{2}\theta(x)    \end{cases}\]</span></p><p>令 <spanclass="math inline">\(\theta(x)=x+\varphi(x)\)</span>，代回上面的第二个方程，并取从1到 <span class="math inline">\(x\)</span>的积分，可以得到极限 <spanclass="math inline">\(\lim_{x \to\infty}\varphi(x)=\alpha_n\)</span>存在. 从而得到渐进式 <spanclass="math display">\[    \theta(x)=x+\alpha_n+o(1)\]</span> 再对上面方程的第一式积分（注意它有通积分），得极限 <spanclass="math display">\[    \lim_{x \to \infty}r(x)=A_n&gt;0\]</span> 也存在. 故 <span class="math inline">\(r(x)\)</span>得渐进式为<span class="math display">\[    r(x)=A_n+o(!)\]</span> 将上两个渐进式代回极坐标形式知 <span class="math display">\[    \begin{cases}        u(x)=A_n\sin (x+\alpha_n) +o(1) \\        u&#39;(x)=A_n\cos (x+\alpha_n) +o(1)            \end{cases}\]</span> 从而有原渐进式成立.并且计算可以发现求导与求渐进可以交换次序.</p><p>渐进式的一个意义在于可以帮助我们分析零点的性质. <spanclass="math inline">\(J_n(x)\)</span>和 <spanclass="math inline">\(Y_n(x)\)</span>都有无穷多个零点，而且它们的零点<strong>互相交错</strong>（留作以后证明）.可以推得 - <span class="math inline">\(J_0(0)=1\)</span> - <spanclass="math inline">\(J_n(0)=0 \quad n\geqslant 1\)</span> - <spanclass="math inline">\(\lim_{x \to o^{+}}Y_n(x)=-\infty\)</span></p><p>这也说明 <span class="math inline">\(J_n(x)\)</span>和 <spanclass="math inline">\(Y_n(x)\)</span>线性无关.</p><p>设 <spanclass="math inline">\(J_n(x)\)</span>的无穷多个零点依次排列为 <spanclass="math display">\[    0&lt;\beta_1&lt;\beta_2&lt;\cdots &lt;\beta_k&lt;\cdots (\to \infty)\]</span> 它们与 <span class="math inline">\(n\)</span>有关.</p><p>在区间 <span class="math inline">\([0,1]\)</span>上考虑函数系 <spanclass="math display">\[    J_n(\beta_1t),J_n(\beta_2t),\cdots ,J_n(\beta_kt),\cdots \tag{20}\]</span> 则有 <strong>性质2</strong>：函数系（20）在区间 <spanclass="math inline">\([0,1]\)</span>上是一个以 <spanclass="math inline">\(t\)</span>为权的<strong>正交系</strong>，即 <spanclass="math display">\[    \int_{0}^{1} t J_n(\beta_jt)J_n(\beta_kt) \mathrm{d}t=    \begin{cases}        0,\quad j\neq k \\        \tau_{n,k}&gt;0,\quad j=k    \end{cases} \tag{21}\]</span> 并且可以算出 <spanclass="math inline">\(\tau_{n,k}=\frac{1}{2}[J_n&#39;(\beta_k)]^{2}\)</span></p><p><strong>证</strong>：令 <span class="math display">\[    u=J_n(at), \quad v=J_n(bt)\]</span> 代入Bessel方程，然后消去 <spanclass="math inline">\(n^{2}\)</span>得 <span class="math display">\[    t^{2}(vu&#39;&#39;-uv&#39;&#39;)+t(vu&#39;-u&#39;v)+t^{2}(a^{2}-b^{2})uv=0\]</span> 凑积分，化为 <span class="math display">\[    \frac{\mathrm{d}}{\mathrm{d}t}[t(vu&#39;-uv&#39;)]+t(a^{2}-b^{2})uv=0\]</span> 积分上式，并注意 <spanclass="math inline">\(u(0),v(0),u&#39;(0),v&#39;(0)\)</span>有界，得到<span class="math display">\[    (a^{2}-b^{2})\int_{0}^{1} tuv\mathrm{d}t=bJ_n(a)J_n&#39;(b)-aJ_n(b)J_n&#39;(a) \tag{22}\]</span> 得到（21）的第一部分. 在（22）中取 <spanclass="math inline">\(b=\beta_k\)</span>和 <spanclass="math inline">\(a\neq \beta_k\)</span>，再令 <spanclass="math inline">\(a\to \beta_k\)</span>，运用L'Hospital法则就得到（21）的第二部分.</p><p>考虑区间 <span class="math inline">\([0,1]\)</span>上可积的 <spanclass="math inline">\(f(x)\)</span>关于Bessel正交函数系的如下展开 <spanclass="math display">\[    f(x) ~ \sum_{k=1}^{\infty} a_kJ_n(\beta_kx) \tag{23}\]</span> 其中广义Fourier系数 <span class="math display">\[    a_k=\frac{2}{[J_n&#39;(\beta_k)]^{2}}\int_{0}^{1} tf(t)J_n(\beta_kt)\mathrm{d}t\]</span></p><div class="note note-success">            <p>如果 <span class="math inline">\(\sqrt{x}f(x)\)</span>在 <spanclass="math inline">\(0\leqslant x\leqslant1\)</span>上绝对可积，且满足Dirichlet/Dini/Holder条件之一，则右侧的广义Fourier级数在<spanclass="math inline">\(x=x_0\)</span>点收敛到两单侧极限的平均值；则当<span class="math inline">\(f(x)\)</span>在 <spanclass="math inline">\(x=x_0\)</span>处连续时，该级数收敛到 <spanclass="math inline">\(f(x_0)\)</span></p>          </div>]]></content>
    
    
    <categories>
      
      <category>分析学</category>
      
      <category>微分方程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>常微分方程（6）</title>
    <link href="/2022/04/12/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%886%EF%BC%89/"/>
    <url>/2022/04/12/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%886%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="线性微分方程组">线性微分方程组</h1><h2 id="一般理论">一般理论</h2><p>考虑标准形式的 <span class="math inline">\(n\)</span>阶线性微分方程组<span class="math display">\[    \frac{\mathrm{d}y_i}{\mathrm{d}x}=\sum_{j=1}^{n} a_{ij}(x)y_j+f_i(x)\quad (i=1,2,\cdots ,n)\]</span> 其中系数函数 <span class="math inline">\(a_{ij}(x)\)</span>和<span class="math inline">\(f_i(x)(i,j=1,2,\cdots ,n)\)</span>在区间<span class="math inline">\(a&lt;x&lt;b\)</span>上都是连续的. 令矩阵<span class="math display">\[    \mathbf{A}(x)=(a_{ij}(x))_{n \times n}\]</span> 和向量 <span class="math display">\[    \mathbf{y}=\begin{pmatrix}        y_1 \\        y_2 \\        \vdots \\        y_n    \end{pmatrix}    , \quad    \mathbf{f}(x)=\begin{pmatrix}        f_1(x) \\        f_2(x) \\        \vdots \\        f_n(x)    \end{pmatrix}\]</span> 就可以将线性微分方程组写成向量形式 <spanclass="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{A}(x)\mathbf{y}+\mathbf{f}(x)\tag{1}\]</span> 当 <spanclass="math inline">\(\mathbf{f}(x)\)</span>不恒为零<spanclass="math inline">\((a&lt;x&lt;b)\)</span>时，称（1）是<strong>非齐次</strong>的线性微分方程组；当<span class="math inline">\(\mathbf{f}(x)\equiv\mathbf{0}\)</span>时，亦即 <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{A}(x)\mathbf{y}\tag{2}\]</span> 称它是（相应）<strong>齐次</strong>的线性微分方程组.</p><p>注意由 <spanclass="math inline">\(\mathbf{A}(x)\)</span>的连续性可知，在 <spanclass="math inline">\(x\)</span>的任意有限闭区间上，函数 <spanclass="math inline">\(\mathbf{A}(x)\mathbf{y}\)</span>对 <spanclass="math inline">\(\mathbf{y}\)</span>满足Lipschitz条件.</p><p><strong>存在和唯一性定理</strong>：线性微分方程组（1）满足初值条件<span class="math display">\[    \mathbf{y}(x_0)=\mathbf{y}_0 \tag{3}\]</span> 的解 <spanclass="math inline">\(\mathbf{y}=\mathbf{y}(x)\)</span>在区间 <spanclass="math inline">\(a&lt;x&lt;b\)</span>上是存在和唯一的，其中初值<span class="math inline">\(x_0 \in (a,b)\)</span>和 <spanclass="math inline">\(\mathbf{y}_0 \in\mathbb{R}^n\)</span>是任意给定的.</p><h3 id="齐次线性微分方程组">齐次线性微分方程组</h3><p><strong>引理6.1</strong>：齐次线性微分方程组的解满足线性性.</p><p>令（2）在区间 <spanclass="math inline">\((a,b)\)</span>上所有的解所组成的集合为 <spanclass="math inline">\(S\)</span>. 显然 <spanclass="math inline">\(S\)</span>是一个线性空间.</p><p><strong>引理6.2</strong>：线性空间 <spanclass="math inline">\(S\)</span>是 <spanclass="math inline">\(n\)</span>维的（<spanclass="math inline">\(n\)</span>是（2）的阶数）.</p><p><strong>证</strong>：作初值到 <spanclass="math inline">\(S\)</span>的同构.</p><p><strong>定理6.1</strong>：齐次线性微分方程组（2）在区间 <spanclass="math inline">\(a&lt;x&lt;b\)</span>上有 <spanclass="math inline">\(n\)</span>个线性无关的解 <spanclass="math display">\[    \mathbf{\varphi}_1,\mathbf{\varphi}_2,\cdots,\mathbf{\varphi}_n\tag{4}\]</span> 而且它的通解为 <span class="math display">\[    \mathbf{y}=C_1\mathbf{\varphi}_1(x)+\cdots +C_n\mathbf{\varphi}_n(x)\tag{5}\]</span> 其中 <span class="math inline">\(C_1,\cdots,C_n\)</span>是任意常数.</p><p><strong>证</strong>：利用引理6.2可得 <spanclass="math inline">\(S\)</span>的一个基，它的线性组合就生成整个线性空间<span class="math inline">\(S\)</span>.</p><p>称（2）的 <spanclass="math inline">\(n\)</span>个线性无关的解为一个<strong>基本解组</strong>.假设已知 <span class="math display">\[    \mathbf{y}_1(x),\cdots ,\mathbf{y}_n(x) \tag{6}\]</span> 是（2）的 <span class="math inline">\(n\)</span>个解.有一个在理论上比较简明的判别法</p><p><strong>定理6.2</strong>：设在（6）中诸解的分量形式为 <spanclass="math display">\[    \mathbf{y}_1(x)=\begin{pmatrix}    y_{11}(x) \\    y_{21}(x) \\    \vdots \\    y_{n1}(x)    \end{pmatrix},    \cdots    \mathbf{y}_n(x)=\begin{pmatrix}    y_{1n}(x) \\    y_{2n}(x) \\    \vdots \\    y_{nn}(x)    \end{pmatrix}\]</span> 称行列式 <span class="math display">\[    \begin{vmatrix}    y_{11}(x) &amp; y_{12}(x) &amp; \cdots  &amp; y_{1n}(x) \\    \vdots    &amp; \vdots    &amp;         &amp; \vdots \\    y_{n1}(x) &amp; y_{n2}(x) &amp; \cdots  &amp; y_{nn}(x) \\    \end{vmatrix}\]</span> 为解组（6）的<strong>朗斯基(Wronsky)行列式</strong>.</p><p><strong>引理6.3</strong>：上述Wronsky行列式满足下面的<strong>Liouville公式</strong>：<span class="math display">\[    W(x)=W(x_0) \mathrm{e}^ {\int_{x_0}^{x} \mathrm{tr} [\mathbf{A}(x)]\mathrm{d}x} \quad (a&lt;x&lt;b) \tag{7}\]</span> 其中 <span class="math inline">\(x_0 \in (a,b)\)</span>，<strong>证</strong>：利用行列式的求导公式得到 <spanclass="math display">\[    \mathbf{W}&#39;=\mathrm{tr}[\mathbf{A}(x)]W\]</span> 这是关于 <spanclass="math inline">\(W\)</span>的一阶线性微分方程. 由此解出 <spanclass="math inline">\(W\)</span>.</p><p>【附注】由Liouville公式知，Wronsky行列式 <spanclass="math inline">\(W(x)\)</span>在区间 <spanclass="math inline">\(a&lt;x&lt;b\)</span>上要么恒为零，要么恒不为零.</p><p><strong>定理6.2</strong>：线性微分方程组（2）的解组（6）是线性无关的充要条件为<span class="math display">\[    W(x) \neq 0 \quad (a&lt;x&lt;b) \tag{8}\]</span></p><p><strong>证</strong>：用引理6.2中的同构即可.</p><p><strong>推论6.1</strong>：解组（6）是线性相关的充要条件为 <spanclass="math display">\[    W(x) \equiv 0 \quad (a&lt;x&lt;b)\]</span></p><p>而这也等价于在某一特殊点 <span class="math inline">\(x_0\)</span>的<span class="math inline">\(W(x_0)=0\)</span>.</p><p>对于解组（6），令矩阵 <spanclass="math inline">\(\mathbf{Y}(x)=(y_{ij}(x))_{n \timesn}\)</span>，它叫作方程组（2）的<strong>解矩阵</strong>. 易知 <spanclass="math inline">\(\mathbf{Y}(x)\)</span>是方程组（2）的矩阵解.</p><p>当（6）是一个基本解组时，称相应的解矩阵 <spanclass="math inline">\(\mathbf{Y}(x)\)</span>为一个<strong>基本解矩阵</strong>.则若已知（2）的一个基解矩阵 <spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>，则由定理6.1知，它的通解为<span class="math display">\[    \mathbf{y}=\mathbf{\Phi}(x)\mathbf{c} \tag{9}\]</span> 其中 <span class="math inline">\(\mathbf{c}\)</span>是 <spanclass="math inline">\(n\)</span>维的任意常数列向量.</p><p><strong>推论6.2</strong>：（1）设 <spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>是方程组（2）的一个基解矩阵，则对于任一个非奇异的<span class="math inline">\(n\)</span>阶常数矩阵 <spanclass="math inline">\(\mathbf{C}\)</span>，矩阵 <spanclass="math display">\[    \mathbf{\Psi}(x)=\mathbf{\Phi}(x) \mathbf{C} \tag{10}\]</span> 也是（2）的一个基解矩阵；</p><p>（2）设 <span class="math inline">\(\mathbf{\Phi}(x)\)</span>和 <spanclass="math inline">\(\mathbf{\Psi}(x)\)</span>都是方程组（2）的基解矩阵，则必存在一个非奇异的<span class="math inline">\(n\)</span>阶常数矩阵 <spanclass="math inline">\(\mathbf{C}\)</span>，使得（10）成立.</p><p><strong>证</strong>：这几乎是显然的，就是个线性方程组.</p><h3 id="非齐次线性微分方程组">非齐次线性微分方程组</h3><p><strong>引理6.4</strong>：（2）的一个基解是 <spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>，<spanclass="math inline">\(\mathbf{\phi}^*(x)\)</span>是（1）的一个特解，则（1）的任意解<spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x)\)</span>可以表示为<span class="math display">\[    \mathbf{\varphi}(x)=\mathbf{\Phi}(x)\mathbf{c}+\mathbf{\varphi}^*(x)\]</span> 其中 <span class="math inline">\(\mathbf{c}\)</span>是一个与<spanclass="math inline">\(\mathbf{\varphi}(x)\)</span>有关的常数列向量.</p><p>事实上，利用<strong>常数变易法</strong>，只要知道 <spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>就够了.</p><p>可以知道 <span class="math display">\[    \mathbf{\varphi}^*(x)=\mathbf{\Phi}(x)\int_{x_0}^{x}\mathbf{\Phi}^{-1}(s)\mathbf{f}(s) \mathrm{d}s \tag{11}\]</span> 是（1）的一个特解.</p><p><strong>引理6.5</strong>：设 <spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>是（2）的一个基解矩阵，则（11）式给出非齐次线性微分方程组（1）的一个特解.</p><p><strong>定理6.3</strong>：（1）的通解可以表示为 <spanclass="math display">\[    \mathbf{y}=\mathbf{\Phi}(x) \biggl( \mathbf{c}+\int_{x_0}^{x}\mathbf{\Phi}^{-1}(s)\mathbf{f}(s) \mathrm{d}s \biggr) \tag{12}\]</span> 其中 <span class="math inline">\(\mathbf{c}\)</span>是 <spanclass="math inline">\(n\)</span>维的任意常数列向量；而且（1）满足初值条件<span class="math inline">\(\mathbf{y}(x_0)=\mathbf{y}_0\)</span> 的解为<span class="math display">\[    \mathbf{y}=\mathbf{\Phi}(x)\mathbf{\Phi}^{-1}(x_0)\mathbf{y}_0+\mathbf{\Phi}(x)\int_{x_0}^{x} \mathbf{\Phi}^{-1}(x)\mathbf{f}(s) \mathrm{d}s \tag{13}\]</span> 其中 <span class="math inline">\(x_0 \in (a,b)\)</span>.</p><div class="note note-success">            <p>一般来说， <spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>比较难解.但其仍有很重要的理论意义.</p>          </div><h2 id="常系数线性微分方程组">常系数线性微分方程组</h2><p><strong>常系数线性微分方程组</strong>：指的是 <spanclass="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{A}\mathbf{y}+\mathbf{f}(x)\tag{14}\]</span> 其中系数矩阵 <span class="math inline">\(\mathbf{A}\)</span>为<span class="math inline">\(n\)</span>阶<strong>常数矩阵</strong>，而<span class="math inline">\(\mathbf{f}(x)\)</span>是在 <spanclass="math inline">\(a&lt;x&lt;b\)</span>上连续的向量函数.</p><p>相应的线性齐次微分方程组为 <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{A}\mathbf{y}\tag{15}\]</span></p><p>当 <span class="math inline">\(n\)</span>=1时，相应的线性齐次微分方程组成为 <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=ay\]</span> 其通解为 <spanclass="math inline">\(y=C\mathrm{e}^{ax}\)</span>，我们尝试推广到高维情形.</p><h3 id="矩阵指数函数">矩阵指数函数</h3><p><strong>命题1</strong>：矩阵 <spanclass="math inline">\(\mathbf{A}\)</span>的幂级数 <spanclass="math display">\[    \mathbf{E}+\mathbf{A}+\frac{1}{2!}\mathbf{A}^{2}+\cdots+\frac{1}{k!}\mathbf{A}^{k}+\cdots\]</span> 是绝对收敛的. 以记号 <spanclass="math inline">\(\mathrm{e}^{\mathbf{A}}\)</span>（或 <spanclass="math inline">\(\exp \mathbf{A}\)</span>）表示上述矩阵幂级数的和.它也是一个矩阵.</p><p><strong>命题2</strong>：关于矩阵指数函数的性质 - 若 <spanclass="math inline">\(\mathbf{AB}=\mathbf{BA}\)</span>，则 <spanclass="math display">\[    \mathrm{e}^{\mathbf{A}+\mathbf{B}}=\mathrm{e}^{\mathbf{A}}+\mathrm{e}^{\mathbf{B}}    \]</span> - 对任何矩阵 <spanclass="math inline">\(\mathbf{A}\)</span>，<spanclass="math inline">\(\mathrm{e}^{\mathbf{A}}\)</span>可逆且 <spanclass="math display">\[    (\mathrm{e}^{\mathbf{A}})^{-1}=\mathrm{e}^{-\mathbf{A}}     \]</span> - 若 <spanclass="math inline">\(\mathbf{P}\)</span>是一个非奇异的 <spanclass="math inline">\(n\)</span>阶矩阵，则 <span class="math display">\[    \mathrm{e}^{\mathbf{PAP^{-1}}}=\mathbf{P}\mathrm{e}^{\mathbf{A}}\mathbf{P}^{-1}\]</span> 证明略</p><h3id="常系数齐次微分方程组的基解矩阵">常系数齐次微分方程组的基解矩阵</h3><p><strong>定理6.4</strong>：<spanclass="math inline">\(\mathbf{\Phi}(x)=\mathrm{e}^{x\mathbf{A}}\)</span>是（15）的一个<strong>标准基解矩阵</strong>（即<span class="math inline">\(\mathbf{\Phi}(0)=\mathbf{E}\)</span>）</p><p>证明略</p><p><strong>推论6.3</strong>：常系数非齐次线性微分方程组（14）在区间<span class="math inline">\((a,b)\)</span>上的通解为 <spanclass="math display">\[    \mathbf{y}=\mathrm{e}^{x \mathbf{A}}\mathbf{c}+\int_{x_0}^{x}\mathrm{e}^{(x-s) \mathbf{A}} \mathbf{f}(s) \mathrm{d}s \tag{16}\]</span></p><p>其中 <spanclass="math inline">\(\mathbf{c}\)</span>为一任意的常数列向量；而（14）满足初值条件<span class="math inline">\(\mathbf{y}(x_0)=\mathbf{y}_0\)</span>的解为<span class="math display">\[    \mathbf{y}=\mathrm{e}^{(x-x_0)\mathbf{A}}\mathbf{y}_0+\int_{x_0}^{x} \mathrm{e}^{(x-s) \mathbf{A}}\mathbf{f}(s) \mathrm{d}s \tag{17}\]</span> 其中 <span class="math inline">\(x_0 \in (a,b)\)</span></p><h3 id="利用jordan标准型求基解矩阵">利用Jordan标准型求基解矩阵</h3><p>对每个 <span class="math inline">\(n\)</span>阶矩阵 <spanclass="math inline">\(\mathbf{A}\)</span>，存在一个 <spanclass="math inline">\(n\)</span>阶非奇异矩阵 <spanclass="math inline">\(\mathbf{P}\)</span>，使得 <spanclass="math display">\[    \mathbf{A}=\mathbf{PJP}^{-1}\]</span> 其中 <span class="math inline">\(J\)</span>为Jordan标准型</p><p>此时有 <span class="math display">\[    \mathrm{e}^{x \mathbf{A}}=\mathrm{e}^{x\mathbf{PJP}^{-1}}=\mathbf{P} \mathrm{e}^{x\mathbf{J}}\mathbf{P}^{-1}      \]</span> 而 <span class="math inline">\(\mathrm{e}^{x\mathbf{A}}\mathbf{P} = \mathbf{P} \mathrm{e}^{x\mathbf{J}}\)</span>也是一个基解矩阵.</p><p>然而 <span class="math inline">\(J\)</span>和 <spanclass="math inline">\(P\)</span>都很难求.</p><h3 id="待定指数函数法">待定指数函数法</h3><p>尝试把（16）应用于<strong>待定系数法</strong></p><p>（一）<span class="math inline">\(A\)</span>只有单的特征根 此时由<span class="math display">\[    \mathbf{\Phi}(x)=\mathrm{e}^{x \mathbf{A}} \mathbf{P}=\mathbf{P}    \begin{pmatrix}        \mathrm{e}^{\lambda_1x} &amp;  &amp;  &amp;  \\         &amp; \mathrm{e}^{\lambda_2x} &amp; &amp; \\         &amp; &amp; \ddots &amp; \\         &amp; &amp; &amp; \mathrm{e}^{\lambda_nx}    \end{pmatrix}\]</span> 且 <spanclass="math inline">\(\mathbf{\Phi}(0)=\mathbf{P}\)</span>. 由此可见<span class="math display">\[    \mathrm{e}^{x \mathbf{A}}=\mathbf{\Phi}(x)\mathbf{\Phi}^{-1}(0)\tag{18}\]</span> 令 <span class="math inline">\(\mathbf{r}_i\)</span>表示 <spanclass="math inline">\(\mathbf{P}\)</span>的第 <spanclass="math inline">\(i\)</span>列的向量，则基解矩阵 <spanclass="math display">\[    \mathbf{\Phi}(x)=(\mathrm{e}^{\lambda_1x}\mathbf{r}_1,\mathrm{e}^{\lambda_2x}\mathbf{r}_2,\cdots,\mathrm{e}^{\lambda_nx}\mathbf{r}_n)\]</span> 下面寻找 <spanclass="math inline">\(\mathbf{r}_i\)</span>的方法</p><p><strong>引理</strong>：微分方程组（15）有非零解 <spanclass="math inline">\(\mathbf{y}=\mathrm{e}^{\lambdax}\mathbf{r}\)</span>，当且仅当 <spanclass="math inline">\(\lambda\)</span>是矩阵 <spanclass="math inline">\(\mathbf{A}\)</span>的特征根，而 <spanclass="math inline">\(\mathbf{r}\)</span>是与 <spanclass="math inline">\(\lambda\)</span>相应的特征向量.</p><p><strong>定理6.5</strong>：设 <spanclass="math inline">\(n\)</span>阶矩阵 <spanclass="math inline">\(\mathbf{A}\)</span>有 <spanclass="math inline">\(n\)</span>个互不相同的特征根 <spanclass="math inline">\(\lambda_1,\lambda_2,\cdots,\lambda_n\)</span>，则矩阵函数<span class="math display">\[    \mathbf{\Phi}(x)=(\mathrm{e}^{\lambda_1x}\mathbf{r}_1,\mathrm{e}^{\lambda_2x}\mathbf{r}_2,\cdots,\mathrm{e}^{\lambda_nx}\mathbf{r}_n)\]</span> 是（15）的一个基解矩阵，其中 <spanclass="math inline">\(\mathbf{r}_i\)</span>是 <spanclass="math inline">\(\mathbf{A}\)</span>的与 <spanclass="math inline">\(\lambda_i\)</span>相应特征向量.</p><p><strong>证</strong>：注意不同特征根的特征向量线性无关.</p><p>我们可以加强定理6.5 <strong>定理6.5</strong>*：将 <spanclass="math inline">\(\mathbf{r}_1,\mathbf{r}_2,\cdots,\mathbf{r}_n\)</span>改为矩阵<span class="math inline">\(\mathbf{A}\)</span>的 <spanclass="math inline">\(n\)</span>个线性无关的特征向量，它们对应的特征根不必互不相同.</p><p>另外，定理6.5中的 <spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>可能是复矩阵，可以利用公式（18）求出实矩阵<span class="math inline">\(\mathrm{e}^{x \mathbf{A}}\)</span></p><p>假设（15）有一个复值解 <span class="math display">\[    \mathbf{y}_1=\mathbf{u}(x)+i \mathbf{v}(x)\]</span> 则 <span class="math display">\[    \mathbf{y}_2=\mathbf{u}(x)-i \mathbf{v}(x)\]</span> 也是 (15)的一个复值解. 则它们的实部和复部都是（15）的解.</p><p>（二） <span class="math inline">\(\mathbf{A}\)</span>有相重的特征根在（15）的基解矩阵 <span class="math inline">\(\mathrm{e}^{x\mathbf{A}}\mathbf{P}\)</span>的所有列向量中，与 <spanclass="math inline">\(\lambda_i\)</span>相关的 <spanclass="math inline">\(n_i\)</span>列都具有下列形式 <spanclass="math display">\[    \mathbf{y}=e^{\lambda_i x}\biggl(\mathbf{r}_0+\frac{x}{1!}\mathbf{r}_1+\cdots+\frac{x^{n_i-1}}{(n_i-1)!}\mathbf{r}_{n_i-1} \biggr) \tag{19}\]</span> 其中 <span class="math inline">\(\mathbf{r}_j(j=0,1,\cdots,n_i-1)\)</span>是 <spanclass="math inline">\(n\)</span>维常数列向量.</p><p><strong>引理6.7</strong>：设 <spanclass="math inline">\(\lambda_i\)</span>是矩阵 <spanclass="math inline">\(\mathbf{A}\)</span>的 <spanclass="math inline">\(n_i\)</span>重特征根，则（15）有形如（19）的非零解的充要条件是：<span class="math inline">\(\mathbf{r}_0\)</span>是齐次线性代数方程组<span class="math display">\[    (\mathbf{A}-\lambda_i \mathbf{E})^{n_i}\mathbf{r}=\mathbf{0}\tag{20}\]</span> 的一个非零解，而且（19）中的 <spanclass="math inline">\(\mathbf{r}_1,\mathbf{r}_2,\cdots,\mathbf{r}_n\)</span>是由下面的关系式逐次确定的：<span class="math display">\[    \begin{cases}        \mathbf{r}_1=(\mathbf{A}-\lambda_i \mathbf{E})\mathbf{r}_0 \\        \mathbf{r}_2=(\mathbf{A}-\lambda_i \mathbf{E})\mathbf{r}_1 \\        \cdots \cdots  \\        \mathbf{r}_{n_i-1}=(\mathbf{A}-\lambda_i\mathbf{E})\mathbf{r}_{n_i-2}    \end{cases} \tag{21}\]</span></p><p>证明略</p><p><strong>定理6.6</strong>：设 <spanclass="math inline">\(n\)</span>阶实矩阵 <spanclass="math inline">\(\mathbf{A}\)</span>在 <spanclass="math inline">\(\mathbb{C}\)</span>中的互不相同的特征根为 <spanclass="math inline">\(\lambda_1,\lambda_2,\cdots,\lambda_s\)</span>，它们的重数分别是<spanclass="math inline">\(n_1,n_2,\cdots,n_s\)</span>，则常系数齐次线性微分方程组（15）有基解矩阵<span class="math inline">\(\mathbf{\Phi}(x)\)</span>为 <spanclass="math display">\[    \biggl( \mathrm{e}^{\lambda_1x}\mathbf{P}_1^{(1)}(x),\cdots,\mathrm{e}^{\lambda_1x}\mathbf{P}_{n_1}^{1}(x);\cdots;\mathrm{e}^{\lambda_s x}\mathbf{P}_1^{(s)}(x),\cdots,\mathrm{e}^{\lambda_s x}\mathbf{P}_{n_s}^{(s)}(x) \biggr) \tag{22}\]</span> 其中 <span class="math display">\[    \mathbf{P}_j^{(i)}(x)=\mathbf{r}_{j0}^{(i)}+\frac{x}{1!}\mathbf{r}_{j1}^{(i)}+\frac{x^{2}}{2!}\mathbf{r}_{j2}^{(i)}+\cdots+\frac{x^{n_i-1}}{(n_i-1)!}\mathbf{r}_{jn_{i-1}}^{(i)} \tag{23}\]</span> 是与 <span class="math inline">\(\lambda_i\)</span>对应的第<span class="math inline">\(j\)</span>个向量多项式 <spanclass="math inline">\((i=1,2,\cdots ,s;j=1,2,\cdots ,n_i)\)</span>，而<span class="math inline">\(\mathbf{r}_{10}^{(i)},\cdots,\mathbf{r}_{n_i 0}^{(i)}\)</span>是齐次线性代数方程组（20）的 <spanclass="math inline">\(n_i\)</span>个线性无关的解，且 <spanclass="math inline">\(\mathbf{r}_{jk}^{(i)}(i=1,2,\cdots ,s;j=1,2,\cdots,n_i;k=1,2,\cdots ,n_i-1)\)</span>是把 <spanclass="math inline">\(\mathbf{r}_{j0}^{(i)}\)</span>代替（21）中的 <spanclass="math inline">\(\mathbf{r}_0\)</span>而依次得出的 <spanclass="math inline">\(\mathbf{r}_k\)</span>. 即 <spanclass="math inline">\(\mathbf{r}_{j,k}^{(i)}=(\mathbf{A-\lambda_iE})\mathbf{r}_{j,k-1}^{(i)}\)</span></p><p>此外，当所得出的 <spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>是复值时，可利用之前的结论从<spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>提取实值基解矩阵.</p><p>这定理挺不讲人话的，得到的每一列中的多项式系数，即诸 <spanclass="math inline">\(r\)</span>在三个坐标的意义下是不相同的，每一类中最多到<span class="math inline">\(x^{n_i-1}\)</span>是因为 <spanclass="math inline">\(n_i\)</span>重特征根 <spanclass="math inline">\(\lambda_i\)</span>，求 <spanclass="math inline">\(n_i-1\)</span>阶导后 <spanclass="math inline">\(\lambda_i\)</span>仍为根.</p><p><strong>证</strong>：由引理6.7知（22）中每一列确实是（15）的解.只需证明 <spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>的各列线性无关. 这是因为<span class="math display">\[    \mathbf{\Phi}(0)= \biggl(\mathbf{r}_{10}^{(1)},\cdots,\mathbf{r}_{n_1 0}^{(1)};\cdots ;\mathbf{r}_{10}^{(s)},\cdots,\mathbf{r}_{n_s 0}^{(s)} \biggr)\]</span> 取适当的 <span class="math inline">\({\mathbf{r}_{j0}^{(i)}}\)</span>使得它们彼此间线性无关，由Wronsky行列式相关结论知，<spanclass="math inline">\(\mathbf{\Phi}(x)\)</span>是（15）的一个基解矩阵</p><h2 id="高阶线性微分方程式">高阶线性微分方程式</h2><p>本节仅讨论含一个未知函数 <spanclass="math inline">\(y=y(x)\)</span>的 <spanclass="math inline">\(n\)</span>阶线性微分方程式 <spanclass="math display">\[    y^{(n)}+a_1(x)y^{(n-1)}+\cdots +a_{n-1}y&#39;+a_n(x)y=f(x) \tag{24}\]</span> 其中 <span class="math inline">\(a_1(x),\cdots,a_n(x)\)</span>和 <span class="math inline">\(f(x)\)</span>都是区间<span class="math inline">\(a&lt;x&lt;b\)</span>上的连续函数. 当 <spanclass="math inline">\(f(x)\)</span>不恒为零时，称（24）为<strong>非齐次</strong>线性微分方程组；而与之相应的<strong>齐次</strong>线性方程组为<span class="math display">\[    y^{(n)}+a_1(x)y^{(n-1)}+\cdots +a_{n-1}y&#39;+a_n(x)y=0 \tag{25}\]</span> 引进新的未知函数 <span class="math display">\[    y_1=y, y_2=y&#39;,\cdots ,y_n=y^{(n-1)} \tag{26}\]</span> 则方程（24）等价于下面的线性微分方程组 <spanclass="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{A}(x)\mathbf{y}+\mathbf{f}(x)\tag{27}\]</span> 其中 <span class="math inline">\(\mathbf{y}\)</span>是诸 <spanclass="math inline">\(\mathbf{y}_i\)</span>的列向量，<spanclass="math inline">\(\mathbf{f}\)</span>是将 <spanclass="math inline">\(f(x)\)</span>置于末位的列向量，<spanclass="math inline">\(\mathbf{A}(x)\)</span>是一个Frobenius矩阵</p><p><span class="math display">\[    \mathbf{A}(x)=      \begin{pmatrix}        0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\        0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \\        \vdots &amp; \vdots &amp; \vdots &amp; &amp; \vdots \\        0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 \\        -a_n(x) &amp; -a_{n-1}(x) &amp; -a_{n-2}(x) &amp; \cdots &amp;-a_1(x)    \end{pmatrix}\]</span></p><p>而（25）等价于 <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{A}(x)\mathbf{y}\tag{28}\]</span> 微分方程（24）满足初值条件的解在区间 <spanclass="math inline">\(a&lt;x&lt;b\)</span>上是存在和唯一的.</p><h3 id="高阶线性微分方程的一般理论">高阶线性微分方程的一般理论</h3><p>假设函数组 <span class="math display">\[    \varphi_1(x),\varphi_2(x),\cdots,\varphi_n(x) \tag{29}      \]</span> 分别是齐次线性微分方程（28）的 <spanclass="math inline">\(n\)</span>个解，其分别求相应阶导数后组合成的行列式称为（29）的Wronsky行列式.</p><p>把之前的定理转述到高阶情形</p><p><strong>定理6.1</strong>*：齐次线性微分方程（25）在区间 <spanclass="math inline">\(a&lt;x&lt;b\)</span>上存在 <spanclass="math inline">\(n\)</span>个线性无关的解，它们具有（29）的形式，则（25）的通解是它们的线性组合.</p><p><strong>定理6.2</strong>*：（29）线性无关的充要条件是它的Wronsky行列式在区间<span class="math inline">\(a&lt;x&lt;b\)</span>上恒不为零.</p><div class="note note-success">            <p>该定理成立需要（29）是一个齐次线性微分方程的解.如果两个函数线性无关，但它们的Wronsky行列式恒为零，则不存在一个齐次线性微分方程的解是（29）</p>          </div><p>注意到 <spanclass="math inline">\(\mathbf{A}(x)\)</span>的迹具有简单形式，故Liouville公式有简单形式<span class="math display">\[    W(x)=W(x_0)\mathrm{e}^{-\int_{x_0}^{x} a_1(s) \mathrm{d}s} \quad(a&lt;x&lt;b) \tag{30}        \]</span> 其中 <spanclass="math inline">\(W(x)\)</span>是方程（25）的Wronsky行列式.</p><p>对于二阶齐次线性微分方程，可以从（25）的一个非零解导出它的通解.</p><p>设 <span class="math inline">\(y=\varphi(x)\)</span>是方程 <spanclass="math display">\[    y&#39;&#39;+p(x)y&#39;+q(x)y=0 \tag{31}\]</span> 的一个非零解，其中 <span class="math inline">\(p(x)\)</span>和<span class="math inline">\(q(x)\)</span>是区间 <spanclass="math inline">\(a&lt;x&lt;b\)</span>上的连续函数，则（31）的通解为<span class="math display">\[    y=\varphi(x)\biggl[ C_1+C_2 \int_{x_0}^{x}\frac{1}{\varphi(s)^{2}}\mathrm{e}^{-\int_{x_0}^{x} p(t) \mathrm{d}t}\mathrm{d}s \biggr] \tag{32}   \]</span> 其中 <span class="math inline">\(C_1\)</span>和 <spanclass="math inline">\(C_2\)</span>为任意常数.</p><p>这只要写出Liouville公式即可.</p><p>运用常数变易法解非齐次的 <strong>定理6.3</strong>* 设 <spanclass="math inline">\(\varphi_1(x),\varphi_2(x),\cdots,\varphi_n(x)\)</span>是齐次线性微分方程（25）在区间<spanclass="math inline">\(a&lt;x&lt;b\)</span>上的一个基本解组，则（24）的通解为<span class="math display">\[    y=C_1\varphi_1(x)+\cdots +C_n \varphi_n(x) +\varphi^*(x) \tag{33}\]</span> 其中 <span class="math inline">\(C_i\)</span>为常数 <spanclass="math display">\[    \varphi^*(x)=\sum_{k=1}^{n} \varphi_k(x) \cdot \int_{x_0}^{x}\frac{W_k(s)}{W(s)}f(s) \mathrm{d}s \tag{34}\]</span></p><p>这里 <span class="math inline">\(W(x)\)</span>是诸 <spanclass="math inline">\(\varphi_i(x)\)</span>的Wronsky行列式，而 <spanclass="math inline">\(W_k(x)\)</span>是 <spanclass="math inline">\(W(x)\)</span>中第 <spanclass="math inline">\(n\)</span>行第 <spanclass="math inline">\(k\)</span>列元素的代数余子式.</p><p><strong>证</strong>：只要证明 <span class="math display">\[    \mathbf{\Phi}(x) \int_{x_0}^{x} \mathbf{\Phi}^{-1}(s)\mathbf{f}(s)\mathrm{d}s\]</span> 的第一个分量就是 <spanclass="math inline">\(\varphi^*(x)\)</span>.</p><p>这运用一些矩阵的计算是不难的.</p><p>对于二阶的非齐次情况 设 <spanclass="math inline">\(y=\varphi(x)\)</span>是方程 <spanclass="math display">\[    y&#39;&#39;+p(x)y&#39;+q(x)y=f(x) \tag{35}\]</span> 的相应齐次方程的两个线性无关的特解 <spanclass="math inline">\(y=\varphi_1(x)\)</span>与 <spanclass="math inline">\(y=\varphi_2(x)\)</span>. <spanclass="math inline">\(p(x),q(x),f(x)\)</span>在 <spanclass="math inline">\(a&lt;x&lt;b\)</span>上连续.</p><div class="note note-success">            <p>一般来说，运用常数变易法时，与系数为常数相比求导后会多出几项.我们希望求导后也能有与系数为常数时同样的形式，同时也是为了方便起见，会令多出的几项为零.这相当于主动增加了一个约束条件，但通常是值得的.</p>          </div><h3 id="常系数高阶线性微分方程">常系数高阶线性微分方程</h3><p>对于 <span class="math inline">\(n\)</span>阶常系数微分方程 <spanclass="math display">\[    y^{(n)}+a_1y^{(n-1)}+\cdots +a_{n-1}y&#39;+a_ny=f(x) \tag{36}\]</span> 和相应的 <span class="math display">\[    y^{(n)}+a_1y^{(n-1)}+\cdots +a_{n-1}y&#39;+a_ny=0 \tag{36}\]</span> 其中 <span class="math inline">\(a_1,\cdots,a_n\)</span>是实常数，而 <spanclass="math inline">\(f(x)\)</span>是区间 <spanclass="math inline">\(a&lt;x&lt;b\)</span>上的实值连续函数.</p><p>（36）等价于方程 <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{Ay}\]</span> 其中 <span class="math display">\[    \mathbf{A}=      \begin{pmatrix}        0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\        0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \\        \vdots &amp; \vdots &amp; \vdots &amp; &amp; \vdots \\        0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 \\        -a_n &amp; -a_{n-1} &amp; -a_{n-2} &amp; \cdots &amp; -a_1    \end{pmatrix} \tag{37}\]</span> 矩阵 <spanclass="math inline">\(\mathbf{A}\)</span>的<strong>特征方程</strong>为<span class="math display">\[    \lambda^{n}+a_1\lambda^{n-1}+\cdots +a_{n-1}\lambda+a_n=0 \tag{38}\]</span> 它也叫（36）的特征方程.</p><p><strong>定理6.6</strong>*：设（36）的特征方程（38）在 <spanclass="math inline">\(\mathbb{C}\)</span>上有 <spanclass="math inline">\(s\)</span>个互不相同的根 <spanclass="math inline">\(\lambda_1,\lambda_2,\cdots,\lambda_s\)</span>，且对应的重数分别为<span class="math inline">\(n_i\)</span>. 则函数组 <spanclass="math display">\[    \begin{cases}        \mathrm{e}^{\lambda_1x} , x\mathrm{e}^{\lambda_1x} ,\cdots,x^{n_1-1}\mathrm{e}^{\lambda_1x} ; \\        \cdots \\        \mathrm{e}^{\lambda_sx} ,x\mathrm{e}^{\lambda_sx},\cdots,x^{n_s-1}\mathrm{e}^{\lambda_sx} \tag{39}    \end{cases}\]</span> 是（36）的一个基本解组. （注意这是 <spanclass="math inline">\(n\)</span>个函数）</p><p><strong>证</strong>：找一个基解矩阵使得它的第一行元素恰为（39）.由矩阵论知识知 <spanclass="math inline">\(\mathbf{A}\)</span>的Jordan标准型 <spanclass="math inline">\(\mathbf{J}\)</span>中相应于某个 <spanclass="math inline">\(\lambda_k\)</span>的Jordan块只有一个.</p><p>一个重要的观察是：矩阵 <spanclass="math inline">\(\mathbf{P}=(p_{ij})\)</span>的第一行元素满足下面的性质<span class="math display">\[    p_{1m_j} \neq 0 \quad (j=1,2,\cdots ,s) \tag{40}\]</span> 其中 <span class="math display">\[    m_1=1,m_2=n_1+1,\cdots ,m_s=n_1+\cdots +n_{s-1}+1\]</span> 事实上，若某个 <spanclass="math inline">\(p_{1m_j}=0\)</span>，则观察 <spanclass="math inline">\(\mathbf{AP}\)</span>的第 <spanclass="math inline">\(m_j\)</span>列，该列前 <spanclass="math inline">\(n-1\)</span>行均为零，对应到 <spanclass="math inline">\(\mathbf{PJ}\)</span>中得到 <spanclass="math inline">\(\mathbf{P}\)</span>的第 <spanclass="math inline">\(m_j\)</span>列全为零，与 <spanclass="math inline">\(\mathbf{P}\)</span>非退化矛盾.</p><p>写出基解矩阵 $^{x }= $的第一行，利用（40），可以对 <spanclass="math inline">\(\mathbf{PJ}\)</span>作初等列变换，消去多余项，就可以得到（39）中的形式.</p><p>【注】当特征方程有复根时，复根成对出现，可以提取实部虚部得到相应实值解，应该实际上是实化的想法.</p><p>对于一些特殊的 <spanclass="math inline">\(f(x)\)</span>，可以凭借经验推测 <spanclass="math inline">\(\varphi^*(x)\)</span> <strong>例</strong>： <spanclass="math display">\[    f(x)=P_m(x)\mathrm{e}^{\mu x}\]</span> 其中 <span class="math inline">\(P_m(x)\)</span>表示 <spanclass="math inline">\(x\)</span>的 <spanclass="math inline">\(m\)</span>次多项式. 那么当 <spanclass="math inline">\(\mu\)</span>不是方程（36）的特征根时，预测（35）有如下形式的特解<span class="math display">\[    \varphi^*(x)=Q_m(x)\mathrm{e}^{\mu x}\]</span> 其中 <span class="math inline">\(m\)</span>次多项式 <spanclass="math inline">\(Q_m(x)\)</span>的系数待定.</p><p>当 <span class="math inline">\(\mu\)</span>是 <spanclass="math inline">\(k\)</span>重特征根时，则令 <spanclass="math display">\[    \varphi^*(x)=x^kQ_m(x)\mathrm{e}^{\mu x}\]</span></p><p><strong>例</strong>： <span class="math display">\[    f(x)=[A_m(x)\cos (\beta x)+B_l(x) \sin (\beta x)] \mathrm{e}^{\alphax}\]</span> 其中 <span class="math inline">\(A_m(x)\)</span>和 <spanclass="math inline">\(B_l(x)\)</span>分别是 <spanclass="math inline">\(x\)</span>的 <spanclass="math inline">\(m\)</span>次和 <spanclass="math inline">\(l\)</span>次多项式. 那么相应特解的形式是 <spanclass="math display">\[    \varphi^*(x)=x^{k}[C_n(x)\cos (\beta x)+D_n(x) \sin (\beta x)]\mathrm{e}^{\alpha x}\]</span> 其中非负整数 <span class="math inline">\(k\)</span>是 <spanclass="math inline">\(\alpha \pmi\beta\)</span>作为（36）的特征根的重数（<spanclass="math inline">\(k\)</span>可以取0），<spanclass="math inline">\(n= \max\{m,l\}\)</span>，而 <spanclass="math inline">\(n\)</span>次多项式 <spanclass="math inline">\(C_n(x)\)</span>和 <spanclass="math inline">\(D_n(x)\)</span>系数待定.</p><p><strong>例（Euler方程）</strong>： <span class="math display">\[    x^{n}y^{(n)}+a_1x^{n-1}y^{(n-1)}+\cdots +a_{n-1}xy&#39;+a_ny=0      \]</span> 其中 <span class="math inline">\(a_1,a_2,\cdots,a_n\)</span>都是常数， <span class="math inline">\(x&gt;0\)</span>.</p><p>做代换 $x=^{t} $，记微分算子 <spanclass="math inline">\(D=\frac{\mathrm{d}}{\mathrm{d}t}\)</span></p><p>可以归纳得到 <span class="math display">\[    x^{k}y^{(k)}=D(D-1)\cdots (D-k+1)y\]</span> 代回Euler方程，得到关于 <spanclass="math inline">\(t\)</span>的常系数线性微分方程，最终用 <spanclass="math inline">\(t= \ln x\)</span>反代即得解.</p><p><strong>例</strong>：考虑微分方程 <span class="math display">\[    y&#39;&#39;+p(x)y&#39;+q(x)y=0 \tag{41}\]</span> 其中 <span class="math inline">\(p(x)\)</span>和 <spanclass="math inline">\(q(x)\)</span>是区间 <spanclass="math inline">\(I:a&lt;x&lt;b\)</span>上的连续函数. 设 <spanclass="math inline">\(y=\varphi(x)\)</span>是方程（41）在区间 <spanclass="math inline">\(I\)</span>上的一个非零解，则 <spanclass="math inline">\(\varphi(x)\)</span> 在区间 <spanclass="math inline">\(I\)</span>上只有一阶零点，从而 <spanclass="math inline">\(\varphi(x)\)</span>在任意有限闭区间上至多有有限个零点，从而每个零点是孤立的.</p><p><strong>证</strong>：这可以由解的唯一性得到.</p>]]></content>
    
    
    <categories>
      
      <category>分析学</category>
      
      <category>微分方程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>常微分方程（5）</title>
    <link href="/2022/04/09/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%885%EF%BC%89/"/>
    <url>/2022/04/09/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%885%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="高阶微分方程">高阶微分方程</h1><p>首先各未知函数微商的最高阶数之和叫作该微分方程的<strong>阶</strong>，它可以部分反应求解的难度.因此降阶是非常重要的一步.</p><h2 id="几个例子">几个例子</h2><p>不明显包含自变量的方程叫作<strong>自治</strong>（或<strong>驻定</strong>）的，对它们可以进行降阶.例如，考虑n阶的自治微分方程 <span class="math display">\[    F\biggl(y,\frac{\mathrm{d}y}{\mathrm{d}x},\cdots,\frac{\mathrm{d}^ny}{\mathrm{d}x^n}\biggr)=0 \tag{1}\]</span> 令 <spanclass="math inline">\(z=\frac{\mathrm{d}y}{\mathrm{d}x}\)</span>，则有关系式<span class="math display">\[    \begin{cases}        \displaystyle        \frac{\mathrm{d}^2y}{\mathrm{d}x^2}=\frac{\mathrm{d}z}{\mathrm{d}x}=\frac{\mathrm{d}z}{\mathrm{d}y}\frac{\mathrm{d}y}{\mathrm{d}x}=z\frac{\mathrm{d}z}{\mathrm{d}y}\\        \displaystyle        \frac{\mathrm{d}^3y}{\mathrm{d}x^3}=\frac{\mathrm{d}}{\mathrm{d}x}\biggl(z\frac{\mathrm{d}z}{\mathrm{d}y}\biggr)=z^2\frac{\mathrm{d}^2z}{\mathrm{d}y^2}+z        \biggl(\frac{\mathrm{d}z}{\mathrm{d}y}\biggr) \\        \cdots \\        \displaystyle        \frac{\mathrm{d}^ny}{\mathrm{d}x^n}=\varphi\biggl(z,\frac{\mathrm{d}z}{\mathrm{d}y},\cdots,\frac{\mathrm{d}^{n-1}z}{\mathrm{d}y^{n-1}}\biggr)    \end{cases}\]</span> 然后，把它们代入（1），就得到一个 <spanclass="math inline">\(n-1\)</span>阶的微分方程 <spanclass="math display">\[    F_1\biggl(y,z,\frac{\mathrm{d}z}{\mathrm{d}y},\cdots,\frac{\mathrm{d}^{n-1}z}{\mathrm{d}y^{n-1}}\biggr)=0\]</span> 其中 <span class="math inline">\(z\)</span>是未知函数，而<span class="math inline">\(y\)</span>是自变量.</p><p>与之相关有<strong>轨线</strong>、<strong>相平面</strong>、<strong>相图</strong>的概念.【例1】<strong>单摆方程</strong>：直接给出要解的微分方程 <spanclass="math display">\[    \frac{\mathrm{d}^2x}{\mathrm{d}t^2}+a^{2}\sin x=0\]</span> 其中常数 <spanclass="math inline">\(a=\sqrt{g/l}&gt;0\)</span>.</p><p>两边乘 <span class="math inline">\(\displaystyle\frac{\mathrm{d}x}{\mathrm{d}t}\)</span>，再积分得 <spanclass="math display">\[    \frac{1}{2} \biggl(\frac{\mathrm{d}x}{\mathrm{d}t}\biggr)^{2}-a^{2}\cos x=-\frac{1}{2}C_1  \]</span>把这种由高阶微分方程积分一次得到的关系式称为<strong>首次积分</strong>继续解该方程得到通积分，包含椭圆函数，可以利用 <spanclass="math inline">\(\sin x \thickapproxx\)</span>，接下去变成物理中的推导，略.</p><p>【例2】<strong>悬链线方程</strong>：在数学分析中，已经用变分法求得了悬链线的方程（摆线），这里可以从微分方程的角度再看一下（其实也是类似的）.直接给出要解的微分方程 <span class="math display">\[    y&#39;&#39;=a \sqrt{1+(y&#39;)^{2}} \tag{2}\]</span> 其中 <span class="math inline">\(a=\gamma/H_0\)</span>是常数.且满足<strong>边值条件</strong> <span class="math display">\[    y(x_1)=y_1, \quad y(x_2)=y_2 \tag{3}\]</span> 这是一个<strong>边值问题</strong>. 令 <spanclass="math inline">\(z=y&#39;\)</span>可以降为一阶微分方程 <spanclass="math display">\[    z&#39;=a \sqrt{1+z^{2}}\]</span> 而且它是变量分离的. 容易求出它的通解 <spanclass="math display">\[    z= \sinh a(x+C_1)\]</span> 其中 <span class="math inline">\(C_1\)</span>是一个任意常数.由此再积分得到通解<br /><span class="math display">\[    y=\frac{1}{a} \cosh a(x+C_1) +C_2 \tag{4}\]</span> 其中 <spanclass="math inline">\(C_2\)</span>是第二个任意常数.</p><p>利用边值条件（2）和（3）可唯一确定 <spanclass="math inline">\(C_1\)</span>和 <spanclass="math inline">\(C_2\)</span>. 最终答案是一个双曲余弦函数.问题此时还没有完全解决.还需要利用悬链线的长度解曲线积分，利用边值条件的高度差求解常数.</p><p>【例3】<strong>二体问题</strong>：由Newton'sLaw得到一个6阶的微分方程组，可以推出运动的轨道永远在一个平面上，不妨设为平面<span class="math inline">\(z=0\)</span>. 此时降为一个4阶方程.以下求解过程略.</p><h2 id="n维线性空间中的微分方程">n维线性空间中的微分方程</h2><p>设 <span class="math inline">\(n\)</span>阶微分方程 <spanclass="math display">\[    \frac{\mathrm{d}^ny}{\mathrm{d}x^n}=F\biggl(x,y,\frac{\mathrm{d}y}{\mathrm{d}x},\cdots,\frac{\mathrm{d}^{n-1}y}{\mathrm{d}x^{n-1}} \biggr) \tag{5}\]</span> 这里 <span class="math inline">\(x\)</span>是自变量，而 <spanclass="math inline">\(y\)</span>是未知函数 令 <spanclass="math display">\[    y_1=y,y_2=\frac{\mathrm{d}y}{\mathrm{d}x},\cdots,y_n=\frac{\mathrm{d}^{n-1}y}{\mathrm{d}x^{n-1}}       \]</span> 则微分方程（5）等价于下列 <spanclass="math inline">\(n\)</span>阶标准微分方程组 <spanclass="math display">\[    \begin{cases}        \displaystyle \frac{\mathrm{d}y_1}{\mathrm{d}x}=y_2 \\        \cdots \\        \displaystyle \frac{\mathrm{d}y_{n-1}}{\mathrm{d}x}=y_n \\        \displaystyle\frac{\mathrm{d}y_n}{\mathrm{d}x}=F(x,y_1,y_2,\cdots ,y_n)      \end{cases}\]</span> 对于未知数个数等于微分方程阶数的方程，可以通过换元写成标准形式<span class="math display">\[    \begin{cases}        \displaystyle\frac{\mathrm{d}y_1}{\mathrm{d}x}=f_1(x,y_1,y_2,\cdots,y_n), \\        \displaystyle\frac{\mathrm{d}y_2}{\mathrm{d}x}=f_2(x,y_1,y_2,\cdots,y_n), \\        \cdots  \\        \displaystyle\frac{\mathrm{d}y_n}{\mathrm{d}x}=f_n(x,y_1,y_2,\cdots,y_n),    \end{cases} \tag{6}\]</span> 其中 <spanclass="math inline">\(f_1,f_2,\cdots,f_n\)</span>是变元 <spanclass="math inline">\((x,y_1,y_2,\cdots,y_n)\)</span>在某个区域 <spanclass="math inline">\(D\)</span>内的连续函数.</p><p>可以令 <span class="math inline">\(n\)</span>维的行向量 <spanclass="math display">\[    y=(y_1,y_2,\cdots,y_n) \in  \mathbb{R}^{n}      \]</span> 令 <span class="math display">\[    f_i(x,\mathbf{y})=f_i(x,y_1,y_2,\cdots,y_n)     \]</span> <span class="math inline">\(i=1,2,\cdots ,n\)</span>和 <spanclass="math display">\[    \mathbf{f}(x, \mathbf{y})=(f_1(x,\mathbf{y}),\cdots,f_n(x,\mathbf{y})) \in \mathbb{R}^{n}          \]</span> 而且规定 <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}= \biggl(\frac{\mathrm{d}y_1}{\mathrm{d}x},\frac{\mathrm{d}y_2}{\mathrm{d}x},\cdots,\frac{\mathrm{d}y_n}{\mathrm{d}x}\biggr)\]</span> 则微分方程组（6）的向量形式为 <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{f}(x,\mathbf{y})  \tag{7}\]</span> 其中 <span class="math inline">\(\mathbf{f}(x,\mathbf{y})\)</span>是关于变元 <span class="math inline">\((x,\mathbf{y})\in D\)</span>的一个 <spanclass="math inline">\(n\)</span>维向量值函数. 一般还应给出初值条件 <spanclass="math display">\[    \mathbf{y}(x_0)=\mathbf{y}_0\]</span> 其中的初值点 <span class="math inline">\((x_0,\mathbf{y}_0)\in D \subset \mathbb{R}^{n+1}\)</span>.</p><div class="note note-success">            <p>在 <spanclass="math inline">\(\mathbb{R}^{n}\)</span>中引入模后可以用完全一致的方法证明Picard定理和Peano定理</p>          </div><p>如果在方程（6）中函数 <spanclass="math inline">\(f_1,f_2,\cdots,f_n\)</span>都是对于 <spanclass="math inline">\(y_1,y_2,\cdots,y_n\)</span>的线性函数，即 <spanclass="math display">\[    f_k(x,y_1,y_2,\cdots,y_n)=\sum_{i=1}^{n} a_{ik}(x)y_i+e_k(x)\]</span> <span class="math inline">\((k=1,2,\cdots,n)\)</span>，则称微分方程（6）或（7）是<strong>线性</strong>的；否则称为<strong>非线性</strong>的.</p><p>线性微分方程组 <span class="math display">\[    \frac{\mathrm{d}y_k}{\mathrm{d}x}=\sum_{i=1}^{n} a_{ik}(x)y_i+e_k(x)\]</span> <span class="math inline">\((k=1,2,\cdots,n)\)</span>的向量形式可以写成 <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{y}\mathbf{A}(x)+\mathbf{e}(x)\]</span> 其中向量 <spanclass="math inline">\(\mathbf{y}=(y_1,y_2,\cdots,y_n)\)</span>和 <spanclass="math inline">\(\mathbf{e}(x)=e_1(x),e_2(x),\cdots,e_n(x)\)</span>，而矩阵<span class="math inline">\(\mathbf{A}(x)=(a_{ik}(x))_{n \timesn}\)</span>. 若采用列向量的写法，则 <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{A}(x)\mathbf{y}+\mathbf{e}(x)\tag{8}\]</span> 这种形式比较常见.</p><p>设 <span class="math inline">\(\mathbf{A}(x)\)</span>和 <spanclass="math inline">\(\mathbf{e}(x)\)</span>在区间 <spanclass="math inline">\(a&lt;x&lt;b\)</span>上连续，则线性微分方程（8）满足任何初值条件<span class="math display">\[    \mathbf{y}(x_0)=y_0 \quad (a&lt;x_0&lt;b), \mathbf{y}_0 \in\mathbb{R}^{n}\]</span> 的解 <spanclass="math inline">\(\mathbf{y}=\mathbf{y}(x)\)</span>在整个区间 <spanclass="math inline">\(a&lt;x&lt;b\)</span>上存在且唯一.</p><h2 id="解对初值和参数的连续依赖性">解对初值和参数的连续依赖性</h2><p>不失一般性，只讨论初值问题 <span class="math display">\[    (E_{\lambda}): \quad\frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{f}(x,\mathbf{y},\mathbf{\lambda}),\quad\mathbf{y}(0)=\mathbf{0}\]</span> 的解 <spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x,\mathbf{\lambda})\)</span>对参量<span class="math inline">\(\mathbf{\lambda}\)</span>的依赖性，其中<span class="math inline">\(\mathbf{\lambda}\)</span>是 <spanclass="math inline">\(m\)</span>维的参数向量.</p><p><strong>定理5.1</strong>：设 <spanclass="math inline">\(n\)</span>维向量值函数 <spanclass="math inline">\(\mathbf{f}(x,\mathbf{y},\mathbf{\lambda})\)</span>在区域<span class="math display">\[    G: \quad \left\vert x \right\vert \leqslant a, \quad \left\vert\mathbf{y} \right\vert \leqslant b, \quad \left\vert\mathbf{\lambda}-\mathbf{\lambda}_0 \right\vert \leqslant c    \]</span> 上是连续的，而且对 <spanclass="math inline">\(\mathbf{y}\)</span>满足Lipschitz条件 <spanclass="math display">\[    \left\vert\mathbf{f}(x,\mathbf{y}_1,\mathbf{\lambda})-\mathbf{f}(x,\mathbf{y}_2,\mathbf{\lambda})\right\vert \leqslant L\left\vert \mathbf{y}_1-\mathbf{y}_2\right\vert  \]</span> 其中常数 <span class="math inline">\(L\geqslant 0\)</span>.令正数 <span class="math inline">\(M\)</span>为 $(x,,) $在区域 <spanclass="math inline">\(G\)</span>的一个上界，而且令 <spanclass="math display">\[    h= \min \biggl(a,\frac{b}{M} \biggr)\]</span> 则初值问题 <spanclass="math inline">\((E_\lambda)\)</span>的解 <spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x,\mathbf{\lambda})\)</span>在区域<span class="math display">\[    D: \quad \left\vert x \right\vert \leqslant h, \quad \left\vert\mathbf{\lambda}-\mathbf{\lambda}_0 \right\vert \leqslant c\]</span> 上是连续的.</p><p><strong>证</strong>：先证明初值问题 <spanclass="math inline">\((E_\lambda)\)</span>的Picard序列 <spanclass="math inline">\(\{\mathbf{\varphi}_k(x,\mathbf{\lambda})\}\)</span>对参数<spanclass="math inline">\(\mathbf{\lambda}\)</span>的连续性（和可微性）；再证明<spanclass="math inline">\(\mathbf{\varphi}_k(x,\mathbf{\lambda})\)</span>是一致收敛的，而且它的极限函数<spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x,\mathbf{\lambda})\)</span>是 <span class="math inline">\((E_\lambda)\)</span>的解.</p><p><strong>推论</strong>：设 <spanclass="math inline">\(n\)</span>维向量值函数 <spanclass="math inline">\(f(x,\mathbf{y})\)</span>在区域 <spanclass="math display">\[    R: \quad \left\vert x-x_0 \right\vert \leqslant a, \quad \left\verty-y_0 \right\vert \leqslant b\]</span> 上连续，而且对 <spanclass="math inline">\(\mathbf{y}\)</span>满足Lipschitz条件.则微分方程初值问题 <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{f}(x,\mathbf{y}),\quad\mathbf{y}(x_0)=\eta \tag{9}\]</span> 的解 <spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x,\mathbf{\eta})\)</span>在区域<span class="math display">\[    Q: \quad \left\vert x-x_0 \right\vert \leqslant \frac{h}{2}, \quad\left\vert \mathbf{\eta}-\mathbf{y_0} \right\vert \leqslant \frac{b}{2}\]</span> 上是连续的，其中 <span class="math display">\[    h=\min \biggl(a,\frac{b}{M} \biggr)\]</span> 而正数 <span class="math inline">\(M\)</span>为 $f(x,) $在区域<span class="math inline">\(R\)</span>上的一个上界.</p><p>解的存在性可由局部延伸到大范围.解对初值（或参数）的连续性（和可微性）也有类似的如下结论.</p><p><strong>定理5.2</strong>：设 <spanclass="math inline">\(n\)</span>维向量值函数 <spanclass="math inline">\(\mathbf{f}(x,\mathbf{y})\)</span>在 <spanclass="math inline">\((x,\mathbf{y})\)</span>空间内的某个开区域上是连续的，而且对<span class="math inline">\(\mathbf{y}\)</span>满足局部 Lipschitz条件.假设 <span class="math inline">\(\mathbf{y}=\mathbf{\xi}(x)\)</span>是微分方程 <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{f}(x,\mathbf{y})\tag{10}\]</span> 的一个解，令它的存在区间为 <spanclass="math inline">\(J\)</span>. 现在，在区间 <spanclass="math inline">\(J\)</span>内任取一个有界闭区间 <spanclass="math inline">\(a\leqslant x\leqslant b\)</span>. 则存在常数 <spanclass="math inline">\(\delta&gt;0\)</span>，使得对任何初值 <spanclass="math inline">\((x_0,\mathbf{y}_0)\)</span>， <spanclass="math display">\[    a\leqslant x_0\leqslant b, \quad \left\vert\mathbf{y}_0-\mathbf{\xi}(x_0) \right\vert \leqslant \delta\]</span> Cauchy问题 <span class="math display">\[    (E): \quad\frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{f}(x,\mathbf{y}),\quad\mathbf{y}(x_0)=y_0\]</span> 的解 <spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x;x_0,\mathbf{y}_0)\)</span>也至少在区间<span class="math inline">\(a\leqslant x\leqslantb\)</span>上存在，并且它在闭区域 <span class="math display">\[    D_\delta: \quad a\leqslant x\leqslant b, a\leqslant x_0 \leqslant b,\left\vert \mathbf{y}_0-\mathbf{\xi}(x_0) \right\vert \leqslant \delta\]</span> 上是连续的.</p><p><strong>证</strong>：用有限覆盖定理将局部Lipschitz条件化为整体Lipschitz条件，然后构造Picard序列证明.</p><p>最后提一下习题中的一个问题：如果 <spanclass="math inline">\(\mathbf{f}(x,\mathbf{y})\)</span>在区域 <spanclass="math inline">\(R\)</span>连续，且微分方程 <spanclass="math inline">\(\displaystyle\frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}\mathbf{f}(x,\mathbf{y})\)</span>经过区域<spanclass="math inline">\(R\)</span>任意一点的解都存在且唯一，则这些解关于初值连续依赖.</p><p>设经过 <span class="math inline">\((x_0,\mathbf{y}_0)\)</span>的解是<spanclass="math inline">\(\mathbf{y}=\mathbf{\phi}(x;x_0,\mathbf{y}_0)=\mathbf{\psi}_{x_0,\mathbf{y}_0}(x)\)</span>.首先 <span class="math inline">\(\mathbf{\psi}\)</span>当然是关于 <spanclass="math inline">\(x\)</span>连续的（因为可微），且 <spanclass="math inline">\(\mathbf{\psi}_{x_0,\mathbf{y}_0}(x_0)=\mathbf{y}_0\)</span>.假设 <span class="math inline">\(\mathbf{\varphi}\)</span>关于 <spanclass="math inline">\((x_0, \mathbf{y}_0)\)</span>不连续，间断点是 <spanclass="math inline">\((x_0^*,\mathbf{y}_0^*\)</span>. 任取 <spanclass="math inline">\(\varepsilon&gt;0\)</span>，由唯一性和连续性知不连续的表达式在<spanclass="math inline">\(d((\widetilde{x}_0,\mathbf{\widetilde{y}}_0),(x_0^*,\mathbf{y}_0^*))\)</span>足够小时不可能成立.</p><h2 id="解对初值和参数的连续可微性">解对初值和参数的连续可微性</h2><p>不失一般性，只考虑微分方程 <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{f}(x,\mathbf{y},\mathbf{\lambda})\tag{11}\]</span> 满足初值条件 <span class="math display">\[    \mathbf{y}(0)=\mathbf{0}\]</span> 的解 <spanclass="math inline">\(\mathbf{y}=\varphi(x,\mathbf{\lambda})\)</span>对参数<span class="math inline">\(\mathbf{\lambda}\)</span>的连续可微性.</p><p><strong>定理5.3</strong>：设 <spanclass="math inline">\(\mathbf{f}(x.\mathbf{y}.\mathbf{\lambda})\)</span>在区域<span class="math display">\[    G: \quad \left\vert x \right\vert \leqslant a, \quad \left\vert\mathbf{y} \right\vert \leqslant b, \quad \left\vert\mathbf{\lambda}-\mathbf{\lambda}_0 \right\vert\leqslant c\]</span> 上连续，且对 <span class="math inline">\(\mathbf{y}\)</span>和<span class="math inline">\(\lambda\)</span>有连续偏微商.则微分方程（11）满足初值条件 <spanclass="math inline">\(\mathbf{y}(0)=\mathbf{0}\)</span>的解 <spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x,\mathbf{\lambda})\)</span>在区域<span class="math display">\[    D: \quad \left\vert x \right\vert \leqslant h, \quad \left\vert\mathbf{\lambda}-\mathbf{\lambda}_0 \right\vert \leqslant c\]</span> 上是连续可微的，其中正数 <spanclass="math inline">\(h\)</span>的定义同定理5.1.</p><p><strong>证</strong>：首先化成积分方程 <span class="math display">\[    \mathbf{y}=\int_{0}^{x} \mathbf{f}(x,\mathbf{y},\mathbf{\lambda})\mathrm{d}x\]</span> 构造它的Picard序列，由定理5.1知Picard序列 <spanclass="math inline">\(\mathbf{\varphi}_k(x,\mathbf{\lambda})\)</span>在区域<span class="math inline">\(D\)</span>上一致收敛到方程（11）的唯一解<spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x,\mathbf{\lambda})\)</span>.其次，为了证明 <spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x,\mathbf{\lambda})\)</span>对<span class="math inline">\(\mathbf{\lambda}\)</span>有连续的偏微商<span class="math inline">\(\frac{\partial \mathbf{\varphi}}{\partial\mathbf{\lambda}}(x,\mathbf{\lambda})\)</span>，证明序列 <spanclass="math inline">\(\frac{\partial \mathbf{\varphi}_k}{\partial\mathbf{\lambda}}(x,\mathbf{\lambda})\)</span>对 <spanclass="math inline">\((x,\mathbf{\lambda}) \in D\)</span>一致收敛.归纳证明辅助不等式 <span class="math display">\[    \left\vert \frac{\partial \mathbf{\varphi}}{\partial\mathbf{\lambda}} \right\vert \leqslant \exp (\alpha h)\]</span> 其中 <span class="math inline">\(\alpha\)</span>是区域 <spanclass="math inline">\(G\)</span>上 <spanclass="math inline">\(\displaystyle \frac{\partial \mathbf{f}}{\partial\mathbf{y}}(x,\mathbf{y},\mathbf{\lambda})\)</span>和 <spanclass="math inline">\(\displaystyle \frac{\partial \mathbf{f}}{\partial\mathbf{\lambda}}(x,\mathbf{y},\mathbf{\lambda})\)</span>的共同上界.</p><p>再次，利用Cauchy收敛准则证明 <span class="math display">\[    v_{k,s}=\left\vert \frac{\partial \mathbf{\varphi}_{k+s}}{\partial\mathbf{\lambda}}-\frac{\partial \mathbf{\varphi}_k}{\partial\mathbf{\lambda}} \right\vert\]</span> 一致趋于0.</p><p>最后，由 <span class="math display">\[    \frac{\partial \mathbf{\varphi}}{\partialx}(x,\mathbf{\lambda})=\mathbf{f}(x,\mathbf{\varphi}(x,\mathbf{\lambda}),\mathbf{\lambda})\]</span> 知 <span class="math inline">\(\displaystyle \frac{\partial\mathbf{\varphi}}{\partial x}(x,\mathbf{\lambda})\)</span>对 <spanclass="math inline">\((x,\mathbf{\lambda}) \in D\)</span>也是连续的.</p><p><strong>推论</strong>：设 <spanclass="math inline">\(n\)</span>维向量值函数 <spanclass="math inline">\(\mathbf{f}(x,\mathbf{y})\)</span>在区域 <spanclass="math display">\[    R: \quad \left\vert x-x_0 \right\vert \leqslant a, \quad \left\verty-y_0 \right\vert \leqslant b\]</span> 上连续，而且对 <spanclass="math inline">\(\mathbf{y}\)</span>有连续的偏微商 <spanclass="math inline">\(\mathbf{f}&#39;_{\mathbf{y}}(x,\mathbf{y})\)</span>.则初值问题 <span class="math display">\[    \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}=\mathbf{f}(x,\mathbf{y}),\quad \mathbf{y}(x_0)=\mathbf{\eta}\]</span> 的解 <spanclass="math inline">\(\mathbf{y}=\mathbf{\varphi}(x,\mathbf{\eta})\)</span>在区域<span class="math display">\[    D: \quad \left\vert x-x_0 \right\vert \leqslant \frac{h}{2}, \quad\left\vert \mathbf{\eta}-\mathbf{y}_0 \right\vert \leqslant\frac{b}{2}      \]</span> 上是连续可微的.</p><p>【附注】假设 <span class="math inline">\(y\)</span>和 <spanclass="math inline">\(\lambda\)</span>都是一维的，并且定理5.3及其推论的条件成立，则初值问题<span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y, \lambda), \quad y(x_0)=y_0\tag{12}\]</span> 的解 <spanclass="math inline">\(y=\varphi(x;x_0,y_0,\lambda)\)</span>对初值 <spanclass="math inline">\(x_0\)</span>，<spanclass="math inline">\(y_0\)</span>及参数 <spanclass="math inline">\(\lambda\)</span>的偏导数 <spanclass="math inline">\(\displaystyle \frac{\partial \varphi}{\partialx_0}\)</span>， <span class="math inline">\(\displaystyle \frac{\partial\varphi}{\partial y_0}\)</span>和 <spanclass="math inline">\(\displaystyle \frac{\partial \varphi}{\partial\lambda}\)</span>分别在它们有定义的区域内连续可微.</p><p>在与之等价的积分方程 <span class="math display">\[    \varphi(x;x_0,y_0,\lambda)=y_0+\int_{x_0}^{x}f(x,\varphi(x;x_0,y_0,\lambda),\lambda) \mathrm{d}x \tag{13}\]</span> 两端分别对 <span class="math inline">\(x_0\)</span>，<spanclass="math inline">\(y_0\)</span>和 <spanclass="math inline">\(\lambda\)</span>求偏导数可以得到三个一阶线性微分方程，把它们叫（12）关于初值<span class="math inline">\(x_0\)</span>，<spanclass="math inline">\(y_0\)</span>和参数 <spanclass="math inline">\(\lambda\)</span>的<strong>变分方程</strong>（回忆变分法的两个经典例子）</p>]]></content>
    
    
    <categories>
      
      <category>分析学</category>
      
      <category>微分方程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>常微分方程（4）</title>
    <link href="/2022/04/06/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%884%EF%BC%89/"/>
    <url>/2022/04/06/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%884%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="奇解">奇解</h1><h2 id="一阶隐式微分方程">一阶隐式微分方程</h2><p>本节讨论一阶隐式方程 <span class="math display">\[    F(x, y,\frac{\mathrm{d}y}{\mathrm{d}x})=0 \tag{1}\]</span> 的几个特殊解法，隐式指的是 <spanclass="math inline">\(\frac{\mathrm{d}y}{\mathrm{d}x}\)</span>没有预先表示为<span class="math inline">\((x, y)\)</span>的显函数.</p><h3 id="微分法">微分法</h3><p>设从微分方程（1）中可显式解出未知函数 <span class="math display">\[    y=f(x, p) \tag{2}       \]</span> 其中 <spanclass="math inline">\(p=\frac{\mathrm{d}y}{\mathrm{d}x}\)</span>. 设函数<span class="math inline">\(f(x,p)\)</span>对 <spanclass="math inline">\((x,p)\)</span>是连续可微的. 则由方程（2）对 <spanclass="math inline">\(x\)</span>进行微分，得 <spanclass="math display">\[    p=f&#39;_x(x,p)+f&#39;_p(x,p)\frac{\mathrm{d}p}{\mathrm{d}x}\]</span> 或 <span class="math display">\[    [f&#39;_(x,p)-p]\mathrm{d}x+f&#39;_p(x,p)\mathrm{d}p=0 \tag{3}\]</span> 这是一个关于变量 <span class="math inline">\(x\)</span>和<span class="math inline">\(p\)</span>的一阶显式微分方程.</p><p>求得（3）的通解 <spanclass="math inline">\(p=u(x,C)\)</span>，得到（2）的通解 <spanclass="math display">\[    y=f(x,u(x,C))\]</span> 其中 <spanclass="math inline">\(C\)</span>是一个任意常数；另外若（3）有特解 <spanclass="math inline">\(p=w(x)\)</span>则方程（2）有相应的特解 <spanclass="math display">\[    y=f(x,w(x))\]</span> 另一方面，若（3）的通解可写成 <spanclass="math inline">\(x=v(p,C)\)</span>的形式，则（2）的通解可写成 <spanclass="math display">\[    \begin{cases}        x=v(p,C) \\        y=f(v(p,C),p)    \end{cases}\]</span> 这里 <spanclass="math inline">\(p\)</span>视作一个参变量；同样如果（3）有特解<span class="math inline">\(x=z(p)\)</span>，则（2）有相应的特解 <spanclass="math display">\[    \begin{cases}        x=z(p) \\        y=f(z(p),)         \end{cases}\]</span></p><h3 id="参数法">参数法</h3><p>设微分方程不明显包含自变量，即 <span class="math display">\[    F(y,p)=0 \quad \biggl(p=\frac{\mathrm{d}y}{\mathrm{d}x}\biggr)\tag{4}\]</span> 作为变元 <span class="math inline">\(y\)</span>和 <spanclass="math inline">\(p\)</span>之间的联系，方程（4）在 <spanclass="math inline">\((y,p)\)</span>平面上一般表示若干条曲线. 设 <spanclass="math display">\[    y=g(t),\quad p=h(t) \tag{5}\]</span> 是其中一条. 称（5）为（4）的一个<strong>参数表示</strong>.一般来说 <span class="math inline">\(g(t)\)</span>，<spanclass="math inline">\(g&#39;(t)\)</span>和 <spanclass="math inline">\(h(t)\)</span>都是参数 <spanclass="math inline">\(t\)</span>的连续函数，并且 <spanclass="math inline">\(h(t) \neq 0\)</span>.根据上述微分方程的参数表示，我们有 <span class="math display">\[    \mathrm{d}x=\frac{1}{p}\mathrm{d}y=\frac{g&#39;(t)}{h(t)}\mathrm{d}t\]</span> 再利用积分，可得 <span class="math display">\[    x=\int_{}^{} \frac{g&#39;(t)}{h(t)} \mathrm{d}t+C\]</span> 因此，微分方程（4）有通解 <span class="math display">\[    x=\int_{}^{} \frac{g&#39;(t)}{h(t)} \mathrm{d}t+C, \quad y=g(t)\tag{6}\]</span></p><p>一般而言，一阶隐式微分方程 <span class="math display">\[    F(x, y,p)=0 \quad \biggl( p=\frac{\mathrm{d}y}{\mathrm{d}x} \biggr)\tag{7}\]</span> 在 <span class="math inline">\((x, y, p)\)</span>空间表示曲面.设它的参数表达式为 <span class="math display">\[    x=f(u,v), \quad y=g(u,v), \quad p=h(u,v)\]</span> 这里 <span class="math inline">\(u\)</span>和 <spanclass="math inline">\(v\)</span> 是两个参数. 则 <spanclass="math inline">\(\mathrm{d}y=p\mathrm{d}x\)</span>可写成如下形式<span class="math display">\[    M(u,v)\mathrm{d}u+N(u,v)\mathrm{d}v=0 \tag{8}\]</span> 其中 <span class="math display">\[    \begin{cases}        M(u,v)=g&#39;_u(u,v)-h(u,v)f&#39;_u(u,v) \\        N(u,v)=g&#39;_v(u,v)-h(u,v)f&#39;_v(u,v)    \end{cases}\]</span> 若能求得一阶显式微分方程（8）的通解 <spanclass="math display">\[    v=Q(u,C) \tag{9}\]</span> 则微分方程（7）有通解 <span class="math display">\[    x=f(u,Q(u,C)), \quad y=g(u,Q(u,C))\]</span> 其中 <span class="math inline">\(u\)</span>是参变量，而 <spanclass="math inline">\(C\)</span>是一个积分常数；若其有特解 <spanclass="math inline">\(v=S(u)\)</span>，则 <span class="math display">\[    x=f(u,S(u)),\quad y=g(u,S(u))\]</span> 是微分方程（7）的特解.</p><h2 id="奇解-1">奇解</h2><p>【定义】设一阶微分方程 <span class="math display">\[    F(x, y, \frac{\mathrm{d}y}{\mathrm{d}x})=0 \tag{10}\]</span> 有一特解 <span class="math display">\[    \Gamma: y=\varphi(x) \quad(x\in J)\]</span> 如果对每一点 <span class="math inline">\(Q\in\Gamma\)</span>，在 <spanclass="math inline">\(Q\)</span>任何领域内方程（10）有一个不同于 <spanclass="math inline">\(\Gamma\)</span>的解在 <spanclass="math inline">\(Q\)</span>点与 <spanclass="math inline">\(\Gamma\)</span>相切，则称 <spanclass="math inline">\(\Gamma\)</span>是微分方程（10）的<strong>奇解</strong>.</p><p><strong>定理4.1（奇解存在的必要条件）</strong>：设函数 <spanclass="math inline">\(F(x, y,p)\)</span>对 <spanclass="math inline">\((x, y, p)\in G\)</span>是连续的，而且对 <spanclass="math inline">\(y\)</span>和 <spanclass="math inline">\(p\)</span>有连续的偏导数 <spanclass="math inline">\(F&#39;_y\)</span>和 <spanclass="math inline">\(F&#39;_p\)</span>，若函数 <spanclass="math inline">\(y=\varphi(x),(x\inJ)\)</span>是微分方程（10）的一个奇解，并且 <spanclass="math display">\[    (x,\varphi(x)\varphi&#39;(x))\in G \quad(x\in J)\]</span> 则奇解 <spanclass="math inline">\(y=\varphi(x)\)</span>满足一个称之为<strong>p-判别式</strong>的联立方程<span class="math display">\[    F(x, y, p)=0, \quad F&#39;_p(x, y, p)=0\quad(p=\frac{\mathrm{d}y}{\mathrm{d}x}) \tag{11}\]</span> 设从（11）中消去 <spanclass="math inline">\(p\)</span>得到方程 <span class="math display">\[    \Delta(x, y)=0 \tag{12}\]</span> 则称由此所决定的曲线为方程（10）的<strong>P-判别曲线</strong>.因此，微分方程（10）的奇解是一条p-判别曲线.</p><p><strong>证</strong>：用反证法，假设 <spanclass="math inline">\(\exists x_0\in J\)</span>，使得 <spanclass="math display">\[    F&#39;_p(x_0, y_0, p_0) \neq 0  \]</span> 利用隐函数定理，<span class="math inline">\(F\)</span>关于<span class="math inline">\(p\)</span>的偏导数不为零保证在 <spanclass="math inline">\((x_0,y_0)\)</span>附近唯一确定函数 <spanclass="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y)\]</span> 推得 <span class="math inline">\(f(x, y)\)</span>在 <spanclass="math inline">\((x_0, y_0)\)</span>附近关于 <spanclass="math inline">\(y\)</span>有连续的偏导数，再用Picard定理知在 <spanclass="math inline">\((x_0,y_0)\)</span>附近有唯一解，从而与奇解的假设矛盾.</p><p>注意，由p-判别式确定的函数 <spanclass="math inline">\(y=\psi(x)\)</span>不一定是相应微分方程的解；即使是解，也不一定是奇解.</p><p>上面的定理要求出通解才能验证奇解与否，下面的定理在某种条件下克服了这一困难.</p><p><strong>定理4.2</strong>：设函数 <span class="math inline">\(F(x, y,p)\)</span>对 <span class="math inline">\((x, y,p)\inG\)</span>是二阶连续可微的. 又设由微分方程（10）的p-判别式 <spanclass="math display">\[    F(x, y, p)=0, \quad F&#39;_p(x, y, p)=0\]</span> 消去<span class="math inline">\(p\)</span>后得到的函数 <spanclass="math inline">\(y=\psi(x)(x\inJ)\)</span>是微分方程（10）的解（注意上面定理的注）.而且设条件 <spanclass="math display">\[    F&#39;_y(x,\psi(x),\psi&#39;(x)) \neq 0,\quadF&#39;&#39;_{pp}(x,\psi(x),\psi&#39;(x)) \neq 0 \tag{13}\]</span> 以及 <span class="math display">\[    F&#39;_p(x,\psi(x),\psi&#39;(x))=0 \tag{14}\]</span> 对 <span class="math inline">\(x\in J\)</span>成立. 则 <spanclass="math inline">\(y=\psi(x)\)</span>是微分方程（10）的奇解.</p><p><strong>证</strong>：放到了文章最后.</p><p>我们说明该定理中（13）（14）都是不可去的.</p><p>考虑方程 <span class="math display">\[    \biggl( \frac{\mathrm{d}y}{\mathrm{d}x} \biggr)^{2}-y^{2}=0\]</span> 得到 <span class="math inline">\(y=0\)</span>，此时有 <spanclass="math inline">\(F&#39;_y(x,0,0)=0\)</span>，而原方程的通解为 <spanclass="math inline">\(y=x\exp {\pm x}\)</span>.</p><p>考虑方程 <span class="math display">\[    \sin (y \frac{\mathrm{d}y}{\mathrm{d}x})=y\]</span> 得到 <span class="math inline">\(y=0\)</span>，此时有 <spanclass="math inline">\(F&#39;&#39;_{pp}(x,0,0)=0\)</span>，<spanclass="math inline">\(y=0\)</span>不是原方程的奇解.</p><p>考虑方程 <span class="math display">\[    y=2x+y&#39;-\frac{1}{3}(y&#39;)^3\]</span> 此时 <spanclass="math inline">\(F&#39;_p(x,\psi(x),\psi&#39;(x))=0\)</span></p><p>另一个类似于2.2中的结果：设连续函数 <spanclass="math inline">\(E(y)\)</span>满足条件： <spanclass="math display">\[    \begin{cases}        E(y)=0, \quad y=0\\        E(y)\neq 0, \quad 0&lt;y\leqslant 1    \end{cases}\]</span> 则 <span class="math inline">\(y=0\)</span>是微分方程 <spanclass="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=E(y)    \]</span> 的奇解当且仅当瑕积分 <span class="math display">\[    \int_{0}^{1} \frac{\mathrm{d}y}{E(y)}       \]</span> 收敛. <strong>证</strong>：<spanclass="math inline">\(\Rightarrow\)</span>：若 <spanclass="math inline">\(y=0\)</span>为奇解，则存在局部解使得 <spanclass="math inline">\(y\neq 0\)</span>，故有 <spanclass="math inline">\(\frac{\mathrm{d}y}{E}=\mathrm{d}x \Rightarrow\int_{0}^{1} \frac{\mathrm{d}y}{E}\)</span>收敛.</p><p><span class="math inline">\(\Leftarrow\)</span>：$ _{0}^{1}$收敛，可令 <span class="math inline">\(x(y)=x_0+\int_{0}^{y}\frac{\mathrm{d}y}{E}\)</span>是异于 <spanclass="math inline">\(y=0\)</span>的解，由于 <spanclass="math inline">\(y&#39;(0)=0\)</span>，得到 <spanclass="math inline">\(y=0\)</span>是奇解.</p><h2 id="包络">包络</h2><p>这一节想利用有关曲线族的包络的概念来阐明奇解与通解之间的联系，以及讨论寻求奇解的方法.</p><p>设单参数 <span class="math inline">\(C\)</span>的曲线族 <spanclass="math display">\[    K(C): \qquad V(x,y,C)=0 \tag{15}\]</span> 其中函数 <span class="math inline">\(V(x, y, C)\)</span>对<span class="math inline">\((x, y, C)\in D\)</span>是连续可微的.</p><p>【定义】设在平面上有一条连续可微的曲线 <spanclass="math inline">\(\Gamma\)</span>. 如果对于任一点 <spanclass="math inline">\(q\in \Gamma\)</span>，在曲线族（15）中都有一条曲线<span class="math inline">\(K(C^*)\)</span>通过 <spanclass="math inline">\(q\)</span>点且在该点与 <spanclass="math inline">\(\Gamma\)</span>相切，而且 <spanclass="math inline">\(K(C^*)\)</span>在 <spanclass="math inline">\(q\)</span>点的某一领域内不同于 <spanclass="math inline">\(\Gamma\)</span>. 则称曲线 <spanclass="math inline">\(\Gamma\)</span>为曲线族（15）的一支<strong>包络</strong>.</p><p>注：这个定义与微分几何中的定义稍有不同.</p><p><strong>定理4.3</strong>：设微分方程 <span class="math display">\[    F(x, y, \frac{\mathrm{d}y}{\mathrm{d}x})=0 \tag{16}\]</span> 有通积分为 <span class="math display">\[    U(x, y, C)=0 \tag{17}\]</span> 又设（积分）曲线族（17）有包络为 <span class="math display">\[    \Gamma: \qquad y=\varphi(x) \quad (x\in J)\]</span> 则包络 <spanclass="math inline">\(y=\varphi(x)\)</span>是微分方程（16）的奇解.</p><p><strong>证</strong>：任取 <spanclass="math inline">\(\Gamma\)</span>上一点，由包络的性质即可.</p><div class="note note-success">            <p>由奇解的定义可知，奇解是通解的包络.因此，由上定理可知，求微分方程的奇解归结到求它的通积分的包络.通积分的包络就是原方程的奇解.</p>          </div><p><strong>定理4.4</strong>：设 <spanclass="math inline">\(\Gamma\)</span>是曲线族（15）的一支包络.则它满足如下的<strong>C-判别式</strong>： <span class="math display">\[    V(x, y, C)=0, \quad V&#39;_C(x, y, C)=0 \tag{18}\]</span> 或（消去 <spanclass="math inline">\(C\)</span>，所得到的关系式） <spanclass="math display">\[    \Omega(x, y)=0 \tag{19}\]</span></p><p><strong>证</strong>：设包络的参数方程为 <span class="math display">\[    \begin{cases}        x=f(C) \\        y=g(C)    \end{cases}\]</span> 根据切向量共线得出一个式子，然后对 <spanclass="math inline">\(V(x, y,C)=0\)</span>对 <spanclass="math inline">\(C\)</span>求导即可.</p><p>满足C-判别式的曲线未必是相应曲线族的包络，如 <spanclass="math display">\[    (y-1)^{2}\biggl(\frac{\mathrm{d}y}{\mathrm{d}x}\biggr)^{2}=\frac{4}{9}y  \]</span> 解它的通积分和C-判别式得 <spanclass="math inline">\(y=0\)</span>或 <spanclass="math inline">\(y=3\)</span>，然而只有前一个是曲线的包络.</p><p>下面的定理给出了包络的一个充分条件.<strong>定理4.5</strong>：设由曲线族（15）的C-判别式 <spanclass="math display">\[    V(x, y,C)=0 \quad V&#39;_C(x, y, C)=0   \]</span> 确定一支连续可微且不含于族（15）的曲线 <spanclass="math display">\[    \Lambda: \quad x=\varphi(C), \quad y=\psi(C) \quad (C\in J)\]</span> 而且它满足<strong>非蜕化性条件</strong>（即切向量不退化）<span class="math display">\[    (\varphi&#39;(C),\psi&#39;(C))\neq (0,0), \quad(V&#39;_x,V&#39;_y)\neq (0,0)\]</span> 其中 <spanclass="math inline">\(V&#39;_x=V&#39;_x(\varphi(C),\psi(C),C)\)</span>与<spanclass="math inline">\(V&#39;_y=V&#39;_y(\varphi(C),\psi(C),C)\)</span>.则 <span class="math inline">\(\Lambda\)</span>是曲线族（15）的一支包络.<strong>证</strong>：在 <spanclass="math inline">\(\Lambda\)</span>上任取一点，在这一点附近用隐函数定理找到一条曲线族中的曲线，然后证明它在这一点与<span class="math inline">\(\Lambda\)</span>相切，即其切向量共线.</p><div class="note note-success">            <p>该定理基本上只适用于能求出通积分的情形，使用时通常配合4.3，这是因为若能判定C-曲线不是积分曲线，则它不是解，从而不是奇解，从而不是包络.</p>          </div><h2 id="奇解的存在定理">奇解的存在定理</h2><p>这是定理4.2的证明，可以参考教材117-120页 或者如下链接： ---</p><p>常微分方程学习笔记（4） https://zhuanlan.zhihu.com/p/97414951</p><hr /><p>下一章就进入高阶微分方程了.</p>]]></content>
    
    
    <categories>
      
      <category>分析学</category>
      
      <category>微分方程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>常微分方程（3）</title>
    <link href="/2022/04/05/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%883%EF%BC%89/"/>
    <url>/2022/04/05/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%883%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="存在和唯一性定理">存在和唯一性定理</h1><h2 id="picard-存在和唯一性定理">Picard 存在和唯一性定理</h2><p><strong>定理3.1</strong> 设初值问题 <span class="math display">\[    (E):\qquad \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y),\quad y(x_0)=y_0\tag{1}\]</span> 其中 <span class="math inline">\(f(x, y)\)</span>在矩形区域<span class="math display">\[    R: \qquad \left\vert x-x_0 \right\vert \leqslant a,\quad \left\verty-y_0 \right\vert \leqslant b       \]</span> 内连续，而且对 <span class="math inline">\(y\)</span>满足Lipschitz条件，则 <span class="math inline">\((E)\)</span>在区间<spanclass="math inline">\(I=[x_0-h,x_0+h]\)</span>上有且仅有一个解，其中常数<span class="math display">\[    h= \min\biggl\{a,\frac{b}{M}\biggr\},\quad M&gt;\max_{(x, y)\in R}\left\vert f(x, y) \right\vert     \]</span></p><p><strong>证</strong>：将微分方程(1)化为积分方程，构造Picard序列 <spanclass="math display">\[    y_{n+1}(x)=y_0+\int_{x_0}^{x} f(x,y_n(x)) \mathrm{d}x  \]</span> 用上确界判则证明 $y_{n+1}(x)-y_n(x) $在 <spanclass="math inline">\(I\)</span>上一致收敛.注意用Lipschitz条件作指数型估计.证明唯一性也运用Lipschitz条件迭代得到.</p><p>有了Picard定理，对于一般微分方程 <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y) \tag{2}\]</span> 只要能判别函数 <span class="math inline">\(f(x,y)\)</span>在某个区域 <span class="math inline">\(D\)</span>内连续并且对 <spanclass="math inline">\(y\)</span>有连续偏导数或满足Lipschitz条件，我们就可以断言区域<span class="math inline">\(D\)</span>内经过每一点有并且只有一个解.</p><p><strong>Osgood 条件</strong>：设函数 <span class="math inline">\(f(x,y)\)</span>在区域 <spanclass="math inline">\(G\)</span>内连续，而且满足不等式 <spanclass="math display">\[    \left\vert f(x, y_1)-f(x, y-2)\right\vert\leqslant F(\left\verty_1-y_2 \right\vert)  \]</span> 其中 <span class="math inline">\(F(r)&gt;0\)</span>是 <spanclass="math inline">\(r&gt;0\)</span>的连续函数，而且瑕积分 <spanclass="math display">\[    \int_{0}^{r_1} \frac{1}{F(r)} \mathrm{d}r=\infty\]</span> <span class="math inline">\(r_1&gt;0\)</span>为常数. 则称<span class="math inline">\(f(x, y)\)</span>在 <spanclass="math inline">\(G\)</span>内对 <spanclass="math inline">\(y\)</span>满足<strong>Osgood</strong>条件. 取<spanclass="math inline">\(F(r)=Lr\)</span>知，Lipschitz条件是Osgood条件的特例.</p><p><strong>定理3.2</strong>：设 <span class="math inline">\(f(x,y)\)</span>在区域 <span class="math inline">\(G\)</span>内对 <spanclass="math inline">\(y\)</span>满足Osgood条件，则微分方程(2)在 <spanclass="math inline">\(G\)</span>内经过每一点的解都是唯一的.</p><p><strong>证</strong>：反证法.</p><p><strong>Muller的反例</strong>：不满足Lipschitz条件时，定理3.1中构造的Picard序列不一定是收敛的.</p><p>设初值问题 <span class="math display">\[    (E_0):\qquad \frac{\mathrm{d}y}{\mathrm{d}x}=F(x, y),\quad y(0)=0\]</span> 其中函数 <span class="math display">\[    F(x, y)=    \begin{cases}        0, \qquad x=0,-\infty&lt;y&lt;\infty; \\        2x,\qquad 0&lt;x\leqslant 1, -\infty&lt;y&lt;0; \\        2x-\displaystyle{\frac{4y}{x}},\qquad 0&lt;x\leqslant 1,0\leqslant y&lt;x^{2}; \\        -2x,\qquad 0&lt;x\leqslant 1,x^{2}\leqslant y&lt;\infty \\    \end{cases}\]</span> 容易验证，函数 <span class="math inline">\(F(x,y)\)</span>在条形区域 <span class="math display">\[    S:\qquad 0\leqslant x\leqslant 1,\quad -\infty&lt;y&lt;\infty\]</span> 内是连续的，可是对 <spanclass="math inline">\(y\)</span>不满足Lipschitz条件.</p><p>对于上述初值问题 <spanclass="math inline">\((E_0)\)</span>，我们有Picard序列 <spanclass="math display">\[    y_{n+1}(x)=y_0+\int_{0}^{x} f(x,y_n(x)) \mathrm{d}x \quad(y_0(x)=0)\]</span> <span class="math inline">\((0\leqslant x\leqslant1;n=0,1,2,\cdots )\)</span>. 而且容易推出 <span class="math display">\[    y_n(x)=(-1)^{n+1}x^2 \quad (0\leqslant x\leqslant 1)\]</span> <span class="math inline">\((n=1,2,\cdots )\)</span>.由此可见，初值问题 <spanclass="math inline">\((E_0)\)</span>的Picard序列是不收敛的. 然而 <spanclass="math inline">\(y=\frac{1}{3}x^2(0\leqslant x\leqslant1)\)</span>是 <span class="math inline">\((E_{0})\)</span>的唯一解.</p><h2 id="peano-存在定理">Peano 存在定理</h2><p>说明：这个定理说明了，在Picard定理中如果只假定 <spanclass="math inline">\(f(x, y)\)</span>在 <spanclass="math inline">\(R\)</span>内的连续性，那么利用Euler折线仍可证明初值问题<span class="math inline">\((E)\)</span>的解在区间 <spanclass="math inline">\(\left\vert x-x_0 \right\vert \leqslanth\)</span>上是存在的（但不一定是唯一的）.</p><h3 id="euler-折线">Euler 折线</h3><p>定义比较繁琐，大概就是将区间划分，从中心点开始向左右，各作一条折线，在每个分点沿线素方向构造一条小折线段.</p><p>令Euler折线 <span class="math inline">\(\gamma_n\)</span>的表达式为<span class="math display">\[    y=\varphi_n(x) \quad (\left\vert x-x_0 \right\vert &lt;h) \tag{3}\]</span></p><p>Euler折线的计算公式： <span class="math display">\[    \varphi_n(x)=y_0+\sum_{k=0}^{s-1} f(x_k, y_k)(x_{k+1}-x_k)+f(x_s,y_s)(x-x_s), \quad x_s&lt;x\leqslant x_{0}+h\]</span> 和 <span class="math display">\[    \varphi_n(x)=y_0+\sum_{k=0}^{-s+1} f(x_k,y_k)(x_{k-1}-x_k)+f(x_{-s}, y{-s})(x-x_{-s}), \quad x_0-h\leqslantx&lt;x_0\]</span> 下面证明Euler折线在区间 <span class="math inline">\(\left\vertx-x_0 \right\vert \leqslanth\)</span>上收敛（或至少有一个收敛的子序列）.</p><h3 id="ascoli-引理">Ascoli 引理</h3><p>设函数列 <span class="math display">\[    f_1(x),f_2(x),\cdots,f_n(x)\]</span> 在有限闭区间 <spanclass="math inline">\(I\)</span>上是一致有界和等度连续的，则可以选取它的一个子序列<span class="math display">\[    f_{n_1}(x),f_{n_2}(x),\cdots,f_{n_k}(x)\]</span> 使它在区间 <spanclass="math inline">\(I\)</span>上是一致收敛的.</p><h3 id="peano-存在定理-1">Peano 存在定理</h3><p><strong>引理3.1</strong> Euler序列(3)在区间 <spanclass="math inline">\(\left\vert x-x_0 \right\vert \leqslanth\)</span>上至少有一个一致收敛的子序列.</p><p><strong>证</strong>：由Ascoli 引理显然.</p><p><strong>引理3.2</strong> Euler折线 <spanclass="math inline">\(y=\varphi_n(x)\)</span>在区间 <spanclass="math inline">\(\left\vert x-x_0 \right\vert \leqslanth\)</span>上满足关系式 <span class="math display">\[    \varphi_n(x)=y_0+\int_{x_0}^{x} f(x,\varphi_n(x))\mathrm{d}x+\delta_n(x)\]</span> 其中函数 <spanclass="math inline">\(\delta_n(x)\)</span>趋于零，即 <spanclass="math display">\[    \lim_{n \to \infty}\delta_n(x)=0 \quad(\left\vert x-x_0 \right\vert\leqslant h)\]</span> <strong>证</strong>：omitted.</p><p><strong>定理3.3（Peano 存在定理）</strong>：设函数 <spanclass="math inline">\(f(x, y)\)</span>在矩形区域 <spanclass="math inline">\(R\)</span>内连续，则初值问题 <spanclass="math display">\[    (E):\qquad \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y),\quad y(x_0)=y_0\tag{4}\]</span> 在区间 <span class="math inline">\(\left\vert x-x_0\right\vert \leqslant h\)</span>上至少有一个解 <spanclass="math inline">\(y=y(x)\)</span>，这里矩形区域 <spanclass="math inline">\(R\)</span>和正数 <spanclass="math inline">\(h\)</span>的定义同定理3.1.</p><p><strong>证</strong>：用引理3.1取一个一致收敛的子函数列，再用引理3.2取<span class="math inline">\(k \to \infty\)</span>即可.</p><p><strong>反例</strong>：若不要求 <span class="math inline">\(f(x,y)\)</span>的连续性，那么上面的初值问题 <spanclass="math inline">\((E)\)</span>可能是无解的. 例如，设函数 <spanclass="math display">\[    f^*(x, y)=    \begin{cases}        1,\qquad 1\leqslant \left\vert x+y \right\vert &lt;\infty \\        (-1)^{n}, \qquad \frac{1}{n+1}\leqslant \left\vert x+y\right\vert \leqslant \frac{1}{n} \quad (n=1,2,\cdots ) \\        0, \qquad \left\vert x+y \right\vert =0    \end{cases}\]</span> 则用反证法易证初值问题 <span class="math display">\[    (E^*): \qquad \frac{\mathrm{d}y}{\mathrm{d}x}=f^*(x, y), \quady(0)=0\]</span> 没有（连续的）解.</p><h2 id="解的延伸">解的延伸</h2><p>本节的目的是把初值问题解的存在性从局部扩大到整体.</p><p>设微分方程 <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y) \tag{5}\]</span> 其中函数 <span class="math inline">\(f(x, y)\)</span>在区域<span class="math inline">\(G\)</span>内连续.</p><p><strong>定理3.4</strong>：设 <spanclass="math inline">\(P_0\)</span>为区域 <spanclass="math inline">\(G\)</span>内任一点，并设 <spanclass="math inline">\(\Gamma\)</span>为微分方程(5)经过 <spanclass="math inline">\(P_0\)</span>点的任一条积分曲线. 则积分曲线 <spanclass="math inline">\(\Gamma\)</span>将在区域 <spanclass="math inline">\(G\)</span>内延伸到边界（换句话说，对于任何有界闭区域<span class="math inline">\(G(P_0 \in G_1 \subset G)\)</span>，积分曲线<span class="math inline">\(\Gamma\)</span>将延伸到 <spanclass="math inline">\(G_1\)</span>之外）.</p><p><strong>推论</strong>：设函数 <span class="math inline">\(f(x,y)\)</span>在区域 <span class="math inline">\(G\)</span>内连续，而且对<spanclass="math inline">\(y\)</span>满足局部Lipschitz条件，则微分方程(5)经过<span class="math inline">\(G\)</span>内任一点 <spanclass="math inline">\(P_0\)</span>存在唯一的积分曲线 <spanclass="math inline">\(\Gamma\)</span>，并且 <spanclass="math inline">\(\Gamma\)</span>在 <spanclass="math inline">\(G\)</span>内延伸到边界.</p><p>这两个结论说明了最大存在区间的形式：开区间. 比如微分方程 <spanclass="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=x^{2}+y^{2}\]</span> 任一解的存在区间是有界的.</p><div class="note note-info">            <p>这个推论十分重要.一般遇到的大量微分方程都满足推论的条件.</p>          </div><p>在特定的条件下，对解的存在区间可以作出先验断言.<strong>定理3.5</strong>：设微分方程 <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y) \tag{6}\]</span> 其中函数 <span class="math inline">\(f(x,y)\)</span>在条形区域 <span class="math display">\[    S: \qquad \alpha&lt;x&lt;\beta,\quad -\infty&lt;y&lt;\infty\]</span> 内连续，而且满足不等式 <span class="math display">\[    \left\vert f(x, y) \right\vert \leqslant A(x)\left\vert y\right\vert +B(x)\]</span> 其中 <span class="math inline">\(A(x)\geqslant 0\)</span>和<span class="math inline">\(B(x)\geqslant 0\)</span>在区间 <spanclass="math inline">\(\alpha&lt;x&lt;\beta\)</span>上是连续的.则微分方程(6)的每一个解都以区间 <spanclass="math inline">\(\alpha&lt;x&lt;\beta\)</span>为最大存在区间.</p><h2 id="比较定理及其应用">比较定理及其应用</h2><p>在上节看到，仅应用延伸定理无法对微分方程的解的存在区间作出估计.下面几个定理是对此的讨论<strong>定理3.6（第一比较定理）</strong>：设函数 <spanclass="math inline">\(f(x, y)\)</span>与 <spanclass="math inline">\(F(x, y)\)</span>都在平面区域 <spanclass="math inline">\(G\)</span>内连续且满足不等式 <spanclass="math display">\[    f(x, y)&lt;F(x, y),\quad (x, y) \in G\]</span> 又设函数 <span class="math inline">\(y=\varphi(x)\)</span>与<span class="math inline">\(y=\Phi(x)\)</span>在区间 <spanclass="math inline">\(a&lt;x&lt;b\)</span>上分别是初值问题 <spanclass="math display">\[    (E_1):\qquad \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y),\quady(x_0)=y_0\]</span> 与 <span class="math display">\[    (E_2):\qquad \frac{\mathrm{d}y}{\mathrm{d}x}=F(x, y),\quady(x_0)=y_0\]</span> 的解，其中 <span class="math inline">\((x_0,y_0)\inG\)</span>. 则我们有 <span class="math display">\[    \begin{cases}        \varphi(x)&lt;\Phi(x),\qquad x_0&lt;x&lt;b \\        \varphi(x)&gt;\Phi(x),\qquad a&lt;x&lt;x_0    \end{cases}\]</span></p><p><strong>证</strong>：omitted</p><p>对一般的初值问题(E)，有两个解 <spanclass="math inline">\(y=Z(x)\)</span>和 <spanclass="math inline">\(y=W(x)\)</span>，使得(E)的任何解 <spanclass="math inline">\(y=y(x)\)</span>都满足不等式： <spanclass="math display">\[    W(x)\leqslant y(x)\leqslant Z(x),\quad(\left\vert x-x_0 \right\vert\leqslant h)\]</span> 则称 <span class="math inline">\(y=W(x)\)</span>和 <spanclass="math inline">\(y=Z(x)\)</span>分别为初值问题(E)的<strong>最小解</strong>和<strong>最大解</strong>.</p><p><strong>定理3.7</strong>：存在正数 <spanclass="math inline">\(\sigma&lt;h\)</span>，使得在区间 <spanclass="math inline">\(\left\vert x-x_0 \right\vert \leqslant\sigma\)</span>上，上述初值问题(E)有最小解和最大解.</p><p>注意，由于初值问题(E)的所有解在 <spanclass="math inline">\((x_0,y_0)\)</span>点均相切，所以(E)的左行最大（小）解和右行最大（小）解就可拼接为整个区间上的最大（小）解.</p><p>(E)的解是唯一的，当且仅当它的最小解和最大解是恒同的.</p><p><strong>定理3.8（第二比较定理）</strong>：设函数 <spanclass="math inline">\(f(x, y)\)</span>与 <spanclass="math inline">\(F(x, y)\)</span>都在平面区域 <spanclass="math inline">\(G\)</span>内连续且满足 <spanclass="math display">\[    f(x, y)\leqslant F(x, y),\quad (x, y) \in G\]</span> 又设函数 <span class="math inline">\(y=\varphi(x)\)</span>与<span class="math inline">\(y=\Phi(x)\)</span>在区间 <spanclass="math inline">\(a&lt;x&lt;b\)</span>上分别是初值问题 <spanclass="math display">\[    (E_1):\qquad \frac{\mathrm{d}y}{\mathrm{d}x}=f(x, y),\quady(x_0)=y_0\]</span> 与 <span class="math display">\[    (E_2):\qquad \frac{\mathrm{d}y}{\mathrm{d}x}=F(x, y),\quady(x_0)=y_0\]</span> 的解，并且 <span class="math inline">\(y=\varphi(x)\)</span>是<spanclass="math inline">\((E_1)\)</span>的右行最小解和左行最大解（或者：<spanclass="math inline">\(y=\Phi(x)\)</span>是 <spanclass="math inline">\((E_2)\)</span>的右行最大解和左行最小解），则有如下比较关系：<span class="math display">\[    \varphi(x)\leqslant \Phi(x),\qquad x_0\leqslant x&lt;b; \\    \varphi(x)\geqslant \Phi(x),\qquad a&lt;x\leqslant x_0\]</span> <strong>证</strong>：此定理容易从定理3.6和定理3.7推出.</p>]]></content>
    
    
    <categories>
      
      <category>分析学</category>
      
      <category>微分方程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>常微分方程（2）</title>
    <link href="/2022/04/01/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%882%EF%BC%89/"/>
    <url>/2022/04/01/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%882%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="初等积分法">初等积分法</h1><h2 id="恰当方程">恰当方程</h2><p>考虑对称形式的一阶微分方程 <span class="math display">\[P(x,y)\mathrm{d}x+Q(x,y)\mathrm{d}y=0\]</span> 如果存在一个可微函数<spanclass="math inline">\(\Phi(x,y)\)</span>使得它的全微分为 <spanclass="math display">\[\mathrm{d}\Phi(x,y)=P(x,y)\mathrm{d}x+Q(x,y)\mathrm{d}y\]</span>则称该方程为<strong>恰当方程</strong>或<strong>全微分方程</strong>. 此时<span class="math display">\[\Phi(x,y)=C\]</span> 是该方程的一个通解. 这一点的验证需要用到隐函数定理.</p><p><strong>定理2.1</strong> 设函数<spanclass="math inline">\(P(x,y)\)</span>和<spanclass="math inline">\(Q(x,y)\)</span>在区域 <spanclass="math display">\[R:\quad \alpha&lt;a&lt;\beta,\quad \gamma&lt;y&lt;\delta\]</span> 上连续，且有连续的一阶偏导数<spanclass="math inline">\(\frac{\partial P}{\partial y}\)</span>与<spanclass="math inline">\(\frac{\partial Q}{\partial x}\)</span>，则上微分方程是恰当方程的充要条件为恒等式 <span class="math display">\[\frac{\partial P}{\partial y}(x,y)=\frac{\partial Q}{\partial x}(x,y)\]</span> 在<span class="math inline">\(R\)</span>内成立.此时可以给出方程的通积分： <span class="math display">\[\int_{x_0}^xP(x,y)\mathrm{d}x+\int_{y_0}^yQ(x_0,y)\mathrm{d}y=C\]</span> 或者 <span class="math display">\[\int_{x_0}^xP(x,y_0)\mathrm{d}x+\int_{y_0}^yQ(x,y)\mathrm{d}y=C\]</span> 其中<span class="math inline">\((x_0,y_0)\)</span>是<spanclass="math inline">\(R\)</span>中任意取定的一点.</p><h2 id="变量分离的方程">变量分离的方程</h2><p>如果微分方程 <span class="math display">\[    P(x,y)\mathrm{d}x+Q(x,y)\mathrm{d}y=0\]</span> 中的函数<span class="math inline">\(P(x,y)\)</span>和<spanclass="math inline">\(Q(x,y)\)</span>均可分别表示为<spanclass="math inline">\(x\)</span>的函数与<spanclass="math inline">\(y\)</span>的函数的乘积，则称上方程为<strong>变量分离的方程</strong>.令 <span class="math display">\[    P(x,y)=X(x)Y_1(y),\quad Q(x,y)=X_1(x)Y(y)\]</span> 以因子<spanclass="math inline">\(X_1(x)Y_1(y)\)</span>去除上式两端，再积分，得到通积分<span class="math display">\[\int_{}^{}\frac{X(x)}{X_1(x)}\mathrm{d}x+\int_{}^{}\frac{Y(y)}{Y_1(y)}\mathrm{d}y=C\]</span> 但注意要补上如下形式的特解（如果它们不在上述通积分之内的话）：<span class="math display">\[x=a_i,\quad (i=1,2,\cdots),\text{其中$a_i$是$X_1(x)=0$的根}\]</span> 和 <span class="math display">\[y=b_j,\quad (i=1,2,\cdots),\text{其中$b_j$是$Y_1(y)=0$的根}\]</span></p><p>习题中有一个有趣的结果： 设微分方程 <span class="math display">\[\frac{\mathrm{d}y}{\mathrm{d}x}=f(y)\]</span> 其中<span class="math inline">\(f(y)\)</span>在<spanclass="math inline">\(y=a\)</span>的某领域（例如区间<spanclass="math inline">\(\lvert y-a\rvert \leqslant\varepsilon\)</span>）内连续，而且<spanclass="math inline">\(f(y)=0\)</span>当且仅当<spanclass="math inline">\(y=a\)</span>. 则在直线<spanclass="math inline">\(y=a\)</span>上的每一点，上方程的解是局部唯一的当且仅当瑕积分<span class="math display">\[\biggl\lvert \int_{a}^{a\pm \varepsilon}\frac{\mathrm{d}y}{f(y)}\biggr\rvert=\infty \text{（发散）}\]</span></p><p>proof 设<span class="math inline">\(y(x_0)=a\)</span>.首先，常值函数<spanclass="math inline">\(y(x)=a\)</span>显然是方程的解.</p><p><span class="math inline">\(\Leftarrow\)</span>：假设方程还有解<spanclass="math inline">\(y=g(x)\neq a\)</span>，则<spanclass="math inline">\(\exists x_1\)</span>，<spanclass="math inline">\(h=g(x_1)-a\neq 0\)</span>，则有 <spanclass="math display">\[    \biggl\lvert \int_{a}^{a+h}\frac{1}{f(y)}\mathrm{d}y\biggr\rvert=\biggl\lvert\int_{x_0}^{x_1}\mathrm{d}x\biggr\rvert          \]</span> 右侧积分有限，与瑕积分发散矛盾.</p><p><span class="math inline">\(\Rightarrow\)</span>：假设方程只有唯一解<span class="math inline">\(y(x)=a\)</span>，若右侧瑕积分收敛，令 <spanclass="math inline">\(g(x)=\int_{a}^{x}\frac{1}{f(y)}\mathrm{d}y\)</span> 则由 <span class="math display">\[    g&#39;(y)=\frac{1}{f(y)} ,y\neq a    \]</span> 可知有隐函数 <span class="math inline">\(y=h(g)\neqa\)</span>，从而有 <span class="math display">\[    \frac{\mathrm{d}h(g)}{\mathrm{d}g}=\frac{\mathrm{d}y}{\mathrm{d}g(y)} =f(y)=f(h(g))\]</span> 即 <span class="math inline">\(y=h(g)\)</span> 也是解，但由<span class="math inline">\(h(g)\neq a\)</span> 知与解唯一矛盾！</p><h2 id="一阶线性方程">一阶线性方程</h2><p><strong>一阶线性方程</strong>即 <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x} +p(x)y=q(x) \qquad (1)\]</span> 其中函数<span class="math inline">\(p(x)\)</span>和<spanclass="math inline">\(q(x)\)</span>在区间 <spanclass="math inline">\(I=(a,b)\)</span>上连续，当<spanclass="math inline">\(q(x)\equiv 0\)</span>时，方程成为 <spanclass="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x} +p(x)y=0 \qquad (2)\]</span> 当<spanclass="math inline">\(q(x)\)</span>不恒等于零时，称(1)为<strong>非齐次</strong>线性方程；而(2)为相应的<strong>齐次</strong>线性方程.</p><p>(2)的通解形如 <span class="math display">\[    y=C\mathrm{e}^{-\int_{}^{}p(x)\mathrm{d}x}\]</span> 其中<span class="math inline">\(C\)</span>是任意常数</p><p>(1)的通解形如 <span class="math display">\[    y=\mathrm{e}^{-\int_{}^{}p(x)\mathrm{d}x}\biggl(C+\int_{}^{}q(x)\mathrm{e}^{\int_{}^{}p(x)\mathrm{d}x}\mathrm{d}x    \biggr)\]</span> 其中<spanclass="math inline">\(C\)</span>是一个任意常数，取原函数时取同一个.上方法称为<strong>积分因子法</strong>. 这里的积分因子 <spanclass="math display">\[    \mu(x)=\mathrm{e}^{\int_{}^{}p(x)\mathrm{d}x}\]</span> 注意积分因子的奇点作特殊讨论. 对于给定的初值<spanclass="math inline">\(y(x_0)=y_0\)</span>，解为 <spanclass="math display">\[    y=y_0\mathrm{e}^{-\int_{x_0}^{x}p(x)\mathrm{d}x}+\int_{x_0}^{x}q(s)\mathrm{e}^{-\int_{s}^{x}p(t)\mathrm{d}t}\mathrm{d}s\]</span> 其中<span class="math inline">\(p(x)\)</span>和<spanclass="math inline">\(q(x)\)</span>在区间<spanclass="math inline">\(I\)</span>上连续.</p><h2 id="初等变换法">初等变换法</h2><h3 id="齐次方程"><strong>齐次方程</strong>：</h3><p>如果微分方程 <span class="math display">\[    P(x,y)\mathrm{d}x+Q(x,y)\mathrm{d}y=0         \]</span> 中的函数<span class="math inline">\(P(x)\)</span>和<spanclass="math inline">\(Q(x)\)</span>都是<spanclass="math inline">\(x\)</span>和<spanclass="math inline">\(y\)</span>的同次（例如<spanclass="math inline">\(m\)</span>次）齐次函数，即： <spanclass="math display">\[    P(tx,ty)=t^mP(x,y),\quad Q(tx,ty)=t^mQ(x,y),\]</span>则称上方程为<strong>齐次方程</strong>（注意这与上节定义的齐次线性方程不是一回事）.做替换 <span class="math display">\[    y=ux\]</span> 得到 <span class="math display">\[    x^m[P(1,u)+uQ(1,u)]\mathrm{d}x+x^{m+1}Q(1,u)\mathrm{d}u=0     \]</span> 这是一个变量分离的方程. 齐次方程的另一个等价定义是 <spanclass="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=\Phi(\frac{y}{x} )  \]</span> 注意：做代换时要排除<spanclass="math inline">\(x=0\)</span>的情况，因<spanclass="math inline">\(x=0\)</span>时代换不可逆.</p><h3 id="伯努利bernoulli方程">伯努利(Bernoulli)方程</h3><p>形如 <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}+p(x)y=q(x)y^n  \]</span> 的方程称为伯努利方程，其中<spanclass="math inline">\(n\)</span>为常数，而且<spanclass="math inline">\(n\neq 0\)</span>和<spanclass="math inline">\(1\)</span>. 以<spanclass="math inline">\((1-n)y^{-n}\)</span> 乘方程两边，然后令 <spanclass="math inline">\(z=y^{1-n}\)</span>， 有 <spanclass="math display">\[    \frac{\mathrm{d}z}{\mathrm{d}x} +(1-n)p(x)z=(1-n)q(x)\]</span> 这是关于未知函数<spanclass="math inline">\(z\)</span>的一阶线性方程.</p><h3 id="里卡蒂riccati方程">里卡蒂(Riccati)方程</h3><p>加入一阶微分方程 <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x} =f(x,y)\]</span> 的右端函数<spanclass="math inline">\(f(x,y)\)</span>是一个关于<spanclass="math inline">\(y\)</span>的二次多项式，则称此方程为二次方程；它可写成如下形式：<span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x} =p(x)y^2+q(x)y+r(x)\]</span> 其中<span class="math inline">\(p(x)\)</span>不恒为零.</p><p><strong>定理</strong> 设已知里卡蒂方程的一个特解<spanclass="math inline">\(y=\phi_1(x)\)</span>，则可用积分法求得它的通解.</p><p>证：做代换<spanclass="math inline">\(y=u+\phi_1(x)\)</span>可将里卡蒂方程化为伯努利方程.</p><p><strong>定理</strong> 设里卡蒂方程 <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x} +ay^2=bx^m\]</span> 其中<span class="math inline">\(a\neq 0\)</span>，<spanclass="math inline">\(b,m\)</span>都是常数. 又设<spanclass="math inline">\(x\neq 0\)</span>和<span class="math inline">\(y\neq 0\)</span>. 则当且仅当 <span class="math display">\[    m=0,-2,\frac{-4k}{2k+1} ,\frac{-4k}{2k-1}\quad(k=1,2,\cdots)  \]</span> 时，上方程可通过适当的变换化为变量分离的方程.</p><h2 id="积分因子法">积分因子法</h2><p>想法是对一般的方程 <span class="math display">\[    P(x,y)\mathrm{d}x+Q(x,y)\mathrm{d}y=0 \tag{1}    \]</span> 设法找一个可微的非零函数<span class="math inline">\(\mu =\mu(x,y)\)</span>，使得用它乘上方程后，所得方程 <spanclass="math display">\[    \mu (x,y)P(x,y)\mathrm{d}x+\mu(x,y)Q(x,y)\mathrm{d}y=0 \tag{2}\]</span> 成为恰当方程，亦即 <span class="math display">\[    \frac{\partial (\mu P)}{\partial y} = \frac{\partial (\muQ)}{\partial x} \tag{3}\]</span> 这时，函数<spanclass="math inline">\(\mu=\mu(x,y)\)</span>叫作上方程的一个<strong>积分因子</strong>.(3)也可以写成 <span class="math display">\[    P\frac{\partial \mu}{\partial y} -Q\frac{\partial \mu}{\partial x}=\biggl(\frac{\partial Q}{\partial x} -\frac{\partial P}{\partial y}\biggr)\mu\]</span> <strong>定理</strong> 微分方程(1)有一个只依赖于<spanclass="math inline">\(x\)</span>的积分因子的充要条件是 <spanclass="math display">\[    \frac{1}{Q(x,y)} \biggl(\frac{\partial P(x,y)}{\partial y}-\frac{\partial Q(x,y)}{\partial x} \biggr) \tag{4}\]</span> 只依赖于<span class="math inline">\(x\)</span>，而与<spanclass="math inline">\(y\)</span>无关. 此时记(4)为<spanclass="math inline">\(G(x)\)</span>，则此时 <spanclass="math display">\[    \mu(x)=\mathrm{e}^{\int G(x)\mathrm{d}x}\]</span> 是(1)的一个积分因子.</p><p>类似的，微分方程(1)有一个只依赖于<spanclass="math inline">\(y\)</span>的积分因子的充要条件是 <spanclass="math display">\[    \frac{1}{P(x,y)} \biggl(\frac{\partial Q(x,y)}{\partial x}-\frac{\partial P(x,y)}{\partial y} \biggr) =H(y)\]</span> 此时 <span class="math display">\[    \mu(y)=\mathrm{e}^{\int H(y)\mathrm{d}y}\]</span> 是(1)的一个积分因子.</p><p>还可以<strong>分组求积分因子</strong>. 理由是下述定理：</p><p><strong>定理</strong> 若<span class="math inline">\(\mu =\mu(x,y)\)</span>是方程(1)的一个积分因子，使得 <spanclass="math display">\[    \mu P(x,y)\mathrm{d}x+\mu Q(x,y)\mathrm{d}y=\mathrm{d} \Phi (x,y)\]</span> 则<spanclass="math inline">\(\mu(x,y)g(\Phi(x,y))\)</span>也是(1)的一个积分因子，其中<spanclass="math inline">\(g(\cdot)\)</span>是任一可微的（非零）函数.</p><p>证：验证恰当方程的条件即可. 事实上有 <span class="math display">\[    \mu g(\Phi) P\mathrm{d}x+\mu g(\Phi)Q\mathrm{d}y=\mathrm{d}(\intg)(\Phi)\]</span></p><p>最后，若<spanclass="math inline">\(P(x,y)\mathrm{d}x+Q(x,y)\mathrm{d}y=0\)</span>是齐次方程，则函数<span class="math display">\[    \mu(x,y)=\frac{1}{xP(x,y)+yQ(x,y)}  \]</span> 是一个积分因子. 这是齐次函数的Euler定理的直接应用.</p><h2 id="应用举例">应用举例</h2><p>假设在 <span class="math inline">\((x, y)\)</span>平面上由方程 <spanclass="math display">\[    \Phi(x,y,C)=0 \tag{5}   \]</span> 给出一个以 <spanclass="math inline">\(C\)</span>为参数的曲线族. 称另一族曲线 <spanclass="math display">\[    \Psi(x,y,K) \tag{6}\]</span> 其中 <spanclass="math inline">\(K\)</span>为参数，使得族(6)中的任一条曲线与族(5)中的每一条曲线相交成定角<span class="math inline">\(\alpha\)</span> (<spanclass="math inline">\(-\frac{\pi}{2}&lt;\alpha&lt;\frac{\pi}{2}\)</span>,以逆时针方向为正).称这样的曲线族(6)为已知曲线族(5)的<strong>等角轨线族</strong>. 特别，当<spanclass="math inline">\(\alpha=\frac{\pi}{2}\)</span>时，称曲线族(6)为(5)的<strong>正交轨线族</strong>.</p><p>(5)满足的微分方程 <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=H(x, y)\]</span> 其中 <span class="math display">\[    H(x, y)=-\frac{\Phi&#39;_x (x, y,C(x, y))}{\Phi&#39;_y (x,y,C(x,y))}\]</span> 这里 <span class="math inline">\(C=C(x, y)\)</span>是由 <spanclass="math inline">\(\Phi(x,y,C)\)</span>决定的函数.</p><p>当 <span class="math inline">\(\alpha\neq\frac{\pi}{2}\)</span>时，有 <span class="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=\frac{H(x, y)+\tan\alpha}{1-H(x,y)\tan \alpha}\]</span> 而当 <spanclass="math inline">\(\alpha=\frac{\pi}{2}\)</span>时，就有 <spanclass="math display">\[    \frac{\mathrm{d}y}{\mathrm{d}x}=-\frac{1}{H(x, y)}\]</span> 这些 <span class="math inline">\(H(x,y)\)</span>都是相同的.</p>]]></content>
    
    
    <categories>
      
      <category>分析学</category>
      
      <category>微分方程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>常微分方程（1）</title>
    <link href="/2022/04/01/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%881%EF%BC%89/"/>
    <url>/2022/04/01/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%881%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>最近其他东西读得有点累了，开个ODE的新坑，这主要是因为一直在读的NeuronalDynamics里用到了很多ODE。参考书基本是丁同仁的《常微分方程教程》，会查知乎上大佬整理的笔记，可能也会附加一点Arnold的《OrdinaryDifferentialEquations》（大概吧），大概就不指望会有动力系统的东西了。</p><p>【定义1.1】方程 <span class="math display">\[F(x,y,y&#39;,\cdots,y^{(n)})\]</span> 叫作常微分方程，其中导数实际出现的最高阶数<spanclass="math inline">\(n\)</span>叫作常微分方程的阶.</p><p>一般会给一个初值条件： <span class="math display">\[y(x_0)=y_0,y&#39;(x_0)=y&#39;_0,\cdots,y^{(n-1)}(x_0)=y^{(n-1)}_0\]</span></p><p>【定义1.2】微分方程的解的概念. 设<spanclass="math inline">\(n\)</span>阶微分方程的解 <spanclass="math display">\[y=\varphi(x,C_1,\cdots,C_n)\]</span> 包含<spanclass="math inline">\(n\)</span>个独立的任意常数<spanclass="math inline">\(C_1,\cdots,C_n\)</span>（这是因为每做一次不定积分就会多一个常数），则称它为通解，独立的意思是Jacobi行列式<span class="math display">\[\det\begin{bmatrix}    \frac{\partial \varphi}{\partial C_1} &amp; \frac{\partial\varphi}{\partial C_2} &amp; \cdots &amp; \frac{\partial\varphi}{\partial C_n} \\    \frac{\partial \varphi&#39;}{\partial C_1} &amp; \frac{\partial\varphi&#39;}{\partial C_2} &amp; \cdots &amp; \frac{\partial\varphi&#39;}{\partial C_n} \\    \vdots &amp; \vdots &amp; &amp; \vdots \\    \frac{\partial \varphi^{(n-1)}}{\partial C_1} &amp; \frac{\partial\varphi^{(n-1)}}{\partial C_2} &amp; \cdots &amp; \frac{\partial\varphi^{(n-1)}}{\partial C_n} \\\end{bmatrix}\]</span> 不为零，若解<spanclass="math inline">\(y=\varphi(x)\)</span>不包含任意常数，则称它为特解.这个定义的合理性以后再说.</p><p>利用初值条件，可以确定通解中的任意常数<spanclass="math inline">\(C_1,\cdots,C_n\)</span>.注意：由于分析方法的限制（如隐函数存在定理的局部性），一般只能在局部范围内讨论通解.常数独立的条件保证可以反解出<spanclass="math inline">\(C_1,\cdots,C_n\)</span>. 为此要求<spanclass="math inline">\(F\)</span>是<spanclass="math inline">\(C^1\)</span>的，不过一般遇到的ODE都是充分光滑的.</p><p>考虑一阶微分方程 <span class="math display">\[\frac{\textrm{d}y}{\textrm{d}x}=f(x,y)\]</span> 其中<span class="math inline">\(f(x,y)\)</span>是平面区域<spanclass="math inline">\(G\)</span>内的连续函数. 它的解<spanclass="math inline">\(y=\varphi(x)\)</span>在<spanclass="math inline">\((x,y)\)</span>平面上的图形——一条光滑曲线<spanclass="math inline">\(\Gamma\)</span>为微分方程的<strong>积分曲线</strong>.即使并不知道积分曲线是什么，也可以知道在积分曲线任意一点处的切线方程.书中给出了<strong>线素</strong>和<strong>线素场</strong>或<strong>方向场</strong>的概念，其实就是切向量场，可以回忆一下切丛等微分流形中的概念.</p><p>利用由关系式<spanclass="math inline">\(f(x,y)=k\)</span>确定的曲线<spanclass="math inline">\(L_k\)</span>，称它为线素场的<strong>等斜线</strong>.一般会利用等斜线上点的切向量方向来画出向量场的简图.</p><p>如果一阶微分方程取如下形式： <span class="math display">\[\frac{\textrm{d}y}{\textrm{d}x}=-\frac{P(x,y)}{Q(x,y)}\]</span> 那么当<spanclass="math inline">\(P(x_0,y_x)=Q(x_0,y_0)=0\)</span>时，称这样的点<spanclass="math inline">\((x_0,y_0)\)</span>为相应微分方程的<strong>奇异点</strong>.线素场在奇异点没有意义. 奇异点的概念十分关键.</p>]]></content>
    
    
    <categories>
      
      <category>分析学</category>
      
      <category>微分方程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高代课外题2</title>
    <link href="/2022/03/29/%E9%AB%98%E4%BB%A3%E8%AF%BE%E5%A4%96%E9%A2%982/"/>
    <url>/2022/03/29/%E9%AB%98%E4%BB%A3%E8%AF%BE%E5%A4%96%E9%A2%982/</url>
    
    <content type="html"><![CDATA[<h1 id="高代课外题2">高代课外题2</h1><h3id="华罗庚不等式let-abin-mathbbcntimes-n-be-such-that-i-aa-geqslant-0-i-bb-geqslant-0.-then-deti-ab2geqslant-deti-aa-cdot-deti-bb">1.（华罗庚不等式）Let<span class="math inline">\(A,B\in \mathbb{C}^{n\times n}\)</span> besuch that <span class="math inline">\(I-AA^{*} \geqslant 0\)</span>,<span class="math inline">\(I-BB^{*} \geqslant 0\)</span>. Then <spanclass="math display">\[ (\det(I-AB^*))^2\geqslant \det(I-AA^*) \cdot\det(I-BB^*)\]</span></h3><p><strong>proof1</strong><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="《矩阵论中不等式》 王松桂 贾忠贞 著 安徽教育出版社">[1]</span></a></sup>：首先证明一个引理：</p><p>Lemma 设Hermite阵 <span class="math inline">\(A\geqslant0\)</span>，<span class="math inline">\(B\geqslant 0\)</span>.则 <spanclass="math display">\[ \det(A+B) \geqslant \det(A) + \det(B)\]</span>等号成立 <span class="math inline">\(\Longleftrightarrow\)</span> <spanclass="math inline">\(A=0\)</span>或<spanclass="math inline">\(B=0\)</span>或 <spanclass="math inline">\(\det(A+B)=0\)</span></p><p>proof 由谱定理，存在酉阵<span class="math inline">\(P\)</span>，使得<span class="math inline">\(PBP^*=\Lambda=\text{diag}(\lambda_1,\cdots,\lambda_n), \lambda_1 \geqslant \cdots \geqslant\lambda_n \geqslant 0\)</span> 为<spanclass="math inline">\(B\)</span>的特征值. 因为 <spanclass="math inline">\(\det(A+B)=\det(PAP^*+\Lambda)\)</span>，<spanclass="math inline">\(\det(A)=\det(PAP^*)\)</span>，<spanclass="math inline">\(\det(B)=\det(\Lambda)\)</span>，且 <spanclass="math inline">\(PAP^*\geqslant 0\)</span>，故不失一般性，可假设<span class="math inline">\(B=\Lambda\)</span>.</p><p>将 <span class="math inline">\(\det(A+\Lambda)\)</span>展开，得 <spanclass="math display">\[\det(A+\Lambda)=\det(A)+\sum_{i=1}^n\lambda_id_i+\cdots+\prod_{i=1}^n\lambda_i\]</span></p><p>其中 <span class="math inline">\(d_{i_1\cdots i_k}\)</span>表示从<span class="math inline">\(A\)</span>中剔除第 <spanclass="math inline">\(i_1,\cdots,i_k\)</span>行和列之后剩下方阵的行列式，因为<span class="math inline">\(A\geqslant 0\)</span>，故所有的 <spanclass="math inline">\(d_{i_1\cdots i_k}\geqslant 0\)</span>. 又 <spanclass="math inline">\(\det(B)=\prod_{i=1}^n\lambda_i\)</span>，故 <spanclass="math display">\[\det(A+\Lambda)\geqslant\det(A)+\det(\Lambda)\]</span></p><p>现在证明等号成立的充要条件. 充分性是显然的.以下证必要性，分两种情况来考虑.</p><p>(1)若 <span class="math inline">\(\det(B)\neq 0\)</span>，此时每个<span class="math inline">\(\lambda_i\neq 0\)</span>.此时必有对一切可能的指标，<span class="math inline">\(d_{i_1\cdotsi_n}=0\)</span>. 特别当<spanclass="math inline">\(k=n-1\)</span>时知道<spanclass="math inline">\(A\)</span>的所有对角元为零. 但 <spanclass="math inline">\(A\geqslant 0\)</span>，故<spanclass="math inline">\(A=0\)</span>.</p><p>(2)若 <span class="math inline">\(\det(B)=0\)</span>，但 <spanclass="math inline">\(B\neq 0\)</span>. 此时至少有一个 <spanclass="math inline">\(\lambda_i \neq 0\)</span>. 因所有 <spanclass="math inline">\(d_{i_1\cdots i_k} \geqslant0\)</span>，但从等号成立知，至少有一个 <spanclass="math inline">\(d_{i_1\cdots i_k} = 0\)</span>. 故<spanclass="math inline">\(A\)</span>不可能是正定的，于是 <spanclass="math inline">\(\det(A)=0\)</span>. 由等号成立知，<spanclass="math inline">\(\det(A+B)=0\)</span>，证毕.</p><p>回到原题，一个重要的观察是（不妨<spanclass="math inline">\(I-AA^*,I-BB^*\)</span>均正定） <spanclass="math display">\[    (I-AB^*)(I-BB^*)^{-1}(I-AB^*)^*    =(I-AA^*)+(A-B)(I-B^*B)^{-1}(A-B)^*\]</span></p><p>这个恒等式可以用（不严格的）等比数列展开理解，并用相关的（严格的）恒等式证明。注意<span class="math inline">\(\det(I-BB^*)=\det(I-B^*B)\)</span>，<spanclass="math inline">\(I-AA^* &gt; 0\)</span>，<spanclass="math inline">\((A-B)(I-BB^*)^{-1}(A-B)^* &gt; 0\)</span>. 则<span class="math display">\[\begin{aligned}    &amp;\lvert \det(I-AB^*) \rvert^2[\det(I-BB^*)]^{-1} \\    =&amp;\det[(I-AA^*)+(A-B)(I-B*B)^{-1}(A-B)^*] \\    \geqslant&amp;\det(I-AA^*)+\det(A-B)(I-B^*B)^{-1}(A-B)^* \\    =&amp;\det(I-AA^*)+\lvert\det(A-B)\rvert^2[\det(I-BB^*)]^{-1}\end{aligned}\]</span> 由Lemma知，等号成立当且仅当 <span class="math display">\[(A-B)(I-B^*B)^{-1}(A-B)^*=[(A-B)(I-B^*B)^{-\frac{1}{2}}][(A-B)(I-B^*B)^{-1\frac{1}{2}}]^*=0\]</span> 当且仅当 <spanclass="math inline">\((A-B)(I-B^*B)^{-\frac{1}{2}}=0\)</span> 当且仅当<span class="math inline">\(A=B\)</span>. 证毕.</p><p>注：从Lemma可以得到一个推论</p><p>prop. 设Hermite阵<span class="math inline">\(A\)</span>和<spanclass="math inline">\(B\)</span>皆为半正定阵，且 <spanclass="math inline">\(A\geqslant B\)</span>. 且 <spanclass="math inline">\(A-B \geqslant 0\)</span>，则 <spanclass="math display">\[\det(A)\geqslant\det(B)\]</span> 这是引理的直接应用.</p><p><strong>proof 2</strong>：考虑分块矩阵 <span class="math display">\[    M=    \begin{bmatrix}        I &amp; A^* \\        B &amp; I \\    \end{bmatrix}      \begin{bmatrix}        I &amp; -B^* \\        -A &amp; I    \end{bmatrix}\]</span> 和上面的引理.</p><h3id="let-h_1h_2in-mathbbcntimes-n-such-that-h_1h_1-h_2h_2-h_10-h_20.-then">2.Let <span class="math inline">\(H_1,H_2\in \mathbb{C^{n\timesn}}\)</span> such that <span class="math inline">\(H_1^*=H_1\)</span>,<span class="math inline">\(H_2^*=H_2\)</span>, <spanclass="math inline">\(H_1&gt;0\)</span>, <spanclass="math inline">\(H_2&gt;0\)</span>. Then</h3><h3 id="if-h_2-h_10-then-h_1-1-h_2-10.">(1) If <spanclass="math inline">\(H_2-H_1&gt;0\)</span>, then <spanclass="math inline">\(H_1^{-1}-H_2^{-1}&gt;0\)</span>.</h3><h3id="minkowski-lvert-h_1-rvertfrac1nlvert-h_2-rvertfrac1n-leqslant-lvert-h_1h_2-rvertfrac1n-and-the-equality-holds-iff-h_2ah_1-for-some-ain-mathbbr.">(2)(Minkowski) <span class="math inline">\(\lvert H_1\rvert^{\frac{1}{n}}+\lvert H_2 \rvert^{\frac{1}{n}} \leqslant \lvertH_1+H_2 \rvert^{\frac{1}{n}}\)</span> and the equality holds iff <spanclass="math inline">\(H_2=aH_1\)</span>, for some <spanclass="math inline">\(a\in \mathbb{R}^+\)</span>.</h3><p>：首先证明一个引理：</p><p>Lemma 设<span class="math inline">\(A\)</span>和<spanclass="math inline">\(B\)</span>为两个<spanclass="math inline">\(n\)</span>阶Hermite阵. <spanclass="math inline">\(A&gt;0\)</span>，<spanclass="math inline">\(B\geqslant 0\)</span>，则</p><ol type="1"><li><p><span class="math inline">\(A\geqslant B \Longleftrightarrow\lambda_1(BA^{-1})\leqslant 1\)</span>，</p></li><li><p><span class="math inline">\(A &gt; B \Longleftrightarrow\lambda_1(BA^{-1})&lt; 1\)</span>.</p></li></ol><p>这里 <span class="math inline">\(\lambda_1(A)\)</span>表示<spanclass="math inline">\(A\)</span>的最大特征值.</p><p><strong>proof</strong>： (1)我们有 <span class="math display">\[\begin{aligned}    A\geqslant B &amp;\LongleftrightarrowA^{-\frac{1}{2}}(A-B)A^{-\frac{1}{2}}\geqslant 0 \\    &amp;\LongleftrightarrowI_n-A^{-\frac{1}{2}}BA^{-\frac{1}{2}}\geqslant 0 \\    &amp;\Longleftrightarrow\lambda_1(I_n-A^{-\frac{1}{2}}BA^{-\frac{1}{2}})\geqslant 0 \\    &amp;\Longleftrightarrow\lambda_1(A^{-\frac{1}{2}}BA^{-\frac{1}{2}})\leqslant1 \\    &amp;\Longleftrightarrow\lambda_1(BA^{-1})\leqslant 1\end{aligned}\]</span></p><p>故(1)得证. 同法可证(2)，引理证毕.</p><p>利用引理，<span class="math inline">\(H_2&gt; H_1\Longleftrightarrow\lambda_1(H_1H_2^{-1})&lt; 1\Longleftrightarrow\lambda_1(H_2^{-1}H_1)&lt; 1 \Longleftrightarrow H_1^{-1}&gt;H_2^{-1}\)</span></p><ol start="2" type="1"><li>不失一般性，可以假设<spanclass="math inline">\(H_1=I\)</span>，于是问题归结为证明 <spanclass="math display">\[(\det(I_n+H_2))^{\frac{1}{n}}\geqslant 1+(\det(B))^{\frac{1}{n}}\]</span></li></ol><p>设 <spanclass="math inline">\(\lambda_1\geqslant\cdots\geqslant\lambda_n&gt;0\)</span>为<spanclass="math inline">\(H_2\)</span>的特征值，则上面的不等式等价于 <spanclass="math display">\[\prod_{i=1}^n(1+\lambda_i)\geqslant(1+\sqrt[n]{\lambda_1\cdots\lambda_n})^n\]</span> 这是Holder不等式的特殊情况. 等号成立当且仅当<spanclass="math inline">\(\lambda_1=\cdots\lambda_n=a\)</span>，此即<spanclass="math inline">\(H_2=aH_1\)</span>.</p><h3id="assume-that-n-leqslant-m-and-ain-mathbbcntimes-m-bin-mathbbcmtimes-p-ran.-show-that-detbi_m-aaa-1ab-leqslant-detbb">3.Assume that <span class="math inline">\(n \leqslant m\)</span>, and<span class="math inline">\(A\in \mathbb{C}^{n\times m}\)</span>, <spanclass="math inline">\(B\in \mathbb{C^{m\times p}}\)</span>, <spanclass="math inline">\(r(A)=n\)</span>. Show that <spanclass="math display">\[\det[B^*(I_m-A^*(AA^*)^{-1}A)B] \leqslant\det(B^*B)\]</span></h3><p><strong>proof1</strong>：我们的目标是利用第一题中的Lemma. 注意<spanclass="math inline">\(B^*B\)</span>正定，<spanclass="math inline">\(B^*A^*(AA^*)^{-1}AB\)</span>正定，故我们只需要证明<spanclass="math inline">\(I_m-A^*(AA^*)^{-1}A\)</span>正定即可，不妨设<spanclass="math inline">\(AA^*=\text{diag}(\lambda_1,\cdots,\lambda_n)\)</span>，其中<spanclass="math inline">\(\lambda_i\)</span>均为实数. 我们有 <spanclass="math display">\[A^*\text{diag}(\frac{1}{\lambda_1},\cdots\frac{1}{\lambda_n})A=\frac{1}{\lambda_1\cdots\lambda_n}A^*A\]</span> 故 <span class="math display">\[(A^*A)^2=A^*AA^*A=A^*\text{diag}(\lambda_1\cdots\lambda_n)A=\lambda_1\cdots\lambda_nA^*A\]</span> 于是<spanclass="math inline">\(A^*(AA^*)^{-1}A\)</span>的特征值都不大于1. 故<spanclass="math inline">\(I_m-A^*(AA^*)^{-1}A\)</span>正定，证毕.</p><p><strong>proof 2</strong>：事实上 <spanclass="math inline">\(A^*(AA^*)^{-1}A\)</span>与 <spanclass="math inline">\((AA^*)^{-1}AA^*=I_n\)</span>有相同的非零特征值，故事实上<spanclass="math inline">\(A^*(AA^*)^{-1}A\)</span>所有非零特征值都为 <spanclass="math inline">\(1\)</span>.</p><h3id="let-hin-mathbbcntimes-n-be-such-that-hh-and-hgeqslant-0-then-there-exists-a-unique-semipositive-definite-hermite-matrix-s-such-that-hs2.-furthermore-s-is-a-polynomial-of-h.">4.Let <span class="math inline">\(H\in \mathbb{C}^{n\times n}\)</span> besuch that <span class="math inline">\(H^*=H\)</span> and <spanclass="math inline">\(H\geqslant 0\)</span>, then there exists a uniquesemipositive-definite Hermite matrix <spanclass="math inline">\(S\)</span> such that <spanclass="math inline">\(H=S^2\)</span>. Furthermore, <spanclass="math inline">\(S\)</span> is a polynomial of <spanclass="math inline">\(H\)</span>.</h3><p>proof：首先由<spanclass="math inline">\(H\)</span>自伴知存在酉矩阵<spanclass="math inline">\(P\)</span>，使得<spanclass="math inline">\(H=P\text{diag}(\lambda_1,\cdots,\lambda_n)P^*\)</span>，其中<spanclass="math inline">\(\lambda_i\)</span>均为正实数. 易知<spanclass="math inline">\(S=P\text{diag}(\sqrt{\lambda_1},\cdots,\sqrt{\lambda_n})P^*\)</span>满足<spanclass="math inline">\(H=S^2\)</span>. 唯一性是容易的.</p><p>其次，不妨设<spanclass="math inline">\(\sqrt{\lambda_1},\cdots,\sqrt{\lambda_n}\)</span>中有且仅有<spanclass="math inline">\(t\)</span>个互不相同的值<spanclass="math inline">\(\sqrt{\lambda_1},\cdots,\sqrt{\lambda_t}\)</span>，构造一个Lagrange插值多项式<span class="math display">\[f(\lambda)=\sum_{i=1}^t\sqrt{\lambda_i}\prod_{k\neqi}\frac{\lambda-\lambda_k}{\lambda_i-\lambda_k}\]</span> 于是 <span class="math display">\[\text{diag}(\sqrt{\lambda_1},\cdots,\sqrt{\lambda_n})=f(\text{diag}(\lambda_1,\cdots,\lambda_n))\]</span> 从而 <span class="math display">\[S=P\text{diag}(\sqrt{\lambda_1},\cdots,\sqrt{\lambda_n})P^*=f(H)\]</span></p><h3 id="let-h-in-mathbbc2n-be-such-that-hh-h0-and-hjjbarh-where">5. Let<span class="math inline">\(H \in \mathbb{C}^{2n}\)</span> be such that<span class="math inline">\(H^*=H\)</span>, <spanclass="math inline">\(H&gt;0\)</span>, and <spanclass="math inline">\(HJ=J\bar{H}\)</span>, where</h3><p><span class="math display">\[J=\begin{bmatrix}    0 &amp; I_n \\    -I_n &amp; 0 \\\end{bmatrix}\]</span></p><h3id="show-that-there-exists-qin-gl_2nmathbbc-such-that-qjjbarq-and-hqq.">showthat there exists <span class="math inline">\(Q\inGL_{2n}(\mathbb{C})\)</span> such that <spanclass="math inline">\(QJ=J\bar{Q}\)</span>, and <spanclass="math inline">\(H=Q^*Q\)</span>.</h3><p><strong>解</strong>：设 <span class="math inline">\(Q\)</span>是<span class="math inline">\(H\)</span>的唯一的平方根，<spanclass="math inline">\(Q^*=Q\)</span>. 则 <spanclass="math inline">\(Q\)</span>是 <spanclass="math inline">\(H\)</span>的多项式，可设 <spanclass="math inline">\(Q=g(H)\)</span>，其中 <spanclass="math inline">\(g(x)\in \mathbb{C}[x]\)</span>. 则 <spanclass="math display">\[    QJ=g(H)J=Jg(\bar{H})=J \overline{g(H)}=J \bar{Q}\]</span></p><h3id="let-v-be-a-n-dimensional-inner-space-and-v_1cdotsv_s-are-finitely-many-proper-subspaces-of-v.-then-there-exists-an-orthonormal-basis-alpha_1cdotsalpha_n-of-v-such-that-alpha_inotin-bigcup_j1sv_j-i12cdotsn.">6.Let V be a n-dimensional inner space, and <spanclass="math inline">\(V_1,\cdots,V_s\)</span> are finitely many propersubspaces of <span class="math inline">\(V\)</span>. Then there existsan orthonormal basis <spanclass="math inline">\(\{\alpha_1,\cdots,\alpha_n\}\)</span> of <spanclass="math inline">\(V\)</span> such that <spanclass="math inline">\(\alpha_i\notin \bigcup_{j=1}^sV_j,i=1,2,\cdots,n\)</span>.</h3><p>proof：先取一个<spanclass="math inline">\(\alpha_1\)</span>，使得<spanclass="math inline">\(\alpha_1\inV/\bigcup_{j=1}^sV_j/\bigcup_{j=1}^sV_j^\bot\)</span>，这是经典的结果.于是<span class="math inline">\(\langle\alpha_1\rangle^\bot\)</span>与诸<spanclass="math inline">\(V_j\)</span>相交得到的线性空间仍然是<spanclass="math inline">\(\langle\alpha_1\rangle ^\bot\)</span>中的真子空间.可以继续取<spanclass="math inline">\(\alpha_2,\cdots\)</span>直到取到<spanclass="math inline">\(\alpha_n\)</span>.</p><h3id="let-abin-on-i.e.-abin-mathbbrntimes-n-and-amathsft-ai-bmathsft-bi.-if-detadetb0-then-detab0.">7.Let <span class="math inline">\(A,B\in O(n)\)</span> i.e. <spanclass="math inline">\(A,B\in \mathbb{R}^{n\times n}\)</span>, and <spanclass="math inline">\(A^{\mathsf{T}} A=I\)</span>, <spanclass="math inline">\(B^{\mathsf{T}} B=I\)</span>. If <spanclass="math inline">\(\det(A)+\det(B)=0\)</span>, then <spanclass="math inline">\(\det(A+B)=0\)</span>.</h3><p>proof：不妨<spanclass="math inline">\(\det(A)=-\det(B)=1\)</span>，注意到<spanclass="math inline">\((A+B)^\top=A^\top(A+B)B^\top\)</span>即可.</p><p>这个问题还可以这么理解：将行列式看成是线性变换下的有向体积，那么<spanclass="math inline">\(A\)</span>和<spanclass="math inline">\(B\)</span>是两个定向相反的线性映射，故<spanclass="math inline">\(A+B\)</span>自然就将一个单位正方体映成两个体积为<spanclass="math inline">\(1\)</span>和<spanclass="math inline">\(-1\)</span>的平行<spanclass="math inline">\(2n\)</span>面体的叠加，自然有<spanclass="math inline">\(\det(A+B)=0\)</span>.</p><h3id="assume-that-pin-on-and-atextdiaga_1cdotsa_n-a_iin-mathbbr-then-the-eigenvalues-lambda_j1leqslant-j-leqslant-n-of-pa-satisfies-mleqslantlvertlambda_j-rvert-leqslant-m-where-mmin_1leqslant-jleqslant-na_j-mmax_1leqslant-jleqslant-na_j.">8.Assume that <span class="math inline">\(P\in O(n)\)</span>, and <spanclass="math inline">\(A=\text{diag}(a_1,\cdots,a_n)\)</span>, <spanclass="math inline">\(a_i\in \mathbb{R}\)</span>, then the eigenvalues<span class="math inline">\(\lambda_j,1\leqslant j \leqslant n\)</span>of <span class="math inline">\(PA\)</span> satisfies <spanclass="math inline">\(m\leqslant\lvert\lambda_j \rvert \leqslantM\)</span>, where <span class="math inline">\(m=\min_{1\leqslantj\leqslant n}\{a_j\}\)</span>, <spanclass="math inline">\(M=\max_{1\leqslant j\leqslantn}\{a_j\}\)</span>.</h3><p>proof：简单的放缩，具体过程略.</p><h3id="let-ain-on-and-b-a-square-submatrix-of-a.-then-any-complex-eigenvalue-lambda-of-b-satisfies-that-barlambdalambda-leqslant-1.">9.Let <span class="math inline">\(A\in O(n)\)</span>, and <spanclass="math inline">\(B\)</span> a square submatrix of <spanclass="math inline">\(A\)</span>. Then any complex eigenvalue <spanclass="math inline">\(\lambda\)</span> of <spanclass="math inline">\(B\)</span> satisfies that <spanclass="math inline">\(\bar{\lambda}\lambda \leqslant 1\)</span>.</h3><p>proof：一个重要的观察是，因为交换正交矩阵的任意两行、任意两列不改变矩阵的正交性质，故可以不妨假设<spanclass="math inline">\(B\)</span>是<spanclass="math inline">\(A\)</span>的左上角的<spanclass="math inline">\(k\)</span>阶主子式. 将A写成: <spanclass="math display">\[ A=\begin{bmatrix}    B &amp; A_{12} \\    A_{13} &amp; A_{14} \\\end{bmatrix}\]</span></p><p>那么，由<span class="math inline">\(A^{\mathsf{T}}A=I\)</span>，可以得到 <span class="math display">\[B^{\mathsf{T}} B+A_{13}^{\mathsf{T}} A_{13}=I_k\]</span></p><p>假设<span class="math inline">\(\lambda\)</span>是<spanclass="math inline">\(B\)</span>的任意一个（复）特征值，对应的特征向量为<spanclass="math inline">\(\alpha\)</span>. 在上式两边左乘<spanclass="math inline">\(\alpha^*\)</span>，右乘<spanclass="math inline">\(\alpha\)</span>，移项，得： <spanclass="math display">\[    \alpha^*A_{13}^{\mathsf{T}}A_{13}\alpha=(1-\bar{\lambda}\lambda)\alpha^*\alpha\]</span> 显然有<spanclass="math inline">\(\bar{\lambda}\lambda\leqslant 1\)</span>.</p><h3id="give-the-maximal-r-such-that-there-exist-beta_1cdotsbeta_r-satisfying-beta_i-beta_j0-1leqslant-ij-leqslant-r.">10.Give the maximal <span class="math inline">\(r\)</span> such that thereexist <span class="math inline">\(\beta_1,\cdots,\beta_r\)</span>satisfying <span class="math inline">\((\beta_i | \beta_j)&lt;0,1\leqslant i&lt;j \leqslant r\)</span>.</h3><p>proof: 题目意思应该是求<spanclass="math inline">\(n\)</span>维欧氏空间中两两成钝角的向量最多有几个，答案是<spanclass="math inline">\(n+1\)</span>个.</p><p>考虑<span class="math inline">\(n+1\)</span>个向量<spanclass="math inline">\(v_1=(n+\sqrt{1+n},-1,-1,\cdots,-1)\)</span>,<spanclass="math inline">\(v_2=(-1,n+\sqrt{1+n},-1,\cdots,-1)\)</span>,<spanclass="math inline">\(\cdots\)</span>,<spanclass="math inline">\(v_n=(-1,-1,\cdots,n+\sqrt{1+n})\)</span>和<spanclass="math inline">\(v_{n+1}=(-1-\sqrt{1+n},\cdots,-1-\sqrt{1+n})\)</span>，易见他们之间两两成钝角.</p><p>也可以用归纳法：假设对 <spanclass="math inline">\(n-1\)</span>维空间满足，考虑由一组标准正交基 <spanclass="math inline">\(\alpha_1,\alpha_2,\cdots,\alpha_n\)</span>张成的<span class="math inline">\(n\)</span>维空间. 由假设，<spanclass="math inline">\(\tilde{\beta}_1,\tilde{\beta}_2,\cdots,\tilde{\beta}_n\in L(\alpha_1,\alpha_2,\cdots,\alpha_{n-1})\)</span> 两两成钝角，令<span class="math display">\[    \begin{cases}        \beta_i=\tilde{\beta}_i \quad 1\leqslant i\leqslant n-1\\        \beta_n=\tilde{\beta}_n+\alpha_n \\        \tilde{\beta}_{n+1}=\tilde{\beta}_n-2(\tilde{\beta}_n|\tilde{\beta}_n)\alpha_n    \end{cases}\]</span> 则它们两两成钝角.</p><p>假设有<span class="math inline">\(n+2\)</span>个向量<spanclass="math inline">\(a_1,\cdots,a_{n+2}\)</span>两两成钝角，因存在不全为零的系数<spanclass="math inline">\(k_1,k_2,\cdots,k_{n+1}\)</span>使得 <spanclass="math display">\[k_1a_1+\cdots+k_{n+1}a_{n+1}=0\]</span> 上式两端点乘<spanclass="math inline">\(a_{n+2}\)</span>，假设<spanclass="math inline">\(k_1,\cdots,k_i\)</span>为正，<spanclass="math inline">\(k_{i+1},\cdots,k_j\)</span>为负，其余为零，则有<span class="math display">\[k_1a_1+\cdots+k_ia_i=-k_{i+1}a_{i+1}-\cdots-k_ja_j=V\]</span> 那么 <span class="math display">\[V\cdotV=(k_1a_1+\cdots+k_ia_i)(-k_{i+1}a_{i+1}-\cdots-k_ja_j)=-k_1k_{i+1}a_1\cdota_{i+1}-\cdots&lt;0\]</span> 矛盾！故最多有<spanclass="math inline">\(n+1\)</span>个两两之间成钝角的向量.</p><p>其实该结论在Hermite空间中也对，上面的做法用到了实数的序关系，因此不具有推广价值，可以利用下面的归纳法证明：</p><p>假设对 <span class="math inline">\(n-1\)</span>维空间，至多有 <spanclass="math inline">\(n\)</span>个两两成钝角的向量. 考虑 <spanclass="math inline">\(n\)</span>维空间的情况.</p><p>假设存在 <spanclass="math inline">\(\beta_1,\beta_2,\cdots,\beta_{n+2}\)</span>使得它们之间两两成钝角，对<spanclass="math inline">\(\beta_1,\beta_2,\cdots,\beta_{n-1}\)</span>找一组子空间的标准正交基<spanclass="math inline">\(\alpha_1,\alpha_2,\cdots,\alpha_{n-1}\)</span>，再将其扩充为原空间的一组标准正交基<spanclass="math inline">\(\alpha_1,\alpha_2,\cdots,\alpha_n\)</span>，设<span class="math display">\[    \beta_n=\gamma_n+a_n \alpha_n \\    \beta_{n+1}=\gamma_{n+1}+a_{n+1}\alpha_n \\    \beta_{n+2}=\gamma_{n+2}+a_{n+2}\alpha_n \\\]</span> 那么 <spanclass="math inline">\((\gamma_i|\beta_j)&lt;0,n\leqslant i\leqslantn+2,1\leqslant j\leqslant n-1\)</span>. 由 <spanclass="math inline">\(n-1\)</span>维的归纳假设， <spanclass="math display">\[    (\gamma_i|\gamma_j)\geqslant 0, \quad n\leqslant i,j \leqslant n+2\]</span> 但是 <spanclass="math inline">\((\beta_i|\beta_j)&lt;0,q\leqslant i,j\leqslantn+2\)</span>. 故 <span class="math inline">\(a_n\bar{a}_{n+2}&lt;0,a_{n+1}\bar{a}_{n+2}&lt;0,a_n\bar{a}_{n+1}&lt;0\)</span>，从中易得矛盾.</p><p>另注：该问题还可以放到<spanclass="math inline">\(n+1\)</span>维空间中一个标准的<spanclass="math inline">\(n\)</span>维单纯形上看；另一个对偶的问题是<spanclass="math inline">\(n\)</span>维欧式空间中大于<spanclass="math inline">\(2^n\)</span>个点的集合一定存在钝角.</p><h3id="let-ain-mathbbcntimes-n-and-lambda-be-an-eigenvalue-of-a.-then-lvert-lambda-rvert-leqslant-lvert-a-rvert-for-any-matrix-norm-of-a.">11.Let <span class="math inline">\(A\in \mathbb{C}^{n\times n}\)</span>,and <span class="math inline">\(\lambda\)</span> be an eigenvalue of<span class="math inline">\(A\)</span>. Then <spanclass="math inline">\(\lvert \lambda \rvert \leqslant \lVert A\rVert\)</span>, for any matrix norm of <spanclass="math inline">\(A\)</span>.</h3><p><strong>proof</strong>：对 <span class="math inline">\(\varepsilon&gt;0\)</span>，令 <span class="math display">\[    B=\frac{A}{\left\| A \right\|_{}+\varepsilon }\]</span> 则 <span class="math inline">\(\left\| B\right\|_{}&lt;1\)</span>. 故有 <span class="math inline">\(B^n \to0\)</span>，即任何 <span class="math inline">\(B\)</span>的特征值 <spanclass="math inline">\(\mu\)</span>都有 <spanclass="math inline">\(\left\vert \mu \right\vert &lt;1\)</span>.故对任何 <span class="math inline">\(A\)</span>的特征值 <spanclass="math inline">\(\lambda\)</span>，有 <span class="math display">\[    \frac{\left\vert \lambda \right\vert }{\left\| A\right\|_{}+\varepsilon }&lt;1\]</span> 令 <span class="math inline">\(\varepsilon \to0\)</span>，就有结论成立.</p><h2 id="参考">参考</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>《矩阵论中不等式》 王松桂贾忠贞 著 安徽教育出版社<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>代数学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>测试LaTeX公式支持</title>
    <link href="/2022/03/29/LaTeX%E5%85%AC%E5%BC%8F%E6%B5%8B%E8%AF%95/"/>
    <url>/2022/03/29/LaTeX%E5%85%AC%E5%BC%8F%E6%B5%8B%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<div class="markdown-body"><p><span class="math display">\[E=mc^2\]</span></p></div>]]></content>
    
    
    <categories>
      
      <category>杂项</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
